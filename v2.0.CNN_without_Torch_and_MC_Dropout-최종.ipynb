{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24443fc4",
   "metadata": {},
   "source": [
    "# <font color=Red># Prologue : CNNs implementation without Torch </font>\n",
    "\n",
    "해당 프로젝트에서는 PyTorch 없이 CNNs를 구현하고, </br>\n",
    "간단한 이미지(MNIST 데이터세트)를 분류하는 Classification Task를 수행합니다. </br>\n",
    "해당 프로젝트의 Experiment Setting은 CNNs Implementation with Torch를 참고합니다. </br> \n",
    "</br>\n",
    "### Preliminary \n",
    "모든 Neural Networks는 세 개의 요소로 구성되어 있습니다. </br>\n",
    "- ```Architecture``` : Neuron과 Layer를 Initialization하고, Forward Propagation을 수행합니다.</br>\n",
    "- ```Cost function``` : Model과 Label간의 차이를 계산합니다. </br>\n",
    "- ```Optimization``` : Backward Propagation을 수행하고, weight와 bias로 구성된 Parameter를 Update합니다. </br>\n",
    "\n",
    "이 세 가지의 구성요소를 코드로 구현하는 것은 매우 복잡하고 지루한 과정이기 때문에, </br>\n",
    "자주 사용되는 method는 ```Tensorflow```나 ```PyTorch```와 같은 Library에서 제공되는 것을 사용하는 것이 일반적입니다. </br>\n",
    "</br>\n",
    "그러나 만약 Library를 사용하지 않고 CNNs를 구현한다면, 어떤 부분이 기존 Library에서 제공되었는지를 파악해야 합니다, </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e63d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d747097",
   "metadata": {},
   "source": [
    "# <font color=Red># Modulization </font>\n",
    "\n",
    "먼저, 신경망에 사용되는 여러 가지 함수들을 ```Module```화 하는 것이 중요합니다. </br>\n",
    "여기서 Module이란 여러 학문 분야에서 사용하는 용어인데, Computer Science에서 사용되는 경우 </br>\n",
    "특정 프로그램을 구성하는 기본 단위를 의미합니다. </br>\n",
    "</br>\n",
    "신경망을 구현하는 데 필요한 각종 Module들은 앞서 설명되었듯이, 여러 Library에서 제공되지만 </br>\n",
    "해당 프로젝트에서는 Torch 없이 Module들을 구현하는 것을 목표로 하므로, 어떤 Module이 사용되는지 먼저 살펴봅니다. </br>\n",
    "신경망에서 자주 사용되는 Module들에는 다음과 같은 것들이 있습니다. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929c46d",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "가장 먼저, 이미지의 각종 특징(Feature)들을 추출하는 ```Convolution``` 연산을 수행합니다.</br>\n",
    "Convolution은, ```Filter``` 혹은 ```Kernel```이라 불리는 Matrix를 원본 이미지에 곱함으로써 수행됩니다.</br>\n",
    "이때, Filter Size는 ```VGGNet```의 설정을 참고하여, 모든 레이어에서 3x3으로 고정합니다.\n",
    "\n",
    "**Ref:Very Deep Convolutional Networks for Large-Scale Image Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2539895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "def im2col(input, filter_height, filter_width, stride, padding):\n",
    "    batch_size, in_channel, in_height, in_width = input.shape\n",
    "    out_height = (in_height - filter_height + 2 * padding) // stride + 1\n",
    "    out_width = (in_width - filter_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    padded_input = cp.pad(input, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    \n",
    "    i0 = cp.repeat(cp.arange(filter_height), filter_width)\n",
    "    i0 = cp.tile(i0, in_channel)\n",
    "    i1 = stride * cp.repeat(cp.arange(out_height), out_width)\n",
    "    j0 = cp.tile(cp.arange(filter_width), filter_height * in_channel)\n",
    "    j1 = stride * cp.tile(cp.arange(out_width), out_height)\n",
    "    \n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    \n",
    "    k = cp.repeat(cp.arange(in_channel), filter_height * filter_width).reshape(-1, 1)\n",
    "    \n",
    "    cols = padded_input[:, k, i, j]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(filter_height * filter_width * in_channel, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im(cols, input_shape, filter_height, filter_width, stride, padding):\n",
    "    batch_size, in_channel, in_height, in_width = input_shape\n",
    "    padded_height = in_height + 2 * padding\n",
    "    padded_width = in_width + 2 * padding\n",
    "    padded_input = cp.zeros((batch_size, in_channel, padded_height, padded_width))\n",
    "    \n",
    "    out_height = (in_height - filter_height + 2 * padding) // stride + 1\n",
    "    out_width = (in_width - filter_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    i0 = cp.repeat(cp.arange(filter_height), filter_width)\n",
    "    i0 = cp.tile(i0, in_channel)\n",
    "    i1 = stride * cp.repeat(cp.arange(out_height), out_width)\n",
    "    j0 = cp.tile(cp.arange(filter_width), filter_height * in_channel)\n",
    "    j1 = stride * cp.tile(cp.arange(out_width), out_height)\n",
    "    \n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    \n",
    "    k = cp.repeat(cp.arange(in_channel), filter_height * filter_width).reshape(-1, 1)\n",
    "    \n",
    "    cols_reshaped = cols.reshape(in_channel * filter_height * filter_width, -1, batch_size)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    \n",
    "    cp.add.at(padded_input, (slice(None), k, i, j), cols_reshaped)\n",
    "    \n",
    "    if padding == 0:\n",
    "        return padded_input\n",
    "    return padded_input[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv2d(input, filters, bias, stride=1, padding=1):\n",
    "    batch_size, in_channel, in_height, in_width = input.shape\n",
    "    out_channel, _, filter_height, filter_width = filters.shape\n",
    "    \n",
    "    out_height = (in_height - filter_height + 2 * padding) // stride + 1\n",
    "    out_width = (in_width - filter_width + 2 * padding) // stride + 1\n",
    "    \n",
    "    cols = im2col(input, filter_height, filter_width, stride, padding)\n",
    "    reshaped_filters = filters.reshape(out_channel, -1)\n",
    "    \n",
    "    out = reshaped_filters @ cols + bias.reshape(-1, 1)\n",
    "    out = out.reshape(out_channel, out_height, out_width, batch_size)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "    return out\n",
    "\n",
    "def conv2d_backward(dout, input, filters, stride=1, padding=1):\n",
    "    batch_size, in_channel, in_height, in_width = input.shape\n",
    "    out_channel, _, filter_height, filter_width = filters.shape\n",
    "    \n",
    "    cols = im2col(input, filter_height, filter_width, stride, padding)\n",
    "    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(out_channel, -1)\n",
    "    \n",
    "    dfilters = dout_reshaped @ cols.T\n",
    "    dfilters = dfilters.reshape(filters.shape)\n",
    "    \n",
    "    dbias = cp.sum(dout, axis=(0, 2, 3))\n",
    "    \n",
    "    dcols = filters.reshape(out_channel, -1).T @ dout_reshaped\n",
    "    dinput = col2im(dcols, input.shape, filter_height, filter_width, stride, padding)\n",
    "    \n",
    "    return dinput, dfilters, dbias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0537d2",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "해당 이미지의 특정 영역의 값을 압축하는 ```Pooling```연산을 수행합니다.</br>\n",
    "Classification Task에서는 다른 Pooling 방법에 비해 특정 영역을 해당 영역의 최댓값으로 압축하는 ```Max Polling```이 일반적으로 더 좋은 성능을 보여주므로,</br>\n",
    "해당 코드에서도 Max Pooling을 기본적으로 구현합니다.</br>\n",
    "Max Pooling 역시 Convolution과 동일하게 Pooling Kernel을 설정하지만 </br>\n",
    "Convoltuion과는 다르게 Parameter를 저장할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3afce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool2d(input, pool_size=2, stride=2):\n",
    "    batch_size, channel, height, width = input.shape  # 입력 텐서의 형태 (배치 크기, 채널 수, 높이, 너비) 구함\n",
    "    out_height = (height - pool_size) // stride + 1  # 출력 텐서의 높이 계산\n",
    "    out_width = (width - pool_size) // stride + 1  # 출력 텐서의 너비 계산\n",
    "    output = cp.zeros((batch_size, channel, out_height, out_width))  # 출력 텐서 초기화\n",
    "    max_indices = cp.zeros_like(input)  # 최대값 인덱스 초기화\n",
    "\n",
    "    # 출력 텐서의 각 위치에 대해 최대 풀링 수행\n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            h_start = i * stride  # 입력 텐서에서 현재 위치의 시작 높이\n",
    "            h_end = h_start + pool_size  # 입력 텐서에서 현재 위치의 끝 높이\n",
    "            w_start = j * stride  # 입력 텐서에서 현재 위치의 시작 너비\n",
    "            w_end = w_start + pool_size  # 입력 텐서에서 현재 위치의 끝 너비\n",
    "            region = input[:, :, h_start:h_end, w_start:w_end]  # 입력 텐서에서 현재 위치의 영역 추출\n",
    "            output[:, :, i, j] = cp.max(region, axis=(2, 3))  # 영역 내의 최대값 계산하여 출력 텐서에 저장\n",
    "            max_indices[:, :, h_start:h_end, w_start:w_end] += (region == output[:, :, i:i+1, j:j+1])  # 최대값 위치 저장\n",
    "\n",
    "    return output, max_indices  # 최대 풀링 결과와 최대값 인덱스 반환\n",
    "\n",
    "def max_pool2d_backward(dout, max_indices, pool_size=2, stride=2):\n",
    "    batch_size, channel, height, width = max_indices.shape  # 최대값 인덱스의 형태 (배치 크기, 채널 수, 높이, 너비) 구함\n",
    "    dinput = cp.zeros_like(max_indices)  # 입력에 대한 변화도 초기화\n",
    "\n",
    "    out_height = dout.shape[2]  # dout의 높이 구함\n",
    "    out_width = dout.shape[3]  # dout의 너비 구함\n",
    "\n",
    "    # 역전파 수행\n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            h_start = i * stride  # 입력 텐서에서 현재 위치의 시작 높이\n",
    "            h_end = h_start + pool_size  # 입력 텐서에서 현재 위치의 끝 높이\n",
    "            w_start = j * stride  # 입력 텐서에서 현재 위치의 시작 너비\n",
    "            w_end = w_start + pool_size  # 입력 텐서에서 현재 위치의 끝 너비\n",
    "            dinput[:, :, h_start:h_end, w_start:w_end] += (max_indices[:, :, h_start:h_end, w_start:w_end] * dout[:, :, i:i+1, j:j+1])  # 최대값 위치에 dout 적용\n",
    "\n",
    "    return dinput  # 입력에 대한 변화도 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650fdc8",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "- Sigmoid, ReLU, Softmax\n",
    "기본적으로 Activation Function으로는 ```ReLU```를 사용하고, </br>\n",
    "Classification Task를 수행하기 때문에</br>\n",
    "Classifier의 마지막 Layer에서는 ```Softmax```를 사용합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc96234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return cp.maximum(0, x)\n",
    "\n",
    "def relu_backward(dout, x):\n",
    "    return dout * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17eff2f",
   "metadata": {},
   "source": [
    "**Convolution과 Pooling으로 구성된 Feature Extractor의 설정은 다음과 같습니다.**\n",
    "\n",
    "|Layer|in_channel|out_chnnel|kernel_size|stride|padding|\n",
    "|---|---|---|---|---|---|\n",
    "|Conv1|1|32|3|1|1|\n",
    "|Conv2|32|32|3|1|1|\n",
    "|Pool1|2|2|0|\n",
    "|Conv3|32|128|1|1|1|\n",
    "|Conv4|128|128|3|1|1|\n",
    "|Pool2|2|2|0|\n",
    "|Conv5|128|256|3|1|1|\n",
    "|Conv6|256|256|3|1|1|\n",
    "|Pool3|2|2|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993d965",
   "metadata": {},
   "source": [
    "### Linear Combination(FC layer)\n",
    "Feature extractor에서 생성한 Feature의 정보를 모아서 분류를 수행하는 Classifier를 정의합니다. </br>\n",
    "Classifier에서는 Feature의 정보를 모으기 위해 Convolutional Layer를 Flatten(Vectorization)하기 때문에 </br>\n",
    "각 Neuron들을 ```Linear Combination```으로 연결하는 ```Fully Connected(FC) Layer```를 구현해야 합니다. </br>\n",
    "\n",
    "**Classifier의 설정은 다음과 같습니다.**\n",
    "\n",
    "|Layer|in_features|out_features|\n",
    "|---|---|---|\n",
    "|FC1|3x3x256|4096|\n",
    "|Dropout(0.5)|\n",
    "|FC2|4096|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fc478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input):\n",
    "    return input.reshape(input.shape[0], -1)  # 입력 텐서를 평탄화하여 2차원 텐서로 변환 (배치 크기, 나머지 차원)\n",
    "\n",
    "def fully_connected(input, weights, bias):\n",
    "    return cp.dot(input, weights) + bias  # 입력 텐서와 가중치를 행렬 곱셈하고 바이어스를 더하여 선형 변환 수행\n",
    "\n",
    "def fully_connected_backward(dout, input, weights):\n",
    "    dinput = cp.dot(dout, weights.T)  # dout과 가중치의 전치를 곱하여 입력에 대한 변화도 계산\n",
    "    dweights = cp.dot(input.T, dout)  # 입력의 전치와 dout을 곱하여 가중치에 대한 변화도 계산\n",
    "    dbias = cp.sum(dout, axis=0)  # dout의 모든 배치에 대해 합을 구하여 바이어스에 대한 변화도 계산\n",
    "    return dinput, dweights, dbias  # 입력, 가중치, 바이어스에 대한 변화도 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120b478",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "더 이상의 자세한 설명은 생략한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8c1c7",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdca506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.num_features = num_features\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = cp.ones((1, num_features, 1, 1))\n",
    "        self.beta = cp.zeros((1, num_features, 1, 1))\n",
    "        self.running_mean = cp.zeros((1, num_features, 1, 1))\n",
    "        self.running_var = cp.ones((1, num_features, 1, 1))\n",
    "        self.dgamma = cp.zeros_like(self.gamma)\n",
    "        self.dbeta = cp.zeros_like(self.beta)\n",
    "    \n",
    "    def forward(self, x, train=True):\n",
    "        if train:\n",
    "            mean = cp.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "            var = cp.var(x, axis=(0, 2, 3), keepdims=True)\n",
    "            self.x_centered = x - mean\n",
    "            self.stddev_inv = 1. / cp.sqrt(var + self.epsilon)\n",
    "            x_norm = self.x_centered * self.stddev_inv\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "            out = self.gamma * x_norm + self.beta\n",
    "            return out\n",
    "        else:\n",
    "            x_norm = (x - self.running_mean) / cp.sqrt(self.running_var + self.epsilon)\n",
    "            return self.gamma * x_norm + self.beta\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, C, H, W = dout.shape\n",
    "        \n",
    "        x_norm = self.x_centered * self.stddev_inv\n",
    "        self.dbeta = cp.sum(dout, axis=(0, 2, 3), keepdims=True)\n",
    "        self.dgamma = cp.sum(dout * x_norm, axis=(0, 2, 3), keepdims=True)\n",
    "        \n",
    "        dx_norm = dout * self.gamma\n",
    "        dvar = cp.sum(dx_norm * self.x_centered * -0.5 * self.stddev_inv**3, axis=(0, 2, 3), keepdims=True)\n",
    "        dmean = cp.sum(dx_norm * -self.stddev_inv, axis=(0, 2, 3), keepdims=True) + dvar * cp.mean(-2. * self.x_centered, axis=(0, 2, 3), keepdims=True)\n",
    "        \n",
    "        dx = (dx_norm * self.stddev_inv) + (dvar * 2 * self.x_centered / N) + (dmean / N)\n",
    "        \n",
    "        return dx\n",
    "    def update_params(self, optimizer, layer_index):\n",
    "        optimizer.update_params(layer_index, self.gamma, self.dgamma)\n",
    "        optimizer.update_params(layer_index + 1, self.beta, self.dbeta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b7a28",
   "metadata": {},
   "source": [
    "# <font color=Red># Class </font>\n",
    "\n",
    "Python에서는 Class를 이용하여 Modulization을 수행할 수 있습니다. </br>\n",
    "이때, 중요한 점은 해당 Class의 method는, 각 module의 ```Forward Propagation``` 부분과</br> \n",
    "```Backpropagation```이 모두 구현되어 있어야 한다는 점입니다.</br>\n",
    "\n",
    "따라서, 모든 클래스에는 Forward Propagation method에서 해당 함수의 연산을 수행하는 부분을 정의하고, </br>\n",
    "Backpropagation method에서 해당 함수의 ```gradient```를 계산하는 연산을 수행하는 부분을 정의합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad176efa",
   "metadata": {},
   "source": [
    "### Dropout Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c65f3",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "define p : dropout probability </br>\n",
    "\n",
    "$$\\text{mask} = \\frac{\\mathbf{r}}{1-p}$$\n",
    "r = uniformly distributed matrix for range [0, 1]\n",
    "\n",
    "- **During Training**\n",
    "$$\\mathbf{y} = \\mathbf{x} \\odot \\text{mask}$$\n",
    "여기서, $\\mathbf{y}$는 Dropout이 적용된 출력, $\\mathbf{x}$는 입력, $\\odot$는 element-wise multiplication을 나타냄.\n",
    "\n",
    "- **During Test**\n",
    "$$\\mathbf{y} = \\mathbf{x}$$\n",
    "Test 시에는 Dropout이 적용되지 않음.\n",
    "\n",
    "#### Backward Propagation\n",
    "$$\\mathbf{dout} = \\mathbf{dout} \\odot \\text{mask}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36cfef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p  # 드롭아웃 확률 설정\n",
    "        self.mask = None  # 마스크 초기화\n",
    "\n",
    "    def forward(self, input, train=True):\n",
    "        if train:  # 학습 중일 때\n",
    "            self.mask = (cp.random.rand(*input.shape) > self.p) / (1 - self.p)  # 드롭아웃 마스크 생성\n",
    "            return input * self.mask  # 입력에 마스크 적용하여 드롭아웃 수행\n",
    "        else:  # 평가 중일 때\n",
    "            return input  # 입력 그대로 반환\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask  # 역전파 시 마스크 적용하여 그래디언트 계산\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba23a8f",
   "metadata": {},
   "source": [
    "### Convolution Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce50308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):\n",
    "        self.stride = stride  # 스트라이드 값 설정\n",
    "        self.padding = padding  # 패딩 값 설정\n",
    "        # He 초기화를 사용하여 필터 가중치 초기화\n",
    "        self.weights = cp.random.randn(out_channels, in_channels, kernel_size, kernel_size) * cp.sqrt(2 / (in_channels * kernel_size * kernel_size))\n",
    "        self.bias = cp.zeros(out_channels)  # 바이어스 초기화\n",
    "        self.dweights = cp.zeros_like(self.weights)  # 필터 가중치 변화도 초기화\n",
    "        self.dbias = cp.zeros_like(self.bias)  # 바이어스 변화도 초기화\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input  # 입력값 저장 (역전파를 위해)\n",
    "        # 컨벌루션 연산 수행 후 ReLU 활성화 함수 적용하여 출력 계산\n",
    "        self.output = relu(conv2d(input, self.weights, self.bias, self.stride, self.padding))\n",
    "        return self.output  # 최종 출력 반환\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # ReLU의 역전파 수행\n",
    "        dout = relu_backward(dout, self.output)\n",
    "        # 컨벌루션 연산의 역전파 수행하여 입력, 필터 가중치, 바이어스에 대한 변화도 계산\n",
    "        dinput, self.dweights, self.dbias = conv2d_backward(dout, self.input, self.weights, self.stride, self.padding)\n",
    "        return dinput  # 입력에 대한 변화도 반환\n",
    "    \n",
    "    def update_params(self, optimizer, layer_index):\n",
    "        optimizer.update_params(layer_index, self.weights, self.dweights)\n",
    "        optimizer.update_params(layer_index + 1, self.bias, self.dbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d25ce",
   "metadata": {},
   "source": [
    "### Maxpool Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90cb6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size  # 풀링 크기 설정\n",
    "        self.stride = stride  # 스트라이드 값 설정\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input  # 입력값 저장 (역전파를 위해)\n",
    "        # 최대 풀링 연산 수행, 출력값과 최대값 인덱스 저장\n",
    "        self.output, self.max_indices = max_pool2d(input, self.pool_size, self.stride)\n",
    "        return self.output  # 최종 출력 반환\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # 최대 풀링의 역전파 수행하여 입력에 대한 변화도 계산\n",
    "        return max_pool2d_backward(dout, self.max_indices, self.pool_size, self.stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d465be79",
   "metadata": {},
   "source": [
    "### ReLU Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812e9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return relu(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(dout, x):\n",
    "        return relu_backward(dout, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c3bc4",
   "metadata": {},
   "source": [
    "### Vectorization Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25184683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape\n",
    "        output = flatten(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3210a",
   "metadata": {},
   "source": [
    "### Linear Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0deaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # He initialization을 사용하여 가중치 초기화\n",
    "        self.weights = cp.random.randn(in_features, out_features) * cp.sqrt(2 / in_features)\n",
    "        self.bias = cp.zeros(out_features)  # 바이어스 초기화\n",
    "        self.dweights = cp.zeros_like(self.weights)  # 가중치 변화도 초기화\n",
    "        self.dbias = cp.zeros_like(self.bias)  # 바이어스 변화도 초기화\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input  # 입력값 저장 (역전파를 위해)\n",
    "        self.output = fully_connected(input, self.weights, self.bias)  # 선형 변환 수행\n",
    "        return relu(self.output)  # ReLU 활성화 함수 적용 후 반환\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = relu_backward(dout, self.output)  # ReLU의 역전파 수행\n",
    "        # 선형 변환의 역전파 수행하여 입력, 가중치, 바이어스에 대한 변화도 계산\n",
    "        dinput, self.dweights, self.dbias = fully_connected_backward(dout, self.input, self.weights)\n",
    "        return dinput  # 입력에 대한 변화도 반환\n",
    "    def update_params(self, optimizer, layer_index):\n",
    "        optimizer.update_params(layer_index, self.weights, self.dweights)\n",
    "        optimizer.update_params(layer_index + 1, self.bias, self.dbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eee485",
   "metadata": {},
   "source": [
    "### MC Dropout for ESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4b426c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(test_batches, model, num_samples=10):\n",
    "    predictions = []\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        batch_predictions = []\n",
    "        for input, _ in test_batches:\n",
    "            output = model.forward(input, train=True)\n",
    "            output = softmax(output)\n",
    "            batch_predictions.append(output)\n",
    "        \n",
    "        batch_predictions = cp.concatenate(batch_predictions, axis=0)\n",
    "        predictions.append(batch_predictions)\n",
    "\n",
    "    mean_predictions = cp.mean(cp.stack(predictions), axis=0)\n",
    "    final_predictions = cp.argmax(mean_predictions, axis=1)\n",
    "    \n",
    "    return final_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa962d6e",
   "metadata": {},
   "source": [
    "# <font color=Red># Architecture </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e16c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Conv2D(1, 32, 3, stride=1, padding=1),  # 입력 채널 1, 출력 채널 32, 커널 크기 3x3\n",
    "            BatchNormalization(32),\n",
    "            MaxPool2D(2, 2),  # 최대 풀링, 풀링 크기 2x2\n",
    "            Conv2D(32, 64, 3, stride=1, padding=1),  # 입력 채널 32, 출력 채널 64, 커널 크기 3x3\n",
    "            BatchNormalization(64),\n",
    "            MaxPool2D(2, 2),  # 최대 풀링, 풀링 크기 2x2\n",
    "            Flatten(),  # 평탄화\n",
    "            FullyConnected(64 * 7 * 7, 128),  # 완전 연결 계층\n",
    "            Dropout(0.5),  # 드롭아웃\n",
    "            FullyConnected(128, 10)  # 출력 클래스 수 10개\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Conv2D, FullyConnected)):\n",
    "                params.append({'value': layer.weights, 'grad': layer.dweights})\n",
    "                params.append({'value': layer.bias, 'grad': layer.dbias})\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                params.append({'value': layer.gamma, 'grad': layer.dgamma})\n",
    "                params.append({'value': layer.beta, 'grad': layer.dbeta})\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e549d8",
   "metadata": {},
   "source": [
    "# <font color=Red># Cost Function </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d4e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(output, target, model, l2_lambda=0.005):\n",
    "    batch_size = target.shape[0]\n",
    "    output = cp.clip(output, 1e-12, 1. - 1e-12)\n",
    "    loss = -cp.sum(target * cp.log(output)) / batch_size\n",
    "    \n",
    "    # L2 정규화 항 추가\n",
    "    l2_loss = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (Conv2D, FullyConnected)):\n",
    "            l2_loss += cp.sum(layer.weights ** 2)\n",
    "    loss += l2_lambda * l2_loss / 2\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_derivative(output, target):\n",
    "    return output - target\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = cp.exp(x - cp.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / cp.sum(exp_x, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798564f",
   "metadata": {},
   "source": [
    "# <font color=Red># Data Loader </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21a65878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "def load_data(batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # 전체 MNIST 데이터셋을 로드합니다.\n",
    "    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # 훈련 데이터셋에서 10,000개 샘플만 선택합니다.\n",
    "    train_indices = list(range(60000))\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    \n",
    "    # 테스트 데이터셋에서 2,000개 샘플만 선택합니다.\n",
    "    test_indices = list(range(10000))\n",
    "    test_subset = Subset(test_dataset, test_indices)\n",
    "    \n",
    "    # DataLoader를 생성합니다.\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # DataLoader에서 배치를 가져와 CuPy 배열로 변환합니다.\n",
    "    train_batches = [(cp.array(data.numpy()), cp.array(labels.numpy())) for data, labels in train_loader]\n",
    "    test_batches = [(cp.array(data.numpy()), cp.array(labels.numpy())) for data, labels in test_loader]\n",
    "    \n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc0023",
   "metadata": {},
   "source": [
    "# <font color=Red># Optimization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e778d7",
   "metadata": {},
   "source": [
    "### ADAM(Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ebe92",
   "metadata": {},
   "source": [
    "**0. Initialize 1st momentum and 2nd momentum**\n",
    "$$m_0 = 0 $$ \n",
    "$$v_0 = 0 $$ </br>\n",
    "\n",
    "**1. Update Time step** </br>\n",
    "$$t = t + 1$$\n",
    "\n",
    "\n",
    "**2. Update 1st Momentum and 2nd Momentum(using the gradient g)**\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n",
    "\n",
    "**3. Update estimation**\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "**4. Update parameters(w, b)**\n",
    "\n",
    "$$theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "- Momentum coefficient(Friction)$(\\beta_1,\\,\\,\\beta_2)$ , Generally choosen 0.9 and 0.999, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "499621fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, params, lr=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.params = params  # w, b (parameters to optimize)\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.beta1 = beta1  # coefficient of 1st momentum\n",
    "        self.beta2 = beta2  # coefficient of 2nd momentum\n",
    "        self.epsilon = epsilon  # avoid to devided by 0\n",
    "        \n",
    "        self.m = [cp.zeros_like(p['value']) for p in params]  # initialize 1st momentum\n",
    "        self.v = [cp.zeros_like(p['value']) for p in params]  # initialize 2nd momentum\n",
    "        self.t = 0  # initialize time step\n",
    "\n",
    "    def update_params(self, index, param, grad):\n",
    "        self.t += 1  # 타임스텝 증가\n",
    "        # Update 1st momentum\n",
    "        self.m[index] = self.beta1 * self.m[index] + (1 - self.beta1) * grad\n",
    "        # Update 2nd momentum\n",
    "        self.v[index] = self.beta2 * self.v[index] + (1 - self.beta2) * (grad ** 2)\n",
    "        # update 1st momentum estimation\n",
    "        m_hat = self.m[index] / (1 - self.beta1 ** self.t)\n",
    "        # update 2nd momentum estimation\n",
    "        v_hat = self.v[index] / (1 - self.beta2 ** self.t)\n",
    "        # update parameters\n",
    "        param -= self.lr * m_hat / (cp.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8727d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLearningRateScheduler:\n",
    "    def __init__(self, optimizer, decay_rate=0.85):\n",
    "        self.optimizer = optimizer\n",
    "        self.decay_rate = decay_rate\n",
    "        self.initial_lr = optimizer.lr\n",
    "\n",
    "    def step(self, epoch):\n",
    "        new_lr = self.initial_lr * (self.decay_rate ** epoch)\n",
    "        self.optimizer.lr = new_lr\n",
    "        print(f\"Epoch {epoch + 1}, Updated Learning Rate: {self.optimizer.lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ec1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, current_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        \n",
    "        if current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c32396a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_batches, model, optimizer, scheduler, early_stopping, epochs=10):\n",
    "\n",
    "def train(train_batches, model, optimizer, scheduler, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (input, target) in enumerate(train_batches):\n",
    "            # Forward pass\n",
    "            output = model.forward(input)\n",
    "            # Compute loss\n",
    "            output = softmax(output)\n",
    "            loss = cross_entropy_loss(output, target, model, l2_lambda=0.0005)\n",
    "            total_loss += loss\n",
    "            # Backward pass\n",
    "            dout = cross_entropy_derivative(output, target)\n",
    "            model.backward(dout)\n",
    "            # Update weights and biases for each layer using Adam optimizer\n",
    "            layer_index = 0\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, (Conv2D, FullyConnected, BatchNormalization)):\n",
    "                    layer.update_params(optimizer, layer_index)\n",
    "                    layer_index += 2  # weights and bias are treated separately\n",
    "            # Check early stopping condition after each batch\n",
    "        \n",
    "#             if early_stopping(loss):\n",
    "#                 print(f\"Early stopping triggered within epoch {epoch + 1}, batch {i + 1}\")\n",
    "#                 break\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss}, Batch Size: {input.shape[0]}, Learning Rate: {optimizer.lr}')\n",
    "        # Update learning rate\n",
    "        scheduler.step(epoch)\n",
    "        print(f'Epoch {epoch + 1}, Average Loss: {total_loss / len(train_batches)}, Learning Rate: {optimizer.lr}')\n",
    "\n",
    "def test(test_batches, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, target in test_batches:\n",
    "        output = model.forward(input)\n",
    "        output = softmax(output)\n",
    "        predictions = cp.argmax(output, axis=1)\n",
    "        labels = cp.argmax(target, axis=1)\n",
    "        correct += cp.sum(predictions == labels)\n",
    "        total += labels.size\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34885d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mc(test_batches, model, num_samples=10):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for input, target in test_batches:\n",
    "        output = mc_dropout_predict([(input, target)], model, num_samples=num_samples)\n",
    "        labels = cp.argmax(target, axis=1)\n",
    "        correct += cp.sum(output == labels)\n",
    "        total += labels.size\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy with MC Dropout: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3262bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, num_classes=10):\n",
    "    return cp.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cd87cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Load data\n",
    "train_batches, test_batches = load_data()\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_batches = [(data, to_one_hot(labels)) for data, labels in train_batches]\n",
    "test_batches = [(data, to_one_hot(labels)) for data, labels in test_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9ea97e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 3.4628065750427903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 2, Loss: 4.323901005669167, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 3, Loss: 3.6663169092463104, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 4, Loss: 2.940170927448304, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 5, Loss: 2.769407604313205, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 6, Loss: 2.758892532918841, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 7, Loss: 2.882656745102573, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 8, Loss: 2.29088381969205, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 9, Loss: 2.2242606965334235, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 10, Loss: 2.7911693464005345, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 11, Loss: 1.8991437823788406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 12, Loss: 2.3105165804421333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 13, Loss: 2.0479768234199276, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 14, Loss: 2.031471819247259, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 15, Loss: 2.0835911315390607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 16, Loss: 2.228157928338721, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 17, Loss: 2.182318253427178, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 18, Loss: 2.0341634491422327, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 19, Loss: 2.337043841974452, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 20, Loss: 1.9369773452723138, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 21, Loss: 1.7079270040171224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 22, Loss: 1.9455860270886294, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 23, Loss: 1.6699937154432605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 24, Loss: 2.0304240479415436, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 25, Loss: 2.286773693888213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 26, Loss: 1.8132113085065444, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 27, Loss: 1.6239267069595529, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 28, Loss: 1.599053043695239, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 29, Loss: 1.9327966968631656, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 30, Loss: 1.7834816975649788, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 31, Loss: 2.201132686723208, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 32, Loss: 1.661449217043956, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 33, Loss: 1.7118037086734355, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 34, Loss: 1.649740008871744, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 35, Loss: 1.793941207661037, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 36, Loss: 1.7646469090824914, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 37, Loss: 1.758137757713735, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 38, Loss: 1.3680524647730803, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 39, Loss: 1.7454015988097704, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 40, Loss: 1.8037815360560918, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 41, Loss: 1.7962323323968419, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 42, Loss: 0.9978888976897374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 43, Loss: 1.8055209412234576, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 44, Loss: 1.639209702933561, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 45, Loss: 1.1195624603200829, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 46, Loss: 1.5867640288866445, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 47, Loss: 1.1397049045800394, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 48, Loss: 1.776639959674583, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 49, Loss: 1.2792453434161866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 50, Loss: 1.0593379362791877, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 51, Loss: 1.5050752769680429, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 52, Loss: 1.2092617731668178, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 53, Loss: 1.6239222184736046, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 54, Loss: 1.605199267584289, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 55, Loss: 1.3509384257879409, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 56, Loss: 1.48805817017181, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 57, Loss: 1.4328432707080543, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 58, Loss: 1.1265528385338905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 59, Loss: 1.2659407043843278, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 60, Loss: 0.805227303058276, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 61, Loss: 1.0474586334740785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 62, Loss: 1.1954209008308598, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 63, Loss: 0.8508182749116663, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 64, Loss: 1.060871305029551, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 65, Loss: 1.273966180069537, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 66, Loss: 1.0035515928813328, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 67, Loss: 0.8205110661898962, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 68, Loss: 0.9471084388426441, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 69, Loss: 1.0349813290049499, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 70, Loss: 1.1222829779190104, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 71, Loss: 1.1374178117094969, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 72, Loss: 1.2112063929594192, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 73, Loss: 1.1702697563109956, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 74, Loss: 0.9705767458454683, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 75, Loss: 0.8341380083527503, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 76, Loss: 0.5937746110613908, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 77, Loss: 1.0083145244265819, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 78, Loss: 1.2113891566667556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 79, Loss: 0.9862622745901104, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 80, Loss: 1.051911634814822, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 81, Loss: 1.3517160718588257, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 82, Loss: 1.0012636880816097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 83, Loss: 0.7937935192535402, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 84, Loss: 0.9340432228201693, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 85, Loss: 1.2970947729977653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 86, Loss: 0.9613367025290271, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 87, Loss: 0.8618458428076099, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 88, Loss: 0.5275481199729828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 89, Loss: 0.6869473648095245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 90, Loss: 0.9520953632273155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 91, Loss: 0.5290173409659725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 92, Loss: 0.8240101944752218, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 93, Loss: 0.9566972795202165, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 94, Loss: 0.7521206159660059, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 95, Loss: 0.9338256577163777, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 96, Loss: 0.740582527155371, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 97, Loss: 0.9585118036266809, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 98, Loss: 0.5333794866457463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 99, Loss: 0.7734803411841716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 100, Loss: 0.9052504912590994, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 101, Loss: 0.7607590640051163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 102, Loss: 0.5892272124619622, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 103, Loss: 1.274557852847102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 104, Loss: 0.809858050469866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 105, Loss: 0.7122868282451462, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 106, Loss: 1.1846081561290749, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 107, Loss: 0.6042588398871513, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 108, Loss: 0.7331739650601151, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 109, Loss: 1.4182627013085864, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 110, Loss: 1.0333247664239718, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 111, Loss: 0.6091570207495259, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 112, Loss: 0.6824147753531868, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 113, Loss: 0.940039690707822, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 114, Loss: 1.2029760017566906, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 115, Loss: 0.9773178511845542, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 116, Loss: 0.6451843504280262, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 117, Loss: 1.041892898831187, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 118, Loss: 1.0865284705114515, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 119, Loss: 1.1178246665251286, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 120, Loss: 1.0556189803704996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 121, Loss: 0.9589107516558406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 122, Loss: 0.6549873830780139, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 123, Loss: 1.0013114066061173, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 124, Loss: 0.9917342329347693, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 125, Loss: 0.9491196389018935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 126, Loss: 0.6493267004252811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 127, Loss: 0.8082289403479428, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 128, Loss: 0.685459066883641, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 129, Loss: 0.8269958882221284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 130, Loss: 0.8005236894385794, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 131, Loss: 0.5932973968348999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 132, Loss: 0.9164609515395801, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 133, Loss: 0.8875872109508448, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 134, Loss: 0.6296549574861696, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 135, Loss: 0.6665991418559217, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 136, Loss: 1.041367074427467, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 137, Loss: 1.002714836439242, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 138, Loss: 0.9299874266551731, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 139, Loss: 1.0009001925489271, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 140, Loss: 0.7173280033281946, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 141, Loss: 0.6914659909000329, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 142, Loss: 0.4799798355212098, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 143, Loss: 0.6687879413040452, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 144, Loss: 0.7413117197611248, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 145, Loss: 0.899662434193362, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 146, Loss: 0.9410471817459878, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 147, Loss: 0.7742114312285148, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 148, Loss: 0.6991246995103944, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 149, Loss: 1.0570903302288102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 150, Loss: 0.9172116315103155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 151, Loss: 0.8273847826882763, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 152, Loss: 0.5031143876767822, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 153, Loss: 0.781145530871489, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 154, Loss: 0.646360843151039, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 155, Loss: 0.6287615397041482, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 156, Loss: 0.772366999475826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 157, Loss: 0.6785161400875859, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 158, Loss: 0.8382921970096378, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 159, Loss: 0.672952736766052, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 160, Loss: 1.2384121266070531, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 161, Loss: 0.7713199213277326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 162, Loss: 0.4934968485946949, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 163, Loss: 0.732670737904955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 164, Loss: 0.7743244894038868, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 165, Loss: 0.6250005987663512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 166, Loss: 0.9818873577556979, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 167, Loss: 0.7412615165681308, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 168, Loss: 1.0154124159247226, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 169, Loss: 0.7363794547824424, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 170, Loss: 0.9131560582610931, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 171, Loss: 0.7404435975476416, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 172, Loss: 0.6003514598303277, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 173, Loss: 0.5997738970849029, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 174, Loss: 0.8965198265487124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 175, Loss: 0.5601656228194233, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 176, Loss: 0.3677808662464494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 177, Loss: 0.7075403006307155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 178, Loss: 0.46964368077333896, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 179, Loss: 0.6925654291656856, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 180, Loss: 0.7144008456626928, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 181, Loss: 0.8328175632870023, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 182, Loss: 0.5926916165218835, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 183, Loss: 0.795809247799612, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 184, Loss: 0.682028282567498, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 185, Loss: 0.6994404067197986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 186, Loss: 0.777683115381526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 187, Loss: 1.0973128961239929, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 188, Loss: 0.558010088394211, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 189, Loss: 0.7072912902404161, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 190, Loss: 0.47110205541411676, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 191, Loss: 0.767429620424205, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 192, Loss: 0.759391285919201, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 193, Loss: 0.7675531496892796, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 194, Loss: 0.4590586211166516, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 195, Loss: 0.6115585378333882, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 196, Loss: 0.974532134704587, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 197, Loss: 0.5510562493695279, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 198, Loss: 0.7566638637569001, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 199, Loss: 1.0350138195078795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 200, Loss: 0.6401828643333033, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 201, Loss: 0.5697323044572511, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 202, Loss: 0.5670929459725059, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 203, Loss: 0.6381667714501376, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 204, Loss: 0.4196303220965033, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 205, Loss: 0.6347893872119494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 206, Loss: 0.5529566555723539, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 207, Loss: 0.8391256630320123, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 208, Loss: 1.0825176900634106, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 209, Loss: 1.3178288588086, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 210, Loss: 0.5696561418200531, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 211, Loss: 0.955831647675722, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 212, Loss: 0.7600877515327442, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 213, Loss: 0.6866888121276198, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 214, Loss: 0.4726987173374666, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 215, Loss: 0.5597335899682634, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 216, Loss: 0.5332326730011697, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 217, Loss: 1.1606856875469989, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 218, Loss: 0.7272792340736655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 219, Loss: 0.5280156185024638, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 220, Loss: 0.6206506534405873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 221, Loss: 0.5535638396974183, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 222, Loss: 0.5730955142633534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 223, Loss: 0.8088633907125428, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 224, Loss: 0.5452603246811479, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 225, Loss: 0.601072965501124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 226, Loss: 0.7106007828849201, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 227, Loss: 0.9185382432109206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 228, Loss: 0.6059475538775795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 229, Loss: 0.6062141838940672, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 230, Loss: 0.5317392009513408, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 231, Loss: 0.581628726237547, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 232, Loss: 0.6411689481612816, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 233, Loss: 0.8621855542849813, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 234, Loss: 0.8443570195990964, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 235, Loss: 1.0290706096611537, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 236, Loss: 0.6537239150858274, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 237, Loss: 0.7941537562688707, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 238, Loss: 0.4093355321835401, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 239, Loss: 0.6786642589548623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 240, Loss: 0.5904491543384824, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 241, Loss: 0.595770725796327, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 242, Loss: 0.3353073300377082, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 243, Loss: 0.6307993814138952, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 244, Loss: 0.7708907631531867, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 245, Loss: 0.44788619080698866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 246, Loss: 0.801551911319345, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 247, Loss: 0.5779015496318654, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 248, Loss: 0.7350432827975582, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 249, Loss: 0.5532043328071359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 250, Loss: 0.5704147325064919, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 251, Loss: 0.816629662457828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 252, Loss: 1.0664300750100824, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 253, Loss: 0.4399175940836981, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 254, Loss: 0.45245574524992593, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 255, Loss: 0.43296055740329054, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 256, Loss: 0.7803630097982582, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 257, Loss: 0.6051303000917376, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 258, Loss: 1.0267274184988067, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 259, Loss: 0.574968805465089, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 260, Loss: 0.6140796643931746, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 261, Loss: 0.5683929265391785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 262, Loss: 0.5154176643715684, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 263, Loss: 0.4525043472631254, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 264, Loss: 0.7727652236703053, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 265, Loss: 0.44862764650713594, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 266, Loss: 0.592522872371563, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 267, Loss: 0.6140949354968854, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 268, Loss: 0.9176373855260307, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 269, Loss: 0.618443791827834, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 270, Loss: 0.5108811240156228, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 271, Loss: 0.5632050845761744, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 272, Loss: 0.4234474387026075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 273, Loss: 0.783521660973553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 274, Loss: 0.7080538587703018, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 275, Loss: 0.7026466384202419, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 276, Loss: 0.99076967161658, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 277, Loss: 0.4722297042935961, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 278, Loss: 0.7434174664099992, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 279, Loss: 0.7350631781612609, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 280, Loss: 0.7052751069091985, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 281, Loss: 0.37752932376977677, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 282, Loss: 0.3360957358868858, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 283, Loss: 0.5251754482525045, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 284, Loss: 0.851599252845337, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 285, Loss: 0.4745974898106161, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 286, Loss: 0.35253103431355814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 287, Loss: 0.8268720240033767, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 288, Loss: 0.9799869619178218, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 289, Loss: 0.5487450529510431, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 290, Loss: 0.5069927941405173, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 291, Loss: 0.848957186363175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 292, Loss: 0.6655421534139434, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 293, Loss: 0.7434185257354187, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 294, Loss: 0.45395468243062176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 295, Loss: 0.27963369372397273, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 296, Loss: 0.9433755942200843, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 297, Loss: 0.6021574764917423, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 298, Loss: 0.44956095982941796, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 299, Loss: 0.8159475347340523, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 300, Loss: 1.0697177709704544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 301, Loss: 0.46180413795464026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 302, Loss: 0.5284719456106377, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 303, Loss: 0.9186198564119418, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 304, Loss: 0.7953260738563291, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 305, Loss: 0.34418873753188095, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 306, Loss: 0.7932097875626567, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 307, Loss: 0.5871067770568533, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 308, Loss: 0.44959145254037175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 309, Loss: 0.5556682701877268, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 310, Loss: 0.6627735098892555, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 311, Loss: 0.5763412853061227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 312, Loss: 0.3599048240263619, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 313, Loss: 0.6396141407821658, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 314, Loss: 0.3817449654423942, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 315, Loss: 0.5910295456962179, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 316, Loss: 0.5075088284353159, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 317, Loss: 0.6837987964163463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 318, Loss: 0.44998420449732474, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 319, Loss: 0.5434237940716333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 320, Loss: 0.7046757997281552, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 321, Loss: 0.7644894705845584, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 322, Loss: 0.6317752664113772, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 323, Loss: 0.7637579047653887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 324, Loss: 0.7461718618789618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 325, Loss: 0.5644115967912646, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 326, Loss: 0.7477001899699075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 327, Loss: 0.3471773930387272, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 328, Loss: 0.5454300327957717, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 329, Loss: 0.5211664443844239, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 330, Loss: 0.4771897444941383, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 331, Loss: 0.5946395596994891, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 332, Loss: 0.5400262576680119, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 333, Loss: 0.5240443204213163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 334, Loss: 0.23551537636119296, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 335, Loss: 0.44340783763566993, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 336, Loss: 0.5809085834148112, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 337, Loss: 0.35199540906362514, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 338, Loss: 0.5041045227545441, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 339, Loss: 0.7434028494280414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 340, Loss: 0.22250314977445765, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 341, Loss: 0.7681865622375292, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 342, Loss: 0.7087282264545559, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 343, Loss: 0.6189137536759318, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 344, Loss: 0.26808450780742793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 345, Loss: 0.7383533545183618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 346, Loss: 0.4362325084024852, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 347, Loss: 0.5287290731150113, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 348, Loss: 0.6857303544974167, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 349, Loss: 0.48688946904939445, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 350, Loss: 0.6954582686999452, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 351, Loss: 0.49896108563620456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 352, Loss: 0.43052645713747106, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 353, Loss: 0.6861829417605512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 354, Loss: 0.3498367321417428, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 355, Loss: 0.5536053299481913, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 356, Loss: 0.9902966339879368, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 357, Loss: 0.6021326365841209, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 358, Loss: 0.7481885894065222, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 359, Loss: 0.5329138507173409, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 360, Loss: 0.5721897719151037, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 361, Loss: 0.5608547292886807, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 362, Loss: 0.5246237852139589, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 363, Loss: 0.6766959419529082, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 364, Loss: 0.42427566862616817, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 365, Loss: 0.48237164890541184, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 366, Loss: 0.5014148041648938, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 367, Loss: 0.7122132208584387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 368, Loss: 0.7465907202357421, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 369, Loss: 1.0532179058954287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 370, Loss: 0.5927080651964333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 371, Loss: 0.6015234140526793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 372, Loss: 0.5216880488554259, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 373, Loss: 0.5567621691589465, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 374, Loss: 0.44049076643798907, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 375, Loss: 0.6107871897617395, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 376, Loss: 0.3652140297224713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 377, Loss: 0.7349063025564526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 378, Loss: 0.5305075725531764, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 379, Loss: 0.7298660245297484, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 380, Loss: 0.9911529409000779, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 381, Loss: 0.4861170689383283, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 382, Loss: 0.738163480020653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 383, Loss: 0.7104119559087998, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 384, Loss: 0.6512500154587071, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 385, Loss: 0.7167146915938729, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 386, Loss: 0.25843739308707514, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 387, Loss: 0.5520601698782248, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 388, Loss: 0.43271806814981284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 389, Loss: 0.5362770624970421, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 390, Loss: 0.40230239604779405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 391, Loss: 0.6402333785458526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 392, Loss: 0.44673651879455234, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 393, Loss: 0.3706391004190627, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 394, Loss: 0.6615403104373451, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 395, Loss: 0.5667745466030933, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 396, Loss: 0.5925326472853848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 397, Loss: 0.4830356384930077, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 398, Loss: 0.4360722323190102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 399, Loss: 0.2874701816104437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 400, Loss: 0.6180339467120342, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 401, Loss: 0.4699278435210834, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 402, Loss: 0.46853619518082484, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 403, Loss: 0.5220890372988097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 404, Loss: 0.47573284057122556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 405, Loss: 0.764992399676314, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 406, Loss: 0.6027334075449859, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 407, Loss: 0.262324251290507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 408, Loss: 0.3424402288468415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 409, Loss: 0.7065565562138509, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 410, Loss: 0.4794354321622105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 411, Loss: 0.8167093447551482, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 412, Loss: 0.29553753786040715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 413, Loss: 0.7954707302809486, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 414, Loss: 1.113969616677198, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 415, Loss: 0.583189604754748, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 416, Loss: 0.4877794875814325, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 417, Loss: 0.8402434866210768, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 418, Loss: 0.4874343314064381, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 419, Loss: 0.40272372268301715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 420, Loss: 0.7292503775130198, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 421, Loss: 0.5129882303285147, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 422, Loss: 0.6578327059114986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 423, Loss: 0.6161358579867859, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 424, Loss: 0.2646596775366322, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 425, Loss: 0.42375677495859143, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 426, Loss: 0.8269182774085473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 427, Loss: 0.45529807359844215, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 428, Loss: 0.3280419849059787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 429, Loss: 0.45584694113866076, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 430, Loss: 0.7696647774728527, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 431, Loss: 0.5374072724158568, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 432, Loss: 0.5231463222610315, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 433, Loss: 0.5161187557773423, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 434, Loss: 0.5962059253745658, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 435, Loss: 0.43475648065893435, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 436, Loss: 0.7631337553073244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 437, Loss: 0.32753836524700114, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 438, Loss: 0.4887508605694457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 439, Loss: 0.44246116475734, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 440, Loss: 0.5447315764948153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 441, Loss: 0.593488550086458, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 442, Loss: 0.45722433851202254, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 443, Loss: 0.6550598444857102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 444, Loss: 0.47230102450697903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 445, Loss: 0.4453705618563689, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 446, Loss: 0.9695038068178373, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 447, Loss: 0.6871519096432092, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 448, Loss: 0.4868352933137539, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 449, Loss: 0.43547780931329305, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 450, Loss: 0.8656040405260401, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 451, Loss: 0.3093709687356406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 452, Loss: 0.6113452542563811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 453, Loss: 0.3766200818529133, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 454, Loss: 0.5680560606247248, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 455, Loss: 0.45768304276134997, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 456, Loss: 0.44029421308498295, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 457, Loss: 0.44554696819691153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 458, Loss: 0.4126298316616096, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 459, Loss: 0.7722068601570535, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 460, Loss: 0.5888899600905043, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 461, Loss: 0.23434118811910626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 462, Loss: 0.8630557619430645, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 463, Loss: 0.25181033554199506, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 464, Loss: 0.5200107579839628, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 465, Loss: 0.6076205025772, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 466, Loss: 0.5283833196500478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 467, Loss: 0.6876855527603974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 468, Loss: 0.554043837525507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 469, Loss: 0.5253823421686564, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 470, Loss: 0.4953544458795544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 471, Loss: 0.648359553292396, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 472, Loss: 0.613602303044851, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 473, Loss: 0.5231239455364649, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 474, Loss: 0.7872808144182682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 475, Loss: 0.4827679570396915, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 476, Loss: 0.2290366240565309, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 477, Loss: 0.6595272250631903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 478, Loss: 0.4632773417525944, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 479, Loss: 0.5076751150856152, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 480, Loss: 0.4922452675149287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 481, Loss: 0.4701737059230883, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 482, Loss: 0.5517876206624965, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 483, Loss: 0.5976212911963233, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 484, Loss: 0.26487213027666984, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 485, Loss: 0.9072208908610185, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 486, Loss: 0.6221593750458632, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 487, Loss: 0.3268688701761957, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 488, Loss: 0.753317276194284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 489, Loss: 0.6627475414971207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 490, Loss: 0.4817520680197074, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 491, Loss: 0.5443089592474344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 492, Loss: 0.5413242031490684, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 493, Loss: 0.8154165019549534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 494, Loss: 0.6120409270835028, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 495, Loss: 0.3098988459314974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 496, Loss: 0.6168032455648584, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 497, Loss: 0.3224477375813206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 498, Loss: 0.41574832330538913, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 499, Loss: 0.38193403321848424, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 500, Loss: 0.4222018104297629, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 501, Loss: 0.3443304695396625, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 502, Loss: 0.43143501381110605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 503, Loss: 0.9685843635431329, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 504, Loss: 0.7539241627205999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 505, Loss: 0.4765721912054256, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 506, Loss: 0.3548296463769356, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 507, Loss: 0.48474075381877707, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 508, Loss: 0.9253073513158534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 509, Loss: 0.6194245528788599, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 510, Loss: 0.39581074017765405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 511, Loss: 0.4947737913131368, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 512, Loss: 0.6968817022172298, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 513, Loss: 0.46013405348214387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 514, Loss: 0.6813462589465319, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 515, Loss: 0.5443244207931344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 516, Loss: 0.3931316510077528, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 517, Loss: 0.3805584649419388, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 518, Loss: 0.7913127807806964, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 519, Loss: 0.5429535100974784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 520, Loss: 0.8763146984375414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 521, Loss: 0.44692578360501056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 522, Loss: 0.563160710627121, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 523, Loss: 0.3991383219661399, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 524, Loss: 0.5305736859378327, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 525, Loss: 0.275921182513154, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 526, Loss: 0.31423684771511917, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 527, Loss: 0.2845343357643527, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 528, Loss: 0.45738828724277814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 529, Loss: 0.6211835914807283, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 530, Loss: 0.5281138380068513, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 531, Loss: 0.7073759679409055, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 532, Loss: 0.5216288920708967, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 533, Loss: 0.575349433584478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 534, Loss: 0.5505467105192227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 535, Loss: 0.49250981099670244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 536, Loss: 0.47328179953885596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 537, Loss: 0.4091006889250101, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 538, Loss: 0.8343191141078565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 539, Loss: 0.7388483374121795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 540, Loss: 0.5459828786776835, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 541, Loss: 0.4426418684878468, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 542, Loss: 0.42500641229222674, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 543, Loss: 0.5496215950719799, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 544, Loss: 0.7493397151793015, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 545, Loss: 0.5038131976445481, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 546, Loss: 0.3782611781551716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 547, Loss: 0.22082730842891166, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 548, Loss: 0.44295248376463503, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 549, Loss: 0.5718231036436768, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 550, Loss: 0.5678444135852454, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 551, Loss: 0.7250119879546832, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 552, Loss: 0.6656998737744403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 553, Loss: 0.2688763263382336, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 554, Loss: 0.5688670179792396, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 555, Loss: 0.43362709588696974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 556, Loss: 0.789399353090937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 557, Loss: 0.5923353518418031, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 558, Loss: 0.777954634150664, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 559, Loss: 0.45590158312748685, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 560, Loss: 0.49510486601543924, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 561, Loss: 0.5301540247267865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 562, Loss: 0.3469352815567658, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 563, Loss: 0.6876171973805356, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 564, Loss: 0.3091057819156766, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 565, Loss: 0.5816284737245505, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 566, Loss: 0.5613935358589203, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 567, Loss: 0.458525506721887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 568, Loss: 0.678386155942962, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 569, Loss: 0.28088533337504507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 570, Loss: 0.31081425472064655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 571, Loss: 0.31145185888250687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 572, Loss: 0.5536965414246016, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 573, Loss: 0.4757336105737886, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 574, Loss: 0.5924078708801418, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 575, Loss: 0.5644327339479637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 576, Loss: 0.29827285678635307, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 577, Loss: 0.31837001524083514, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 578, Loss: 0.5169423230585881, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 579, Loss: 0.5127453517530293, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 580, Loss: 0.5124546642974952, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 581, Loss: 0.5937386085559786, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 582, Loss: 0.8139912546841227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 583, Loss: 0.573272263258655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 584, Loss: 0.6382553276632357, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 585, Loss: 0.32455645382015696, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 586, Loss: 0.38852622002561693, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 587, Loss: 0.6091273222342417, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 588, Loss: 1.300005374754368, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 589, Loss: 0.3722157992309174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 590, Loss: 0.7271083284162052, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 591, Loss: 0.8571629346810878, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 592, Loss: 0.5714627443893913, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 593, Loss: 0.5082199814848885, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 594, Loss: 0.5351548602041669, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 595, Loss: 0.5295551698457626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 596, Loss: 0.41725533230585776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 597, Loss: 0.4833784737903194, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 598, Loss: 0.47827430719109165, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 599, Loss: 0.24489567661937164, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 600, Loss: 0.85930726156957, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 601, Loss: 0.8007822261569076, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 602, Loss: 0.6293218630783173, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 603, Loss: 0.7008268301054843, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 604, Loss: 0.6623088219389588, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 605, Loss: 0.6177017455380684, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 606, Loss: 0.37625369241627316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 607, Loss: 0.5487427478946775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 608, Loss: 0.30743147398746457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 609, Loss: 0.5130998098408347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 610, Loss: 0.4835352399084701, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 611, Loss: 0.2134677952158682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 612, Loss: 0.39443815691879197, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 613, Loss: 0.2656220931480643, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 614, Loss: 0.5215883121800153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 615, Loss: 0.5534709072602589, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 616, Loss: 0.7952607390963455, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 617, Loss: 0.5790568042886938, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 618, Loss: 0.37995584920891545, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 619, Loss: 0.34999488482530466, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 620, Loss: 0.6669039162252268, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 621, Loss: 0.5388835299758983, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 622, Loss: 0.4450094518760432, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 623, Loss: 0.43556940208967937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 624, Loss: 0.4855485623871322, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 625, Loss: 0.47133288580043287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 626, Loss: 0.38959338009780425, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 627, Loss: 0.398878623315703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 628, Loss: 0.26001187401294823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 629, Loss: 0.24102894256803054, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 630, Loss: 0.5895750214102768, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 631, Loss: 0.27968912069139196, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 632, Loss: 0.39691539821770844, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 633, Loss: 0.40016225392532645, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 634, Loss: 0.26673583064406026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 635, Loss: 0.5152840855318372, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 636, Loss: 0.5303547209549665, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 637, Loss: 0.3075541061515861, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 638, Loss: 0.34043969283670955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 639, Loss: 0.4096397081337746, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 640, Loss: 0.5541577176773792, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 641, Loss: 0.5160454928290531, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 642, Loss: 0.4727647246583395, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 643, Loss: 0.5871245712430405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 644, Loss: 0.7540105361488332, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 645, Loss: 0.720981659253928, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 646, Loss: 0.5112199826403397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 647, Loss: 0.35968303480084507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 648, Loss: 0.3161207237838475, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 649, Loss: 0.43611863837336895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 650, Loss: 0.46085103094474417, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 651, Loss: 0.507211501535012, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 652, Loss: 0.247944970492999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 653, Loss: 0.23891053166971016, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 654, Loss: 0.3381659611150507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 655, Loss: 0.29332942361685876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 656, Loss: 0.7377154047626497, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 657, Loss: 0.3614950347390036, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 658, Loss: 1.1777003578945995, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 659, Loss: 0.6217827294165196, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 660, Loss: 0.5525573171275355, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 661, Loss: 0.25791479280300955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 662, Loss: 0.2663261663364447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 663, Loss: 0.33997565248398764, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 664, Loss: 0.5527461237222642, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 665, Loss: 0.609368043843076, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 666, Loss: 0.7353257151600262, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 667, Loss: 0.6452874184755741, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 668, Loss: 0.635244648101003, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 669, Loss: 0.5255940768059058, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 670, Loss: 0.2925506934751553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 671, Loss: 0.3123798139188612, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 672, Loss: 0.5097372827167626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 673, Loss: 0.3661961922580693, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 674, Loss: 0.5005244506411495, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 675, Loss: 0.6109983074428238, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 676, Loss: 0.46618882979147414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 677, Loss: 0.7175035739363049, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 678, Loss: 0.44961762801102045, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 679, Loss: 0.500169043171837, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 680, Loss: 0.6096810299174102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 681, Loss: 0.312870286406554, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 682, Loss: 0.38244770689825563, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 683, Loss: 0.44897821897839596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 684, Loss: 0.4081023907568549, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 685, Loss: 0.3739887743092891, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 686, Loss: 0.560399116569271, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 687, Loss: 0.40660516013948655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 688, Loss: 0.6389741661899416, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 689, Loss: 0.32832390064567163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 690, Loss: 0.7189001651111927, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 691, Loss: 0.3773352651446174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 692, Loss: 1.292219067140108, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 693, Loss: 0.5293022311213508, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 694, Loss: 0.6325192702849558, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 695, Loss: 0.7532384836673112, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 696, Loss: 0.3961778732413156, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 697, Loss: 0.34427928387016665, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 698, Loss: 0.31147000699123845, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 699, Loss: 0.3039008618009393, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 700, Loss: 0.3710815389535157, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 701, Loss: 0.8618988296982955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 702, Loss: 0.5291694340067143, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 703, Loss: 0.473200663242592, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 704, Loss: 0.5396968474487226, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 705, Loss: 0.26297078920323014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 706, Loss: 0.9772639349247588, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 707, Loss: 0.36280699324512566, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 708, Loss: 0.49780327925236406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 709, Loss: 0.6318365050997055, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 710, Loss: 0.4237145644003729, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 711, Loss: 0.4900799428756831, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 712, Loss: 0.616841418621757, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 713, Loss: 0.30386678893227737, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 714, Loss: 0.5046276860995811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 715, Loss: 0.41644906935911385, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 716, Loss: 0.5443372230432771, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 717, Loss: 0.3846650951916437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 718, Loss: 0.9799132048673351, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 719, Loss: 0.5126793953092705, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 720, Loss: 0.8442231610367716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 721, Loss: 0.46951904335369876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 722, Loss: 0.710674851057842, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 723, Loss: 0.6253923674475064, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 724, Loss: 0.40430428656880446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 725, Loss: 0.40283947737267356, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 726, Loss: 0.4928425105046821, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 727, Loss: 0.7395276725065654, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 728, Loss: 0.3894841495364667, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 729, Loss: 0.7386226116870536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 730, Loss: 0.8461847048344058, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 731, Loss: 0.4616256677680289, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 732, Loss: 0.3292594559038271, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 733, Loss: 0.6576313854903452, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 734, Loss: 0.7857100321054856, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 735, Loss: 0.46339280746124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 736, Loss: 0.6453115251068295, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 737, Loss: 0.3500081697093667, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 738, Loss: 0.5053037551806819, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 739, Loss: 0.5824963741928413, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 740, Loss: 0.2441486550595792, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 741, Loss: 0.4694976959417594, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 742, Loss: 0.36168064604037264, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 743, Loss: 0.2991792015684781, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 744, Loss: 0.545535467073583, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 745, Loss: 0.34519711760177474, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 746, Loss: 0.7845152989725083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 747, Loss: 0.7368550832833112, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 748, Loss: 0.3903741470755303, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 749, Loss: 0.44508095575229645, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 750, Loss: 0.3159957252962229, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 751, Loss: 0.5643149193255431, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 752, Loss: 0.4367751620426336, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 753, Loss: 0.4496292661729905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 754, Loss: 0.5175479454728602, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 755, Loss: 0.6657488215840999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 756, Loss: 0.5329123245581848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 757, Loss: 0.3407706741303668, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 758, Loss: 0.8874363320879726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 759, Loss: 0.43245144221513004, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 760, Loss: 0.41292592784198057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 761, Loss: 0.33672146495832017, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 762, Loss: 0.4736178170036638, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 763, Loss: 0.41917751362620503, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 764, Loss: 0.17481121991993773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 765, Loss: 0.7133681609180478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 766, Loss: 0.658385971896513, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 767, Loss: 0.6698281489209876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 768, Loss: 0.5027314481718808, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 769, Loss: 0.5035906108724018, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 770, Loss: 0.5505881849713002, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 771, Loss: 0.41963105729936784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 772, Loss: 0.3559647073931815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 773, Loss: 0.5378445587370487, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 774, Loss: 0.5524528520210942, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 775, Loss: 0.4827494399296107, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 776, Loss: 0.4708845945767094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 777, Loss: 0.30411920237407997, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 778, Loss: 0.16028508713826714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 779, Loss: 0.6596530654695791, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 780, Loss: 0.5311867356663313, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 781, Loss: 0.867810150517519, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 782, Loss: 0.3628080357374025, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 783, Loss: 0.41994070430465863, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 784, Loss: 0.31314329073746616, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 785, Loss: 0.19715484220794965, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 786, Loss: 0.573982504050742, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 787, Loss: 0.5623367436842972, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 788, Loss: 0.3654687503638502, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 789, Loss: 0.44067418279861115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 790, Loss: 0.5692916513230748, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 791, Loss: 0.5883572968460243, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 792, Loss: 0.3633159140476963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 793, Loss: 1.1171406291535029, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 794, Loss: 0.32263965325248334, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 795, Loss: 0.19769771519440135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 796, Loss: 0.6537871204071682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 797, Loss: 0.5993078451121273, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 798, Loss: 0.29669324113686385, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 799, Loss: 0.400945806981428, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 800, Loss: 0.5637768218135989, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 801, Loss: 0.6223919695377153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 802, Loss: 0.5675189598540942, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 803, Loss: 0.2897688257763102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 804, Loss: 0.8297102343007191, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 805, Loss: 0.32871368207980944, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 806, Loss: 0.25292570816333926, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 807, Loss: 0.61773485272661, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 808, Loss: 0.5529720969330872, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 809, Loss: 0.39134544963542633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 810, Loss: 0.36895685510963766, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 811, Loss: 0.5755775343236365, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 812, Loss: 0.41570411421544373, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 813, Loss: 0.3687374230170163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 814, Loss: 0.31089726323597344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 815, Loss: 0.3889578547708411, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 816, Loss: 0.4775134491915359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 817, Loss: 0.44905688059972815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 818, Loss: 0.47641767165804405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 819, Loss: 0.35044303362311957, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 820, Loss: 0.34343636734473226, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 821, Loss: 0.6308440144323001, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 822, Loss: 0.3827350793701542, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 823, Loss: 0.6469716536924773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 824, Loss: 0.5411855378354085, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 825, Loss: 0.40801700809885094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 826, Loss: 0.5190515737314709, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 827, Loss: 0.3997466664470338, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 828, Loss: 0.4347274878018119, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 829, Loss: 0.24768390460880146, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 830, Loss: 0.3437591395317332, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 831, Loss: 0.44283137890410923, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 832, Loss: 0.3577156665244171, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 833, Loss: 0.447486233656308, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 834, Loss: 0.28739599389695814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 835, Loss: 0.703306833911249, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 836, Loss: 0.39132296401648287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 837, Loss: 0.6369537318522014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 838, Loss: 0.37465152205909413, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 839, Loss: 0.3868883131419941, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 840, Loss: 0.5986301291033046, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 841, Loss: 0.454433318984625, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 842, Loss: 0.46893385587962694, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 843, Loss: 0.6460365904656248, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 844, Loss: 0.4570666456800905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 845, Loss: 0.36695283552293473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 846, Loss: 0.5569309316432655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 847, Loss: 0.5816419478696195, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 848, Loss: 0.48975165739068605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 849, Loss: 0.5817359017669272, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 850, Loss: 0.772500829520256, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 851, Loss: 0.41393177726812613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 852, Loss: 0.28699207083060474, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 853, Loss: 0.49655887688290384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 854, Loss: 0.5862259230919377, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 855, Loss: 0.5038206588430827, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 856, Loss: 0.5118751697206457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 857, Loss: 0.8441688345264723, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 858, Loss: 0.8635717707996552, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 859, Loss: 0.2974569545497679, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 860, Loss: 0.21976217200542103, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 861, Loss: 0.5244465017052742, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 862, Loss: 0.27863624711883617, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 863, Loss: 0.3259300792990719, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 864, Loss: 0.8357365589799458, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 865, Loss: 0.4316571915437437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 866, Loss: 0.3969763582038105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 867, Loss: 0.6637260213322302, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 868, Loss: 0.5219540798674371, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 869, Loss: 0.4036461064339324, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 870, Loss: 0.4344174507422502, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 871, Loss: 0.30405156521767446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 872, Loss: 0.6082580119807559, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 873, Loss: 0.6540179006674359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 874, Loss: 0.3822975637142826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 875, Loss: 0.38458422860054775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 876, Loss: 0.2833832237488454, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 877, Loss: 0.34590442299976715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 878, Loss: 0.4234377634073655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 879, Loss: 0.2891296148385828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 880, Loss: 0.43792282778768243, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 881, Loss: 0.6691174037649732, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 882, Loss: 0.6100211534461004, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 883, Loss: 0.4557754660017902, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 884, Loss: 0.32422349707464776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 885, Loss: 0.6860952547525077, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 886, Loss: 0.4560481338228344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 887, Loss: 0.7523701050476457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 888, Loss: 0.6066833561107776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 889, Loss: 0.35111934984979976, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 890, Loss: 0.37628357024785386, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 891, Loss: 0.38758264850725366, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 892, Loss: 0.45590660101117714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 893, Loss: 0.31509924152058855, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 894, Loss: 0.44321984375280904, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 895, Loss: 0.4185432114379446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 896, Loss: 0.4607564302292269, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 897, Loss: 0.3274638087150482, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 898, Loss: 0.37416021014537626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 899, Loss: 0.3372620098641291, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 900, Loss: 0.6570612103359748, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 901, Loss: 0.6909067361287645, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 902, Loss: 0.4591753781560425, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 903, Loss: 0.5536967610639669, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 904, Loss: 0.7392997491655828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 905, Loss: 0.60041571810152, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 906, Loss: 0.5788238790551868, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 907, Loss: 0.2474870148687397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 908, Loss: 0.6309987926496581, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 909, Loss: 0.37071889215022075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 910, Loss: 0.6849172283134787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 911, Loss: 0.23547485571204016, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 912, Loss: 0.39457619282799633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 913, Loss: 0.43164070741869787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 914, Loss: 0.33980295381542003, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 915, Loss: 0.9946823607144462, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 916, Loss: 0.4944624586817258, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 917, Loss: 0.7329272495342606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 918, Loss: 0.32070753061330115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 919, Loss: 0.45239458048509573, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 920, Loss: 0.40235297920611557, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 921, Loss: 0.43849628425060794, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 922, Loss: 0.3163091105879447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 923, Loss: 0.3751336709273087, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 924, Loss: 0.642243458177323, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 925, Loss: 0.6602019923869284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 926, Loss: 0.8840429299506092, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 927, Loss: 1.1126561974332418, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 928, Loss: 0.4180199221052244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 929, Loss: 0.2806102998315776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 930, Loss: 0.5492928371326987, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 931, Loss: 0.5704070828987207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 932, Loss: 0.5228490093739352, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 933, Loss: 0.21694174604965036, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 934, Loss: 0.5043184714661093, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 935, Loss: 0.571015226122477, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 936, Loss: 0.5915284831659127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 937, Loss: 0.2949641478579432, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 938, Loss: 0.6935795850284437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 939, Loss: 0.2410991319453665, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 940, Loss: 0.3679055101574996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 941, Loss: 0.3373053280121938, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 942, Loss: 0.2993307088152447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 943, Loss: 0.5466081650306099, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 944, Loss: 0.259609432720936, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 945, Loss: 0.6356010774873251, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 946, Loss: 0.3637060443192064, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 947, Loss: 0.29697226612183736, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 948, Loss: 0.3669855868605266, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 949, Loss: 0.48011998226860164, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 950, Loss: 0.504448273515823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 951, Loss: 0.48113741948938116, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 952, Loss: 0.42793858111630134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 953, Loss: 0.28003437601083797, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 954, Loss: 0.36077116376989493, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 955, Loss: 0.4020161826132425, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 956, Loss: 0.2557302356956811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 957, Loss: 0.5378642701303873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 958, Loss: 0.3055675725751999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 959, Loss: 0.5084978378592695, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 960, Loss: 0.28408643414870793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 961, Loss: 0.8355198235064215, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 962, Loss: 0.33882186944118975, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 963, Loss: 0.49328204892788496, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 964, Loss: 0.4184317515669336, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 965, Loss: 0.3352758059944173, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 966, Loss: 0.5035868815072772, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 967, Loss: 0.47410880639102293, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 968, Loss: 0.25554039020005337, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 969, Loss: 0.5320159817296292, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 970, Loss: 0.41581143750107996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 971, Loss: 0.4769246807873073, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 972, Loss: 0.6240047588338133, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 973, Loss: 0.505588348948108, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 974, Loss: 1.149498588998084, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 975, Loss: 0.39073625958191915, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 976, Loss: 0.3555312652053636, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 977, Loss: 0.32558582658782087, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 978, Loss: 0.5540467063846783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 979, Loss: 0.2571803545074691, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 980, Loss: 0.3080417167139471, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 981, Loss: 0.5021228879202149, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 982, Loss: 0.22852808931119262, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 983, Loss: 0.39506121976019637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 984, Loss: 0.3342084239978631, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 985, Loss: 0.41983935623033963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 986, Loss: 0.7375786799360714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 987, Loss: 0.39028831283955906, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 988, Loss: 0.45205427658787933, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 989, Loss: 0.36641090428523165, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 990, Loss: 0.7630577973949557, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 991, Loss: 0.44458669636551407, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 992, Loss: 0.2082405757352762, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 993, Loss: 0.24416464736417381, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 994, Loss: 0.35183934274318873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 995, Loss: 0.4400803058213175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 996, Loss: 0.6684729542342123, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 997, Loss: 0.3314740302659309, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 998, Loss: 0.39309353661406077, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 999, Loss: 0.3019624575242852, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1000, Loss: 0.4272744193601973, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1001, Loss: 0.6699285928062569, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1002, Loss: 0.711522112679776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1003, Loss: 0.27987186715678847, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1004, Loss: 0.3022096868522256, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1005, Loss: 0.43245467471449567, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1006, Loss: 0.3992190917889159, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1007, Loss: 0.5116603816321164, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1008, Loss: 0.3215397491475658, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1009, Loss: 0.7461962300224737, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1010, Loss: 0.429670214049322, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1011, Loss: 0.8212008275397186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1012, Loss: 0.4929705808187399, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1013, Loss: 0.5509839078326225, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1014, Loss: 0.41445825890191307, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1015, Loss: 0.46209498935052556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1016, Loss: 0.44716449702590333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1017, Loss: 0.5173640698349219, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1018, Loss: 0.5442029107993966, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1019, Loss: 0.42953232330605173, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1020, Loss: 0.19624858264493572, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1021, Loss: 0.7594232272871504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1022, Loss: 0.41446118540468535, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1023, Loss: 0.2918174857488166, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1024, Loss: 0.5060984714809169, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1025, Loss: 0.3537033284908079, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1026, Loss: 0.34735246412057463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1027, Loss: 0.3645491959271021, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1028, Loss: 0.6772861302227956, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1029, Loss: 0.33845250998164567, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1030, Loss: 0.2661999600482249, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1031, Loss: 0.6028178994240131, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1032, Loss: 0.5635863464284923, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1033, Loss: 0.6424509400557621, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1034, Loss: 0.2937642974509084, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1035, Loss: 0.5580266230376373, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1036, Loss: 0.6700000008977349, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1037, Loss: 0.6194809317535668, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1038, Loss: 0.30486485296128185, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1039, Loss: 0.20922253769334925, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1040, Loss: 0.3740090412261877, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1041, Loss: 0.19396301500992374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1042, Loss: 0.18337657346858469, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1043, Loss: 0.666679359693134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1044, Loss: 0.7448569891359228, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1045, Loss: 0.7384491833312057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1046, Loss: 0.33487894014176056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1047, Loss: 0.813750596001847, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1048, Loss: 0.22490565839544596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1049, Loss: 0.6602037725419, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1050, Loss: 0.6257088009831429, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1051, Loss: 0.3544579686580466, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1052, Loss: 0.5607497630753601, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1053, Loss: 0.41721018765483553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1054, Loss: 0.33912720336780655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1055, Loss: 0.22947911591875897, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1056, Loss: 0.4061738879109792, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1057, Loss: 0.32097426303442467, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1058, Loss: 0.570084385374172, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1059, Loss: 0.2742033152813509, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1060, Loss: 0.40710852946529585, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1061, Loss: 0.4234032335196326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1062, Loss: 0.7292049982435314, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1063, Loss: 0.32164855563856243, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1064, Loss: 0.527183719765006, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1065, Loss: 0.45100132223263023, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1066, Loss: 0.42375528559687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1067, Loss: 0.29153201021717057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1068, Loss: 0.3738602134866177, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1069, Loss: 0.4593907948616994, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1070, Loss: 0.19709706502743857, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1071, Loss: 0.27139045776125365, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1072, Loss: 0.23834216076791478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1073, Loss: 0.41769969578416954, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1074, Loss: 0.47207253786868963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1075, Loss: 0.4207769364044007, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1076, Loss: 0.5267486309489757, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1077, Loss: 0.2109098706552979, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1078, Loss: 0.34699394537347944, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1079, Loss: 0.6161152551519882, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1080, Loss: 0.2860139509534898, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1081, Loss: 0.37772574748382115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1082, Loss: 0.23906172134505896, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1083, Loss: 0.2504553633054099, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1084, Loss: 0.3366220628791364, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1085, Loss: 0.33139438595373627, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1086, Loss: 0.3952429127525572, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1087, Loss: 0.5821983868062988, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1088, Loss: 0.8131243575825504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1089, Loss: 0.37561404414647315, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1090, Loss: 0.23285434341755284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1091, Loss: 0.6918954827093287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1092, Loss: 0.7007155833144969, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1093, Loss: 0.3993240059192348, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1094, Loss: 0.6399006955056297, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1095, Loss: 0.3040381865955666, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1096, Loss: 0.3186955639261517, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1097, Loss: 0.5778022718038722, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1098, Loss: 0.6581054987285118, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1099, Loss: 0.4206385485712147, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1100, Loss: 0.19164622698634956, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1101, Loss: 0.7585575158300781, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1102, Loss: 0.44102099711844317, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1103, Loss: 0.6057233652764099, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1104, Loss: 0.46819482346841623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1105, Loss: 0.3074508043814087, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1106, Loss: 0.45089356916963086, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1107, Loss: 0.6474230538058088, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1108, Loss: 0.6330577825947774, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1109, Loss: 0.3706781360830754, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1110, Loss: 0.2610218015724206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1111, Loss: 0.6534052668963939, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1112, Loss: 0.4826431096813339, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1113, Loss: 0.282531365601258, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1114, Loss: 0.5690610572184667, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1115, Loss: 0.479417333222295, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1116, Loss: 0.6169802420109284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1117, Loss: 0.44265328567200213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1118, Loss: 0.34129566719302806, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1119, Loss: 0.49418762406370703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1120, Loss: 0.5267125495986619, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1121, Loss: 0.4225898873666134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1122, Loss: 0.3992677848718553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1123, Loss: 0.40271307254921174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1124, Loss: 0.3790669855266594, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1125, Loss: 0.38371306779763287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1126, Loss: 0.6097490483232455, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1127, Loss: 0.6665992698087443, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1128, Loss: 0.4058919275479075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1129, Loss: 0.35078812552095073, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1130, Loss: 0.3864656328686056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1131, Loss: 0.768881029312446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1132, Loss: 0.3912756696521409, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1133, Loss: 0.34143901009446653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1134, Loss: 0.5090322067341756, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1135, Loss: 0.23042702292257458, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1136, Loss: 0.3254554762868618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1137, Loss: 0.5322306072262063, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1138, Loss: 0.7840761225701127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1139, Loss: 0.35717868413290665, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1140, Loss: 0.5065047314770661, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1141, Loss: 0.7421062030833651, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1142, Loss: 0.6555322135293414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1143, Loss: 0.42661225295388866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1144, Loss: 0.6269333415751107, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1145, Loss: 0.49070980978380396, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1146, Loss: 0.534500750732619, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1147, Loss: 0.2561361356794724, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1148, Loss: 0.31701466576875903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1149, Loss: 0.7677152837183647, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1150, Loss: 0.5095039462808258, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1151, Loss: 0.5697595351786562, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1152, Loss: 0.4251060710327673, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1153, Loss: 0.31989772380323633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1154, Loss: 0.491264930714945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1155, Loss: 0.27262593527222767, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1156, Loss: 0.31821798280614155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1157, Loss: 0.6018522423452244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1158, Loss: 0.5091210389834059, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1159, Loss: 0.48376464041855516, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1160, Loss: 0.7424781605441075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1161, Loss: 0.32494486696928776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1162, Loss: 0.5250764793169609, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1163, Loss: 0.341506296627213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1164, Loss: 0.6779081912752083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1165, Loss: 0.4577186145575546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1166, Loss: 0.42781189369547057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1167, Loss: 0.38503714177095144, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1168, Loss: 0.4816826815030522, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1169, Loss: 0.5014745935096733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1170, Loss: 0.31080979175242757, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1171, Loss: 0.3399767866026714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1172, Loss: 0.43362858773091695, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1173, Loss: 0.5109545165485382, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1174, Loss: 0.5860374931060467, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1175, Loss: 0.6606331346720513, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1176, Loss: 0.20194370857232558, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1177, Loss: 0.2621216394994108, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1178, Loss: 0.24061631199120823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1179, Loss: 0.5385759171224125, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1180, Loss: 0.39738764104568886, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1181, Loss: 0.41964839913264906, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1182, Loss: 0.6794236809722997, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1183, Loss: 0.4761271558289919, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1184, Loss: 0.522154031370961, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1185, Loss: 0.3029113636382479, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1186, Loss: 0.18478848576531348, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1187, Loss: 0.43842947977825386, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1188, Loss: 0.3271419017636714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1189, Loss: 0.25030492681473515, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1190, Loss: 0.4385620262236757, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1191, Loss: 0.5754473752842441, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1192, Loss: 0.3320459820700775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1193, Loss: 0.5398091607605375, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1194, Loss: 0.5603418465347316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1195, Loss: 0.32022806826992384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1196, Loss: 0.44686138000262327, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1197, Loss: 0.32048647114331, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1198, Loss: 0.5473291347589573, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1199, Loss: 0.6548170099559245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1200, Loss: 0.39444245816688217, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1201, Loss: 0.39385914167215264, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1202, Loss: 0.2850738716608003, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1203, Loss: 0.5425602289612458, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1204, Loss: 0.24672497429788434, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1205, Loss: 0.2852672946402126, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1206, Loss: 0.4120363247456488, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1207, Loss: 0.5460090058573982, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1208, Loss: 0.3612561777420491, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1209, Loss: 0.19716468780318036, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1210, Loss: 0.5910696434174945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1211, Loss: 0.7702108103074496, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1212, Loss: 0.46777548475651676, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1213, Loss: 0.25602488835758774, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1214, Loss: 0.39792020560086466, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1215, Loss: 0.39774642139327154, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1216, Loss: 0.36121756990091086, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1217, Loss: 0.35196303096356607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1218, Loss: 0.6171398931274674, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1219, Loss: 0.18499836018730453, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1220, Loss: 0.6353798715365826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1221, Loss: 1.063949553273783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1222, Loss: 0.5518361023816242, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1223, Loss: 0.3963859298851756, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1224, Loss: 0.48377660827762037, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1225, Loss: 0.5424549240763967, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1226, Loss: 0.46598899377057534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1227, Loss: 0.509724382727651, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1228, Loss: 0.246023895666786, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1229, Loss: 0.6493480045732922, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1230, Loss: 0.4146744047915011, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1231, Loss: 0.573989151048048, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1232, Loss: 0.22131687016486135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1233, Loss: 0.479180965452231, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1234, Loss: 0.5579070898635787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1235, Loss: 0.6880357172317806, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1236, Loss: 0.6965050865081298, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1237, Loss: 0.28692820348832404, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1238, Loss: 0.8452016429779569, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1239, Loss: 0.7147789385904675, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1240, Loss: 0.6580671371516479, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1241, Loss: 0.3826760070794528, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1242, Loss: 0.41583561725666107, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1243, Loss: 0.5702522483430024, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1244, Loss: 0.30770396328584076, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1245, Loss: 0.5558703500235522, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1246, Loss: 0.3010344481464824, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1247, Loss: 0.38030980574257456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1248, Loss: 0.388187570331685, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1249, Loss: 0.3650371979930388, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1250, Loss: 0.7308458842215069, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1251, Loss: 0.6890504770984984, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1252, Loss: 0.6475038336423264, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1253, Loss: 0.3333594413572928, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1254, Loss: 0.5877281200693505, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1255, Loss: 0.45957767471901967, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1256, Loss: 0.49496652621781845, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1257, Loss: 0.43424714454830415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1258, Loss: 0.2518073773457449, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1259, Loss: 0.4051601942503813, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1260, Loss: 0.26631367644079484, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1261, Loss: 0.4410952350298968, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1262, Loss: 0.634354390716499, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1263, Loss: 0.5397192628148996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1264, Loss: 0.31509214130107754, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1265, Loss: 0.5072486898571097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1266, Loss: 0.45295621775478534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1267, Loss: 0.42781812841771905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1268, Loss: 0.5826645093310328, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1269, Loss: 0.5194886337068207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1270, Loss: 0.5189230080767299, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1271, Loss: 0.47743076769072745, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1272, Loss: 0.3169104789276923, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1273, Loss: 0.5116935395951513, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1274, Loss: 0.46892477194876586, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1275, Loss: 0.41040968018963686, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1276, Loss: 0.737091748456021, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1277, Loss: 0.24845455110595438, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1278, Loss: 0.3608779977325597, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1279, Loss: 0.31409133841184916, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1280, Loss: 0.4849550705693415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1281, Loss: 0.3310087162457338, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1282, Loss: 0.4509438209255074, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1283, Loss: 0.33503768511140736, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1284, Loss: 0.498923811810963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1285, Loss: 0.4566116248794132, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1286, Loss: 0.5067961229436119, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1287, Loss: 0.6696816996512449, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1288, Loss: 0.8577677821137548, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1289, Loss: 0.3080713670668349, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1290, Loss: 0.685419617534255, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1291, Loss: 0.4643516720458205, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1292, Loss: 0.36610316776809726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1293, Loss: 0.843744547560269, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1294, Loss: 0.4856355496201498, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1295, Loss: 0.24281972837044466, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1296, Loss: 0.6318226323093055, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1297, Loss: 0.4245621496468941, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1298, Loss: 0.8926079773630995, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1299, Loss: 0.5582805890276148, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1300, Loss: 0.3193728990227691, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1301, Loss: 0.33256279647809206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1302, Loss: 0.6812046546723636, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1303, Loss: 0.3505198598480408, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1304, Loss: 0.21616699105584547, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1305, Loss: 0.41392387995598523, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1306, Loss: 0.35004962497752895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1307, Loss: 0.709238362359607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1308, Loss: 0.3983774786809571, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1309, Loss: 0.48102490463341246, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1310, Loss: 0.34556648181756755, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1311, Loss: 0.3275507959364867, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1312, Loss: 0.37141227369955176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1313, Loss: 0.462486220988083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1314, Loss: 0.7808750519985646, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1315, Loss: 0.28543967710333706, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1316, Loss: 0.36300583486995197, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1317, Loss: 0.5014983313032918, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1318, Loss: 0.2038222390947716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1319, Loss: 0.312578506386615, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1320, Loss: 0.3113634819976536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1321, Loss: 0.416215638791538, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1322, Loss: 0.39614874107642856, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1323, Loss: 0.36115003067211837, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1324, Loss: 0.5270375648079467, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1325, Loss: 0.41237314209846593, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1326, Loss: 0.6883057001164733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1327, Loss: 0.5039097496040582, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1328, Loss: 0.3929821180637241, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1329, Loss: 0.416910064380862, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1330, Loss: 0.6648828394480439, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1331, Loss: 0.2793468382338796, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1332, Loss: 0.5454131361600799, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1333, Loss: 0.6284404486378257, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1334, Loss: 0.3232673920462207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1335, Loss: 0.3821180834390129, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1336, Loss: 1.121053603473308, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1337, Loss: 0.38220224083429566, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1338, Loss: 0.3912102913476406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1339, Loss: 0.7605730723356094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1340, Loss: 0.3520169482831108, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1341, Loss: 0.27840649416611474, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1342, Loss: 0.49356370892283113, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1343, Loss: 0.3767819121313974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1344, Loss: 0.29035123348347325, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1345, Loss: 0.4232750431492657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1346, Loss: 0.33840875652592517, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1347, Loss: 0.4935140082879784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1348, Loss: 0.6170649925669515, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1349, Loss: 0.6692010923815614, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1350, Loss: 0.34756275272897075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1351, Loss: 0.5005255448375672, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1352, Loss: 0.36774841136226655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1353, Loss: 0.5093215864262689, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1354, Loss: 0.37333338756468853, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1355, Loss: 0.3867980224066222, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1356, Loss: 0.3210339086374403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1357, Loss: 0.5521233192432259, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1358, Loss: 0.3340237464595966, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1359, Loss: 0.5566216169609058, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1360, Loss: 0.5073676151601128, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1361, Loss: 0.5912214465599135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1362, Loss: 0.39513126224227846, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1363, Loss: 0.6559500478702405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1364, Loss: 0.3409972036182879, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1365, Loss: 0.418048904777945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1366, Loss: 0.4943013491459441, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1367, Loss: 0.33493447656068553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1368, Loss: 0.5873778170228556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1369, Loss: 0.3238062123096829, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1370, Loss: 0.4526146553506095, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1371, Loss: 0.5276475842736542, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1372, Loss: 0.4634544245569384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1373, Loss: 0.5310088078373417, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1374, Loss: 0.5420779162414439, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1375, Loss: 0.29979447212507426, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1376, Loss: 0.4152410748768124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1377, Loss: 0.6220742516595399, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1378, Loss: 0.4912435345211786, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1379, Loss: 0.3992070050934492, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1380, Loss: 0.31410998317174454, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1381, Loss: 0.5209267350260745, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1382, Loss: 0.2527677600866065, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1383, Loss: 0.5516001403772333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1384, Loss: 0.34973551265747915, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1385, Loss: 0.43014315052016094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1386, Loss: 0.3722879586232558, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1387, Loss: 0.3688706590018891, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1388, Loss: 0.3481252100785457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1389, Loss: 0.39709933125828384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1390, Loss: 0.31770991882119454, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1391, Loss: 0.437382527722176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1392, Loss: 0.440194446708967, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1393, Loss: 0.5372758496999519, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1394, Loss: 0.46808961078706657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1395, Loss: 0.4659513337297545, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1396, Loss: 0.24077290276327462, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1397, Loss: 0.26487431172326803, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1398, Loss: 0.3742045504187316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1399, Loss: 0.7079628556372923, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1400, Loss: 0.4218216469746807, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1401, Loss: 0.25552555339411054, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1402, Loss: 0.39991023693644223, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1403, Loss: 0.3254384522504069, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1404, Loss: 0.36451486969921176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1405, Loss: 0.22876381253143352, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1406, Loss: 0.5873804894721195, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1407, Loss: 0.511657120282425, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1408, Loss: 0.5371670904087738, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1409, Loss: 0.21972714843586316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1410, Loss: 0.32072600340907703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1411, Loss: 0.3872656610903559, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1412, Loss: 0.8378547218316605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1413, Loss: 0.5917055175842542, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1414, Loss: 0.39265145638924503, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1415, Loss: 0.6040650372266152, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1416, Loss: 0.4098294480355775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1417, Loss: 0.359399970064066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1418, Loss: 0.3979311090244334, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1419, Loss: 0.3008772231328484, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1420, Loss: 0.5437848105341538, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1421, Loss: 0.2835842910437829, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1422, Loss: 0.47932910308577875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1423, Loss: 0.4687039669532871, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1424, Loss: 0.8024860824645409, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1425, Loss: 0.7743436945432479, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1426, Loss: 0.2678650651787696, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1427, Loss: 0.5420755161026067, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1428, Loss: 0.5476741287961961, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1429, Loss: 0.47374437873145075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1430, Loss: 0.36857147602834184, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1431, Loss: 0.263220500628863, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1432, Loss: 0.7474330358076745, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1433, Loss: 0.30083856277032633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1434, Loss: 0.5599541673581603, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1435, Loss: 0.44010232020240925, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1436, Loss: 0.40226153677553617, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1437, Loss: 0.41012917531892495, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1438, Loss: 0.3340929855777691, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1439, Loss: 0.31259607106295983, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1440, Loss: 0.5700942706092557, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1441, Loss: 0.6433024789145871, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1442, Loss: 0.7890191571951835, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1443, Loss: 0.48171461965889384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1444, Loss: 0.3786237649115043, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1445, Loss: 0.5704364971405186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1446, Loss: 0.27209535519278605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1447, Loss: 0.18706544914455642, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1448, Loss: 0.356763161403689, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1449, Loss: 0.5920566422152647, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1450, Loss: 0.4637170051321966, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1451, Loss: 0.38836128579190066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1452, Loss: 0.40932570218132275, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1453, Loss: 0.2717480587319442, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1454, Loss: 0.3239288462225177, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1455, Loss: 0.44276831956067764, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1456, Loss: 0.3806679065822819, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1457, Loss: 0.21550737643172627, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1458, Loss: 0.5437853528493999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1459, Loss: 0.6747700211539818, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1460, Loss: 0.3632620013686382, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1461, Loss: 1.066613139462448, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1462, Loss: 0.45903286134589516, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1463, Loss: 0.44280947088040357, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1464, Loss: 0.4335215770880186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1465, Loss: 0.3124016482723725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1466, Loss: 0.6202104997626017, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1467, Loss: 0.460769512769309, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1468, Loss: 0.4250732211476298, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1469, Loss: 0.42202045904445507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1470, Loss: 0.5681785486661983, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1471, Loss: 0.32374315962145955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1472, Loss: 0.39287351793350483, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1473, Loss: 0.254059987584878, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1474, Loss: 0.40147140227574696, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1475, Loss: 0.3072961292224773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1476, Loss: 0.6904222261088813, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1477, Loss: 0.4881048911390369, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1478, Loss: 0.3878593237293555, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1479, Loss: 0.4715786032333126, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1480, Loss: 0.861799760798124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1481, Loss: 0.4847163830256726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1482, Loss: 0.29557365140555075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1483, Loss: 0.38476824116248465, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1484, Loss: 0.49906927260368505, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1485, Loss: 0.5624735698242523, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1486, Loss: 0.20399183133521204, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1487, Loss: 0.2671881170968392, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1488, Loss: 0.36377710971908245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1489, Loss: 0.2866955475975912, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1490, Loss: 0.47739582821708304, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1491, Loss: 0.3262538351374129, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1492, Loss: 0.3600151942042147, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1493, Loss: 0.25396846288570496, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1494, Loss: 0.28575352046013014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1495, Loss: 0.30424949067501267, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1496, Loss: 0.29014136921018685, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1497, Loss: 0.558914503132177, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1498, Loss: 0.49738993982734386, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1499, Loss: 0.4408348722538316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1500, Loss: 0.43771669576128847, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1501, Loss: 0.3807288847605427, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1502, Loss: 0.5059412509581008, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1503, Loss: 0.2803330677273065, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1504, Loss: 0.26165022045654407, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1505, Loss: 0.7679111296651864, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1506, Loss: 0.3614191156976212, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1507, Loss: 0.22497354866717706, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1508, Loss: 0.6702894501474982, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1509, Loss: 0.3252487533566806, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1510, Loss: 0.28939280831751657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1511, Loss: 0.5280518995919464, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1512, Loss: 1.3081549477421293, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1513, Loss: 0.4020572264106261, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1514, Loss: 0.3471512110432895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1515, Loss: 0.557642667402445, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1516, Loss: 0.5963844820210062, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1517, Loss: 0.3065987040189084, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1518, Loss: 0.47852560302751546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1519, Loss: 0.4499263573712496, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1520, Loss: 0.7775347733190059, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1521, Loss: 0.4504198852989325, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1522, Loss: 0.628851342667307, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1523, Loss: 0.3174950219349879, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1524, Loss: 0.38372803899775476, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1525, Loss: 0.5186758933156576, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1526, Loss: 0.24278920699288875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1527, Loss: 0.5212919065110151, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1528, Loss: 0.4746811849211323, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1529, Loss: 0.5563138179637482, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1530, Loss: 0.3383081191805546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1531, Loss: 0.21748590876075596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1532, Loss: 0.49357636718334064, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1533, Loss: 0.47985095369414144, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1534, Loss: 0.26675219437745656, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1535, Loss: 0.528667035761325, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1536, Loss: 0.659853668777745, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1537, Loss: 0.43779087871452843, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1538, Loss: 0.40352173976969546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1539, Loss: 0.25308520551020364, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1540, Loss: 0.42226046656637656, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1541, Loss: 0.599497506694435, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1542, Loss: 0.2712702907374403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1543, Loss: 0.39332566116953066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1544, Loss: 0.43630305412570347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1545, Loss: 0.3622941748517468, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1546, Loss: 0.24650381989242381, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1547, Loss: 0.6090542077407741, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1548, Loss: 0.43372114157818253, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1549, Loss: 0.3423017071306195, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1550, Loss: 0.2934586276495299, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1551, Loss: 0.5816706569845321, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1552, Loss: 0.33614578952716184, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1553, Loss: 0.5932905961995063, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1554, Loss: 0.33753417005519687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1555, Loss: 0.38485135644838125, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1556, Loss: 0.3985486480197289, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1557, Loss: 0.4231811639380622, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1558, Loss: 0.3642771266573783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1559, Loss: 0.3590143382288427, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1560, Loss: 0.5852468760703007, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1561, Loss: 0.3027946662196367, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1562, Loss: 0.7882278112453249, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1563, Loss: 0.4673066309743238, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1564, Loss: 0.23810241872401, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1565, Loss: 0.4322930150240435, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1566, Loss: 0.22539223663185723, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1567, Loss: 0.24853596428596036, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1568, Loss: 0.2667001531699366, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1569, Loss: 0.8267872749468526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1570, Loss: 0.27856278330458073, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1571, Loss: 0.16085620401651973, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1572, Loss: 0.24843801869824406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1573, Loss: 0.6291419157038971, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1574, Loss: 0.24352264263024512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1575, Loss: 0.5264931124571006, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1576, Loss: 0.42696353261976894, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1577, Loss: 0.2327367756323137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1578, Loss: 0.5132141324680093, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1579, Loss: 0.526586253969979, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1580, Loss: 0.40880989082828556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1581, Loss: 0.2710222218612204, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1582, Loss: 0.5412525408821528, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1583, Loss: 0.4828817154657222, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1584, Loss: 0.5026320467137689, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1585, Loss: 0.35813750479314443, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1586, Loss: 0.3012746182348843, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1587, Loss: 0.2450118565895088, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1588, Loss: 0.3140191942082532, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1589, Loss: 0.5106549433518323, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1590, Loss: 0.42646747757188647, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1591, Loss: 0.1497239517969704, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1592, Loss: 0.33938947623515114, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1593, Loss: 0.38079766584599617, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1594, Loss: 0.2900574729207971, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1595, Loss: 0.34102297567447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1596, Loss: 0.2343795061192916, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1597, Loss: 0.341154832353482, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1598, Loss: 0.630658687800391, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1599, Loss: 0.2916594291534239, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1600, Loss: 0.4138101715957002, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1601, Loss: 0.5855980447190623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1602, Loss: 0.3237218697140344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1603, Loss: 0.7256408834470167, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1604, Loss: 0.5947545911606176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1605, Loss: 0.27496678509044337, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1606, Loss: 0.3185295217355734, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1607, Loss: 0.7022553467525269, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1608, Loss: 0.5610497616496121, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1609, Loss: 0.28721075885212244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1610, Loss: 0.3184668355013407, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1611, Loss: 0.365937238673141, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1612, Loss: 0.7482340800544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1613, Loss: 0.34115440254407536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1614, Loss: 0.3838682960069024, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1615, Loss: 0.26600366907966283, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1616, Loss: 0.7177568259228372, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1617, Loss: 0.33502412498758005, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1618, Loss: 0.7744490885164276, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1619, Loss: 0.33291985686781284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1620, Loss: 0.6040230602517901, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1621, Loss: 0.5067462039869577, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1622, Loss: 0.24807094499683785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1623, Loss: 0.4026527727733985, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1624, Loss: 0.342601602318483, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1625, Loss: 0.7144771669414575, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1626, Loss: 0.19660282753492672, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1627, Loss: 0.3509717615885872, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1628, Loss: 0.29950869589212725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1629, Loss: 0.33133725667452396, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1630, Loss: 0.3664865649594313, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1631, Loss: 0.30029926699772663, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1632, Loss: 0.38932690238893597, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1633, Loss: 0.2296598116255023, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1634, Loss: 0.34491542181838986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1635, Loss: 0.4758715702082365, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1636, Loss: 0.3948811947959626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1637, Loss: 0.3946223860445285, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1638, Loss: 0.44441897787108725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1639, Loss: 0.36450335394024014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1640, Loss: 0.32277087343581734, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1641, Loss: 0.37045626771753926, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1642, Loss: 0.37450217849062073, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1643, Loss: 0.17088745575554448, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1644, Loss: 0.4732357352088689, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1645, Loss: 0.3456082979009306, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1646, Loss: 0.4490478363204571, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1647, Loss: 0.34068320009189174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1648, Loss: 0.2055747289455783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1649, Loss: 0.3185181570256277, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1650, Loss: 0.43260397792355476, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1651, Loss: 0.588238095303539, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1652, Loss: 0.4470753853236512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1653, Loss: 0.25829027548376315, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1654, Loss: 0.698125560036823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1655, Loss: 0.5551794930149954, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1656, Loss: 0.671380549596512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1657, Loss: 0.35979733427587135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1658, Loss: 0.5471303597038799, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1659, Loss: 0.503430168672755, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1660, Loss: 0.34235498060342384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1661, Loss: 0.45440748407978815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1662, Loss: 0.460449618349028, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1663, Loss: 0.3529470137574713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1664, Loss: 0.308313647685037, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1665, Loss: 0.3476844571094875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1666, Loss: 0.6067617868251185, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1667, Loss: 0.35351351614514676, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1668, Loss: 0.551935941462353, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1669, Loss: 0.28744824311091116, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1670, Loss: 0.3407562300905642, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1671, Loss: 0.6107018180194118, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1672, Loss: 0.5010630820390156, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1673, Loss: 0.3363735820674021, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1674, Loss: 0.4499784366764738, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1675, Loss: 0.46009416727382824, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1676, Loss: 0.5232349061723853, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1677, Loss: 0.7114634675672242, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1678, Loss: 0.5574096973459584, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1679, Loss: 0.328317006690843, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1680, Loss: 0.43536725993652614, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1681, Loss: 0.3245805495826808, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1682, Loss: 0.41449374076304224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1683, Loss: 0.44839456785692566, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1684, Loss: 0.28411803127626156, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1685, Loss: 0.4940045994420948, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1686, Loss: 0.5465836774915609, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1687, Loss: 0.1933526544526199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1688, Loss: 0.2841704372267198, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1689, Loss: 0.3740250006502148, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1690, Loss: 0.31625821535272103, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1691, Loss: 0.850906821471351, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1692, Loss: 0.4617892125184412, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1693, Loss: 0.2654537502415012, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1694, Loss: 0.4536184241141966, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1695, Loss: 0.2099996480562842, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1696, Loss: 0.6100371656478296, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1697, Loss: 0.5901173522971249, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1698, Loss: 0.19478665667510875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1699, Loss: 0.3003901045219139, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1700, Loss: 0.2880562229064066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1701, Loss: 0.30141342297335105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1702, Loss: 0.397091985622092, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1703, Loss: 0.3114091899085786, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1704, Loss: 0.4155921123688111, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1705, Loss: 0.3835077543827069, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1706, Loss: 0.4315409280692756, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1707, Loss: 0.7407703554878726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1708, Loss: 0.2529027328851309, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1709, Loss: 0.19677472701514312, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1710, Loss: 0.3676084411058709, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1711, Loss: 0.36364330927138033, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1712, Loss: 0.42794187725949046, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1713, Loss: 0.31021470155817743, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1714, Loss: 0.28993243366134447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1715, Loss: 0.5575307739064446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1716, Loss: 0.25840233167417076, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1717, Loss: 0.39601277464430185, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1718, Loss: 0.6237833987519688, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1719, Loss: 0.38916152703374085, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1720, Loss: 0.6669914831273586, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1721, Loss: 0.533844468352035, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1722, Loss: 0.42552425134023375, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1723, Loss: 0.30221456429790017, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1724, Loss: 0.18276199947529465, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1725, Loss: 0.48276783800783996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1726, Loss: 0.486461411476742, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1727, Loss: 0.2757684911154952, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1728, Loss: 0.2481711449647812, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1729, Loss: 0.33625410992404814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1730, Loss: 0.7362161094267861, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1731, Loss: 0.3100343497611525, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1732, Loss: 0.27525023736473403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1733, Loss: 0.6411316849124832, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1734, Loss: 0.6344808871470333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1735, Loss: 0.3773571041710203, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1736, Loss: 0.35568953208072607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1737, Loss: 0.7150250674924911, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1738, Loss: 0.21223997773552897, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1739, Loss: 0.23058219802128616, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1740, Loss: 0.39406824248308125, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1741, Loss: 0.3032308873207691, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1742, Loss: 0.2763075530632613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1743, Loss: 0.43246749733527523, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1744, Loss: 0.747707606336266, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1745, Loss: 0.5573414610125582, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1746, Loss: 0.3405785587046384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1747, Loss: 0.47847670247164187, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1748, Loss: 0.5028808826230042, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1749, Loss: 0.25031776829213165, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1750, Loss: 0.5726450655712307, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1751, Loss: 0.3432268773431296, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1752, Loss: 0.35190201980453106, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1753, Loss: 0.233964746880897, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1754, Loss: 0.33577522702153756, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1755, Loss: 0.23411648675088692, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1756, Loss: 0.2806582424622529, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1757, Loss: 0.6857897801038026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1758, Loss: 0.2933511204909895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1759, Loss: 0.2766698385215606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1760, Loss: 0.3292115553683306, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1761, Loss: 0.25075507260105945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1762, Loss: 0.5386247629662572, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1763, Loss: 0.2906297386448673, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1764, Loss: 0.3890800902205984, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1765, Loss: 0.36578633671792815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1766, Loss: 0.23384021582394865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1767, Loss: 0.4333232384760646, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1768, Loss: 0.4641902578279236, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1769, Loss: 0.5761638717990577, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1770, Loss: 0.3690768703156543, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1771, Loss: 0.46507662702742636, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1772, Loss: 0.3521562296376352, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1773, Loss: 0.4724031596146739, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1774, Loss: 0.35814961164694686, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1775, Loss: 0.33376713044342776, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1776, Loss: 0.23339061087775526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1777, Loss: 0.45145953065420824, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1778, Loss: 0.37039672480950336, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1779, Loss: 0.230676623273485, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1780, Loss: 0.26227829540648184, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1781, Loss: 0.28000566988425224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1782, Loss: 0.7037232866507662, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1783, Loss: 0.3087264333914972, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1784, Loss: 0.33796185531432354, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1785, Loss: 0.5393059321368744, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1786, Loss: 0.4834713592925342, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1787, Loss: 0.3515127365993139, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1788, Loss: 0.6534755317965565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1789, Loss: 0.510954409149108, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1790, Loss: 0.6198867493963529, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1791, Loss: 0.3407404454386217, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1792, Loss: 0.3902687059038826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1793, Loss: 0.42159474297747335, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1794, Loss: 0.2514049835323618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1795, Loss: 0.3784886766686254, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1796, Loss: 0.42568637390471536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1797, Loss: 0.22465309047528115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1798, Loss: 0.21922286272057986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1799, Loss: 0.292232277681568, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1800, Loss: 0.34127935513938934, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1801, Loss: 0.2864186328226855, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1802, Loss: 0.43443532962247244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1803, Loss: 0.16492033506592624, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1804, Loss: 0.41850313083791546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1805, Loss: 0.2442116578812119, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1806, Loss: 0.2927147367061983, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1807, Loss: 0.4315631112370731, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1808, Loss: 0.6535084008343175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1809, Loss: 0.31357365600604736, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1810, Loss: 0.43822077009625793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1811, Loss: 0.44271234400478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1812, Loss: 0.4617552955323216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1813, Loss: 0.3832871458749779, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1814, Loss: 0.5116106254191433, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1815, Loss: 0.4043217118142144, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1816, Loss: 0.3352443201033025, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1817, Loss: 0.3442109667860668, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1818, Loss: 0.3393711536196347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1819, Loss: 0.265849619210742, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1820, Loss: 0.30747196332055193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1821, Loss: 0.4410221146287126, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1822, Loss: 0.5121260076659789, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1823, Loss: 0.37974396795328597, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1824, Loss: 0.20118850299897384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1825, Loss: 0.2678027191304678, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1826, Loss: 0.6446965889518621, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1827, Loss: 0.2745792682574849, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1828, Loss: 0.5902560495286919, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1829, Loss: 0.5037543814601645, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1830, Loss: 0.45279225859779604, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1831, Loss: 0.23584396580729333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1832, Loss: 0.34660626585958515, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1833, Loss: 0.20962838840609072, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1834, Loss: 0.3783908494452446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1835, Loss: 0.5549244069740735, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1836, Loss: 0.3170015054874853, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1837, Loss: 0.23983971760140355, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1838, Loss: 0.5309657570877939, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1839, Loss: 0.3544379914385012, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1840, Loss: 0.6402319572620064, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1841, Loss: 0.35533094884908917, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1842, Loss: 0.3022026581587416, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1843, Loss: 0.31479196212183935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1844, Loss: 0.3292978786250939, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1845, Loss: 0.436290820141903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1846, Loss: 0.2640434738017641, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1847, Loss: 0.3717023300932133, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1848, Loss: 0.1885925195377544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1849, Loss: 1.095774311174375, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1850, Loss: 0.46373755940064026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1851, Loss: 0.2296859754762473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1852, Loss: 0.705852287621209, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1853, Loss: 0.35214362903723373, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1854, Loss: 0.354322500057978, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1855, Loss: 0.2108603629395151, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1856, Loss: 0.23054613964949414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1857, Loss: 0.39872554409932787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1858, Loss: 0.40082614741510264, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1859, Loss: 0.6691681368918216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1860, Loss: 0.6178299880896493, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1861, Loss: 0.6539122624728176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1862, Loss: 0.2936662981009093, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1863, Loss: 0.3824090473333617, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1864, Loss: 0.3355683365839938, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1865, Loss: 0.5118091641076441, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1866, Loss: 0.25398662950917306, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1867, Loss: 0.813384699488672, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1868, Loss: 0.2771762731728199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1869, Loss: 0.7274749272310694, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1870, Loss: 0.25111399693939723, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1871, Loss: 0.3369701266242532, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1872, Loss: 0.2879419927964207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1873, Loss: 0.36079847807510124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1874, Loss: 0.40189539428812715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Batch 1875, Loss: 0.8653098835473738, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 1, Updated Learning Rate: 0.0003\n",
      "Epoch 1, Average Loss: 0.5539271707043723, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1, Loss: 0.9851034998300736, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 2, Loss: 0.4224235072875733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 3, Loss: 0.45433305071777164, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 4, Loss: 0.6912463255365743, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 5, Loss: 0.35528618370153914, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 6, Loss: 0.356201759726657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 7, Loss: 0.245197716272149, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 8, Loss: 0.5516790416763322, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 9, Loss: 0.2917946039813074, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 10, Loss: 0.5486940595821309, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 11, Loss: 0.3284354222081026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 12, Loss: 0.6871449806120189, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 13, Loss: 0.23289699874118402, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 14, Loss: 0.33331834048382, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 15, Loss: 0.42785454434087145, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 16, Loss: 0.6420681305652338, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 17, Loss: 0.4589871001860437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 18, Loss: 0.47427223023348253, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 19, Loss: 0.5803368547490402, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 20, Loss: 0.4283538614199637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 21, Loss: 0.5368879435809857, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 22, Loss: 0.4978462937082462, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 23, Loss: 0.39418911715929555, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 24, Loss: 0.7613473313275912, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 25, Loss: 0.45250827424498963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 26, Loss: 0.32067376118481916, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 27, Loss: 0.4078354175306591, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 28, Loss: 0.28616209772805623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 29, Loss: 1.0097484252188587, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 30, Loss: 0.4038862172459542, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 31, Loss: 0.30939434487968887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 32, Loss: 0.21843183515833803, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 33, Loss: 0.8265417622830791, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 34, Loss: 0.36577641491540885, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 35, Loss: 0.39628391079091624, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 36, Loss: 0.45899865489844477, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 37, Loss: 0.4515715614535206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 38, Loss: 0.438740942281941, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 39, Loss: 0.43035931703042374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 40, Loss: 0.56274933834494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 41, Loss: 0.2944635663056557, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 42, Loss: 0.18374134418538432, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 43, Loss: 0.5616906151805348, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 44, Loss: 0.4983219564832232, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 45, Loss: 0.34178670100996444, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 46, Loss: 0.41797311344648014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 47, Loss: 0.3553859427874443, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 48, Loss: 0.4382771403142769, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 49, Loss: 0.5356113820523044, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 50, Loss: 0.3135306746695775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 51, Loss: 0.7826183589922011, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 52, Loss: 0.3397609464466316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 53, Loss: 0.45990485516355983, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 54, Loss: 0.4601533423262231, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 55, Loss: 0.2597911635349199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 56, Loss: 0.490335895311683, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 57, Loss: 0.4269542481677173, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 58, Loss: 0.24792230353225905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 59, Loss: 0.25387959866878596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 60, Loss: 0.2627283971843232, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 61, Loss: 0.41966727715413793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 62, Loss: 0.36015475117439444, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 63, Loss: 0.49746937031181687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 64, Loss: 0.5203772908003303, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 65, Loss: 0.35726450464812587, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 66, Loss: 0.8006420960336825, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 67, Loss: 0.5463843672480194, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 68, Loss: 0.25361628955451654, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 69, Loss: 0.38541776188473575, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 70, Loss: 0.316058940577127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 71, Loss: 0.6626757366174656, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 72, Loss: 0.23491120726875642, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 73, Loss: 0.5182777988415335, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 74, Loss: 0.3601529741720909, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 75, Loss: 0.5119259084865827, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 76, Loss: 0.331740703073848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 77, Loss: 0.4305420749959119, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 78, Loss: 0.5837474442283866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 79, Loss: 0.3554127818050974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 80, Loss: 0.6361340813460694, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 81, Loss: 0.45947345235971104, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 82, Loss: 0.6578640556776716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 83, Loss: 0.28765296043513733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 84, Loss: 0.31390380791139283, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 85, Loss: 0.5391264460904206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 86, Loss: 0.48888882653534105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 87, Loss: 0.348344163790944, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 88, Loss: 0.24890207558704158, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 89, Loss: 0.26723510113928706, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 90, Loss: 0.28486731828332823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 91, Loss: 0.2766535535153247, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 92, Loss: 0.3429896689217544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 93, Loss: 0.3260570420684443, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 94, Loss: 0.3369568611664531, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 95, Loss: 0.8376064388610613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 96, Loss: 0.6495575008994893, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 97, Loss: 0.28253325756342196, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 98, Loss: 0.3519248267732733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 99, Loss: 0.3207859237119496, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 100, Loss: 0.2689306490208798, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 101, Loss: 0.3787957660200537, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 102, Loss: 0.2468744448543193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 103, Loss: 0.5478789590710149, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 104, Loss: 0.27363198691596163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 105, Loss: 0.26904628276077247, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 106, Loss: 0.25784671574420837, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 107, Loss: 0.24978587934286828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 108, Loss: 0.19612610643875578, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 109, Loss: 0.36241017894112937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 110, Loss: 0.3920337708581384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 111, Loss: 0.39289371597475964, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 112, Loss: 0.29291551598146565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 113, Loss: 0.592638462769441, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 114, Loss: 0.5521677954542213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 115, Loss: 0.32289229705552636, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 116, Loss: 0.25304551534198527, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 117, Loss: 0.6979401123655042, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 118, Loss: 0.41870749398046536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 119, Loss: 0.27861750696626775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 120, Loss: 0.5388553194944149, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 121, Loss: 0.29587107660435674, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 122, Loss: 0.32442221518121683, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 123, Loss: 0.6466931753047191, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 124, Loss: 0.5113861456937703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 125, Loss: 0.8060059896646463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 126, Loss: 0.36761581212916855, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 127, Loss: 0.29560140266642254, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 128, Loss: 0.3461963764948356, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 129, Loss: 0.720370220615679, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 130, Loss: 0.37182346443506953, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 131, Loss: 0.31447583468148693, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 132, Loss: 0.5682497803364204, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 133, Loss: 0.5507747782172174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 134, Loss: 0.3566785922791238, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 135, Loss: 0.25910841529006867, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 136, Loss: 0.4976748720332669, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 137, Loss: 0.21245796644244494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 138, Loss: 0.32004157519876225, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 139, Loss: 0.37821724300491644, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 140, Loss: 0.44583985865379255, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 141, Loss: 0.24150483606886147, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 142, Loss: 0.3942548975476508, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 143, Loss: 0.4184737668678331, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 144, Loss: 0.4170563977654371, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 145, Loss: 0.32085718839728816, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 146, Loss: 0.3218405051496056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 147, Loss: 0.6981925136091036, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 148, Loss: 0.4240050661694911, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 149, Loss: 0.43313337782649325, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 150, Loss: 0.553808215152901, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 151, Loss: 0.3624000090379368, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 152, Loss: 0.2562137719499379, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 153, Loss: 0.4360374523501265, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 154, Loss: 0.4087279809715818, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 155, Loss: 0.32371006020048015, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 156, Loss: 0.37440571156397195, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 157, Loss: 0.19847783833713767, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 158, Loss: 0.23594824346581608, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 159, Loss: 0.24388026934728935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 160, Loss: 0.42553051168188094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 161, Loss: 0.2923643108126972, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 162, Loss: 0.2972953599699968, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 163, Loss: 0.38089580586017385, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 164, Loss: 0.5806431323078827, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 165, Loss: 0.23297597742275605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 166, Loss: 0.3798694158724918, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 167, Loss: 0.3459587637767151, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 168, Loss: 0.738893615396097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 169, Loss: 0.44775102438191544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 170, Loss: 0.3718891658922787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 171, Loss: 0.2311180791545716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 172, Loss: 0.2261315282179698, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 173, Loss: 0.47066104220594096, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 174, Loss: 0.3556814811388864, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 175, Loss: 0.24245186855943035, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 176, Loss: 0.24861395187695473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 177, Loss: 0.42483537466333066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 178, Loss: 0.5370582476691945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 179, Loss: 0.4083133188182199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 180, Loss: 0.5335272037067978, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 181, Loss: 0.3925221306922498, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 182, Loss: 0.4070426013675951, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 183, Loss: 0.3352643432908878, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 184, Loss: 0.41949482813476385, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 185, Loss: 0.48690038068012365, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 186, Loss: 0.4116328724747198, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 187, Loss: 0.9526798412782314, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 188, Loss: 0.657901586649686, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 189, Loss: 0.6981578489937887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 190, Loss: 0.5468274683864267, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 191, Loss: 0.27564496613334466, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 192, Loss: 0.44051277482858564, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 193, Loss: 0.25843567588746075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 194, Loss: 0.4001514111593225, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 195, Loss: 0.24473192687628387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 196, Loss: 0.44698592798121917, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 197, Loss: 0.5622359665974839, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 198, Loss: 0.32949068474405246, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 199, Loss: 0.40697451869507045, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 200, Loss: 0.28401583959572063, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 201, Loss: 0.34270451835629046, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 202, Loss: 0.4930135186913616, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 203, Loss: 0.3203019966561298, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 204, Loss: 0.4359132775908797, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 205, Loss: 0.2929200489899487, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 206, Loss: 0.31065071758992, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 207, Loss: 0.3983139995561886, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 208, Loss: 0.594785888219525, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 209, Loss: 0.7714335025371695, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 210, Loss: 0.24714659761262553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 211, Loss: 0.44637186684733315, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 212, Loss: 0.29742937856010265, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 213, Loss: 0.4126146062544556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 214, Loss: 0.21269134983306115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 215, Loss: 0.3445647488685178, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 216, Loss: 0.32165520744033727, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 217, Loss: 0.3486403066002265, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 218, Loss: 0.45125871506006127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 219, Loss: 0.32206922756173556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 220, Loss: 0.4115294992677765, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 221, Loss: 0.2338792553402314, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 222, Loss: 0.25383889999503206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 223, Loss: 0.4884371615648922, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 224, Loss: 0.25313084356464727, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 225, Loss: 0.4618218425514862, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 226, Loss: 0.6143228203627957, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 227, Loss: 0.5437348162859563, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 228, Loss: 0.5875450922557865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 229, Loss: 0.37510826751080195, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 230, Loss: 0.4445070884443014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 231, Loss: 0.33141156672348715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 232, Loss: 0.6230596750971924, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 233, Loss: 0.5191408671631457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 234, Loss: 0.45737626833734946, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 235, Loss: 0.8232906405915708, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 236, Loss: 0.26430198649886555, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 237, Loss: 0.2602177127656692, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 238, Loss: 0.24893604371087785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 239, Loss: 0.3456431137525787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 240, Loss: 0.2734535825691905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 241, Loss: 0.3009234810741106, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 242, Loss: 0.31185961011122426, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 243, Loss: 0.496607258373404, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 244, Loss: 0.300697337985832, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 245, Loss: 0.6580718707425027, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 246, Loss: 0.32552789262316606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 247, Loss: 0.3758996177259536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 248, Loss: 0.31320165613112555, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 249, Loss: 0.4118897140983087, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 250, Loss: 0.5861316385252712, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 251, Loss: 0.43319854935726565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 252, Loss: 0.7122899055140476, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 253, Loss: 0.28021487969135406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 254, Loss: 0.2327308892260601, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 255, Loss: 0.4802974815338269, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 256, Loss: 0.3841423897608808, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 257, Loss: 0.6450031835489574, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 258, Loss: 0.41199057986516063, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 259, Loss: 0.2672037319319733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 260, Loss: 0.31410227485704423, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 261, Loss: 0.41464030009102154, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 262, Loss: 0.3243942706043321, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 263, Loss: 0.34841876089754586, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 264, Loss: 0.46877716296566346, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 265, Loss: 0.36391169632991904, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 266, Loss: 0.243762970745945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 267, Loss: 0.5715389423803924, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 268, Loss: 0.3968872350040447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 269, Loss: 0.3336896566148085, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 270, Loss: 0.39291607862580724, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 271, Loss: 0.6079447142604071, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 272, Loss: 0.41444064823860527, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 273, Loss: 0.40113962097666334, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 274, Loss: 0.3560133778112822, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 275, Loss: 0.3396713015799287, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 276, Loss: 0.3320772732831012, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 277, Loss: 0.2695892679033697, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 278, Loss: 0.7382280382732427, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 279, Loss: 0.38579759738576114, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 280, Loss: 0.346600744262113, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 281, Loss: 0.29843124884949507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 282, Loss: 0.2144715861545592, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 283, Loss: 0.46681687001830163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 284, Loss: 0.3520234600353468, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 285, Loss: 0.27818838380355054, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 286, Loss: 0.21582016283356165, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 287, Loss: 0.42044423626917, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 288, Loss: 0.5314831144940126, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 289, Loss: 0.3268582464806712, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 290, Loss: 0.24564065227203624, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 291, Loss: 0.543099222791956, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 292, Loss: 0.36281826976970083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 293, Loss: 0.3024812898206265, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 294, Loss: 0.27957664316031705, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 295, Loss: 0.16670063502034224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 296, Loss: 0.5614800474731989, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 297, Loss: 0.36794638939254787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 298, Loss: 0.18669883873873538, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 299, Loss: 0.41250947165042884, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 300, Loss: 0.4925776184049939, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 301, Loss: 0.2640909181726864, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 302, Loss: 0.3092269558137672, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 303, Loss: 0.7136387535107971, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 304, Loss: 0.5653122745186442, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 305, Loss: 0.2792833990558607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 306, Loss: 0.45429310898302155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 307, Loss: 0.5111687474298987, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 308, Loss: 0.3360581718780413, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 309, Loss: 0.2731316394969934, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 310, Loss: 0.4429321228054788, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 311, Loss: 0.2812897801668807, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 312, Loss: 0.2579747401042671, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 313, Loss: 0.463578074712807, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 314, Loss: 0.22819678338907806, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 315, Loss: 0.296241184761733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 316, Loss: 0.24554890108399963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 317, Loss: 0.4644325468681172, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 318, Loss: 0.33774571756670635, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 319, Loss: 0.5334079934693731, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 320, Loss: 0.45213693133864147, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 321, Loss: 0.40151018286293405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 322, Loss: 0.5396007723629628, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 323, Loss: 0.5093295538556848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 324, Loss: 0.4052185010938587, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 325, Loss: 0.3957067820253982, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 326, Loss: 0.3032033277532053, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 327, Loss: 0.35249100435949277, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 328, Loss: 0.1866257047677127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 329, Loss: 0.4634356447818406, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 330, Loss: 0.26514577539800194, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 331, Loss: 0.3461536092920505, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 332, Loss: 0.43771049382831706, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 333, Loss: 0.35088353897388513, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 334, Loss: 0.3297216013639256, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 335, Loss: 0.46230580107162833, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 336, Loss: 0.36592864746762604, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 337, Loss: 0.20588720426360504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 338, Loss: 0.4638419535314773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 339, Loss: 0.4612093591117175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 340, Loss: 0.1946119986358943, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 341, Loss: 0.5920305412384507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 342, Loss: 0.2531956342827959, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 343, Loss: 0.31757254446070193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 344, Loss: 0.19721246632706188, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 345, Loss: 0.4460135782681725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 346, Loss: 0.3065056341028778, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 347, Loss: 0.38717064677509166, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 348, Loss: 0.27455468208896083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 349, Loss: 0.21442996225870825, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 350, Loss: 0.43295809786107986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 351, Loss: 0.6246544921591818, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 352, Loss: 0.21178671530459992, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 353, Loss: 0.5003384938750739, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 354, Loss: 0.36650317522903675, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 355, Loss: 0.27250005532532456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 356, Loss: 0.642393434117497, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 357, Loss: 0.5189148159530093, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 358, Loss: 0.5790166517107405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 359, Loss: 0.39817539920501516, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 360, Loss: 0.2665694429781565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 361, Loss: 0.36026781879070524, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 362, Loss: 0.4300645832751374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 363, Loss: 0.39903323737367735, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 364, Loss: 0.4502008945924596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 365, Loss: 0.3780629518046446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 366, Loss: 0.3570559965070061, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 367, Loss: 0.4707345292668836, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 368, Loss: 0.24663435229396308, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 369, Loss: 0.7207304315537537, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 370, Loss: 0.47790370415177463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 371, Loss: 0.5054986575608236, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 372, Loss: 0.4592506296184988, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 373, Loss: 0.7564452741829576, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 374, Loss: 0.37456471038345596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 375, Loss: 0.35718062850331733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 376, Loss: 0.24058852920184737, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 377, Loss: 0.808322914883036, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 378, Loss: 0.22035573467632533, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 379, Loss: 0.20711529740513798, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 380, Loss: 0.7873765953026762, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 381, Loss: 0.3018142427691841, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 382, Loss: 0.5695939600109569, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 383, Loss: 0.7786832519764172, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 384, Loss: 0.4563959039236811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 385, Loss: 0.40095502629523094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 386, Loss: 0.2367371571569954, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 387, Loss: 0.4392244054963692, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 388, Loss: 0.35397747843990846, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 389, Loss: 0.5067075728273942, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 390, Loss: 0.352984842443415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 391, Loss: 0.6152482880073556, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 392, Loss: 0.3098236640812311, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 393, Loss: 0.3440718080102938, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 394, Loss: 0.24045387989682948, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 395, Loss: 0.36066487673761394, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 396, Loss: 0.41877619253934717, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 397, Loss: 0.4209910091226322, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 398, Loss: 0.2124047563703969, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 399, Loss: 0.25512548067203433, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 400, Loss: 0.222666680688207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 401, Loss: 0.2792666572028709, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 402, Loss: 0.3057990093527677, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 403, Loss: 0.39996375137811935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 404, Loss: 0.346660647469722, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 405, Loss: 0.530053225454207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 406, Loss: 0.4849898858675733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 407, Loss: 0.36981138906887623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 408, Loss: 0.24434576082226345, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 409, Loss: 0.31838813376727193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 410, Loss: 0.37236778093128853, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 411, Loss: 0.4176608709573089, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 412, Loss: 0.26544641922132584, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 413, Loss: 0.4315259906885801, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 414, Loss: 0.8961871726938944, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 415, Loss: 0.5280021078733391, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 416, Loss: 0.32266139654510484, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 417, Loss: 0.26174555864442056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 418, Loss: 0.27371142595833087, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 419, Loss: 0.3450056259962002, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 420, Loss: 0.6350093272469849, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 421, Loss: 0.3976381089264745, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 422, Loss: 0.4691396697178448, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 423, Loss: 0.5151030167666532, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 424, Loss: 0.2513077589319298, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 425, Loss: 0.27079442470535964, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 426, Loss: 0.3347318439764588, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 427, Loss: 0.3842638842455639, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 428, Loss: 0.3595449184385558, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 429, Loss: 0.26386981659970493, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 430, Loss: 0.6718179256975443, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 431, Loss: 0.26590214216611324, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 432, Loss: 0.5767156242605816, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 433, Loss: 0.28671914125589204, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 434, Loss: 0.40975707594148386, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 435, Loss: 0.4677626968875457, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 436, Loss: 0.5403294033312531, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 437, Loss: 0.3725293254597579, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 438, Loss: 0.49082725006855893, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 439, Loss: 0.25940840753317346, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 440, Loss: 0.3384175656716671, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 441, Loss: 0.43740552569818725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 442, Loss: 0.30797098780645366, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 443, Loss: 0.5867865329207931, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 444, Loss: 0.29968244786339365, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 445, Loss: 0.22567023256175706, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 446, Loss: 0.44488162495211814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 447, Loss: 0.2998872923108573, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 448, Loss: 0.5140935068008612, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 449, Loss: 0.3379707406145064, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 450, Loss: 0.3734380294523262, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 451, Loss: 0.28751107089500766, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 452, Loss: 0.39538831129803576, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 453, Loss: 0.3103107135460754, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 454, Loss: 0.5320582501802615, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 455, Loss: 0.2779041000113118, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 456, Loss: 0.2848592187376329, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 457, Loss: 0.34896433681522543, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 458, Loss: 0.21262628897339683, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 459, Loss: 0.262811694494713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 460, Loss: 0.4073412882969759, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 461, Loss: 0.16071942278386925, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 462, Loss: 0.24962183305136082, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 463, Loss: 0.18895089833090692, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 464, Loss: 0.36117835018228717, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 465, Loss: 0.5165876581067955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 466, Loss: 0.5252303888741859, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 467, Loss: 0.4806062138799122, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 468, Loss: 0.4221587313860997, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 469, Loss: 0.3202071053001911, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 470, Loss: 0.6141829546262227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 471, Loss: 0.2506717883874398, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 472, Loss: 0.5853648457711483, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 473, Loss: 0.4052763735822249, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 474, Loss: 0.5717487619816728, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 475, Loss: 0.1791104328952827, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 476, Loss: 0.26978276864633927, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 477, Loss: 0.36651111746871784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 478, Loss: 0.3628714021077558, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 479, Loss: 0.5720010291034086, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 480, Loss: 0.48392479741970695, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 481, Loss: 0.26929335569847657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 482, Loss: 0.42996504230865296, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 483, Loss: 0.744284664024577, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 484, Loss: 0.1639912952547546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 485, Loss: 0.6004661380535229, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 486, Loss: 0.36825003754347774, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 487, Loss: 0.6124633332109597, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 488, Loss: 0.42076946762513423, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 489, Loss: 0.6910931712553622, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 490, Loss: 0.23998198253570688, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 491, Loss: 0.22837914768175205, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 492, Loss: 0.46123587458994453, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 493, Loss: 0.5534746457821164, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 494, Loss: 0.5083530750053489, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 495, Loss: 0.47355620396675935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 496, Loss: 0.24144051803639774, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 497, Loss: 0.2680912441855154, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 498, Loss: 0.23015538270410274, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 499, Loss: 0.4819649573335281, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 500, Loss: 0.22966989945572586, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 501, Loss: 0.30497546556480326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 502, Loss: 0.49780500754789225, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 503, Loss: 0.6657649415741298, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 504, Loss: 0.5451335771360637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 505, Loss: 0.4380716671388505, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 506, Loss: 0.279484471405566, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 507, Loss: 0.43757069117352076, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 508, Loss: 0.6416856610588424, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 509, Loss: 0.5798928130044962, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 510, Loss: 0.30957773719194726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 511, Loss: 0.537612082745622, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 512, Loss: 0.38712934482829053, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 513, Loss: 0.5080409812621867, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 514, Loss: 0.5922888184551431, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 515, Loss: 0.4270243184839504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 516, Loss: 0.21995633214290863, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 517, Loss: 0.3115242476728375, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 518, Loss: 0.5988866159222787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 519, Loss: 0.5557626845001821, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 520, Loss: 0.39781253828808405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 521, Loss: 0.5945812018919534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 522, Loss: 0.4347680656676435, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 523, Loss: 0.5235872109824331, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 524, Loss: 0.5153942135267094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 525, Loss: 0.3866109061473727, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 526, Loss: 0.7198667203442538, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 527, Loss: 0.4139048206586733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 528, Loss: 0.3049510425886067, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 529, Loss: 0.30764636002193707, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 530, Loss: 0.35046150761438427, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 531, Loss: 0.7833546161513618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 532, Loss: 0.2996086524688224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 533, Loss: 0.43370805972314785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 534, Loss: 0.3122007805998789, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 535, Loss: 0.2939985937524613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 536, Loss: 0.41570512841244195, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 537, Loss: 0.3784009110955255, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 538, Loss: 0.5938349671823091, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 539, Loss: 0.452216725827199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 540, Loss: 0.36106512485539255, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 541, Loss: 0.33938203110053144, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 542, Loss: 0.22948130523685137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 543, Loss: 0.4132085246222521, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 544, Loss: 0.2695409908961107, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 545, Loss: 0.34950659817141866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 546, Loss: 0.44668256819580077, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 547, Loss: 0.23085260808151256, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 548, Loss: 0.3976736523495946, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 549, Loss: 0.5618888366882128, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 550, Loss: 0.3947256078870529, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 551, Loss: 0.43619073406142367, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 552, Loss: 0.7328051245448575, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 553, Loss: 0.4215825247447199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 554, Loss: 0.5808113614402868, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 555, Loss: 0.22737138740423135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 556, Loss: 0.5674398372613784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 557, Loss: 0.3747052015848267, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 558, Loss: 0.5880927242636672, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 559, Loss: 0.3999856758747783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 560, Loss: 0.5292373239640933, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 561, Loss: 0.48722846077408977, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 562, Loss: 0.356965264418464, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 563, Loss: 0.46969970301671127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 564, Loss: 0.25736108907085775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 565, Loss: 0.40306740207512715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 566, Loss: 0.6848759370441578, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 567, Loss: 0.21983601738286435, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 568, Loss: 0.3809604006579943, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 569, Loss: 0.38042876980549134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 570, Loss: 0.2748755462369237, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 571, Loss: 0.27251756593947785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 572, Loss: 0.27122485988666134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 573, Loss: 0.23296031891993707, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 574, Loss: 0.4885826557305835, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 575, Loss: 0.4538550305202742, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 576, Loss: 0.20036437628502257, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 577, Loss: 0.3462322980247754, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 578, Loss: 0.24570803835664687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 579, Loss: 0.28313919888067374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 580, Loss: 0.4297417281938667, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 581, Loss: 0.5703981334983975, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 582, Loss: 0.5533484894186241, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 583, Loss: 0.39441010784531794, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 584, Loss: 0.7203384675643669, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 585, Loss: 0.29545748309506403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 586, Loss: 0.41642238909264606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 587, Loss: 0.4082588733900105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 588, Loss: 0.654086543956551, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 589, Loss: 0.3752096677861356, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 590, Loss: 0.3432712872877013, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 591, Loss: 0.3321625767763754, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 592, Loss: 0.6481493409582002, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 593, Loss: 0.5143234356963318, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 594, Loss: 0.37720504382404746, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 595, Loss: 0.35016937111322743, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 596, Loss: 0.35601365660157214, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 597, Loss: 0.38017821628832305, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 598, Loss: 0.5484291583200483, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 599, Loss: 0.3174658758032753, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 600, Loss: 0.5711049097568175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 601, Loss: 0.6610923588699813, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 602, Loss: 0.5835340226907458, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 603, Loss: 0.36770856906367383, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 604, Loss: 0.4599157052213666, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 605, Loss: 0.4111343951499833, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 606, Loss: 0.23416677896830404, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 607, Loss: 0.19057851338941956, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 608, Loss: 0.24718234932391675, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 609, Loss: 0.31528203749670347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 610, Loss: 0.3737060648892837, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 611, Loss: 0.27437156031804033, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 612, Loss: 0.2822750761433926, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 613, Loss: 0.2701200033514011, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 614, Loss: 0.5153875787893427, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 615, Loss: 0.5092782943969117, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 616, Loss: 0.5434197305264117, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 617, Loss: 0.3688059932632183, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 618, Loss: 0.2833727861741812, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 619, Loss: 0.3065231712472618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 620, Loss: 0.37699437305423655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 621, Loss: 0.21747361337621185, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 622, Loss: 0.46554042059582657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 623, Loss: 0.28972437609667623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 624, Loss: 0.399324538607453, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 625, Loss: 0.44878452376471434, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 626, Loss: 0.3317600256912723, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 627, Loss: 0.4444158457651558, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 628, Loss: 0.3668312567635497, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 629, Loss: 0.25171463120703397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 630, Loss: 0.283184768856114, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 631, Loss: 0.3767610584814707, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 632, Loss: 0.5457198020087491, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 633, Loss: 0.28133099661518757, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 634, Loss: 0.4733159041098951, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 635, Loss: 0.4504821942659224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 636, Loss: 0.3732095824348597, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 637, Loss: 0.35305643156775446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 638, Loss: 0.4773308193024485, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 639, Loss: 0.29758645282887364, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 640, Loss: 0.24586756400401993, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 641, Loss: 0.5277797170027333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 642, Loss: 0.4966839526055328, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 643, Loss: 0.4303100002736814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 644, Loss: 0.5439982734748652, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 645, Loss: 0.5061242878579713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 646, Loss: 0.4665841418703123, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 647, Loss: 0.19936798982180737, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 648, Loss: 0.3322139043000008, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 649, Loss: 0.21765153887911062, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 650, Loss: 0.4848764323638828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 651, Loss: 0.4621651854508465, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 652, Loss: 0.27913710638119105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 653, Loss: 0.2500011036885328, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 654, Loss: 0.3660491078639295, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 655, Loss: 0.3672789028391724, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 656, Loss: 0.5443101330055787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 657, Loss: 0.27412561482949965, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 658, Loss: 0.5005734190423188, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 659, Loss: 0.3474470041747795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 660, Loss: 0.3541200687661131, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 661, Loss: 0.2786241509678783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 662, Loss: 0.26027740712315156, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 663, Loss: 0.2747991215679799, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 664, Loss: 0.289821215070147, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 665, Loss: 0.32583853422662656, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 666, Loss: 0.716322464017876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 667, Loss: 0.3024963849008855, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 668, Loss: 0.29453893863155073, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 669, Loss: 0.5438099839905453, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 670, Loss: 0.39162326365153244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 671, Loss: 0.3301449848145359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 672, Loss: 0.4348561112045086, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 673, Loss: 0.42496304990741907, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 674, Loss: 0.2933762756991511, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 675, Loss: 0.47684615257659263, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 676, Loss: 0.24558580607579655, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 677, Loss: 0.3828125823949987, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 678, Loss: 0.4318807021741663, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 679, Loss: 0.27516397351012006, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 680, Loss: 0.2533503389091518, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 681, Loss: 0.3708544984346688, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 682, Loss: 0.4115593375360478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 683, Loss: 0.5082981048521372, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 684, Loss: 0.4482575999622347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 685, Loss: 0.3382167624416531, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 686, Loss: 0.7253228118030095, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 687, Loss: 0.31557072828563415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 688, Loss: 0.4404216266310958, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 689, Loss: 0.23951153959577848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 690, Loss: 0.6975108878040821, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 691, Loss: 0.2389999382610117, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 692, Loss: 0.7930344643251366, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 693, Loss: 0.44077488359108885, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 694, Loss: 0.6044259621322333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 695, Loss: 0.5435953179625668, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 696, Loss: 0.4676639067143748, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 697, Loss: 0.24077306033795512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 698, Loss: 0.363007015548069, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 699, Loss: 0.5346920519111658, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 700, Loss: 0.47787665934385326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 701, Loss: 0.6346748969067451, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 702, Loss: 0.28413985281321186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 703, Loss: 0.6087380239661854, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 704, Loss: 0.4434134465089381, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 705, Loss: 0.260838955185096, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 706, Loss: 0.5870131232194777, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 707, Loss: 0.31320547408420546, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 708, Loss: 0.4048219586016727, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 709, Loss: 0.5364308455514996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 710, Loss: 0.2715644959219061, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 711, Loss: 0.6086683795867731, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 712, Loss: 0.7963227121786323, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 713, Loss: 0.23013569805974826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 714, Loss: 0.738266042499027, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 715, Loss: 0.34546696138592226, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 716, Loss: 0.2879655390502945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 717, Loss: 0.4458033097310277, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 718, Loss: 0.6063498416317834, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 719, Loss: 0.3904251267264028, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 720, Loss: 0.5898823942103864, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 721, Loss: 0.47987206827138473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 722, Loss: 0.40691474531010263, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 723, Loss: 0.5380764774437569, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 724, Loss: 0.27445344167474306, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 725, Loss: 0.21677324602411746, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 726, Loss: 0.3165819014616557, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 727, Loss: 0.6266591252362876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 728, Loss: 0.2508857860299514, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 729, Loss: 0.4414042156329018, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 730, Loss: 0.5011786875516544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 731, Loss: 0.41132193923143323, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 732, Loss: 0.2518833353214771, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 733, Loss: 0.36385009506388266, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 734, Loss: 0.4879392140844533, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 735, Loss: 0.3055099204094456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 736, Loss: 0.23713303433588273, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 737, Loss: 0.25477403004585064, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 738, Loss: 0.4213164385392199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 739, Loss: 0.5554923092077283, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 740, Loss: 0.3664688709038808, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 741, Loss: 0.5242898191445263, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 742, Loss: 0.40318829176152504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 743, Loss: 0.5143525370167153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 744, Loss: 0.2828583496086702, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 745, Loss: 0.32562677449462574, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 746, Loss: 0.6657676872938736, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 747, Loss: 0.6418523775544894, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 748, Loss: 0.37183953950080784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 749, Loss: 0.24041889712681305, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 750, Loss: 0.2714565596989048, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 751, Loss: 0.4715038781209755, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 752, Loss: 0.592587281165192, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 753, Loss: 0.5528584380790427, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 754, Loss: 0.2925124581918651, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 755, Loss: 0.47453440940280583, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 756, Loss: 0.4235772228249781, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 757, Loss: 0.4344627245965366, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 758, Loss: 0.5072530124617963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 759, Loss: 0.41701490503604044, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 760, Loss: 0.4964049819205272, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 761, Loss: 0.3563193795938573, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 762, Loss: 0.31407800947528397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 763, Loss: 0.36739772860972875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 764, Loss: 0.18192042815776815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 765, Loss: 0.5746333401852057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 766, Loss: 0.308377457175623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 767, Loss: 0.5228185079115634, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 768, Loss: 0.34917148361345796, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 769, Loss: 0.5692915753891102, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 770, Loss: 0.3803372635280927, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 771, Loss: 0.503051888572114, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 772, Loss: 0.36472917309320446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 773, Loss: 0.347437845818269, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 774, Loss: 0.42388400888097244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 775, Loss: 0.34968451228165653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 776, Loss: 0.22037147543802163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 777, Loss: 0.32976353997244257, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 778, Loss: 0.2895235097018066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 779, Loss: 0.4504003337126148, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 780, Loss: 0.3325426469753874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 781, Loss: 0.538428861995117, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 782, Loss: 0.6857384582303584, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 783, Loss: 0.40545584968843773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 784, Loss: 0.2676427402541174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 785, Loss: 0.4366898309793633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 786, Loss: 0.4752328321709632, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 787, Loss: 0.3950397771112817, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 788, Loss: 0.21992511379760055, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 789, Loss: 0.6471455611102284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 790, Loss: 0.4546806549556399, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 791, Loss: 0.2927672190622625, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 792, Loss: 0.2984784418330456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 793, Loss: 1.0482635227818153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 794, Loss: 0.3003524918147775, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 795, Loss: 0.34168780388844877, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 796, Loss: 0.8162409074728593, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 797, Loss: 0.7745046656355633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 798, Loss: 0.2631879155073428, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 799, Loss: 0.3169576567711712, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 800, Loss: 0.3825647585803016, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 801, Loss: 0.35813340285931733, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 802, Loss: 0.5692920963549934, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 803, Loss: 0.39642119889171595, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 804, Loss: 0.4603050043404577, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 805, Loss: 0.40980747425882336, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 806, Loss: 0.26075121472588997, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 807, Loss: 0.6419460592749212, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 808, Loss: 0.3148098276163628, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 809, Loss: 0.5098117714698189, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 810, Loss: 0.30407898920763043, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 811, Loss: 0.6771355868645166, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 812, Loss: 0.2785914828359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 813, Loss: 0.23496398500114785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 814, Loss: 0.35266602443327566, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 815, Loss: 0.3277506185009828, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 816, Loss: 0.29014275028832504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 817, Loss: 0.4063401195150067, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 818, Loss: 0.4767732171755734, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 819, Loss: 0.357531673600387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 820, Loss: 0.4256059952852196, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 821, Loss: 0.5077779309539613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 822, Loss: 0.45103511752526315, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 823, Loss: 0.5178406789583953, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 824, Loss: 0.5025385333348324, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 825, Loss: 0.2771304365602645, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 826, Loss: 0.5281271683254095, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 827, Loss: 0.311771409727974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 828, Loss: 0.7403137560367565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 829, Loss: 0.5457693266463804, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 830, Loss: 0.34183324933250236, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 831, Loss: 0.3911461542912412, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 832, Loss: 0.184891147953984, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 833, Loss: 0.46335367376266134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 834, Loss: 0.3355918557420588, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 835, Loss: 0.577672517373169, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 836, Loss: 0.3306938072562007, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 837, Loss: 0.2779293951731131, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 838, Loss: 0.256731197178739, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 839, Loss: 0.25194601521995785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 840, Loss: 0.4055074457190061, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 841, Loss: 0.39105087337139766, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 842, Loss: 0.31374363623128554, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 843, Loss: 0.6305126799201713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 844, Loss: 0.5120412335607079, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 845, Loss: 0.28072264273222214, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 846, Loss: 0.39082968486065445, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 847, Loss: 0.5169954165257141, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 848, Loss: 0.3948197896889126, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 849, Loss: 0.26912002564583115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 850, Loss: 0.5753838537627866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 851, Loss: 0.35730693666970303, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 852, Loss: 0.35866299188704265, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 853, Loss: 0.3534169595539618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 854, Loss: 0.6211804919198862, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 855, Loss: 0.5340020918419248, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 856, Loss: 0.4844074997939598, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 857, Loss: 0.7864233385190492, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 858, Loss: 0.5150722797505434, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 859, Loss: 0.27946739328903225, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 860, Loss: 0.3410218002052713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 861, Loss: 0.4287364624338813, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 862, Loss: 0.34172253617974585, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 863, Loss: 0.2261921221415473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 864, Loss: 0.4498431633067138, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 865, Loss: 0.3424045071046343, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 866, Loss: 0.36412715534945633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 867, Loss: 0.3870129274235683, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 868, Loss: 0.27108567270661493, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 869, Loss: 0.3152378215013829, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 870, Loss: 0.4677173191997863, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 871, Loss: 0.24702982875919727, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 872, Loss: 0.34253641715151945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 873, Loss: 0.48219191964552677, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 874, Loss: 0.2997457729760876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 875, Loss: 0.4179947593847022, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 876, Loss: 0.2072625181552129, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 877, Loss: 0.28189164350775253, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 878, Loss: 0.2664623372668456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 879, Loss: 0.2596052423562911, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 880, Loss: 0.2990983658843682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 881, Loss: 0.472924051864385, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 882, Loss: 0.33479903163706626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 883, Loss: 0.2847587091583333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 884, Loss: 0.32747315646024633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 885, Loss: 0.5117644212724884, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 886, Loss: 0.315686473797251, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 887, Loss: 0.5163000277115061, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 888, Loss: 0.6285172472404259, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 889, Loss: 0.3253531875540009, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 890, Loss: 0.27468261380107883, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 891, Loss: 0.4281580002678982, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 892, Loss: 0.3019440757593068, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 893, Loss: 0.27096730010685477, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 894, Loss: 0.4068918770271538, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 895, Loss: 0.23757279641118595, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 896, Loss: 0.47332647250558524, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 897, Loss: 0.31165683489138163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 898, Loss: 0.332324349831823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 899, Loss: 0.24815896424854364, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 900, Loss: 0.5127235716894447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 901, Loss: 0.2698281622617905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 902, Loss: 0.5403376968373665, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 903, Loss: 0.6320999240926665, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 904, Loss: 0.5314897131945682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 905, Loss: 0.7171749496798201, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 906, Loss: 0.4359589656730588, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 907, Loss: 0.25613278589302135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 908, Loss: 0.3242361968857014, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 909, Loss: 0.31812181617934704, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 910, Loss: 0.5544269692770587, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 911, Loss: 0.4276050731533963, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 912, Loss: 0.4762257699287609, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 913, Loss: 0.33137668944779874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 914, Loss: 0.1870362463450616, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 915, Loss: 0.6574775541950929, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 916, Loss: 0.436948319053873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 917, Loss: 0.4964380468313359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 918, Loss: 0.2540502612263282, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 919, Loss: 0.5825081190246946, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 920, Loss: 0.2837054687479367, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 921, Loss: 0.39539330730172084, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 922, Loss: 0.26750271390858216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 923, Loss: 0.4610127332676677, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 924, Loss: 0.3661466066040526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 925, Loss: 0.7127123761039383, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 926, Loss: 0.73347951422455, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 927, Loss: 0.7833096707558863, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 928, Loss: 0.512773289056994, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 929, Loss: 0.3817097161288219, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 930, Loss: 0.23053761674535928, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 931, Loss: 0.33703117011165634, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 932, Loss: 0.5705386223531622, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 933, Loss: 0.23052715144061747, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 934, Loss: 0.3446339056269463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 935, Loss: 0.41171195532703514, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 936, Loss: 0.35127842803325127, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 937, Loss: 0.3088814452410028, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 938, Loss: 0.6975499455459636, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 939, Loss: 0.3112173709317836, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 940, Loss: 0.40544753825587787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 941, Loss: 0.34914876546954204, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 942, Loss: 0.23786585911264818, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 943, Loss: 0.5039126082931458, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 944, Loss: 0.3192383954826009, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 945, Loss: 0.4022486195630865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 946, Loss: 0.5869962125549368, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 947, Loss: 0.34507744397833456, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 948, Loss: 0.34595095464925785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 949, Loss: 0.5283924760954414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 950, Loss: 0.3282505463804847, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 951, Loss: 0.2975589345930163, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 952, Loss: 0.32208871283299045, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 953, Loss: 0.32222764831537304, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 954, Loss: 0.3505009457131968, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 955, Loss: 0.4034407465185895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 956, Loss: 0.3260113253297182, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 957, Loss: 0.4173334063108647, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 958, Loss: 0.3753350462346813, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 959, Loss: 0.5044018055746202, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 960, Loss: 0.4036666985701044, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 961, Loss: 0.5240747954972383, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 962, Loss: 0.27000766956038524, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 963, Loss: 0.48840066293760587, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 964, Loss: 0.32033657211507793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 965, Loss: 0.32478020796835494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 966, Loss: 0.3900218515443389, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 967, Loss: 0.33451816906992954, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 968, Loss: 0.24377970574094135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 969, Loss: 0.4916247984233112, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 970, Loss: 0.27177555034197143, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 971, Loss: 0.39048026145638337, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 972, Loss: 0.48214650014300425, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 973, Loss: 0.48274661269488806, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 974, Loss: 0.8102901991873911, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 975, Loss: 0.38418018977454216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 976, Loss: 0.26176217286637615, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 977, Loss: 0.30163639754844507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 978, Loss: 0.7879288512438039, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 979, Loss: 0.2743441120090735, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 980, Loss: 0.36586385016574885, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 981, Loss: 0.33980121833153687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 982, Loss: 0.21363772528989092, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 983, Loss: 0.2305167753972167, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 984, Loss: 0.3278104232722344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 985, Loss: 0.24894835116480124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 986, Loss: 0.6082528173660232, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 987, Loss: 0.27971190837908505, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 988, Loss: 0.2496503482412355, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 989, Loss: 0.4616897283782891, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 990, Loss: 0.5902772802298653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 991, Loss: 0.21168588259606214, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 992, Loss: 0.199421092017503, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 993, Loss: 0.2942736009613094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 994, Loss: 0.4028419577347844, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 995, Loss: 0.4285108588455705, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 996, Loss: 0.5794943297048241, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 997, Loss: 0.3090487350615282, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 998, Loss: 0.28411291358520496, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 999, Loss: 0.3487579941743548, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1000, Loss: 0.2728783395350487, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1001, Loss: 0.3717341932140865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1002, Loss: 0.44426695546867845, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1003, Loss: 0.38866754704985246, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1004, Loss: 0.21587027986177995, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1005, Loss: 0.4021669905661781, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1006, Loss: 0.5355573922378185, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1007, Loss: 0.37816568637648074, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1008, Loss: 0.22314054058222596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1009, Loss: 0.5981910558758019, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1010, Loss: 0.31684708945330403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1011, Loss: 0.2659714763371614, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1012, Loss: 0.44734982419644964, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1013, Loss: 0.4292800827181387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1014, Loss: 0.3297880047589331, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1015, Loss: 0.3329017986078009, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1016, Loss: 0.3015701391672545, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1017, Loss: 0.5209812553011679, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1018, Loss: 0.5694025199975732, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1019, Loss: 0.340097943865637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1020, Loss: 0.26124547534999154, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1021, Loss: 0.6229420464492734, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1022, Loss: 0.5351951929454812, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1023, Loss: 0.4693503027163787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1024, Loss: 0.2556812941403528, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1025, Loss: 0.19893933111617682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1026, Loss: 0.3097416317004723, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1027, Loss: 0.3109877915106003, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1028, Loss: 0.5550601379874921, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1029, Loss: 0.39100187697921507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1030, Loss: 0.30454217724341726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1031, Loss: 0.4028665134815857, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1032, Loss: 0.5288856839420397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1033, Loss: 0.48210364853935894, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1034, Loss: 0.24144269823207934, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1035, Loss: 0.667468539064126, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1036, Loss: 0.4693521464447101, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1037, Loss: 0.7745903548905592, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1038, Loss: 0.21243279532215137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1039, Loss: 0.48578661303637566, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1040, Loss: 0.6017325952811049, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1041, Loss: 0.18697603410885216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1042, Loss: 0.17565962031014534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1043, Loss: 0.3304567488860186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1044, Loss: 0.2201933825224604, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1045, Loss: 0.6732942440051474, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1046, Loss: 0.42995090761919874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1047, Loss: 0.46634842754875083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1048, Loss: 0.27648594637562907, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1049, Loss: 0.5215110971622275, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1050, Loss: 0.37961169597699757, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1051, Loss: 0.26391362872069235, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1052, Loss: 0.7373097497039152, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1053, Loss: 0.4101973944118419, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1054, Loss: 0.420800160805071, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1055, Loss: 0.25829283872120207, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1056, Loss: 0.24608806701597408, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1057, Loss: 0.3713483666737153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1058, Loss: 0.5362922630662923, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1059, Loss: 0.3037207569854451, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1060, Loss: 0.35790938143742146, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1061, Loss: 0.4516781598651266, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1062, Loss: 0.5863067651359882, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1063, Loss: 0.3735763040471026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1064, Loss: 0.5767735676199429, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1065, Loss: 0.36309393534425216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1066, Loss: 0.47471945010457317, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1067, Loss: 0.3006950388465479, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1068, Loss: 0.32252672272644545, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1069, Loss: 0.36875226508266773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1070, Loss: 0.24121233811246118, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1071, Loss: 0.47619289330084547, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1072, Loss: 0.34518483422728874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1073, Loss: 0.4179733551918932, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1074, Loss: 0.2918509519490172, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1075, Loss: 0.6066052522574945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1076, Loss: 0.4498546183133167, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1077, Loss: 0.2847534013775742, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1078, Loss: 0.3079204252338392, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1079, Loss: 0.4789018412406848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1080, Loss: 0.2561495012488153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1081, Loss: 0.33814769570217557, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1082, Loss: 0.2937724829935511, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1083, Loss: 0.40009419763697596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1084, Loss: 0.4796802243040381, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1085, Loss: 0.8389749903202341, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1086, Loss: 0.5977628793314318, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1087, Loss: 0.7312324544284945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1088, Loss: 0.5427430683736831, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1089, Loss: 0.22302002326778897, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1090, Loss: 0.28479113690670244, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1091, Loss: 0.3649314732410375, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1092, Loss: 0.6706899264391855, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1093, Loss: 0.3790430249581891, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1094, Loss: 0.7821566803655937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1095, Loss: 0.2695076697661965, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1096, Loss: 0.37739206970211614, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1097, Loss: 0.45788692815685145, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1098, Loss: 0.49521591363121725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1099, Loss: 0.37007333910317275, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1100, Loss: 0.2656414203796017, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1101, Loss: 0.7355192478777397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1102, Loss: 0.3395562811762005, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1103, Loss: 0.8751015350372223, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1104, Loss: 0.7984180959588124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1105, Loss: 0.30885146650590783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1106, Loss: 0.27721416628738216, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1107, Loss: 0.43611335222696823, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1108, Loss: 0.30782732729610446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1109, Loss: 0.30985610996069424, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1110, Loss: 0.33195773184244826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1111, Loss: 0.45015409865819606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1112, Loss: 0.5903735622635904, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1113, Loss: 0.279539350280422, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1114, Loss: 0.3309300382338551, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1115, Loss: 0.2707115211713856, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1116, Loss: 0.4898291955725591, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1117, Loss: 0.7070355575323731, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1118, Loss: 0.35128216676461954, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1119, Loss: 0.6612903594941593, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1120, Loss: 0.3476651217013168, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1121, Loss: 0.513145263009579, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1122, Loss: 0.49266982621872046, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1123, Loss: 0.28296600603295596, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1124, Loss: 0.26832827599053577, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1125, Loss: 0.309066788445907, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1126, Loss: 0.532969870490444, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1127, Loss: 0.5099796830692267, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1128, Loss: 0.42091770602993883, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1129, Loss: 0.28479928345364536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1130, Loss: 0.2869087783530548, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1131, Loss: 0.5174114741513999, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1132, Loss: 0.2615432031652095, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1133, Loss: 0.3882156813729939, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1134, Loss: 0.6726340244761364, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1135, Loss: 0.318928698523519, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1136, Loss: 0.41376737527008245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1137, Loss: 0.5030947638891674, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1138, Loss: 0.656699606438324, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1139, Loss: 0.5095965877179107, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1140, Loss: 0.339227441709702, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1141, Loss: 0.36338277819907006, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1142, Loss: 0.5741118700473389, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1143, Loss: 0.2570501559792485, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1144, Loss: 0.612288460460182, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1145, Loss: 0.5226986478775486, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1146, Loss: 0.33827070427142586, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1147, Loss: 0.4241809923358867, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1148, Loss: 0.23765873748677308, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1149, Loss: 0.3626736335544211, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1150, Loss: 0.7178411577228682, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1151, Loss: 0.5039548278741321, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1152, Loss: 0.4914151596122208, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1153, Loss: 0.2226046969959403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1154, Loss: 0.4238495431067685, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1155, Loss: 0.36782923640667153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1156, Loss: 0.379179701016339, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1157, Loss: 0.4111219697868982, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1158, Loss: 0.4306031935437155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1159, Loss: 0.40286569231803027, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1160, Loss: 0.8410870626286605, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1161, Loss: 0.3811594692260367, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1162, Loss: 0.3342431076406478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1163, Loss: 0.25769031894693073, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1164, Loss: 0.34263014361259964, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1165, Loss: 0.3100091859170526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1166, Loss: 0.3910958535376179, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1167, Loss: 0.3716861854808463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1168, Loss: 0.37728308624702805, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1169, Loss: 0.5153552700164628, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1170, Loss: 0.20125010559101464, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1171, Loss: 0.6536912676883415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1172, Loss: 0.3340554578456818, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1173, Loss: 0.5407682585399344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1174, Loss: 0.5308484334590182, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1175, Loss: 0.4705735173207958, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1176, Loss: 0.253604058836047, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1177, Loss: 0.29654121078797213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1178, Loss: 0.2655672102593702, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1179, Loss: 0.3578188776036917, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1180, Loss: 0.25179196416608507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1181, Loss: 0.5426364327859516, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1182, Loss: 0.8690753976735652, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1183, Loss: 0.28355180055686446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1184, Loss: 0.5764895690833417, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1185, Loss: 0.30989835695511947, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1186, Loss: 0.34784536559964113, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1187, Loss: 0.5999110187672192, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1188, Loss: 0.4760401171777424, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1189, Loss: 0.27153027273832714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1190, Loss: 0.2926076507535399, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1191, Loss: 0.5275877991669409, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1192, Loss: 0.2915460839995422, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1193, Loss: 0.442986955060541, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1194, Loss: 0.4215224290130395, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1195, Loss: 0.3363937069558106, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1196, Loss: 0.5739248836344986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1197, Loss: 0.3499072150399321, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1198, Loss: 0.6521744785782961, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1199, Loss: 0.459234610513253, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1200, Loss: 0.41657220938134637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1201, Loss: 0.24664653072349227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1202, Loss: 0.24885140772140268, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1203, Loss: 0.7423285795043074, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1204, Loss: 0.2735405986086186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1205, Loss: 0.21302172240919598, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1206, Loss: 0.4588292330884936, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1207, Loss: 0.38736887305325585, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1208, Loss: 0.379966741355278, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1209, Loss: 0.23184841535296075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1210, Loss: 0.5378702040112515, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1211, Loss: 0.5730889628179622, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1212, Loss: 0.5487724682399758, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1213, Loss: 0.2729217771166206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1214, Loss: 0.19001806611579833, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1215, Loss: 0.5539601677275057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1216, Loss: 0.2785745996207341, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1217, Loss: 0.2470147572307769, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1218, Loss: 0.6127951219354213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1219, Loss: 0.2994424563518213, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1220, Loss: 0.43947583170276744, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1221, Loss: 1.065930404376807, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1222, Loss: 0.6487046488627252, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1223, Loss: 0.35875764070858873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1224, Loss: 0.274364789528381, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1225, Loss: 0.5119150541293037, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1226, Loss: 0.5323834452226652, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1227, Loss: 0.3853379424923403, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1228, Loss: 0.2635270195063576, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1229, Loss: 0.5284953319407479, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1230, Loss: 0.29993586527369187, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1231, Loss: 0.4139541763423161, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1232, Loss: 0.41215778416891696, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1233, Loss: 0.3368086925617729, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1234, Loss: 0.5269678568283012, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1235, Loss: 0.3458185161131951, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1236, Loss: 0.7490609293309809, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1237, Loss: 0.5323727232084199, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1238, Loss: 0.6301685659766545, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1239, Loss: 0.45617658328743615, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1240, Loss: 0.5968125961235072, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1241, Loss: 0.2550334201745573, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1242, Loss: 0.28611633937848857, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1243, Loss: 0.46676576752399945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1244, Loss: 0.25779707499592375, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1245, Loss: 0.47031082471124125, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1246, Loss: 0.3968025146562074, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1247, Loss: 0.3574538561109952, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1248, Loss: 0.470663772079316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1249, Loss: 0.2697365299749567, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1250, Loss: 0.893551709506313, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1251, Loss: 0.41977533467520706, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1252, Loss: 0.43048067461764905, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1253, Loss: 0.4531192853643834, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1254, Loss: 0.3169744036487744, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1255, Loss: 0.5435479824696463, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1256, Loss: 0.5212488649177108, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1257, Loss: 0.44058977030588553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1258, Loss: 0.2277807108955125, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1259, Loss: 0.47149542211784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1260, Loss: 0.3816164129744599, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1261, Loss: 0.374282053906356, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1262, Loss: 0.9882748789444042, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1263, Loss: 0.3581206215381798, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1264, Loss: 0.35424928657620414, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1265, Loss: 0.2543789707429374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1266, Loss: 0.3463631530377504, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1267, Loss: 0.2452544688105335, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1268, Loss: 0.30817358301544184, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1269, Loss: 0.39684185314657716, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1270, Loss: 0.4283407533352891, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1271, Loss: 0.41986934720438684, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1272, Loss: 0.3522378558520607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1273, Loss: 0.35683164860163785, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1274, Loss: 0.6589131240881047, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1275, Loss: 0.2839800935645177, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1276, Loss: 0.675090754980618, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1277, Loss: 0.2925390877172967, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1278, Loss: 0.4343197884668982, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1279, Loss: 0.27756773003915525, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1280, Loss: 0.6133539316808865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1281, Loss: 0.444746768629005, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1282, Loss: 0.21126947618321318, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1283, Loss: 0.37488473717812815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1284, Loss: 0.42688330383101714, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1285, Loss: 0.2768360299559238, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1286, Loss: 0.31738749618563644, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1287, Loss: 0.2764242396868388, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1288, Loss: 0.617070929867925, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1289, Loss: 0.21158569911876937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1290, Loss: 0.5527863062389052, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1291, Loss: 0.6163845116615347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1292, Loss: 0.24080380635102122, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1293, Loss: 1.024169026159793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1294, Loss: 0.402389472205874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1295, Loss: 0.23733743873396598, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1296, Loss: 0.40310698119944743, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1297, Loss: 0.3909131309594174, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1298, Loss: 0.589449982566848, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1299, Loss: 0.5117813823458391, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1300, Loss: 0.5185004758559613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1301, Loss: 0.657154501334402, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1302, Loss: 0.5463903371608617, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1303, Loss: 0.4167111374161311, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1304, Loss: 0.19698198268187161, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1305, Loss: 0.4282279264610903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1306, Loss: 0.3752186055657746, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1307, Loss: 0.740123621856875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1308, Loss: 0.3580955658126521, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1309, Loss: 0.4896800574653136, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1310, Loss: 0.2737488154154497, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1311, Loss: 0.26166292133941343, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1312, Loss: 0.35390433514568137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1313, Loss: 0.5101109663918452, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1314, Loss: 0.6575295172407187, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1315, Loss: 0.2903122105818684, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1316, Loss: 0.19530045262585713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1317, Loss: 0.25195702857820473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1318, Loss: 0.374363592351336, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1319, Loss: 0.33047491209856117, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1320, Loss: 0.2665252521963397, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1321, Loss: 0.7708641982373171, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1322, Loss: 0.40853721506804075, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1323, Loss: 0.32194229491353343, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1324, Loss: 0.5234280905946701, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1325, Loss: 0.3416275223681206, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1326, Loss: 0.5710055117556497, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1327, Loss: 0.45630277740947056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1328, Loss: 0.38849129753773315, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1329, Loss: 0.38186605754297115, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1330, Loss: 0.5843346660311532, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1331, Loss: 0.214766230401097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1332, Loss: 0.2751638454498937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1333, Loss: 0.47434375588708266, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1334, Loss: 0.40941033831125695, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1335, Loss: 0.31117321537413445, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1336, Loss: 0.993759871517321, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1337, Loss: 0.23378633263552326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1338, Loss: 0.2524144133343311, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1339, Loss: 0.3936961080430774, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1340, Loss: 0.4498614768588193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1341, Loss: 0.37611749723609705, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1342, Loss: 0.5187304972485527, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1343, Loss: 0.3642313565244588, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1344, Loss: 0.3045892097496149, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1345, Loss: 0.374430862169876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1346, Loss: 0.30485745954644766, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1347, Loss: 0.4767852534736512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1348, Loss: 0.5702250659026703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1349, Loss: 0.4785497895620784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1350, Loss: 0.3544093815813144, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1351, Loss: 0.3497447242916906, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1352, Loss: 0.7230535403327172, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1353, Loss: 0.2873115760530998, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1354, Loss: 0.5105711283496407, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1355, Loss: 0.35733580942616844, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1356, Loss: 0.39183938409337193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1357, Loss: 0.3325137205388261, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1358, Loss: 0.3661885341910993, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1359, Loss: 0.42815242345387805, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1360, Loss: 0.39813307589732827, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1361, Loss: 0.34316486093991055, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1362, Loss: 0.2760734617031575, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1363, Loss: 0.47139156902242507, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1364, Loss: 0.36525533148868294, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1365, Loss: 0.34377373387103793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1366, Loss: 0.2509541064978231, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1367, Loss: 0.26917708101506077, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1368, Loss: 0.8336251622990795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1369, Loss: 0.38206592642924603, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1370, Loss: 0.4828169603495198, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1371, Loss: 0.47990289369306066, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1372, Loss: 0.5931782167295973, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1373, Loss: 0.6499094061502498, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1374, Loss: 0.5351168730086635, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1375, Loss: 0.21794996360792976, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1376, Loss: 0.38702888454819295, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1377, Loss: 0.6257041482192203, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1378, Loss: 0.5355325665686405, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1379, Loss: 0.5762510292748231, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1380, Loss: 0.5393298970455301, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1381, Loss: 0.4403749074702723, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1382, Loss: 0.3152006976793726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1383, Loss: 0.34623398023334284, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1384, Loss: 0.2638067252020359, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1385, Loss: 0.33807463534306637, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1386, Loss: 0.3320817177658699, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1387, Loss: 0.2917125767149015, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1388, Loss: 0.2602076501513453, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1389, Loss: 0.34898909290910796, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1390, Loss: 0.807936342353581, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1391, Loss: 0.538277180223486, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1392, Loss: 0.6370389302812631, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1393, Loss: 0.27339302974559787, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1394, Loss: 0.3619950934867091, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1395, Loss: 0.3786311484634437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1396, Loss: 0.21726244054387955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1397, Loss: 0.1901819020691639, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1398, Loss: 0.46723327529444986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1399, Loss: 0.7149465685362597, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1400, Loss: 0.3448219280750482, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1401, Loss: 0.34356576717221543, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1402, Loss: 0.22565729467717674, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1403, Loss: 0.2400449961712826, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1404, Loss: 0.7710860052886057, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1405, Loss: 0.28677823697224525, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1406, Loss: 0.4647932735100473, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1407, Loss: 0.5556046797741623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1408, Loss: 0.3205909292829581, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1409, Loss: 0.2682498618061462, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1410, Loss: 0.6969143855131084, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1411, Loss: 0.4761611524501025, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1412, Loss: 0.6115458773409865, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1413, Loss: 0.4710732947024041, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1414, Loss: 0.33865240843901245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1415, Loss: 0.4712311021531935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1416, Loss: 0.3519563920645685, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1417, Loss: 0.2772673407207526, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1418, Loss: 0.28782604721944105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1419, Loss: 0.540667375140085, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1420, Loss: 0.5254162234021226, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1421, Loss: 0.2681345774215606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1422, Loss: 0.3567299017940308, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1423, Loss: 0.3415125501022953, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1424, Loss: 0.4695775919088869, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1425, Loss: 0.48808231489931553, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1426, Loss: 0.3152501002906624, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1427, Loss: 0.3597308599533058, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1428, Loss: 0.5454164396288421, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1429, Loss: 0.45900312123994613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1430, Loss: 0.38139957996522467, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1431, Loss: 0.2752885297944171, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1432, Loss: 0.618576461225705, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1433, Loss: 0.4025024193852971, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1434, Loss: 0.2261680849176907, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1435, Loss: 0.29372710345697217, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1436, Loss: 0.3025666486666124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1437, Loss: 0.2080185464185887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1438, Loss: 0.265449481535931, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1439, Loss: 0.29364651640598816, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1440, Loss: 0.34409003879908096, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1441, Loss: 0.6023670366558138, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1442, Loss: 0.6042847032127042, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1443, Loss: 0.3631882850078745, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1444, Loss: 0.3501482270270874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1445, Loss: 0.4825397011761373, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1446, Loss: 0.2452891217473097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1447, Loss: 0.19640618768805196, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1448, Loss: 0.2548450100304935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1449, Loss: 0.24848826077180475, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1450, Loss: 0.26712809708423657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1451, Loss: 0.5089935896172595, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1452, Loss: 0.3446492557443219, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1453, Loss: 0.26516203670137295, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1454, Loss: 0.32208208763731594, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1455, Loss: 0.4225805718494934, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1456, Loss: 0.2998045117140278, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1457, Loss: 0.2777927899098728, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1458, Loss: 0.38023903219062105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1459, Loss: 0.46309708534983307, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1460, Loss: 0.23573687640025404, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1461, Loss: 0.7717464300249522, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1462, Loss: 0.3453555769274554, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1463, Loss: 0.22157498965345904, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1464, Loss: 0.3975261647366607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1465, Loss: 0.3467763262856235, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1466, Loss: 0.42525664308850275, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1467, Loss: 0.33372032828186177, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1468, Loss: 0.28221330524097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1469, Loss: 0.41084484145925904, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1470, Loss: 0.5417512532625106, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1471, Loss: 0.3437147217099793, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1472, Loss: 0.5561214385981137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1473, Loss: 0.3290431329688822, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1474, Loss: 0.3788329185865792, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1475, Loss: 0.5685492247052885, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1476, Loss: 0.4467571647494385, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1477, Loss: 0.5585333671072753, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1478, Loss: 0.25098223552367044, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1479, Loss: 0.2423033762616965, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1480, Loss: 0.5638907748926835, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1481, Loss: 0.6167983826896046, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1482, Loss: 0.36200874721210186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1483, Loss: 0.36176738185417795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1484, Loss: 0.4804797191150404, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1485, Loss: 0.6434578235688113, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1486, Loss: 0.2535114269618806, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1487, Loss: 0.25689732621440753, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1488, Loss: 0.22434061164982352, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1489, Loss: 0.23528182870299535, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1490, Loss: 0.33673110503489423, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1491, Loss: 0.47292996235031626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1492, Loss: 0.40364253535751965, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1493, Loss: 0.22945408412495955, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1494, Loss: 0.4041950907630976, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1495, Loss: 0.34389179687853394, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1496, Loss: 0.3558281945970324, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1497, Loss: 0.5042524623144768, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1498, Loss: 0.4009498345293134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1499, Loss: 0.6030961445803023, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1500, Loss: 0.44853415424895626, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1501, Loss: 0.3906760523048711, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1502, Loss: 0.2664905267069924, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1503, Loss: 0.256456342637462, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1504, Loss: 0.41366745177736663, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1505, Loss: 0.522645050990227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1506, Loss: 0.4334470559856428, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1507, Loss: 0.18887007467009465, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1508, Loss: 0.3606538377122762, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1509, Loss: 0.3380126425341357, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1510, Loss: 0.2195388592433795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1511, Loss: 0.42124647211972693, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1512, Loss: 1.3131040484612866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1513, Loss: 0.32916781578437027, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1514, Loss: 0.4321243174518028, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1515, Loss: 0.45045921244815534, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1516, Loss: 0.5379085492155607, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1517, Loss: 0.2555517912856472, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1518, Loss: 0.543014982074123, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1519, Loss: 0.41610315338887854, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1520, Loss: 0.37286719892682657, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1521, Loss: 0.3409796989131027, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1522, Loss: 0.6041817984069444, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1523, Loss: 0.32266196014738724, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1524, Loss: 0.3341038554205189, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1525, Loss: 0.3323128258942194, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1526, Loss: 0.34066129564401937, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1527, Loss: 0.5336297131041884, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1528, Loss: 0.48577752720078815, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1529, Loss: 0.5190053202257613, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1530, Loss: 0.35817445051582875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1531, Loss: 0.362077740837065, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1532, Loss: 0.3718692371761068, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1533, Loss: 0.8158836836906574, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1534, Loss: 0.20137710327248065, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1535, Loss: 0.33750962069008494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1536, Loss: 0.34326743479867017, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1537, Loss: 0.4446540219247761, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1538, Loss: 0.3799764377557105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1539, Loss: 0.3140373277329156, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1540, Loss: 0.41486611888157565, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1541, Loss: 0.288756532113157, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1542, Loss: 0.30560323841958154, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1543, Loss: 0.3084468406718184, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1544, Loss: 0.5142142245965952, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1545, Loss: 0.4258862834043112, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1546, Loss: 0.264342236525277, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1547, Loss: 0.658606392645342, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1548, Loss: 0.3597861809239155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1549, Loss: 0.3054954422473998, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1550, Loss: 0.33111789322665386, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1551, Loss: 0.6532468661251176, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1552, Loss: 0.2569171353055089, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1553, Loss: 0.80236575396155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1554, Loss: 0.3688638351030233, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1555, Loss: 0.7847381928658337, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1556, Loss: 0.23829823664054486, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1557, Loss: 0.38108672093699586, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1558, Loss: 0.25375698327725477, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1559, Loss: 0.2780728533440976, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1560, Loss: 0.6577576260684252, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1561, Loss: 0.41720108760318736, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1562, Loss: 1.050872805007331, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1563, Loss: 0.20789134959744116, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1564, Loss: 0.2904888789189881, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1565, Loss: 0.273662118184094, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1566, Loss: 0.4004526550437395, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1567, Loss: 0.30399325013814615, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1568, Loss: 0.24989127364498842, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1569, Loss: 0.548477688213197, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1570, Loss: 0.4186311049234951, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1571, Loss: 0.20472895261515875, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1572, Loss: 0.33949885649088896, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1573, Loss: 0.36634676657949694, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1574, Loss: 0.3721039079253087, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1575, Loss: 0.2639159387764426, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1576, Loss: 0.4033205182471583, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1577, Loss: 0.1951490678636189, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1578, Loss: 0.3548983488690364, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1579, Loss: 0.6221212903978343, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1580, Loss: 0.5648639788475878, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1581, Loss: 0.2793965335027009, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1582, Loss: 0.4540571056008512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1583, Loss: 0.467815388780188, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1584, Loss: 0.360429268706018, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1585, Loss: 0.2443859329920133, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1586, Loss: 0.2789989159360193, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1587, Loss: 0.2521241092275709, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1588, Loss: 0.24340498723414114, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1589, Loss: 0.5038009888570911, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1590, Loss: 0.35635211270629097, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1591, Loss: 0.2041599176705348, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1592, Loss: 0.49188825905746814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1593, Loss: 0.3608826791853763, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1594, Loss: 0.543129293930344, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1595, Loss: 0.2614977121249882, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1596, Loss: 0.20448111335201205, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1597, Loss: 0.32137093944763384, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1598, Loss: 0.513670493283707, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1599, Loss: 0.3552205133626687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1600, Loss: 0.3467071120237783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1601, Loss: 0.3166996408849443, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1602, Loss: 0.4595054852958186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1603, Loss: 0.46314583493224326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1604, Loss: 0.7194529447236063, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1605, Loss: 0.5196683520633345, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1606, Loss: 0.33541794384345447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1607, Loss: 0.5117521328591966, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1608, Loss: 0.6035766248599082, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1609, Loss: 0.3491484613626581, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1610, Loss: 0.2493948054926201, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1611, Loss: 0.28056794698735454, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1612, Loss: 0.6922443127111181, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1613, Loss: 0.39226693905888954, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1614, Loss: 0.531691244933138, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1615, Loss: 0.36497891321382747, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1616, Loss: 0.7648232833163934, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1617, Loss: 0.2804514718457412, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1618, Loss: 0.5152480380276783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1619, Loss: 0.3150239135324022, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1620, Loss: 0.513999539496981, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1621, Loss: 0.41615599352719324, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1622, Loss: 0.4006626134611772, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1623, Loss: 0.39467901376619435, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1624, Loss: 0.356559894072279, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1625, Loss: 0.3998928457205083, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1626, Loss: 0.23118083141003276, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1627, Loss: 0.40980638504600975, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1628, Loss: 0.2241590557148992, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1629, Loss: 0.35125846681701134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1630, Loss: 0.41079898087237, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1631, Loss: 0.24121076096739974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1632, Loss: 0.6060107694426845, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1633, Loss: 0.2742849748928986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1634, Loss: 0.5035930929198602, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1635, Loss: 0.3695758074228871, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1636, Loss: 0.29945580357681784, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1637, Loss: 0.5407314391303611, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1638, Loss: 0.26187496963101903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1639, Loss: 0.4905615356159567, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1640, Loss: 0.3876037028594579, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1641, Loss: 0.33573316073625825, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1642, Loss: 0.5613628550276002, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1643, Loss: 0.24570730799095142, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1644, Loss: 0.34442962455785814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1645, Loss: 0.26514501561997095, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1646, Loss: 0.3931463485604434, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1647, Loss: 0.3670117887421884, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1648, Loss: 0.32530544660195715, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1649, Loss: 0.3744741090789698, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1650, Loss: 0.6218653586830856, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1651, Loss: 0.6910180990833253, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1652, Loss: 0.2981949538319012, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1653, Loss: 0.33104385155511135, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1654, Loss: 0.506998479044124, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1655, Loss: 0.5133542469753584, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1656, Loss: 0.4370519381604099, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1657, Loss: 0.21931492012153245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1658, Loss: 0.5233323208881056, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1659, Loss: 0.4362780134246472, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1660, Loss: 0.34422773845900245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1661, Loss: 0.6607150546175966, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1662, Loss: 0.4446319858802523, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1663, Loss: 0.5897597515810276, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1664, Loss: 0.42158500062337084, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1665, Loss: 0.21014630106905424, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1666, Loss: 0.7422967142951186, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1667, Loss: 0.4461822360584393, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1668, Loss: 0.39178388332981817, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1669, Loss: 0.5315072368836123, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1670, Loss: 0.21078060748316088, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1671, Loss: 0.6477839491469374, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1672, Loss: 0.48247665326922506, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1673, Loss: 0.3371085255591437, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1674, Loss: 0.8431975287390996, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1675, Loss: 0.6182220976445303, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1676, Loss: 0.5875285750149901, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1677, Loss: 0.7110321047283898, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1678, Loss: 0.4964272407566883, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1679, Loss: 0.23617850403874316, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1680, Loss: 0.4991032057009972, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1681, Loss: 0.3458606943356779, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1682, Loss: 0.2966695321476352, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1683, Loss: 0.4067703929174909, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1684, Loss: 0.5813137632057299, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1685, Loss: 0.4192008837008178, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1686, Loss: 0.39890340084807824, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1687, Loss: 0.23863374799684628, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1688, Loss: 0.32159983795830227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1689, Loss: 0.5076555684900866, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1690, Loss: 0.2777451415567829, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1691, Loss: 0.3351328061154656, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1692, Loss: 0.3844738562540544, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1693, Loss: 0.2479023326050897, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1694, Loss: 0.6024510713818887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1695, Loss: 0.21995386842659326, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1696, Loss: 0.5196770611411825, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1697, Loss: 0.9256574843238122, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1698, Loss: 0.40834661133691447, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1699, Loss: 0.3409169936257902, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1700, Loss: 0.29033732965205017, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1701, Loss: 0.2644135705832641, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1702, Loss: 0.2893219789694887, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1703, Loss: 0.35171838746349376, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1704, Loss: 0.35673943438653644, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1705, Loss: 0.3732250512579959, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1706, Loss: 0.3468188340851189, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1707, Loss: 0.72283374115253, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1708, Loss: 0.2264383662725159, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1709, Loss: 0.18887877258658722, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1710, Loss: 0.22813385391542146, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1711, Loss: 0.3553574650960224, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1712, Loss: 0.7087230897870811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1713, Loss: 0.5247234886584532, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1714, Loss: 0.23786501065618393, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1715, Loss: 0.43701109456788767, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1716, Loss: 0.45545058508434033, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1717, Loss: 0.3962852039452391, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1718, Loss: 0.5737191499668621, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1719, Loss: 0.36981039452252773, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1720, Loss: 0.520210074033616, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1721, Loss: 0.29027899871528196, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1722, Loss: 0.3482103864461009, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1723, Loss: 0.23737620933734638, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1724, Loss: 0.25559764050016726, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1725, Loss: 0.4048332209173452, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1726, Loss: 0.32491227432715974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1727, Loss: 0.528444823062704, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1728, Loss: 0.3132974818721395, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1729, Loss: 0.6572766180035687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1730, Loss: 0.6053103621463687, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1731, Loss: 0.41786342069176485, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1732, Loss: 0.48311253641047347, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1733, Loss: 0.7780303160098392, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1734, Loss: 0.6080927174661377, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1735, Loss: 0.3568246655169569, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1736, Loss: 0.4041026530258033, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1737, Loss: 0.3222622901761104, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1738, Loss: 0.23489227328686121, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1739, Loss: 0.31195611049163396, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1740, Loss: 0.28482522632986923, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1741, Loss: 0.28745595048641903, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1742, Loss: 0.23680600298858623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1743, Loss: 0.4040847502890679, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1744, Loss: 0.9030888530207398, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1745, Loss: 0.5017886078393738, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1746, Loss: 0.2612530476455007, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1747, Loss: 0.3339638050452133, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1748, Loss: 0.42956150112704855, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1749, Loss: 0.35100087501167254, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1750, Loss: 0.5693179600190341, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1751, Loss: 0.3452163623085962, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1752, Loss: 0.31488158454486864, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1753, Loss: 0.2925957387595792, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1754, Loss: 0.3015769212152703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1755, Loss: 0.2863899321515266, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1756, Loss: 0.3600552987560157, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1757, Loss: 0.6087247217652387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1758, Loss: 0.3841541471883335, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1759, Loss: 0.42939246373307755, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1760, Loss: 0.2532276326394873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1761, Loss: 0.24782563189015006, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1762, Loss: 0.6318027826110155, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1763, Loss: 0.6255002056049696, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1764, Loss: 0.30646063147345703, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1765, Loss: 0.5008476674357145, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1766, Loss: 0.36880916983976664, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1767, Loss: 0.4440408259236778, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1768, Loss: 0.3324976212743577, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1769, Loss: 0.7517509016638508, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1770, Loss: 0.3315693330213725, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1771, Loss: 0.41889534732317313, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1772, Loss: 0.379091649754536, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1773, Loss: 0.3320462049057942, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1774, Loss: 0.3264528665291022, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1775, Loss: 0.4803366315646299, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1776, Loss: 0.2881667108128935, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1777, Loss: 0.32193123019371983, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1778, Loss: 0.30519812106432653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1779, Loss: 0.24137622582143148, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1780, Loss: 0.2804721297726998, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1781, Loss: 0.398586305184804, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1782, Loss: 0.5263207353793018, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1783, Loss: 0.4091094351952294, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1784, Loss: 0.40615112137387765, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1785, Loss: 0.45073897372744387, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1786, Loss: 0.513275721550968, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1787, Loss: 0.26135233950107256, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1788, Loss: 0.2647658450902569, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1789, Loss: 0.6073462141481649, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1790, Loss: 0.5690153906677162, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1791, Loss: 0.370021926610408, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1792, Loss: 0.2839097612239153, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1793, Loss: 0.6418050916054134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1794, Loss: 0.45682022436179986, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1795, Loss: 0.2658337476663376, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1796, Loss: 0.3933617379451415, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1797, Loss: 0.26031212176553054, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1798, Loss: 0.21880966674005026, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1799, Loss: 0.3404460542272323, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1800, Loss: 0.33311801619682857, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1801, Loss: 0.3405253125993547, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1802, Loss: 0.5704822759092489, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1803, Loss: 0.34716256946992335, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1804, Loss: 0.4706597721463333, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1805, Loss: 0.31572644428721275, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1806, Loss: 0.36124489489024936, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1807, Loss: 0.8434059897161873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1808, Loss: 0.5474264287837914, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1809, Loss: 0.26963618970977105, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1810, Loss: 0.32152030229735523, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1811, Loss: 0.27433770926263595, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1812, Loss: 0.23036582850019227, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1813, Loss: 0.4686339097820167, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1814, Loss: 0.40559401913936366, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1815, Loss: 0.34133649181078574, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1816, Loss: 0.309419760881492, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1817, Loss: 0.35622203666023367, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1818, Loss: 0.35134451448813264, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1819, Loss: 0.3524188648245895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1820, Loss: 0.3180724376523132, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1821, Loss: 0.45287566134602886, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1822, Loss: 0.2499646717072411, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1823, Loss: 0.6134659157948879, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1824, Loss: 0.1896838471963009, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1825, Loss: 0.34529428922735134, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1826, Loss: 0.43281476510893446, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1827, Loss: 0.3830149986862533, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1828, Loss: 0.350629344244488, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1829, Loss: 0.3763423611851332, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1830, Loss: 0.4901723161876478, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1831, Loss: 0.3499608553643175, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1832, Loss: 0.49955356782927945, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1833, Loss: 0.4098265491853661, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1834, Loss: 0.5036291848067579, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1835, Loss: 0.6008685352935421, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1836, Loss: 0.45862571348020814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1837, Loss: 0.217828932927811, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1838, Loss: 0.5319294818404371, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1839, Loss: 0.31133907800820404, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1840, Loss: 0.7878934104192841, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1841, Loss: 0.5310637394237512, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1842, Loss: 0.4895343485085814, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1843, Loss: 0.25101120166322666, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1844, Loss: 0.2677322392570343, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1845, Loss: 0.34794561014746633, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1846, Loss: 0.32132882267839763, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1847, Loss: 0.38775513980324305, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1848, Loss: 0.25793298620927574, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1849, Loss: 0.3446610645878653, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1850, Loss: 0.24551951808262873, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1851, Loss: 0.5687574092725695, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1852, Loss: 0.5259236250773034, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1853, Loss: 0.3861123336198137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1854, Loss: 0.36787763831099685, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1855, Loss: 0.25031016917766974, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1856, Loss: 0.333610980338673, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1857, Loss: 0.35518759317345494, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1858, Loss: 0.33492268698225713, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1859, Loss: 0.4673573984221623, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1860, Loss: 0.6254600903244967, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1861, Loss: 0.8312468535554275, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1862, Loss: 0.33123252641833245, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1863, Loss: 0.3475911352954783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1864, Loss: 0.36708709835598874, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1865, Loss: 0.502657379669595, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1866, Loss: 0.32464959300328877, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1867, Loss: 0.34532662774208606, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1868, Loss: 0.3014523473001895, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1869, Loss: 0.4996725956658522, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1870, Loss: 0.23053896008746783, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1871, Loss: 0.30612777810943137, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1872, Loss: 0.3152752309683795, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1873, Loss: 0.5070576760213121, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1874, Loss: 0.29222739968737876, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Batch 1875, Loss: 0.4258666278043303, Batch Size: 32, Learning Rate: 0.0003\n",
      "Epoch 2, Updated Learning Rate: 0.00025499999999999996\n",
      "Epoch 2, Average Loss: 0.4099228430681232, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1, Loss: 0.9169140214640797, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 2, Loss: 0.33858871437736504, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 3, Loss: 0.2673979189363158, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 4, Loss: 0.619390978855305, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 5, Loss: 0.289541560873528, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 6, Loss: 0.2942761250369524, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 7, Loss: 0.21409548625518457, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 8, Loss: 0.4794162218477898, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 9, Loss: 0.19531939928149794, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 10, Loss: 0.3524349089828728, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 11, Loss: 0.22359699420017104, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 12, Loss: 0.7177725227895924, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 13, Loss: 0.40679033282790245, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 14, Loss: 0.29462803159948836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 15, Loss: 0.3644397493525352, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 16, Loss: 0.5900622927017447, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 17, Loss: 0.5482536878537616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 18, Loss: 0.24434794120910194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 19, Loss: 0.5127561939759645, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 20, Loss: 0.28479844806803567, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 21, Loss: 0.503781636929503, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 22, Loss: 0.38114341160879084, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 23, Loss: 0.31520769522686803, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 24, Loss: 0.9350397070566809, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 25, Loss: 0.5018411131649976, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 26, Loss: 0.34888563647385673, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 27, Loss: 0.45890538137918735, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 28, Loss: 0.25345708842982045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 29, Loss: 0.854666178508194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 30, Loss: 0.4103696841429856, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 31, Loss: 0.36313716402638807, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 32, Loss: 0.26448037830851207, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 33, Loss: 0.9476325470330614, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 34, Loss: 0.27236375437803045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 35, Loss: 0.3469724471185812, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 36, Loss: 0.3575814502437089, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 37, Loss: 0.3995262740543568, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 38, Loss: 0.22696356162678183, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 39, Loss: 0.2470259043500656, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 40, Loss: 0.37202318434074244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 41, Loss: 0.49321092605428385, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 42, Loss: 0.2506318158394779, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 43, Loss: 0.5259937642557735, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 44, Loss: 0.4740836051842321, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 45, Loss: 0.4211248797228574, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 46, Loss: 0.38030886395377617, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 47, Loss: 0.28700307951588644, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 48, Loss: 0.4099199657627962, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 49, Loss: 0.6229714167104119, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 50, Loss: 0.31922288733622073, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 51, Loss: 0.44684598374574414, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 52, Loss: 0.3691161858193507, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 53, Loss: 0.5817033902422686, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 54, Loss: 0.29481654567959226, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 55, Loss: 0.42621230799694876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 56, Loss: 0.3870018204233165, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 57, Loss: 0.6437394916578143, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 58, Loss: 0.4089670237877616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 59, Loss: 0.2858513132415045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 60, Loss: 0.3299394890098086, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 61, Loss: 0.23045532283161313, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 62, Loss: 0.30886205310096104, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 63, Loss: 0.4767248611481608, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 64, Loss: 0.37539140107048596, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 65, Loss: 0.581374204463788, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 66, Loss: 0.37616854295042446, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 67, Loss: 0.4068549453612338, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 68, Loss: 0.24445521368137135, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 69, Loss: 0.31048817689516917, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 70, Loss: 0.3740793241521253, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 71, Loss: 0.5004351144216052, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 72, Loss: 0.24757291584627455, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 73, Loss: 0.41562761050986585, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 74, Loss: 0.41177864550125803, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 75, Loss: 0.384469681712604, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 76, Loss: 0.21476730519086218, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 77, Loss: 0.4578356013776431, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 78, Loss: 0.4634209883403658, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 79, Loss: 0.3099021638403777, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 80, Loss: 0.6919524220431638, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 81, Loss: 0.5947635333757657, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 82, Loss: 0.6477043661346421, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 83, Loss: 0.21737662884598008, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 84, Loss: 0.33913390560488393, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 85, Loss: 0.483934845678089, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 86, Loss: 0.5009200791474914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 87, Loss: 0.30353146203970316, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 88, Loss: 0.305166061344601, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 89, Loss: 0.267748068331723, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 90, Loss: 0.39679149496522187, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 91, Loss: 0.2769710067220893, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 92, Loss: 0.1981760682848015, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 93, Loss: 0.5211896707118431, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 94, Loss: 0.2325242889034481, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 95, Loss: 0.3515686446068801, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 96, Loss: 0.5683455436841176, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 97, Loss: 0.22189826660767764, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 98, Loss: 0.23484362263596636, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 99, Loss: 0.2354190503583538, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 100, Loss: 0.27723152360253894, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 101, Loss: 0.35708713106984835, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 102, Loss: 0.253262359756219, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 103, Loss: 0.28557478435688677, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 104, Loss: 0.2488951696118532, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 105, Loss: 0.3503396242079937, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 106, Loss: 0.5249867299421053, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 107, Loss: 0.24089982427232942, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 108, Loss: 0.2074796851289222, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 109, Loss: 0.4356415141049124, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 110, Loss: 0.5800716124793626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 111, Loss: 0.5017127405046083, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 112, Loss: 0.3519941706992842, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 113, Loss: 0.3927953084031678, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 114, Loss: 0.36215102267210253, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 115, Loss: 0.24860451194221583, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 116, Loss: 0.39810471287805244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 117, Loss: 0.7882813099064369, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 118, Loss: 0.27754130307620706, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 119, Loss: 0.4642612088089956, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 120, Loss: 0.46001950137891934, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 121, Loss: 0.28150162045146354, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 122, Loss: 0.258840842413648, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 123, Loss: 0.3375627877420896, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 124, Loss: 0.3608231142897391, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 125, Loss: 0.44984745818869953, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 126, Loss: 0.44203079434293335, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 127, Loss: 0.3902865668064657, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 128, Loss: 0.456495276555392, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 129, Loss: 0.37600788728275447, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 130, Loss: 0.33485848825974796, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 131, Loss: 0.43916465110662994, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 132, Loss: 0.5710310816318862, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 133, Loss: 0.31101589236413607, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 134, Loss: 0.5459428713258163, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 135, Loss: 0.34746909469342935, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 136, Loss: 0.5748517210211535, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 137, Loss: 0.19187408747608764, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 138, Loss: 0.325846688311769, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 139, Loss: 0.2133720236496801, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 140, Loss: 0.44156328704845405, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 141, Loss: 0.2853602790823042, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 142, Loss: 0.28381862379946227, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 143, Loss: 0.6339963660031429, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 144, Loss: 0.3645670922918672, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 145, Loss: 0.24041510611966097, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 146, Loss: 0.4383586858827605, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 147, Loss: 0.36825763418980323, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 148, Loss: 0.312251680116446, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 149, Loss: 0.3779017224172726, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 150, Loss: 0.6701953186668836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 151, Loss: 0.3835587695917685, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 152, Loss: 0.29521524775056807, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 153, Loss: 0.2733026967898785, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 154, Loss: 0.3472331212324702, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 155, Loss: 0.23618864859109331, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 156, Loss: 0.26280082788638054, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 157, Loss: 0.2510253415135151, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 158, Loss: 0.2688468732925553, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 159, Loss: 0.2893170303371141, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 160, Loss: 0.4119922287285685, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 161, Loss: 0.3598409684667195, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 162, Loss: 0.26600705716835354, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 163, Loss: 0.34233767022658407, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 164, Loss: 0.4817096128551082, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 165, Loss: 0.29128921827765875, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 166, Loss: 0.38473576064581594, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 167, Loss: 0.3112525103533863, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 168, Loss: 0.3539040723679491, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 169, Loss: 0.32025737656694386, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 170, Loss: 0.4267501292883169, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 171, Loss: 0.22028864570258178, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 172, Loss: 0.23890127937991137, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 173, Loss: 0.2344718431571149, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 174, Loss: 0.43498420996606885, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 175, Loss: 0.2695069320812683, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 176, Loss: 0.32228596976517315, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 177, Loss: 0.3924950953173362, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 178, Loss: 0.35924321680379445, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 179, Loss: 0.25433325271607654, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 180, Loss: 0.613379181745374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 181, Loss: 0.28659666922263194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 182, Loss: 0.2500087138443701, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 183, Loss: 0.33679244311343426, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 184, Loss: 0.32174317272468267, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 185, Loss: 0.5874966467679072, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 186, Loss: 0.37767091014076015, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 187, Loss: 0.6948340784663674, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 188, Loss: 0.6310339894341761, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 189, Loss: 0.4303453385254712, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 190, Loss: 0.27731389020542835, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 191, Loss: 0.28837622860746537, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 192, Loss: 0.4195052779806518, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 193, Loss: 0.2899279940061893, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 194, Loss: 0.4137060797400106, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 195, Loss: 0.22596441961302086, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 196, Loss: 0.4603895902839349, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 197, Loss: 0.3895886087043393, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 198, Loss: 0.2293965003214323, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 199, Loss: 0.38514288873266644, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 200, Loss: 0.46295970659499314, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 201, Loss: 0.20799639278628507, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 202, Loss: 0.3331966409305338, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 203, Loss: 0.4572138240659909, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 204, Loss: 0.25936307497380656, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 205, Loss: 0.37122472500196724, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 206, Loss: 0.24981391851037524, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 207, Loss: 0.560587116008667, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 208, Loss: 0.5679729299051481, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 209, Loss: 0.6551390447356289, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 210, Loss: 0.24001482307510164, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 211, Loss: 0.4547824444212871, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 212, Loss: 0.458931327507774, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 213, Loss: 0.3352229650059795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 214, Loss: 0.2406847200904666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 215, Loss: 0.4398818079158633, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 216, Loss: 0.3662161324570983, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 217, Loss: 0.24298154892316093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 218, Loss: 0.3980728731844929, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 219, Loss: 0.2449661545937099, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 220, Loss: 0.5874403227256997, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 221, Loss: 0.27642896459803806, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 222, Loss: 0.30653579474423254, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 223, Loss: 0.5610374618572339, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 224, Loss: 0.18857166904718276, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 225, Loss: 0.6955431310406548, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 226, Loss: 0.6464710641522952, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 227, Loss: 0.4744290904743377, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 228, Loss: 0.6136247126549816, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 229, Loss: 0.39924318998024, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 230, Loss: 0.2122320837563189, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 231, Loss: 0.3110346561687677, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 232, Loss: 0.49727589410016837, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 233, Loss: 0.42490836263398635, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 234, Loss: 0.5426433966257483, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 235, Loss: 0.595641119052405, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 236, Loss: 0.2501529177365715, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 237, Loss: 0.44520908103380097, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 238, Loss: 0.21349731985168147, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 239, Loss: 0.32510382315907904, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 240, Loss: 0.2586445673842498, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 241, Loss: 0.2540604817887646, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 242, Loss: 0.21611936594997994, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 243, Loss: 0.3002923633256319, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 244, Loss: 0.5577433775478108, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 245, Loss: 0.3169842462789876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 246, Loss: 0.4188611174029056, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 247, Loss: 0.4664078219616125, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 248, Loss: 0.37485788812555765, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 249, Loss: 0.4453407922287477, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 250, Loss: 0.7194291599587199, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 251, Loss: 0.3067159883833406, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 252, Loss: 0.5939688181936849, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 253, Loss: 0.2290387522927641, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 254, Loss: 0.32207327774253625, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 255, Loss: 0.35176744880853894, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 256, Loss: 0.5830326823707496, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 257, Loss: 0.4210438362687224, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 258, Loss: 0.3365841376266906, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 259, Loss: 0.3923325633693299, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 260, Loss: 0.22376013231931852, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 261, Loss: 0.29488610270529636, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 262, Loss: 0.26585737882257776, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 263, Loss: 0.2270318956589093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 264, Loss: 0.3173088605774238, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 265, Loss: 0.2717159912283794, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 266, Loss: 0.4704898738194836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 267, Loss: 0.36413619881714276, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 268, Loss: 0.31594953070225473, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 269, Loss: 0.3114176863960315, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 270, Loss: 0.3345142583330407, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 271, Loss: 0.6697667213736367, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 272, Loss: 0.46678617650287424, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 273, Loss: 0.3981196822018754, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 274, Loss: 0.39769823433938967, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 275, Loss: 0.29762100441736006, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 276, Loss: 0.3513186057098577, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 277, Loss: 0.2272950342441798, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 278, Loss: 0.4651046823793422, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 279, Loss: 0.4265433276926042, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 280, Loss: 0.41088802045868994, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 281, Loss: 0.1962101262149843, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 282, Loss: 0.2657722175027062, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 283, Loss: 0.380722398046917, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 284, Loss: 0.34065708095726943, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 285, Loss: 0.38247833881534565, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 286, Loss: 0.2832397088837117, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 287, Loss: 0.45696494076705957, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 288, Loss: 0.4738799046661578, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 289, Loss: 0.27863464978220753, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 290, Loss: 0.2246970120869192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 291, Loss: 0.44031693430623076, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 292, Loss: 0.326243010099009, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 293, Loss: 0.274408029340944, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 294, Loss: 0.18393072480657202, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 295, Loss: 0.31784139001259637, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 296, Loss: 0.4893718502132991, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 297, Loss: 0.39878594723131794, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 298, Loss: 0.36991713156191786, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 299, Loss: 0.3020215882156244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 300, Loss: 0.5481808729492983, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 301, Loss: 0.20278157833338228, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 302, Loss: 0.3033932493621576, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 303, Loss: 0.5363753344695947, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 304, Loss: 0.49140136653146377, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 305, Loss: 0.21826848335835172, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 306, Loss: 0.2796011576564184, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 307, Loss: 0.2775432550131315, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 308, Loss: 0.27599974730187254, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 309, Loss: 0.33810348489980846, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 310, Loss: 0.31529112289687145, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 311, Loss: 0.23396663098834336, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 312, Loss: 0.2371238090968459, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 313, Loss: 0.5793383509711512, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 314, Loss: 0.19844471120662877, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 315, Loss: 0.395841045454153, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 316, Loss: 0.22430262874254872, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 317, Loss: 0.5410660110618403, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 318, Loss: 0.3565609160175891, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 319, Loss: 0.35787990785924984, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 320, Loss: 0.3543598874478361, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 321, Loss: 0.4771398954165551, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 322, Loss: 0.4348646260032177, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 323, Loss: 0.38964361452316054, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 324, Loss: 0.366593678991343, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 325, Loss: 0.37345768941629576, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 326, Loss: 0.24357514394184243, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 327, Loss: 0.3129975547194086, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 328, Loss: 0.26931311235399996, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 329, Loss: 0.2666135696440831, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 330, Loss: 0.18885556783589094, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 331, Loss: 0.4725629230812398, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 332, Loss: 0.7586101565692661, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 333, Loss: 0.2758715520701927, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 334, Loss: 0.30650664177557685, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 335, Loss: 0.2915850450704479, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 336, Loss: 0.4984527073138227, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 337, Loss: 0.2523666278148152, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 338, Loss: 0.3188351720286642, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 339, Loss: 0.5620331780410938, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 340, Loss: 0.21953948936450624, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 341, Loss: 0.6343454467480321, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 342, Loss: 0.4705818708225006, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 343, Loss: 0.3833909005276506, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 344, Loss: 0.2224301077195758, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 345, Loss: 0.44888768802811985, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 346, Loss: 0.29579821517190724, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 347, Loss: 0.38834612282235925, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 348, Loss: 0.34035273273207334, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 349, Loss: 0.2837497149601366, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 350, Loss: 0.49727186519213024, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 351, Loss: 0.5090539917155424, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 352, Loss: 0.35642805018224055, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 353, Loss: 0.4005884219015836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 354, Loss: 0.2908157651259192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 355, Loss: 0.24064478429748087, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 356, Loss: 0.8406558701602265, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 357, Loss: 0.28434825903755173, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 358, Loss: 0.40537127484006263, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 359, Loss: 0.32189222749158297, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 360, Loss: 0.2785435073964658, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 361, Loss: 0.2484713646856218, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 362, Loss: 0.27801846510669065, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 363, Loss: 0.3659276335667432, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 364, Loss: 0.22369490954723487, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 365, Loss: 0.2842721832121591, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 366, Loss: 0.29242816911784475, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 367, Loss: 0.2708389616339708, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 368, Loss: 0.41414351228724305, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 369, Loss: 0.5358741839363538, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 370, Loss: 0.3322047974175516, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 371, Loss: 0.45046540877870844, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 372, Loss: 0.471163690749877, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 373, Loss: 0.31907491309973846, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 374, Loss: 0.27933197193187276, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 375, Loss: 0.21808461246693905, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 376, Loss: 0.26272503545093256, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 377, Loss: 0.8778954775776339, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 378, Loss: 0.2246607951545756, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 379, Loss: 0.20842434801054507, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 380, Loss: 0.9051457528158613, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 381, Loss: 0.33395569924285895, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 382, Loss: 0.6020916642528125, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 383, Loss: 0.8266013794672002, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 384, Loss: 0.4298444438078788, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 385, Loss: 0.459001150058407, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 386, Loss: 0.28982759468752894, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 387, Loss: 0.5028659973103484, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 388, Loss: 0.23895066063623588, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 389, Loss: 0.3702896317982007, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 390, Loss: 0.3883563539839474, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 391, Loss: 0.45484439947901073, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 392, Loss: 0.5132597221293912, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 393, Loss: 0.3744899765381201, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 394, Loss: 0.36461841340366485, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 395, Loss: 0.2022230982643896, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 396, Loss: 0.49953132573769765, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 397, Loss: 0.30993063082364297, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 398, Loss: 0.27896357983743736, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 399, Loss: 0.3532820262266839, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 400, Loss: 0.33432134908249156, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 401, Loss: 0.31532321811104014, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 402, Loss: 0.29827470434510706, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 403, Loss: 0.2682540442646861, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 404, Loss: 0.3632200851386876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 405, Loss: 0.33230541174533246, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 406, Loss: 0.49845720391555803, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 407, Loss: 0.3112310288624441, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 408, Loss: 0.27565690188841196, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 409, Loss: 0.3124242444078112, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 410, Loss: 0.28425596017820337, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 411, Loss: 0.4038526192721319, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 412, Loss: 0.23481939566057522, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 413, Loss: 0.4064181496593702, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 414, Loss: 0.8635183284756813, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 415, Loss: 0.5440244565673764, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 416, Loss: 0.3300088502421659, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 417, Loss: 0.26807279363258657, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 418, Loss: 0.3343206417666287, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 419, Loss: 0.3180673666386955, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 420, Loss: 0.36508107569428605, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 421, Loss: 0.3597309572675883, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 422, Loss: 0.47866199472763293, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 423, Loss: 0.34413349078344047, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 424, Loss: 0.2769410307757067, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 425, Loss: 0.4873502515931504, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 426, Loss: 0.3946714921699159, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 427, Loss: 0.3811451129341311, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 428, Loss: 0.21960386959114367, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 429, Loss: 0.4479694434867651, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 430, Loss: 0.4464718561280517, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 431, Loss: 0.24114965208121408, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 432, Loss: 0.41233793675879427, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 433, Loss: 0.3325685337677999, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 434, Loss: 0.3915225646108192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 435, Loss: 0.4996949900510428, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 436, Loss: 0.7960774419383537, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 437, Loss: 0.28799800275671006, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 438, Loss: 0.4894610517385547, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 439, Loss: 0.34995410341445565, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 440, Loss: 0.31244090092346255, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 441, Loss: 0.5485516091716799, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 442, Loss: 0.2563043165094271, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 443, Loss: 0.5615668928777005, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 444, Loss: 0.2706604320934029, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 445, Loss: 0.28383978228861695, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 446, Loss: 0.5718644979747367, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 447, Loss: 0.395036969235101, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 448, Loss: 0.5473083220851309, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 449, Loss: 0.4412794027775633, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 450, Loss: 0.5971662732415188, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 451, Loss: 0.23121303546454935, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 452, Loss: 0.3764891599682717, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 453, Loss: 0.5837692177943699, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 454, Loss: 0.29489370396054093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 455, Loss: 0.2251735100214904, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 456, Loss: 0.3455851710990965, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 457, Loss: 0.31391683878069876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 458, Loss: 0.23528137036928914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 459, Loss: 0.25041937191846503, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 460, Loss: 0.39801832427124695, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 461, Loss: 0.2194535094521727, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 462, Loss: 0.36108035286460016, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 463, Loss: 0.2501462014369192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 464, Loss: 0.35402986044641716, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 465, Loss: 0.5365993137556153, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 466, Loss: 0.3242695101970797, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 467, Loss: 0.3904164294952626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 468, Loss: 0.361182959452889, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 469, Loss: 0.3112444808292213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 470, Loss: 0.420338947566116, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 471, Loss: 0.25388208973724447, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 472, Loss: 0.387163995055096, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 473, Loss: 0.4633517592705192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 474, Loss: 0.3783259097650955, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 475, Loss: 0.2480235695622594, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 476, Loss: 0.3322563460116328, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 477, Loss: 0.3908275362387349, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 478, Loss: 0.48456796607560426, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 479, Loss: 0.387650911736405, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 480, Loss: 0.31182896987452147, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 481, Loss: 0.3079851212219161, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 482, Loss: 0.22911071851541157, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 483, Loss: 0.3191354422357108, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 484, Loss: 0.2880190340653781, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 485, Loss: 0.35382666533140633, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 486, Loss: 0.4685169295959336, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 487, Loss: 0.4089989824966969, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 488, Loss: 0.4919407099892155, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 489, Loss: 0.4726658567578477, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 490, Loss: 0.21222331516960594, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 491, Loss: 0.21699562513258827, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 492, Loss: 0.5341618061437734, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 493, Loss: 0.5298168961670272, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 494, Loss: 0.417742503582859, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 495, Loss: 0.39092599277987766, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 496, Loss: 0.29140177912175336, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 497, Loss: 0.3449573629969783, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 498, Loss: 0.3462259153726809, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 499, Loss: 0.5508031411760961, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 500, Loss: 0.3160723899819992, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 501, Loss: 0.21336376018208592, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 502, Loss: 0.5166809321758393, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 503, Loss: 0.5521857576286675, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 504, Loss: 0.4402468308807277, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 505, Loss: 0.3470232903056635, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 506, Loss: 0.3057099700449407, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 507, Loss: 0.5055725681559827, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 508, Loss: 0.2970809599066031, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 509, Loss: 0.7765544310905765, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 510, Loss: 0.39624853506883445, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 511, Loss: 0.7722984238358577, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 512, Loss: 0.5534290154339988, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 513, Loss: 0.30491058836427043, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 514, Loss: 0.7869504991440295, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 515, Loss: 0.3028282612982515, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 516, Loss: 0.21413195429201676, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 517, Loss: 0.35767913635733983, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 518, Loss: 0.5555722827976051, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 519, Loss: 0.4106810488271706, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 520, Loss: 0.32585831895877937, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 521, Loss: 0.36086734130984444, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 522, Loss: 0.2768164709929514, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 523, Loss: 0.3893010830163266, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 524, Loss: 0.5793609476924173, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 525, Loss: 0.33558108320348445, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 526, Loss: 0.31482585668879465, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 527, Loss: 0.28223079394182693, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 528, Loss: 0.3936867067777403, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 529, Loss: 0.580298071093903, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 530, Loss: 0.5501255626807536, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 531, Loss: 0.7139281436411339, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 532, Loss: 0.27296250842652187, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 533, Loss: 0.41315585570394525, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 534, Loss: 0.36283190015886235, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 535, Loss: 0.26254395500601535, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 536, Loss: 0.4782640793883103, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 537, Loss: 0.6080567885330946, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 538, Loss: 0.6379156250247678, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 539, Loss: 0.36183484714637226, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 540, Loss: 0.38689121704464413, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 541, Loss: 0.39109456451086677, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 542, Loss: 0.23883578866832939, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 543, Loss: 0.39439013933416733, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 544, Loss: 0.21908870056694218, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 545, Loss: 0.482702585911895, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 546, Loss: 0.3742328851439017, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 547, Loss: 0.22759891968372503, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 548, Loss: 0.3939857600903177, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 549, Loss: 0.465841228079983, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 550, Loss: 0.4502953106350348, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 551, Loss: 0.4867110448288715, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 552, Loss: 0.7567730297580175, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 553, Loss: 0.3355285074000758, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 554, Loss: 0.4661650701187824, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 555, Loss: 0.30029621970346704, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 556, Loss: 0.6607210035410931, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 557, Loss: 0.2960722443155236, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 558, Loss: 0.5079906346013099, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 559, Loss: 0.2508171408991343, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 560, Loss: 0.3236617174215539, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 561, Loss: 0.3990614900460654, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 562, Loss: 0.26594629533282166, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 563, Loss: 0.5947025823376249, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 564, Loss: 0.23628597116840339, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 565, Loss: 0.3884365092742453, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 566, Loss: 0.565709995262049, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 567, Loss: 0.20245061273402334, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 568, Loss: 0.46281037686115406, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 569, Loss: 0.3317559129118229, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 570, Loss: 0.18911846960238124, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 571, Loss: 0.3868610970602767, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 572, Loss: 0.2665012201044057, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 573, Loss: 0.3616451232532273, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 574, Loss: 0.70428265542179, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 575, Loss: 0.5834307674971672, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 576, Loss: 0.19131300037617563, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 577, Loss: 0.33827714116349505, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 578, Loss: 0.3236474417690969, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 579, Loss: 0.4662444214269016, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 580, Loss: 0.3905564246093931, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 581, Loss: 0.28737621562395355, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 582, Loss: 0.3523414180028622, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 583, Loss: 0.3077515991921043, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 584, Loss: 0.5038210904565414, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 585, Loss: 0.34828084470665494, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 586, Loss: 0.28773952877367137, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 587, Loss: 0.3293164071548175, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 588, Loss: 0.5214778486378647, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 589, Loss: 0.5537988093110062, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 590, Loss: 0.29163639875136166, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 591, Loss: 0.5621457685572676, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 592, Loss: 0.47765271626403455, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 593, Loss: 0.3049695232918764, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 594, Loss: 0.37916975244417067, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 595, Loss: 0.25518264345945646, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 596, Loss: 0.243753645082524, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 597, Loss: 0.24177426800982424, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 598, Loss: 0.49594672092018816, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 599, Loss: 0.18044482895280897, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 600, Loss: 0.372318839822785, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 601, Loss: 0.7924341243952674, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 602, Loss: 0.4260851726929899, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 603, Loss: 0.25558181088675935, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 604, Loss: 0.24187928141150122, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 605, Loss: 0.34689210265879045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 606, Loss: 0.25794294048663025, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 607, Loss: 0.2844413033961911, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 608, Loss: 0.32684258465108657, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 609, Loss: 0.33902623725461545, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 610, Loss: 0.3918318565021221, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 611, Loss: 0.38248908789528857, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 612, Loss: 0.2865708877283933, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 613, Loss: 0.25862422249463624, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 614, Loss: 0.45249741380535646, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 615, Loss: 0.37709712943671403, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 616, Loss: 0.8058412987803588, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 617, Loss: 0.4707243187042917, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 618, Loss: 0.21707554017116734, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 619, Loss: 0.22143937812893918, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 620, Loss: 0.21185580132622345, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 621, Loss: 0.3080394014107035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 622, Loss: 0.2585824464576035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 623, Loss: 0.48553206057348386, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 624, Loss: 0.27832300045820035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 625, Loss: 0.3487665083448128, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 626, Loss: 0.27047848294200566, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 627, Loss: 0.2516282913311081, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 628, Loss: 0.4173613524674484, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 629, Loss: 0.23003217876186602, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 630, Loss: 0.31324528971088383, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 631, Loss: 0.4909469707686608, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 632, Loss: 0.5490716989144282, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 633, Loss: 0.3693232453298173, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 634, Loss: 0.2665504859535475, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 635, Loss: 0.32790880508809606, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 636, Loss: 0.5078868840861235, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 637, Loss: 0.23393974838375098, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 638, Loss: 0.33693153165843326, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 639, Loss: 0.2961894476495477, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 640, Loss: 0.4574819230898054, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 641, Loss: 0.347558355604423, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 642, Loss: 0.28034720357881193, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 643, Loss: 0.48868468621753436, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 644, Loss: 0.5327830319226334, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 645, Loss: 0.3612266990244665, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 646, Loss: 0.34755502054913123, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 647, Loss: 0.17722472588145072, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 648, Loss: 0.4166319938962424, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 649, Loss: 0.19669695161762907, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 650, Loss: 0.2683868161167329, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 651, Loss: 0.3473635258772643, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 652, Loss: 0.20627181202036662, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 653, Loss: 0.23123742046884305, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 654, Loss: 0.3047168152070463, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 655, Loss: 0.3221008631252207, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 656, Loss: 0.6229267883621491, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 657, Loss: 0.3186142743944288, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 658, Loss: 0.39722191667102036, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 659, Loss: 0.6820769983539355, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 660, Loss: 0.3179324942233772, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 661, Loss: 0.21657448246035432, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 662, Loss: 0.35187551082779067, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 663, Loss: 0.3222262050113486, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 664, Loss: 0.29263060232965654, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 665, Loss: 0.24290118793276366, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 666, Loss: 0.5090470977796543, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 667, Loss: 0.4663296586684831, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 668, Loss: 0.5054103587679388, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 669, Loss: 0.3848048155462047, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 670, Loss: 0.333185317311265, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 671, Loss: 0.3329467205408886, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 672, Loss: 0.2904418363311002, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 673, Loss: 0.3299746098374068, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 674, Loss: 0.2805789683835673, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 675, Loss: 0.5028446367695587, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 676, Loss: 0.24775479964800567, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 677, Loss: 0.33830832184366955, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 678, Loss: 0.6081273814715836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 679, Loss: 0.21531425783870262, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 680, Loss: 0.3615392255881157, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 681, Loss: 0.37289474912380527, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 682, Loss: 0.339155852614229, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 683, Loss: 0.4673684072109968, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 684, Loss: 0.27857138576248236, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 685, Loss: 0.3739095080333331, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 686, Loss: 0.44399255762928147, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 687, Loss: 0.37002637918074444, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 688, Loss: 0.39546317325766256, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 689, Loss: 0.2806900207684764, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 690, Loss: 0.6744574193709456, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 691, Loss: 0.3905488956398524, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 692, Loss: 0.6762786422166893, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 693, Loss: 0.4693085039265672, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 694, Loss: 0.6754032347490452, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 695, Loss: 0.5742886619749475, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 696, Loss: 0.20825241762368865, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 697, Loss: 0.3907458932327639, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 698, Loss: 0.31303110929352934, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 699, Loss: 0.442004101942507, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 700, Loss: 0.41581560115022087, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 701, Loss: 0.39906505676120374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 702, Loss: 0.28757669793268914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 703, Loss: 0.5517008972780371, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 704, Loss: 0.42958936789772995, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 705, Loss: 0.26191547919326363, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 706, Loss: 0.6569605730336301, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 707, Loss: 0.19881820762007965, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 708, Loss: 0.34741634893431966, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 709, Loss: 0.5174425232739798, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 710, Loss: 0.25938137704472797, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 711, Loss: 0.5357290794874655, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 712, Loss: 0.6229402392364944, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 713, Loss: 0.2697582142568491, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 714, Loss: 0.49672423034133995, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 715, Loss: 0.25648197331304656, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 716, Loss: 0.540457099915058, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 717, Loss: 0.5928119630827378, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 718, Loss: 0.8139192685497023, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 719, Loss: 0.27036754349684244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 720, Loss: 0.7252479084400575, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 721, Loss: 0.5705319525350172, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 722, Loss: 0.4076597275727508, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 723, Loss: 0.4293580912262951, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 724, Loss: 0.257260141009329, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 725, Loss: 0.2563367413812202, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 726, Loss: 0.26363970388379665, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 727, Loss: 0.6275914020146317, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 728, Loss: 0.2679094497901463, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 729, Loss: 0.604043675706075, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 730, Loss: 0.5828548366698504, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 731, Loss: 0.35712407411079905, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 732, Loss: 0.24086935174060575, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 733, Loss: 0.4435539492065801, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 734, Loss: 0.5649349708792, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 735, Loss: 0.29991290889983213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 736, Loss: 0.2160306423369441, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 737, Loss: 0.2873583338199035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 738, Loss: 0.41991692009279813, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 739, Loss: 0.48316294481890243, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 740, Loss: 0.25641723105964276, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 741, Loss: 0.34859521048297015, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 742, Loss: 0.49868543837846346, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 743, Loss: 0.33431787501918303, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 744, Loss: 0.19653031149862116, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 745, Loss: 0.3143564551752346, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 746, Loss: 0.5731140549952504, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 747, Loss: 0.8083110165654135, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 748, Loss: 0.33389787451587644, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 749, Loss: 0.2978658797862036, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 750, Loss: 0.22378322217413837, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 751, Loss: 0.34040525851484493, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 752, Loss: 0.5206281672886376, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 753, Loss: 0.41454650005238863, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 754, Loss: 0.30061578537964717, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 755, Loss: 0.36297114853502194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 756, Loss: 0.5345891688335838, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 757, Loss: 0.2499151240584497, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 758, Loss: 0.5340349656979647, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 759, Loss: 0.38454240318189237, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 760, Loss: 0.3608387822494278, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 761, Loss: 0.36459729645301553, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 762, Loss: 0.26938350159707114, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 763, Loss: 0.3048769852643447, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 764, Loss: 0.2565979712360076, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 765, Loss: 0.31661389946067253, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 766, Loss: 0.24983567834232057, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 767, Loss: 0.45321935978284045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 768, Loss: 0.47519967299661925, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 769, Loss: 0.5346734238663441, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 770, Loss: 0.34002647018852183, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 771, Loss: 0.3049924010918673, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 772, Loss: 0.28706547885764566, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 773, Loss: 0.21691834589036135, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 774, Loss: 0.3817361877521284, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 775, Loss: 0.41785468629322675, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 776, Loss: 0.2898277033500035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 777, Loss: 0.2303523804475136, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 778, Loss: 0.2167493106309621, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 779, Loss: 0.481541102920517, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 780, Loss: 0.42634427267146, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 781, Loss: 0.9453245921328891, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 782, Loss: 0.2948491540539905, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 783, Loss: 0.31701008318185697, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 784, Loss: 0.22764041503560611, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 785, Loss: 0.24666838617761289, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 786, Loss: 0.8895369895335643, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 787, Loss: 0.5196852277325714, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 788, Loss: 0.21786658808882436, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 789, Loss: 0.3023172585820977, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 790, Loss: 0.26765917134884426, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 791, Loss: 0.21978151506052676, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 792, Loss: 0.41526268544082434, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 793, Loss: 0.9465270095797622, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 794, Loss: 0.27787519236643193, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 795, Loss: 0.25037886970669004, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 796, Loss: 0.4721078015173688, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 797, Loss: 0.848676405955435, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 798, Loss: 0.31585565547951244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 799, Loss: 0.48818194501827694, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 800, Loss: 0.3263847832884422, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 801, Loss: 0.41541619978887473, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 802, Loss: 0.5534096005008844, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 803, Loss: 0.3387907772738605, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 804, Loss: 0.36358044924288213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 805, Loss: 0.3432043083174049, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 806, Loss: 0.3669494529116577, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 807, Loss: 0.48750074768890017, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 808, Loss: 0.30673910317361064, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 809, Loss: 0.20275155870044848, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 810, Loss: 0.4091426549633972, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 811, Loss: 0.5570014177311933, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 812, Loss: 0.37921684058329913, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 813, Loss: 0.22557740945158744, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 814, Loss: 0.2911939386540978, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 815, Loss: 0.21995176055337823, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 816, Loss: 0.3699287898732929, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 817, Loss: 0.19616450924630696, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 818, Loss: 0.3742325431590594, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 819, Loss: 0.4032683690748711, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 820, Loss: 0.3829170485984075, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 821, Loss: 0.4735753230593491, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 822, Loss: 0.20113838295852288, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 823, Loss: 0.3640873431334856, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 824, Loss: 0.5516723936886827, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 825, Loss: 0.3284965222777734, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 826, Loss: 0.5340503647784178, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 827, Loss: 0.3663198886602126, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 828, Loss: 0.41432732698203856, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 829, Loss: 0.37012208874104435, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 830, Loss: 0.36623451749533176, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 831, Loss: 0.33963036838301836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 832, Loss: 0.237791347236444, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 833, Loss: 0.23838203737661828, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 834, Loss: 0.3431237929541161, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 835, Loss: 0.4594823419955234, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 836, Loss: 0.5734782570364271, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 837, Loss: 0.3819651094153603, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 838, Loss: 0.2682926940402797, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 839, Loss: 0.4720595901437847, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 840, Loss: 0.5674980330377211, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 841, Loss: 0.4600831541051129, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 842, Loss: 0.41370522438484847, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 843, Loss: 0.32501597013240446, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 844, Loss: 0.33274478395884616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 845, Loss: 0.42852718650072125, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 846, Loss: 0.548041581853556, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 847, Loss: 0.34743416456394804, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 848, Loss: 0.46328525645869656, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 849, Loss: 0.33845606747612733, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 850, Loss: 0.5091481402120088, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 851, Loss: 0.2845957356500957, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 852, Loss: 0.2513395410309113, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 853, Loss: 0.2208559010236081, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 854, Loss: 0.5222084041062763, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 855, Loss: 0.2686172413676339, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 856, Loss: 0.4590945103818677, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 857, Loss: 0.8386244420518081, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 858, Loss: 0.3399974167237897, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 859, Loss: 0.22257498711000762, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 860, Loss: 0.3880912687919412, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 861, Loss: 0.5679182696012142, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 862, Loss: 0.20243890815272544, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 863, Loss: 0.2561852174050422, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 864, Loss: 0.5273349588877235, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 865, Loss: 0.5377084281250666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 866, Loss: 0.22827098597458184, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 867, Loss: 0.40661230490576816, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 868, Loss: 0.4130120041971389, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 869, Loss: 0.3277974051404341, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 870, Loss: 0.3314030839425207, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 871, Loss: 0.30891623755901587, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 872, Loss: 0.40572585387688553, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 873, Loss: 0.3846784930931301, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 874, Loss: 0.3076169328813403, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 875, Loss: 0.37683943169867584, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 876, Loss: 0.27965616407676924, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 877, Loss: 0.21683693476090476, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 878, Loss: 0.4923049291193532, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 879, Loss: 0.30889158734657374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 880, Loss: 0.44808045524022583, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 881, Loss: 0.46470266687003725, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 882, Loss: 0.3340411028377283, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 883, Loss: 0.27422086172764093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 884, Loss: 0.2821283270115205, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 885, Loss: 0.32749882236573347, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 886, Loss: 0.2939668408858921, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 887, Loss: 0.3960034100635943, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 888, Loss: 0.420115045493199, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 889, Loss: 0.5815712935817011, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 890, Loss: 0.2919230846192339, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 891, Loss: 0.35099167436893164, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 892, Loss: 0.231078056823259, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 893, Loss: 0.387594132408702, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 894, Loss: 0.2581497581585952, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 895, Loss: 0.29849178384741454, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 896, Loss: 0.3717993077996483, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 897, Loss: 0.3135250403974477, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 898, Loss: 0.3728572475001749, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 899, Loss: 0.36587168673800163, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 900, Loss: 0.49288785864158857, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 901, Loss: 0.291401382113864, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 902, Loss: 0.45980526145620915, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 903, Loss: 0.43472494360574127, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 904, Loss: 0.44352333949672695, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 905, Loss: 0.8070135513777106, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 906, Loss: 0.3806362089306956, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 907, Loss: 0.2577433099163368, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 908, Loss: 0.33111796156821804, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 909, Loss: 0.3997205404779862, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 910, Loss: 0.6051907215389114, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 911, Loss: 0.29806970796652654, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 912, Loss: 0.31695024094965374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 913, Loss: 0.20155695513155208, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 914, Loss: 0.2148238613928417, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 915, Loss: 0.7898983167429761, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 916, Loss: 0.22401487352425759, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 917, Loss: 0.6319857076807529, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 918, Loss: 0.26012393974231846, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 919, Loss: 0.347729239863214, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 920, Loss: 0.23504348857998042, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 921, Loss: 0.2510481018005306, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 922, Loss: 0.29525586403298265, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 923, Loss: 0.43154954553864544, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 924, Loss: 0.2903494506667244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 925, Loss: 0.7579045757440231, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 926, Loss: 0.7321312670407243, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 927, Loss: 0.6472517491909546, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 928, Loss: 0.3694952171050771, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 929, Loss: 0.24935285831343929, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 930, Loss: 0.2997958222506396, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 931, Loss: 0.35916465871317804, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 932, Loss: 0.35317689781182937, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 933, Loss: 0.20778282935369213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 934, Loss: 0.24001313242680322, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 935, Loss: 0.4505017727988505, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 936, Loss: 0.543005739508373, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 937, Loss: 0.22226171203562045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 938, Loss: 0.5281373496729715, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 939, Loss: 0.5665810744104851, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 940, Loss: 0.3728268435083511, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 941, Loss: 0.29202225652886715, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 942, Loss: 0.24430893043067653, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 943, Loss: 0.6669930512386752, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 944, Loss: 0.2928198132359941, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 945, Loss: 0.599476091733876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 946, Loss: 0.4295574658455483, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 947, Loss: 0.22833859636482748, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 948, Loss: 0.36111528816510885, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 949, Loss: 0.28560915791909997, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 950, Loss: 0.3320111161765689, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 951, Loss: 0.4535001606416154, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 952, Loss: 0.2637258651250773, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 953, Loss: 0.32013583982176697, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 954, Loss: 0.25944370545082773, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 955, Loss: 0.3795879745369647, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 956, Loss: 0.2709371674262383, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 957, Loss: 0.2745706853042319, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 958, Loss: 0.27612584646869043, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 959, Loss: 0.3780073244818433, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 960, Loss: 0.3553820385322912, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 961, Loss: 0.3747860124943659, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 962, Loss: 0.332718905749983, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 963, Loss: 0.5368786389579632, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 964, Loss: 0.22793226705721642, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 965, Loss: 0.32060116821760887, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 966, Loss: 0.4693509329273806, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 967, Loss: 0.32328138611870805, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 968, Loss: 0.36257609938625945, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 969, Loss: 0.5117816253001761, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 970, Loss: 0.3209832943933386, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 971, Loss: 0.2823442923069529, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 972, Loss: 0.45927883858571716, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 973, Loss: 0.22813610937889978, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 974, Loss: 0.8393889365470627, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 975, Loss: 0.3094713869701208, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 976, Loss: 0.3367298323417758, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 977, Loss: 0.3342644119866348, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 978, Loss: 0.4136002885679663, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 979, Loss: 0.32554327347972933, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 980, Loss: 0.2358188134348572, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 981, Loss: 0.4355296009846238, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 982, Loss: 0.2350119590009685, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 983, Loss: 0.24069095343357877, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 984, Loss: 0.2405030815393151, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 985, Loss: 0.18265556869451666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 986, Loss: 0.3854945387156067, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 987, Loss: 0.3201067127678924, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 988, Loss: 0.31614880740644896, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 989, Loss: 0.30575007526314585, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 990, Loss: 0.6790984186386134, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 991, Loss: 0.31970046397495017, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 992, Loss: 0.22873665127905157, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 993, Loss: 0.36761133689752856, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 994, Loss: 0.2817267497751679, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 995, Loss: 0.41747670809810544, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 996, Loss: 0.4678649413868269, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 997, Loss: 0.20393420561396836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 998, Loss: 0.315627411162007, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 999, Loss: 0.4929771674034851, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1000, Loss: 0.46883259884524875, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1001, Loss: 0.6786131491233982, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1002, Loss: 0.29569758385667616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1003, Loss: 0.3267616928136557, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1004, Loss: 0.29795841797836853, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1005, Loss: 0.3474918742721185, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1006, Loss: 0.43879062104553135, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1007, Loss: 0.4630588812856662, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1008, Loss: 0.20418504107056518, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1009, Loss: 0.3676877389308278, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1010, Loss: 0.26967807715213477, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1011, Loss: 0.31603163314521865, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1012, Loss: 0.3975714821159644, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1013, Loss: 0.3619340393365067, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1014, Loss: 0.384756818443849, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1015, Loss: 0.3607638518162617, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1016, Loss: 0.4910259588876793, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1017, Loss: 0.2733953014989643, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1018, Loss: 0.4134233050880808, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1019, Loss: 0.4115925479790873, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1020, Loss: 0.24629481363448438, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1021, Loss: 0.7538589161078382, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1022, Loss: 0.5172824713476997, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1023, Loss: 0.6021343164428581, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1024, Loss: 0.28570680645772606, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1025, Loss: 0.40711384871974743, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1026, Loss: 0.47881704775543343, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1027, Loss: 0.4502397683213687, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1028, Loss: 0.47930204563127976, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1029, Loss: 0.48462757234677795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1030, Loss: 0.29991296199621387, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1031, Loss: 0.5458932215604912, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1032, Loss: 0.2857805498517423, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1033, Loss: 0.3734865690079976, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1034, Loss: 0.27917252307395213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1035, Loss: 0.5898076665480317, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1036, Loss: 0.2758492498872774, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1037, Loss: 0.7789947354867748, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1038, Loss: 0.22900685031951795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1039, Loss: 0.35622629363615527, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1040, Loss: 0.4737566400796248, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1041, Loss: 0.18247858984404297, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1042, Loss: 0.20966587897586636, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1043, Loss: 0.41704702768178925, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1044, Loss: 0.355392329504203, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1045, Loss: 0.6108601083412651, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1046, Loss: 0.2648705047865756, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1047, Loss: 0.4175490182052611, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1048, Loss: 0.21999944510110916, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1049, Loss: 0.6852710688173209, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1050, Loss: 0.4890644628658974, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1051, Loss: 0.37745979870639257, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1052, Loss: 0.376268851491344, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1053, Loss: 0.38216678972315393, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1054, Loss: 0.412118579975291, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1055, Loss: 0.25047724000631866, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1056, Loss: 0.5656187226793458, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1057, Loss: 0.38305585025019273, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1058, Loss: 0.667143940471225, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1059, Loss: 0.30226445018801795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1060, Loss: 0.3221257621797746, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1061, Loss: 0.21075465912255598, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1062, Loss: 0.698232967940666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1063, Loss: 0.21365595525914738, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1064, Loss: 0.4609664483784386, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1065, Loss: 0.3367589615165949, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1066, Loss: 0.29161169390856456, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1067, Loss: 0.20107068224875943, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1068, Loss: 0.28203637369273915, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1069, Loss: 0.34895202381544577, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1070, Loss: 0.24846315243343914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1071, Loss: 0.23291698151043633, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1072, Loss: 0.2524430322080219, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1073, Loss: 0.5164165007929551, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1074, Loss: 0.353903274295012, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1075, Loss: 0.5812204077816626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1076, Loss: 0.46847324331120277, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1077, Loss: 0.24245582190504567, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1078, Loss: 0.30036137791462103, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1079, Loss: 0.4711375416815853, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1080, Loss: 0.19870945609972882, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1081, Loss: 0.27629926306208086, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1082, Loss: 0.32901459185981563, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1083, Loss: 0.3483824521951304, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1084, Loss: 0.24440615509817093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1085, Loss: 0.5084844319794066, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1086, Loss: 0.4445954810977152, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1087, Loss: 0.4936351381456626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1088, Loss: 0.6228793400428561, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1089, Loss: 0.20785850291261382, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1090, Loss: 0.2437293336417757, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1091, Loss: 0.5137334332390744, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1092, Loss: 0.5894251863660213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1093, Loss: 0.4393501664572085, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1094, Loss: 0.639681293313923, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1095, Loss: 0.34517038350940915, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1096, Loss: 0.39540161247545735, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1097, Loss: 0.21807270380421903, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1098, Loss: 0.4924339050673352, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1099, Loss: 0.40703629939909736, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1100, Loss: 0.29848721323987, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1101, Loss: 0.7917277116731596, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1102, Loss: 0.40749975337048594, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1103, Loss: 0.6075706606603526, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1104, Loss: 0.3054198348905398, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1105, Loss: 0.43575121767171937, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1106, Loss: 0.3471879589334841, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1107, Loss: 0.32915346318987593, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1108, Loss: 0.30204571996198204, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1109, Loss: 0.30298149135366714, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1110, Loss: 0.26841794483268155, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1111, Loss: 0.7295963208255529, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1112, Loss: 0.35689647561579724, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1113, Loss: 0.25657286427651915, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1114, Loss: 0.3461001454032795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1115, Loss: 0.27952148402696325, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1116, Loss: 0.6546223681554852, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1117, Loss: 0.550129736086932, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1118, Loss: 0.26811702858024, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1119, Loss: 0.3854711782070469, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1120, Loss: 0.36399743432922027, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1121, Loss: 0.42600009442380504, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1122, Loss: 0.22125246502709506, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1123, Loss: 0.3462372327047363, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1124, Loss: 0.2430198067833717, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1125, Loss: 0.3334189021058055, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1126, Loss: 0.3743372913514741, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1127, Loss: 0.5336636057058987, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1128, Loss: 0.39615843797488537, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1129, Loss: 0.34271055593512667, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1130, Loss: 0.42613549846249626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1131, Loss: 0.7065673099339882, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1132, Loss: 0.3737493084555231, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1133, Loss: 0.2613231676064158, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1134, Loss: 0.5036662954266555, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1135, Loss: 0.25653024867407426, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1136, Loss: 0.2789147776559594, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1137, Loss: 0.38396728234875765, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1138, Loss: 0.7994722652815119, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1139, Loss: 0.24969228584490985, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1140, Loss: 0.3323906289675692, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1141, Loss: 0.675710774360913, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1142, Loss: 0.34370619721007734, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1143, Loss: 0.3590149945661263, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1144, Loss: 0.5418350647257546, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1145, Loss: 0.3952369333911637, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1146, Loss: 0.32034605614398626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1147, Loss: 0.31209884219284234, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1148, Loss: 0.2646020823486666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1149, Loss: 0.45856469005813827, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1150, Loss: 0.44989785368003776, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1151, Loss: 0.35159899410322765, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1152, Loss: 0.21628599763417436, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1153, Loss: 0.28986977756177545, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1154, Loss: 0.4619430206520521, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1155, Loss: 0.4413135072100499, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1156, Loss: 0.392975768277273, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1157, Loss: 0.4315171542554127, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1158, Loss: 0.49498447985780897, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1159, Loss: 0.26110501631894834, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1160, Loss: 0.872724625241205, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1161, Loss: 0.26778999618168553, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1162, Loss: 0.41473474092642015, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1163, Loss: 0.19989228218418778, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1164, Loss: 0.4504996036794666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1165, Loss: 0.24416670581478234, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1166, Loss: 0.3706156397077273, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1167, Loss: 0.18991340263849327, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1168, Loss: 0.24548808231623814, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1169, Loss: 0.5056030311389257, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1170, Loss: 0.3346303514445522, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1171, Loss: 0.4378536271342028, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1172, Loss: 0.28968023859854586, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1173, Loss: 0.5509647204171612, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1174, Loss: 0.4994463851721151, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1175, Loss: 0.266430563150999, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1176, Loss: 0.22150291724332336, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1177, Loss: 0.27237098005769805, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1178, Loss: 0.20924469270109097, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1179, Loss: 0.25874825957307285, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1180, Loss: 0.32126809611667284, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1181, Loss: 0.2963012341545944, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1182, Loss: 0.7052496012508266, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1183, Loss: 0.32283944480680893, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1184, Loss: 0.45308711700858895, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1185, Loss: 0.2376823333860395, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1186, Loss: 0.19886464132886275, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1187, Loss: 0.4253166036506433, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1188, Loss: 0.3916329766076549, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1189, Loss: 0.3112375501064465, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1190, Loss: 0.4068635553149028, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1191, Loss: 0.4661840476410165, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1192, Loss: 0.2905956493812074, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1193, Loss: 0.4191448135309771, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1194, Loss: 0.5720848646104156, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1195, Loss: 0.2690707487724274, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1196, Loss: 0.49597114063902914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1197, Loss: 0.2711776164193547, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1198, Loss: 0.5525107201231249, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1199, Loss: 0.5110137250330615, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1200, Loss: 0.4917627363035145, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1201, Loss: 0.2535345691494497, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1202, Loss: 0.24646376199726172, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1203, Loss: 0.481187236067796, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1204, Loss: 0.24667952092656603, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1205, Loss: 0.2946710944239537, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1206, Loss: 0.36209379510787104, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1207, Loss: 0.4173560662004877, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1208, Loss: 0.4030498790079633, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1209, Loss: 0.20022057641467153, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1210, Loss: 0.26261189986639033, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1211, Loss: 0.4388472255815762, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1212, Loss: 0.4507122426356788, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1213, Loss: 0.29301998067727686, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1214, Loss: 0.34389011373050926, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1215, Loss: 0.3367022564266448, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1216, Loss: 0.3186687782817082, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1217, Loss: 0.22408210093719028, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1218, Loss: 0.7351387582275017, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1219, Loss: 0.17572556677055567, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1220, Loss: 0.48433887079860105, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1221, Loss: 0.9179999793763491, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1222, Loss: 0.6129986437372035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1223, Loss: 0.25674318095565357, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1224, Loss: 0.24890440958058102, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1225, Loss: 0.6971416098652321, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1226, Loss: 0.633892457063803, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1227, Loss: 0.5744029760802751, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1228, Loss: 0.33511067336205635, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1229, Loss: 0.5365885394060561, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1230, Loss: 0.19621291755950698, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1231, Loss: 0.4096478759214547, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1232, Loss: 0.20835619187644044, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1233, Loss: 0.33511319199228384, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1234, Loss: 0.4078254088664472, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1235, Loss: 0.4201909278062257, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1236, Loss: 0.6609846367389804, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1237, Loss: 0.25045614567736285, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1238, Loss: 0.34274645333963955, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1239, Loss: 0.3615404948462517, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1240, Loss: 0.6164381418110083, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1241, Loss: 0.22332407811256258, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1242, Loss: 0.29045393563895694, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1243, Loss: 0.5785077155009266, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1244, Loss: 0.21339253948352943, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1245, Loss: 0.4502512135810086, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1246, Loss: 0.35982124807880245, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1247, Loss: 0.25971250220170616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1248, Loss: 0.3987422280376493, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1249, Loss: 0.3326312830468764, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1250, Loss: 0.7907967658533996, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1251, Loss: 0.4678662066387699, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1252, Loss: 0.500991921206179, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1253, Loss: 0.35414223028816894, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1254, Loss: 0.29117430221570495, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1255, Loss: 0.3179751019999172, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1256, Loss: 0.604309482947188, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1257, Loss: 0.33498771777145914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1258, Loss: 0.2703166180646925, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1259, Loss: 0.2859114783353105, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1260, Loss: 0.27076486190666993, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1261, Loss: 0.34813864183886534, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1262, Loss: 0.579639430843866, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1263, Loss: 0.35752694105816396, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1264, Loss: 0.2344587736776525, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1265, Loss: 0.3331022456642556, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1266, Loss: 0.27965221099636134, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1267, Loss: 0.2258643783555966, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1268, Loss: 0.4250187785596128, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1269, Loss: 0.35247057861058756, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1270, Loss: 0.44095681944775755, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1271, Loss: 0.47513860368248007, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1272, Loss: 0.24241277742930512, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1273, Loss: 0.3425287030308245, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1274, Loss: 0.42494427672546875, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1275, Loss: 0.3224446338406398, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1276, Loss: 0.46363821545515493, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1277, Loss: 0.25004625991363655, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1278, Loss: 0.33696441110876807, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1279, Loss: 0.22928114657057438, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1280, Loss: 0.5294022577228916, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1281, Loss: 0.3015805077622573, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1282, Loss: 0.3829480005712155, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1283, Loss: 0.4191588671990977, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1284, Loss: 0.4570061860265394, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1285, Loss: 0.37622777784738853, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1286, Loss: 0.5242743429556487, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1287, Loss: 0.38197483541433663, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1288, Loss: 0.7196627864551628, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1289, Loss: 0.22803717776874852, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1290, Loss: 0.4494066636655308, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1291, Loss: 0.4724690969199953, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1292, Loss: 0.23964158655675882, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1293, Loss: 0.7675510456015011, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1294, Loss: 0.41891661626679705, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1295, Loss: 0.41490258492241583, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1296, Loss: 0.42695536724835526, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1297, Loss: 0.5445486289473379, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1298, Loss: 0.5947383814291557, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1299, Loss: 0.6599497697365793, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1300, Loss: 0.6538301421640466, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1301, Loss: 0.3385417574018609, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1302, Loss: 0.468903141185443, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1303, Loss: 0.42637046439940507, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1304, Loss: 0.17492679859749605, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1305, Loss: 0.423222413173642, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1306, Loss: 0.36444757006235745, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1307, Loss: 0.5811135600016388, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1308, Loss: 0.3439719484885337, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1309, Loss: 0.3479219084920662, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1310, Loss: 0.3501800741183009, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1311, Loss: 0.3536802808078503, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1312, Loss: 0.41641394274464893, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1313, Loss: 0.3791543637818625, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1314, Loss: 0.8033326861953374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1315, Loss: 0.3596967003045167, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1316, Loss: 0.2779909561554577, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1317, Loss: 0.20173923958601736, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1318, Loss: 0.2330148127514141, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1319, Loss: 0.43443576095773623, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1320, Loss: 0.1926211275800659, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1321, Loss: 0.5057017223810523, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1322, Loss: 0.4609472884129107, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1323, Loss: 0.29722665926610026, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1324, Loss: 0.47937732061200794, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1325, Loss: 0.4853506230139798, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1326, Loss: 0.46867009635518836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1327, Loss: 0.33880155330358286, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1328, Loss: 0.3085893428392099, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1329, Loss: 0.27532718446261967, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1330, Loss: 0.331529647586529, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1331, Loss: 0.42517283134900735, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1332, Loss: 0.3928490520497454, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1333, Loss: 0.5420157855903465, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1334, Loss: 0.4666760633294318, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1335, Loss: 0.24968931328198413, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1336, Loss: 0.6596939166492309, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1337, Loss: 0.2816935792384275, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1338, Loss: 0.3856543332847093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1339, Loss: 0.5621075787462085, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1340, Loss: 0.5546055221922059, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1341, Loss: 0.34880350937569665, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1342, Loss: 0.44279098204664535, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1343, Loss: 0.3058842337601561, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1344, Loss: 0.46968876755550026, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1345, Loss: 0.2913569233039194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1346, Loss: 0.2555449298437521, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1347, Loss: 0.3871457649260939, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1348, Loss: 0.5075232791110469, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1349, Loss: 0.6586213006247935, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1350, Loss: 0.19697326341222973, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1351, Loss: 0.27964352994049335, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1352, Loss: 0.4332629561208745, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1353, Loss: 0.24882587876933432, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1354, Loss: 0.36363946836323624, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1355, Loss: 0.38415490337869107, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1356, Loss: 0.2861971624653409, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1357, Loss: 0.31308079043514125, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1358, Loss: 0.3397997071216785, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1359, Loss: 0.3995430048134493, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1360, Loss: 0.32370564806333885, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1361, Loss: 0.34838598762775264, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1362, Loss: 0.34353820819852704, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1363, Loss: 0.512211987997226, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1364, Loss: 0.2595675856215026, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1365, Loss: 0.28621105291147264, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1366, Loss: 0.2656878985664757, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1367, Loss: 0.24775916300141643, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1368, Loss: 0.39696276482471515, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1369, Loss: 0.23928851749018695, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1370, Loss: 0.37832461454807903, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1371, Loss: 0.36324900188180775, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1372, Loss: 0.4156760924707945, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1373, Loss: 0.558926699446744, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1374, Loss: 0.5695348616689562, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1375, Loss: 0.2466542188125373, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1376, Loss: 0.3104210412211708, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1377, Loss: 0.6735826474834701, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1378, Loss: 0.3949679759449479, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1379, Loss: 0.268683387501224, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1380, Loss: 0.3607354452247358, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1381, Loss: 0.28367027764218883, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1382, Loss: 0.3518274519971568, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1383, Loss: 0.2940420450331914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1384, Loss: 0.19434462351172688, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1385, Loss: 0.3845798778645677, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1386, Loss: 0.35341542914994917, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1387, Loss: 0.23913148595738565, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1388, Loss: 0.27955383762823205, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1389, Loss: 0.32850538853859734, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1390, Loss: 0.5400234517002904, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1391, Loss: 0.46703872870858537, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1392, Loss: 0.48644017752387586, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1393, Loss: 0.4440682202510241, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1394, Loss: 0.6571952940380805, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1395, Loss: 0.3171338425212876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1396, Loss: 0.21019971787668257, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1397, Loss: 0.23011055159240462, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1398, Loss: 0.23019912202944698, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1399, Loss: 0.48826962460418943, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1400, Loss: 0.632691655508268, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1401, Loss: 0.28606728165253537, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1402, Loss: 0.24627899798580394, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1403, Loss: 0.2757445849051, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1404, Loss: 0.25615453436078683, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1405, Loss: 0.2678847979551481, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1406, Loss: 0.33885262249769693, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1407, Loss: 0.3334327989717616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1408, Loss: 0.393907853460373, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1409, Loss: 0.1993134160857746, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1410, Loss: 0.4730106454068541, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1411, Loss: 0.36370977016374445, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1412, Loss: 0.49759271328730514, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1413, Loss: 0.42146461131768687, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1414, Loss: 0.38336063512705654, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1415, Loss: 0.6600592497310473, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1416, Loss: 0.4678991782188435, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1417, Loss: 0.2246574492952488, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1418, Loss: 0.26769692605327944, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1419, Loss: 0.30875079724117205, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1420, Loss: 0.43457363525480464, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1421, Loss: 0.2601802931241193, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1422, Loss: 0.44064163403989454, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1423, Loss: 0.31997452166532464, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1424, Loss: 0.3704564252659436, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1425, Loss: 0.5459123189765638, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1426, Loss: 0.25172092940276103, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1427, Loss: 0.26850780234200194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1428, Loss: 0.4510582350841979, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1429, Loss: 0.387806244215694, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1430, Loss: 0.6064922457761769, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1431, Loss: 0.22789335203149821, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1432, Loss: 0.8290568091974195, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1433, Loss: 0.21706115106459586, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1434, Loss: 0.505658444121169, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1435, Loss: 0.33045534775010366, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1436, Loss: 0.3537596219924888, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1437, Loss: 0.21167459243289322, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1438, Loss: 0.34251518584948015, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1439, Loss: 0.28411051429307194, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1440, Loss: 0.35785326816387814, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1441, Loss: 0.4315383991004085, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1442, Loss: 0.4459245776543711, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1443, Loss: 0.30778133770739086, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1444, Loss: 0.44438852597485456, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1445, Loss: 0.526350698110623, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1446, Loss: 0.2810978179281861, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1447, Loss: 0.22057224845088697, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1448, Loss: 0.23654256666917964, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1449, Loss: 0.26697589171856084, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1450, Loss: 0.254890180234311, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1451, Loss: 0.2964726682407445, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1452, Loss: 0.2997732897279699, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1453, Loss: 0.2029457739096292, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1454, Loss: 0.21890408960218988, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1455, Loss: 0.39446972863869906, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1456, Loss: 0.385294955089641, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1457, Loss: 0.28087092798289237, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1458, Loss: 0.3440285509204001, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1459, Loss: 0.41997781036590043, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1460, Loss: 0.22941199527991035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1461, Loss: 0.6149072316087907, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1462, Loss: 0.4346262737329427, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1463, Loss: 0.3762140077789915, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1464, Loss: 0.3759385624356697, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1465, Loss: 0.2420914894063739, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1466, Loss: 0.2572903205437035, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1467, Loss: 0.35282553946860895, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1468, Loss: 0.35951493193954603, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1469, Loss: 0.33693711818134003, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1470, Loss: 0.3913002935876384, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1471, Loss: 0.23749057956521605, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1472, Loss: 0.5028469490917346, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1473, Loss: 0.3023425518383955, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1474, Loss: 0.4444432187886062, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1475, Loss: 0.4914000594128249, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1476, Loss: 0.31186222513934136, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1477, Loss: 0.4294900578060983, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1478, Loss: 0.38615419641447657, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1479, Loss: 0.2153639804834986, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1480, Loss: 0.5238536624946897, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1481, Loss: 0.42367899668787323, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1482, Loss: 0.4791973060124901, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1483, Loss: 0.28004538887836744, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1484, Loss: 0.4872125486664858, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1485, Loss: 0.4906263617838795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1486, Loss: 0.21791039097632484, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1487, Loss: 0.3074106560700758, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1488, Loss: 0.2414945524835032, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1489, Loss: 0.3200769259632422, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1490, Loss: 0.3328263060713451, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1491, Loss: 0.3483999887936283, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1492, Loss: 0.36067071382093663, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1493, Loss: 0.28954803495107695, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1494, Loss: 0.274143398103891, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1495, Loss: 0.24510083850702205, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1496, Loss: 0.2925690621718924, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1497, Loss: 0.5324860258580494, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1498, Loss: 0.23873810611081048, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1499, Loss: 0.5827995581721448, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1500, Loss: 0.4809750955580763, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1501, Loss: 0.3091696665930084, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1502, Loss: 0.20842501222604098, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1503, Loss: 0.3520438728245365, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1504, Loss: 0.27469069283805725, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1505, Loss: 0.4636274500220705, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1506, Loss: 0.43140645402233346, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1507, Loss: 0.24430933648722586, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1508, Loss: 0.789682801490374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1509, Loss: 0.310753301610992, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1510, Loss: 0.3052873429400835, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1511, Loss: 0.5268670077708023, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1512, Loss: 1.078559060566668, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1513, Loss: 0.5580368887716843, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1514, Loss: 0.3056427486606501, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1515, Loss: 0.29506256294795896, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1516, Loss: 0.4287350112906163, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1517, Loss: 0.383378742394512, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1518, Loss: 0.675092845822094, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1519, Loss: 0.3497785773737505, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1520, Loss: 0.43505950642251967, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1521, Loss: 0.5103354788563934, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1522, Loss: 0.399172246906769, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1523, Loss: 0.32490621538592646, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1524, Loss: 0.35381880500681673, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1525, Loss: 0.42020687409783664, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1526, Loss: 0.309475957248845, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1527, Loss: 0.56590887429468, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1528, Loss: 0.47242171490342355, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1529, Loss: 0.3195121905946813, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1530, Loss: 0.3075877981861377, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1531, Loss: 0.19970293721353072, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1532, Loss: 0.27818009254448456, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1533, Loss: 0.34931425303839203, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1534, Loss: 0.2627398688512826, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1535, Loss: 0.2975017429824113, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1536, Loss: 0.5946631048429143, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1537, Loss: 0.44026250932127986, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1538, Loss: 0.2670922642855585, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1539, Loss: 0.23525249349906882, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1540, Loss: 0.3891598574440219, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1541, Loss: 0.52197245598468, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1542, Loss: 0.25093445644458795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1543, Loss: 0.36723691353182597, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1544, Loss: 0.4976527611171625, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1545, Loss: 0.3611127266526065, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1546, Loss: 0.31054351022707816, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1547, Loss: 0.390617309663042, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1548, Loss: 0.38951592474111235, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1549, Loss: 0.4150892824107665, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1550, Loss: 0.2775657527409674, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1551, Loss: 0.49592278408690305, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1552, Loss: 0.18762626279384964, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1553, Loss: 0.39414221945635725, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1554, Loss: 0.3941692372211495, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1555, Loss: 0.7186251868700626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1556, Loss: 0.23477028414187076, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1557, Loss: 0.2911083209454193, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1558, Loss: 0.2860990147386548, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1559, Loss: 0.2771200699958044, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1560, Loss: 0.4519316919577244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1561, Loss: 0.4119277620011476, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1562, Loss: 0.7971992585770109, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1563, Loss: 0.4319262306909515, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1564, Loss: 0.219601641021398, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1565, Loss: 0.26763984371738464, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1566, Loss: 0.2088970077298014, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1567, Loss: 0.3600525154796297, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1568, Loss: 0.2628843782465162, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1569, Loss: 0.36235388054619133, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1570, Loss: 0.277699399069413, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1571, Loss: 0.21397867855495872, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1572, Loss: 0.2631389130809058, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1573, Loss: 0.43977547368529546, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1574, Loss: 0.2933922390230893, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1575, Loss: 0.2940781642202142, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1576, Loss: 0.22952857302204197, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1577, Loss: 0.17710247563109646, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1578, Loss: 0.4872549526204598, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1579, Loss: 0.9576868821471672, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1580, Loss: 0.5210796920668248, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1581, Loss: 0.3348936749796425, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1582, Loss: 0.4877075806217115, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1583, Loss: 0.4121390580557689, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1584, Loss: 0.5910934637057409, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1585, Loss: 0.27944047566765484, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1586, Loss: 0.22448717130212353, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1587, Loss: 0.20468299964056139, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1588, Loss: 0.3528718395741466, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1589, Loss: 0.24180286676699678, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1590, Loss: 0.2861767320204676, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1591, Loss: 0.24552792208803387, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1592, Loss: 0.18494914353272843, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1593, Loss: 0.24721554622593997, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1594, Loss: 0.25404580970303836, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1595, Loss: 0.22705394939721626, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1596, Loss: 0.2888406456774903, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1597, Loss: 0.24872537573756237, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1598, Loss: 0.35282387916025093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1599, Loss: 0.37934039354565074, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1600, Loss: 0.5897901875279427, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1601, Loss: 0.3699934166760698, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1602, Loss: 0.24742932109379118, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1603, Loss: 0.5569703037266571, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1604, Loss: 0.6594490803046786, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1605, Loss: 0.31348534171652254, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1606, Loss: 0.407578382442552, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1607, Loss: 0.4748049272064565, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1608, Loss: 0.3805991522302359, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1609, Loss: 0.29331774335349037, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1610, Loss: 0.4062142680937364, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1611, Loss: 0.27773314160931917, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1612, Loss: 0.4966776947724454, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1613, Loss: 0.48084522399624696, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1614, Loss: 0.5430443638322494, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1615, Loss: 0.35263044707932767, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1616, Loss: 0.9435382237062835, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1617, Loss: 0.34705460199170896, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1618, Loss: 0.5212596957569369, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1619, Loss: 0.31405807291220134, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1620, Loss: 0.3502170378584002, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1621, Loss: 0.3462007311412676, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1622, Loss: 0.36060587679568257, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1623, Loss: 0.323530144396084, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1624, Loss: 0.3927666123297915, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1625, Loss: 0.4372258170258513, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1626, Loss: 0.19924936813644945, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1627, Loss: 0.29250493079301354, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1628, Loss: 0.324062623780856, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1629, Loss: 0.37566177451779115, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1630, Loss: 0.31171151703504185, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1631, Loss: 0.2163217350967858, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1632, Loss: 0.41979285949850065, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1633, Loss: 0.2495995710318724, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1634, Loss: 0.5951973566325828, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1635, Loss: 0.3640650175216795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1636, Loss: 0.3326718107994796, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1637, Loss: 0.3344460667067275, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1638, Loss: 0.24712467103515973, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1639, Loss: 0.26342924090545605, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1640, Loss: 0.3520833757438291, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1641, Loss: 0.3678217443655933, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1642, Loss: 0.3490657858455458, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1643, Loss: 0.28189031005568654, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1644, Loss: 0.41013771316337666, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1645, Loss: 0.32098095015679395, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1646, Loss: 0.49514115817026716, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1647, Loss: 0.27461412179977346, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1648, Loss: 0.34740614902318123, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1649, Loss: 0.2632014058126265, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1650, Loss: 0.5628450456757608, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1651, Loss: 0.7083465341966471, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1652, Loss: 0.4290615004946682, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1653, Loss: 0.31667940571382325, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1654, Loss: 0.3800766592014462, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1655, Loss: 0.549469840794024, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1656, Loss: 0.47865306153418774, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1657, Loss: 0.2778832269746359, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1658, Loss: 0.2066148287077877, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1659, Loss: 0.4652770174470684, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1660, Loss: 0.24216236202052685, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1661, Loss: 0.48961362278130616, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1662, Loss: 0.509246137001397, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1663, Loss: 0.37158205472777084, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1664, Loss: 0.24092774980532322, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1665, Loss: 0.31656813944764517, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1666, Loss: 0.5910487438484848, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1667, Loss: 0.5097696475710505, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1668, Loss: 0.5628220885178021, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1669, Loss: 0.25699746878245655, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1670, Loss: 0.2958861276850093, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1671, Loss: 0.3965862615245975, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1672, Loss: 0.4300973964665382, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1673, Loss: 0.4565714025868244, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1674, Loss: 0.49419282807235215, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1675, Loss: 0.4323667639279139, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1676, Loss: 0.40551273436638396, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1677, Loss: 0.7860571592242644, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1678, Loss: 0.37218064731101613, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1679, Loss: 0.3109241832757241, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1680, Loss: 0.4464842671977382, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1681, Loss: 0.26550714119300756, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1682, Loss: 0.25173894174528255, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1683, Loss: 0.3279578082766683, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1684, Loss: 0.2872540604181409, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1685, Loss: 0.29821792767260347, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1686, Loss: 0.666324741306643, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1687, Loss: 0.2157605085654638, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1688, Loss: 0.26277946046616507, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1689, Loss: 0.35705056772789373, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1690, Loss: 0.35163129877055865, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1691, Loss: 0.3248597196019142, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1692, Loss: 0.5023119774829565, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1693, Loss: 0.23742895543839043, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1694, Loss: 0.2734394457844494, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1695, Loss: 0.2546896452423643, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1696, Loss: 0.7099244249397652, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1697, Loss: 0.6709604963029587, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1698, Loss: 0.3285570733129551, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1699, Loss: 0.36258172601170635, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1700, Loss: 0.2746241785774318, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1701, Loss: 0.24368235918461584, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1702, Loss: 0.19743634080597422, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1703, Loss: 0.3615852524474923, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1704, Loss: 0.25234715606079694, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1705, Loss: 0.27940003028192245, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1706, Loss: 0.33567999158304723, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1707, Loss: 0.5819586550766704, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1708, Loss: 0.21701702140070966, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1709, Loss: 0.2259339525770579, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1710, Loss: 0.3219491677594636, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1711, Loss: 0.46897993756514567, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1712, Loss: 0.580224163427213, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1713, Loss: 0.39492751396705106, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1714, Loss: 0.48090097191691045, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1715, Loss: 0.4251138095451624, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1716, Loss: 0.42423594569395073, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1717, Loss: 0.21883573706910667, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1718, Loss: 0.7518458841463076, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1719, Loss: 0.2696883575949578, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1720, Loss: 0.6282433280330783, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1721, Loss: 0.6741338887401565, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1722, Loss: 0.48742736650524593, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1723, Loss: 0.2778811514361598, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1724, Loss: 0.21986488361557105, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1725, Loss: 0.5842689711625567, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1726, Loss: 0.35191846852629904, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1727, Loss: 0.48407137157043856, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1728, Loss: 0.22918119363916395, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1729, Loss: 0.3332480940882311, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1730, Loss: 0.30047518214782953, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1731, Loss: 0.3712711262006342, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1732, Loss: 0.33524603666070685, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1733, Loss: 0.5745656203331375, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1734, Loss: 0.5264424234430335, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1735, Loss: 0.27883825830234793, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1736, Loss: 0.4478827744687667, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1737, Loss: 0.40371505621993475, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1738, Loss: 0.27211670230989876, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1739, Loss: 0.3668722399481544, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1740, Loss: 0.2872822964431982, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1741, Loss: 0.3312302534767736, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1742, Loss: 0.2513531454805995, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1743, Loss: 0.5338134367696119, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1744, Loss: 0.5436875730897415, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1745, Loss: 0.4884171820688763, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1746, Loss: 0.20765228314953119, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1747, Loss: 0.3421904587553404, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1748, Loss: 0.3797280303040711, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1749, Loss: 0.25021730830391037, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1750, Loss: 0.5255446731651003, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1751, Loss: 0.2951544791634761, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1752, Loss: 0.32530754615606383, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1753, Loss: 0.22250565380993603, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1754, Loss: 0.43177762619086374, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1755, Loss: 0.2760346461623503, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1756, Loss: 0.2169071802953503, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1757, Loss: 0.46571053676712526, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1758, Loss: 0.2975546066031684, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1759, Loss: 0.3177826755983972, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1760, Loss: 0.3317119541148279, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1761, Loss: 0.2769281474007014, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1762, Loss: 0.7628243644430115, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1763, Loss: 0.6508049764514175, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1764, Loss: 0.23738212662835967, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1765, Loss: 0.27864672020829895, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1766, Loss: 0.2555084387376001, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1767, Loss: 0.2461431930257017, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1768, Loss: 0.3660089050808909, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1769, Loss: 0.4538260069646255, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1770, Loss: 0.5119128463935495, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1771, Loss: 0.715781679816909, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1772, Loss: 0.5204584540136459, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1773, Loss: 0.3129517834517066, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1774, Loss: 0.4465822616173644, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1775, Loss: 0.4221301714060176, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1776, Loss: 0.30423789437248105, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1777, Loss: 0.412996011700011, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1778, Loss: 0.37758490473979817, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1779, Loss: 0.3305889274900422, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1780, Loss: 0.22819313951219056, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1781, Loss: 0.32873826778188164, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1782, Loss: 0.5884297017129845, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1783, Loss: 0.312102724224122, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1784, Loss: 0.2279515140236646, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1785, Loss: 0.45362725950892807, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1786, Loss: 0.517025409321344, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1787, Loss: 0.3169144583038734, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1788, Loss: 0.4695090161333079, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1789, Loss: 0.554329059420552, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1790, Loss: 0.45442260635928305, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1791, Loss: 0.40161622203385766, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1792, Loss: 0.2166288182183232, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1793, Loss: 0.4615085488555973, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1794, Loss: 0.34658334288414816, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1795, Loss: 0.2612023349396771, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1796, Loss: 0.492322539869523, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1797, Loss: 0.2315434573881192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1798, Loss: 0.2097496033689762, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1799, Loss: 0.21374686357244868, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1800, Loss: 0.25602978602920007, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1801, Loss: 0.2587365341050167, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1802, Loss: 0.6016167927063278, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1803, Loss: 0.42155639440871684, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1804, Loss: 0.30976428055755834, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1805, Loss: 0.30495416543553544, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1806, Loss: 0.22093355724666136, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1807, Loss: 0.4880519945659334, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1808, Loss: 0.37764443174123963, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1809, Loss: 0.22371933769727065, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1810, Loss: 0.26775466063205533, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1811, Loss: 0.2777135717010429, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1812, Loss: 0.45345499449388255, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1813, Loss: 0.33417464156994336, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1814, Loss: 0.26180902442181647, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1815, Loss: 0.30760246913453637, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1816, Loss: 0.29696642858907635, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1817, Loss: 0.4026193815112795, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1818, Loss: 0.31338027989184847, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1819, Loss: 0.20373583024611908, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1820, Loss: 0.37475946423553064, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1821, Loss: 0.3933048893487068, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1822, Loss: 0.22314539159281074, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1823, Loss: 0.37273625119512116, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1824, Loss: 0.22091342015357615, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1825, Loss: 0.3391111297037292, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1826, Loss: 0.7327145729750175, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1827, Loss: 0.2555711468755608, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1828, Loss: 0.5082726058485858, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1829, Loss: 0.49528589792159916, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1830, Loss: 0.361792674619869, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1831, Loss: 0.5267560429094192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1832, Loss: 0.5309383294260192, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1833, Loss: 0.20793210013154068, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1834, Loss: 0.26618358527788877, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1835, Loss: 0.4580015953609167, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1836, Loss: 0.20030907271526532, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1837, Loss: 0.2757215164948822, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1838, Loss: 0.4239613775766768, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1839, Loss: 0.2996557710864894, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1840, Loss: 0.4130816567836857, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1841, Loss: 0.3872060442308758, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1842, Loss: 0.2466564236559523, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1843, Loss: 0.30767828698608907, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1844, Loss: 0.25819986341070716, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1845, Loss: 0.35182449068033145, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1846, Loss: 0.26169942512371275, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1847, Loss: 0.26472177012124914, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1848, Loss: 0.3185601304422313, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1849, Loss: 0.4658999203306747, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1850, Loss: 0.4177470987967218, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1851, Loss: 0.5259296603746921, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1852, Loss: 0.6201903112319159, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1853, Loss: 0.23179600346918453, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1854, Loss: 0.4344581352842443, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1855, Loss: 0.29156284573154934, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1856, Loss: 0.3212206851843797, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1857, Loss: 0.531572213286793, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1858, Loss: 0.29669986474419524, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1859, Loss: 0.7177134417103297, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1860, Loss: 0.6061901701559984, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1861, Loss: 0.4516735243751703, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1862, Loss: 0.437707006179936, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1863, Loss: 0.38258696644201506, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1864, Loss: 0.35371440861161335, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1865, Loss: 0.3912362905328989, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1866, Loss: 0.21575007900894824, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1867, Loss: 0.3326114537992531, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1868, Loss: 0.24272338657522957, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1869, Loss: 0.746581775784491, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1870, Loss: 0.3242899007644322, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1871, Loss: 0.23042088682013592, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1872, Loss: 0.29676251670837084, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1873, Loss: 0.34130747878345424, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1874, Loss: 0.35341485450874577, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Batch 1875, Loss: 0.7660727169690881, Batch Size: 32, Learning Rate: 0.00025499999999999996\n",
      "Epoch 3, Updated Learning Rate: 0.00021674999999999996\n",
      "Epoch 3, Average Loss: 0.3830391465409992, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1, Loss: 0.6816053558749078, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 2, Loss: 0.3361254183518324, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 3, Loss: 0.3425944188208818, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 4, Loss: 0.7167056672037098, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 5, Loss: 0.3501947760176978, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 6, Loss: 0.2728406325333756, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 7, Loss: 0.3373299773924122, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 8, Loss: 0.6003665876512017, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 9, Loss: 0.20398930816479013, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 10, Loss: 0.48733943862005014, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 11, Loss: 0.29884291018334364, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 12, Loss: 0.9030526086511309, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 13, Loss: 0.27015413005033095, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 14, Loss: 0.32354338899761603, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 15, Loss: 0.424721324803207, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 16, Loss: 0.5972226888179877, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 17, Loss: 0.2220282732166545, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 18, Loss: 0.35934878178479635, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 19, Loss: 0.28945755961980085, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 20, Loss: 0.4099124849884449, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 21, Loss: 0.528522431608791, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 22, Loss: 0.41295234427892485, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 23, Loss: 0.434782621985317, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 24, Loss: 0.44265231341714556, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 25, Loss: 0.6117643258580655, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 26, Loss: 0.26872417673841587, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 27, Loss: 0.254122013085897, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 28, Loss: 0.27836850710394256, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 29, Loss: 0.7703015949717319, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 30, Loss: 0.3222166128812604, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 31, Loss: 0.2982285208175517, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 32, Loss: 0.3649117365384745, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 33, Loss: 0.6870217225921107, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 34, Loss: 0.3378860270829501, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 35, Loss: 0.25258462181756636, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 36, Loss: 0.427488039959357, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 37, Loss: 0.3976130487596939, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 38, Loss: 0.35468099930809394, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 39, Loss: 0.2210051206101519, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 40, Loss: 0.47945488498954403, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 41, Loss: 0.3511459953987694, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 42, Loss: 0.19016825719031621, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 43, Loss: 0.7800722264024116, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 44, Loss: 0.40853033294687024, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 45, Loss: 0.20398191681562206, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 46, Loss: 0.3730512889730757, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 47, Loss: 0.2865085483840679, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 48, Loss: 0.2998234063908837, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 49, Loss: 0.5095203986254654, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 50, Loss: 0.3845796243467591, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 51, Loss: 0.5660753472673093, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 52, Loss: 0.4324650910402671, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 53, Loss: 0.2722073614579521, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 54, Loss: 0.5932305199904673, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 55, Loss: 0.40080888472193965, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 56, Loss: 0.5945193592654892, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 57, Loss: 0.3903304070342055, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 58, Loss: 0.3637240961442069, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 59, Loss: 0.3714101212391634, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 60, Loss: 0.48234997114412836, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 61, Loss: 0.4207391775690586, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 62, Loss: 0.30238382935888447, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 63, Loss: 0.2684243030618719, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 64, Loss: 0.522607443606943, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 65, Loss: 0.5614580962193868, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 66, Loss: 0.3110984068366983, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 67, Loss: 0.5269218910658284, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 68, Loss: 0.2155289221917581, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 69, Loss: 0.2608865346226662, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 70, Loss: 0.3967678526677413, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 71, Loss: 0.9368120020430977, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 72, Loss: 0.1989779825171659, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 73, Loss: 0.23252607544330176, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 74, Loss: 0.3880675377445183, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 75, Loss: 0.31273331466407184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 76, Loss: 0.27595288707270443, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 77, Loss: 0.33587912697809735, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 78, Loss: 0.3964566579154912, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 79, Loss: 0.3497172679079663, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 80, Loss: 0.42273787354467307, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 81, Loss: 0.45131375119936223, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 82, Loss: 0.5586990594407635, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 83, Loss: 0.23020081456173075, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 84, Loss: 0.3180712482702379, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 85, Loss: 0.6471360139510729, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 86, Loss: 0.35784396264406454, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 87, Loss: 0.2823369506420726, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 88, Loss: 0.2086050455025883, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 89, Loss: 0.3035810600539841, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 90, Loss: 0.2830548775572224, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 91, Loss: 0.2573402807776815, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 92, Loss: 0.2946176273471908, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 93, Loss: 0.29543770726103097, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 94, Loss: 0.24379217185698565, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 95, Loss: 0.4597619870307169, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 96, Loss: 0.38467054968937797, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 97, Loss: 0.3385380362015976, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 98, Loss: 0.23680481343785337, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 99, Loss: 0.22569753341215748, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 100, Loss: 0.28504117765551606, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 101, Loss: 0.3411847002614977, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 102, Loss: 0.366579812589683, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 103, Loss: 0.309150566825243, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 104, Loss: 0.3333493744557334, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 105, Loss: 0.3635230144027648, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 106, Loss: 0.3093353563866207, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 107, Loss: 0.21621748547651992, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 108, Loss: 0.2622008484499717, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 109, Loss: 0.3792249276981548, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 110, Loss: 0.2639705735615714, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 111, Loss: 0.676547428487404, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 112, Loss: 0.33909814492232504, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 113, Loss: 0.3846048444283563, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 114, Loss: 0.4179162096820396, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 115, Loss: 0.35646720291553796, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 116, Loss: 0.22532322755274475, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 117, Loss: 0.6598320598188692, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 118, Loss: 0.359364077192169, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 119, Loss: 0.30913120716584525, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 120, Loss: 0.4879805362560601, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 121, Loss: 0.3163566836361362, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 122, Loss: 0.20962649592248894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 123, Loss: 0.4122966258634151, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 124, Loss: 0.2523174883580187, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 125, Loss: 0.5393365678301334, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 126, Loss: 0.38969624480518017, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 127, Loss: 0.36726546845427427, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 128, Loss: 0.3744360351614607, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 129, Loss: 0.5317192266189602, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 130, Loss: 0.4146294572590013, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 131, Loss: 0.2992893913241321, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 132, Loss: 0.5917590460381035, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 133, Loss: 0.4539862253084751, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 134, Loss: 0.5642826865427049, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 135, Loss: 0.21662663249711311, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 136, Loss: 0.3714713547136038, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 137, Loss: 0.2879776046627418, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 138, Loss: 0.31316310622687965, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 139, Loss: 0.3382682896639433, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 140, Loss: 0.3001881825351622, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 141, Loss: 0.3384526298381608, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 142, Loss: 0.2284330306654125, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 143, Loss: 0.36559643095353056, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 144, Loss: 0.5658472047873124, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 145, Loss: 0.48698421389598157, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 146, Loss: 0.4386093554199393, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 147, Loss: 0.2921265421736369, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 148, Loss: 0.47053276633810703, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 149, Loss: 0.46921414850753007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 150, Loss: 0.36083455632781913, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 151, Loss: 0.34193297881869, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 152, Loss: 0.3711948774577612, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 153, Loss: 0.26015553978734496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 154, Loss: 0.308104165780038, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 155, Loss: 0.17995705085159033, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 156, Loss: 0.293649853029998, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 157, Loss: 0.21112904278098288, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 158, Loss: 0.228092517470962, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 159, Loss: 0.25242116178576623, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 160, Loss: 0.34047409591361866, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 161, Loss: 0.36781425569469106, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 162, Loss: 0.18761672614088948, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 163, Loss: 0.22044773366003212, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 164, Loss: 0.37104361115386086, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 165, Loss: 0.23677923067703188, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 166, Loss: 0.34677781188080836, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 167, Loss: 0.46264578835641146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 168, Loss: 0.4961951619300363, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 169, Loss: 0.23029841122804576, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 170, Loss: 0.22302264211570458, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 171, Loss: 0.21973533547279506, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 172, Loss: 0.3795698344150749, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 173, Loss: 0.3482405610893675, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 174, Loss: 0.2624618782276763, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 175, Loss: 0.34344863031743617, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 176, Loss: 0.2589359761327802, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 177, Loss: 0.22885961848259223, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 178, Loss: 0.3195841658147264, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 179, Loss: 0.3656042039558684, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 180, Loss: 0.47341185113040535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 181, Loss: 0.4306897927019371, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 182, Loss: 0.42811125911717957, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 183, Loss: 0.3925453523435597, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 184, Loss: 0.3491038246503729, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 185, Loss: 0.31897654331622055, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 186, Loss: 0.3510580410473919, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 187, Loss: 0.5807692067900764, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 188, Loss: 0.7382998263114, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 189, Loss: 0.5522191173482098, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 190, Loss: 0.38032169632557566, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 191, Loss: 0.29881335133423176, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 192, Loss: 0.27739834493784266, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 193, Loss: 0.26581547792266563, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 194, Loss: 0.24918172399213911, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 195, Loss: 0.24308027297984955, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 196, Loss: 0.4807935316984936, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 197, Loss: 0.4346163940717096, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 198, Loss: 0.3442570603638937, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 199, Loss: 0.3806795402945793, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 200, Loss: 0.3324904695008458, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 201, Loss: 0.3830078469055439, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 202, Loss: 0.32705470167170975, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 203, Loss: 0.5031732787816086, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 204, Loss: 0.20603566085604286, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 205, Loss: 0.35156979501829216, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 206, Loss: 0.1933602799557739, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 207, Loss: 0.6879244184869047, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 208, Loss: 0.6114499245634459, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 209, Loss: 0.7842124798712531, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 210, Loss: 0.26900443817123276, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 211, Loss: 0.38027907541727457, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 212, Loss: 0.5820179410440236, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 213, Loss: 0.4387689340309253, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 214, Loss: 0.3222201960048623, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 215, Loss: 0.3196132458721008, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 216, Loss: 0.3350889503038258, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 217, Loss: 0.29163634789350873, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 218, Loss: 0.3420730091626984, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 219, Loss: 0.3843152313488357, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 220, Loss: 0.31403179608388554, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 221, Loss: 0.2121926137392528, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 222, Loss: 0.24482112716438936, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 223, Loss: 0.3407241667606705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 224, Loss: 0.37781773415580044, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 225, Loss: 0.4128692884107855, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 226, Loss: 0.7867769156379429, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 227, Loss: 0.3736936678575054, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 228, Loss: 0.4932801519066916, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 229, Loss: 0.3782099469749455, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 230, Loss: 0.2652503603323445, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 231, Loss: 0.43211472562543785, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 232, Loss: 0.8209369529327765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 233, Loss: 0.30991549865365475, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 234, Loss: 0.6416472301912626, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 235, Loss: 0.5727348300794151, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 236, Loss: 0.3573867402269525, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 237, Loss: 0.3022895750370166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 238, Loss: 0.22370440943959946, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 239, Loss: 0.3074476689479677, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 240, Loss: 0.2558955117993389, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 241, Loss: 0.24337448076257978, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 242, Loss: 0.2127346100231537, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 243, Loss: 0.3019795506751767, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 244, Loss: 0.3128227906154799, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 245, Loss: 0.34978440736327765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 246, Loss: 0.39499589640544175, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 247, Loss: 0.2638273597128796, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 248, Loss: 0.4968275101014369, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 249, Loss: 0.4025076346314697, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 250, Loss: 0.3891250152223876, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 251, Loss: 0.30073783876053356, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 252, Loss: 0.6537860064200124, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 253, Loss: 0.21809195054251435, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 254, Loss: 0.20417605869117608, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 255, Loss: 0.2806057402582435, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 256, Loss: 0.2653603751874888, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 257, Loss: 0.45468085313198614, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 258, Loss: 0.2991505691636748, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 259, Loss: 0.3028067080968934, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 260, Loss: 0.2815099447757705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 261, Loss: 0.38005233890287504, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 262, Loss: 0.2999149285766719, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 263, Loss: 0.3186936605081607, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 264, Loss: 0.363120769296093, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 265, Loss: 0.3743661292937924, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 266, Loss: 0.30849815086337284, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 267, Loss: 0.41457912548392867, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 268, Loss: 0.4933815057848022, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 269, Loss: 0.7056901997427221, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 270, Loss: 0.3477276948324528, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 271, Loss: 0.487521142380245, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 272, Loss: 0.37314629243722625, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 273, Loss: 0.3754863238206164, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 274, Loss: 0.3882558109821861, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 275, Loss: 0.28752951176282127, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 276, Loss: 0.5942978821274676, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 277, Loss: 0.23640231335249268, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 278, Loss: 0.3792759132065054, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 279, Loss: 0.5372034463049007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 280, Loss: 0.2648776474249921, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 281, Loss: 0.19277378842673124, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 282, Loss: 0.2594942979058731, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 283, Loss: 0.3373530699887731, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 284, Loss: 0.47891601856941846, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 285, Loss: 0.43268523413418924, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 286, Loss: 0.28118633485752276, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 287, Loss: 0.4423920157317446, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 288, Loss: 0.4149098260827973, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 289, Loss: 0.47155366228593387, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 290, Loss: 0.24431435520357983, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 291, Loss: 0.4207051387111911, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 292, Loss: 0.30453494282695515, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 293, Loss: 0.36832060616498996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 294, Loss: 0.24532551821572196, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 295, Loss: 0.25551471958418714, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 296, Loss: 0.31133109611342835, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 297, Loss: 0.266355878615105, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 298, Loss: 0.28127779417618526, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 299, Loss: 0.41570684355404297, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 300, Loss: 0.4146091007573373, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 301, Loss: 0.32729871071951777, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 302, Loss: 0.2770605000683579, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 303, Loss: 0.8536556762157095, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 304, Loss: 0.4214618850227496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 305, Loss: 0.2538044855742855, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 306, Loss: 0.4077250581941463, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 307, Loss: 0.49433489760477634, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 308, Loss: 0.29983419736320266, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 309, Loss: 0.24028407417225184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 310, Loss: 0.4583897656639687, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 311, Loss: 0.2598366382985684, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 312, Loss: 0.23242695028907803, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 313, Loss: 0.5957971368078102, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 314, Loss: 0.22108625817972602, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 315, Loss: 0.2285844665877148, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 316, Loss: 0.20925028026213136, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 317, Loss: 0.3224736717502355, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 318, Loss: 0.3328697584240226, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 319, Loss: 0.3201771699786615, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 320, Loss: 0.3079622650653332, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 321, Loss: 0.276879129735241, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 322, Loss: 0.5725915337800767, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 323, Loss: 0.5199040778236007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 324, Loss: 0.32274613861910595, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 325, Loss: 0.41689749319425584, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 326, Loss: 0.33281949568844515, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 327, Loss: 0.23749066626529702, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 328, Loss: 0.19141445790962092, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 329, Loss: 0.2523577083052985, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 330, Loss: 0.2709625211630324, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 331, Loss: 0.6532690318859593, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 332, Loss: 0.369505250636449, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 333, Loss: 0.29666354233632003, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 334, Loss: 0.19397227233982198, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 335, Loss: 0.30014503914704455, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 336, Loss: 0.3680731159074851, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 337, Loss: 0.27440877996782537, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 338, Loss: 0.21704878532485136, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 339, Loss: 0.4735371004762612, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 340, Loss: 0.17963810179170225, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 341, Loss: 0.5753825142101299, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 342, Loss: 0.454117884595109, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 343, Loss: 0.3546065617702602, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 344, Loss: 0.28911827405225543, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 345, Loss: 0.31947886044655494, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 346, Loss: 0.4424010077910261, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 347, Loss: 0.35006561982893136, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 348, Loss: 0.5102756623343805, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 349, Loss: 0.3461680311324442, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 350, Loss: 0.4148022305865765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 351, Loss: 0.39400408927017455, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 352, Loss: 0.2571498327391476, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 353, Loss: 0.39136498768587025, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 354, Loss: 0.21102859008606664, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 355, Loss: 0.3154154518318921, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 356, Loss: 0.5264185225135158, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 357, Loss: 0.35305678433285265, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 358, Loss: 0.4781283745588617, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 359, Loss: 0.37681665342932213, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 360, Loss: 0.3536950138266255, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 361, Loss: 0.28696802652772246, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 362, Loss: 0.2262384568824739, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 363, Loss: 0.3543387671395303, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 364, Loss: 0.246725546532575, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 365, Loss: 0.33664520711330764, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 366, Loss: 0.47628350998573843, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 367, Loss: 0.25088297902410933, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 368, Loss: 0.5236871459180075, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 369, Loss: 0.49969243425136023, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 370, Loss: 0.3034948931730854, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 371, Loss: 0.2908216341566112, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 372, Loss: 0.35490316225763074, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 373, Loss: 0.42739948458508703, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 374, Loss: 0.2921240196887398, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 375, Loss: 0.3397534998624566, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 376, Loss: 0.23738192553701698, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 377, Loss: 0.9580076472790542, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 378, Loss: 0.26032088854428714, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 379, Loss: 0.19789121484765768, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 380, Loss: 0.3894470313843495, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 381, Loss: 0.41164817909503937, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 382, Loss: 0.4180939652811473, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 383, Loss: 0.6763087594287613, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 384, Loss: 0.5760325471600326, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 385, Loss: 0.5332525574990493, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 386, Loss: 0.3479649258199344, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 387, Loss: 0.2742513960798221, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 388, Loss: 0.484866730948711, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 389, Loss: 0.45001719101941695, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 390, Loss: 0.22050205608574436, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 391, Loss: 0.36484437116152535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 392, Loss: 0.3890111450174063, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 393, Loss: 0.31863163982684006, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 394, Loss: 0.33138053119000405, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 395, Loss: 0.1999688932150032, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 396, Loss: 0.41168606673002694, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 397, Loss: 0.26033641096191384, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 398, Loss: 0.2768384979194077, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 399, Loss: 0.27664804893974454, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 400, Loss: 0.2288628264319184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 401, Loss: 0.2568999543051166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 402, Loss: 0.36194516072981575, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 403, Loss: 0.36652332427761397, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 404, Loss: 0.2599365117228873, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 405, Loss: 0.45827876640508786, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 406, Loss: 0.39310474058360245, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 407, Loss: 0.3048751481046798, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 408, Loss: 0.3255638300069119, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 409, Loss: 0.44908005451313404, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 410, Loss: 0.2615093267848826, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 411, Loss: 0.43132888713526146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 412, Loss: 0.21695477265779, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 413, Loss: 0.3767401952025424, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 414, Loss: 0.6611783512884629, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 415, Loss: 0.35826048964739815, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 416, Loss: 0.24463567127269506, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 417, Loss: 0.34550720344639735, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 418, Loss: 0.27601529076734826, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 419, Loss: 0.23344215586464234, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 420, Loss: 0.4711231734099226, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 421, Loss: 0.2718907582147915, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 422, Loss: 0.43214689829647635, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 423, Loss: 0.4084144485398823, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 424, Loss: 0.2963914976074487, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 425, Loss: 0.29541691748361854, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 426, Loss: 0.44876643100854186, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 427, Loss: 0.3274928804695933, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 428, Loss: 0.2577959436419688, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 429, Loss: 0.403052862421594, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 430, Loss: 0.3735839385925358, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 431, Loss: 0.2874783839308418, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 432, Loss: 0.43806636973347735, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 433, Loss: 0.2631967321094658, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 434, Loss: 0.29356562811068826, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 435, Loss: 0.31637585615147235, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 436, Loss: 0.41222784675250024, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 437, Loss: 0.31373799125024415, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 438, Loss: 0.37611843146339163, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 439, Loss: 0.37598944790895644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 440, Loss: 0.3638045252636002, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 441, Loss: 0.3526409148202162, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 442, Loss: 0.26329882743135735, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 443, Loss: 0.4130164756496532, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 444, Loss: 0.33069827516751754, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 445, Loss: 0.2590636026066626, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 446, Loss: 0.3819743745512535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 447, Loss: 0.34755943189510174, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 448, Loss: 0.25121319125768277, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 449, Loss: 0.34509761036573594, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 450, Loss: 0.3548271327353665, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 451, Loss: 0.36840613632901265, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 452, Loss: 0.2839812608958807, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 453, Loss: 0.27387836491768086, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 454, Loss: 0.47931831213476606, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 455, Loss: 0.44342842319416825, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 456, Loss: 0.3478231085252709, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 457, Loss: 0.21195988447545322, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 458, Loss: 0.29287491592228176, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 459, Loss: 0.3017290801325787, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 460, Loss: 0.42213146091112813, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 461, Loss: 0.22123417406563703, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 462, Loss: 0.2954017149946646, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 463, Loss: 0.21094338566104545, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 464, Loss: 0.21008838465990443, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 465, Loss: 0.3821131412325932, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 466, Loss: 0.1834017406486142, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 467, Loss: 0.22969735023755372, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 468, Loss: 0.42692169455752393, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 469, Loss: 0.2525129172646541, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 470, Loss: 0.6090714765848336, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 471, Loss: 0.28237166113678386, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 472, Loss: 0.46585375103254434, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 473, Loss: 0.5490981425655974, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 474, Loss: 0.5374159303599875, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 475, Loss: 0.2758208115787881, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 476, Loss: 0.37200548946939593, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 477, Loss: 0.5675694119218291, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 478, Loss: 0.4761672464824215, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 479, Loss: 0.5846461472903934, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 480, Loss: 0.4790455656857685, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 481, Loss: 0.32991793846141815, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 482, Loss: 0.4990519557476844, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 483, Loss: 0.5512291183866638, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 484, Loss: 0.19925178005724706, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 485, Loss: 0.6633084215645557, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 486, Loss: 0.2948419271179279, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 487, Loss: 0.36666617200224844, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 488, Loss: 0.5966280974499187, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 489, Loss: 0.4814902760632467, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 490, Loss: 0.20994467402577827, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 491, Loss: 0.1907265522458494, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 492, Loss: 0.33362033943019487, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 493, Loss: 0.6889776298174816, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 494, Loss: 0.4700680062304783, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 495, Loss: 0.32889839589316705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 496, Loss: 0.2548288267891251, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 497, Loss: 0.3448203330505165, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 498, Loss: 0.3389549725181563, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 499, Loss: 0.45570772949715016, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 500, Loss: 0.2643517604855926, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 501, Loss: 0.337206288248547, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 502, Loss: 0.40048581438525066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 503, Loss: 0.4693913897523086, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 504, Loss: 0.23931882037067598, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 505, Loss: 0.252326863775825, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 506, Loss: 0.31373925215422716, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 507, Loss: 0.34926811166293154, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 508, Loss: 0.7049324557613399, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 509, Loss: 0.5252505238875975, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 510, Loss: 0.34527028821946804, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 511, Loss: 0.3827532175784911, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 512, Loss: 0.30536920390108213, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 513, Loss: 0.339493251756105, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 514, Loss: 0.5291884165753109, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 515, Loss: 0.365311384468333, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 516, Loss: 0.3165462538900715, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 517, Loss: 0.2631417183762012, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 518, Loss: 0.3358302360023871, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 519, Loss: 0.31974129821924535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 520, Loss: 0.3434135540183184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 521, Loss: 0.47105209408800064, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 522, Loss: 0.31829589551936077, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 523, Loss: 0.3502498334584776, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 524, Loss: 0.3815435341321061, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 525, Loss: 0.23362604998706954, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 526, Loss: 0.4860007827085875, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 527, Loss: 0.30617167231641595, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 528, Loss: 0.22275702778613085, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 529, Loss: 0.27501752248112166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 530, Loss: 0.2902588927627026, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 531, Loss: 0.744545117155005, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 532, Loss: 0.1894532998657506, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 533, Loss: 0.4005782556165224, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 534, Loss: 0.32381751676988973, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 535, Loss: 0.2956896671441199, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 536, Loss: 0.49430570835476145, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 537, Loss: 0.3118995319872776, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 538, Loss: 0.6120147969653823, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 539, Loss: 0.4422963974092953, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 540, Loss: 0.2604968163963434, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 541, Loss: 0.2854429864757957, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 542, Loss: 0.32671286531622146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 543, Loss: 0.37074849408964794, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 544, Loss: 0.3668932256221437, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 545, Loss: 0.31662981577877874, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 546, Loss: 0.3002630438716726, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 547, Loss: 0.21296089026147755, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 548, Loss: 0.3034934992576006, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 549, Loss: 0.5019701516892726, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 550, Loss: 0.5255577486742227, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 551, Loss: 0.43325599832003786, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 552, Loss: 0.6803541265074369, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 553, Loss: 0.20640440079191988, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 554, Loss: 0.5578175697741037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 555, Loss: 0.27316758137879676, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 556, Loss: 0.42618067009236676, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 557, Loss: 0.426420903949266, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 558, Loss: 0.6280866320533619, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 559, Loss: 0.31715143014503133, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 560, Loss: 0.34384340604191377, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 561, Loss: 0.49082904665999827, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 562, Loss: 0.32047858239559823, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 563, Loss: 0.3950702426166241, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 564, Loss: 0.23392248215563782, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 565, Loss: 0.3824030673093275, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 566, Loss: 0.6415164508981398, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 567, Loss: 0.2095261253102716, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 568, Loss: 0.5830706822518327, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 569, Loss: 0.5568922027609344, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 570, Loss: 0.26814623358445, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 571, Loss: 0.32199641326592643, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 572, Loss: 0.24257817890610653, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 573, Loss: 0.32189994736221894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 574, Loss: 0.5989079727899149, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 575, Loss: 0.46822010867335334, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 576, Loss: 0.1944640071295911, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 577, Loss: 0.25327773942641907, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 578, Loss: 0.4573396775399927, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 579, Loss: 0.32120545474543094, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 580, Loss: 0.5473470481047534, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 581, Loss: 0.3700209480823329, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 582, Loss: 0.3567080275907224, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 583, Loss: 0.29591235960161427, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 584, Loss: 0.6838787906627695, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 585, Loss: 0.3107886736788614, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 586, Loss: 0.3903916936698184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 587, Loss: 0.21023196949471784, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 588, Loss: 0.5636238367375066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 589, Loss: 0.4043582752221234, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 590, Loss: 0.2516957226635928, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 591, Loss: 0.6214630256784922, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 592, Loss: 0.3960095886401348, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 593, Loss: 0.30500236163823585, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 594, Loss: 0.30312154219100457, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 595, Loss: 0.3178438635631706, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 596, Loss: 0.266087810812415, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 597, Loss: 0.3439244499309291, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 598, Loss: 0.5880501418466506, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 599, Loss: 0.2240393078678309, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 600, Loss: 0.2595326311763505, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 601, Loss: 0.6971374451407724, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 602, Loss: 0.3412545124384858, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 603, Loss: 0.3063163319541342, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 604, Loss: 0.3588413209642184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 605, Loss: 0.44447432030022005, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 606, Loss: 0.24469285293197307, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 607, Loss: 0.36143033008402176, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 608, Loss: 0.2872957287895437, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 609, Loss: 0.4755832601490042, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 610, Loss: 0.2647338525210894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 611, Loss: 0.21621736058455038, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 612, Loss: 0.28131385858820523, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 613, Loss: 0.30883057979696626, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 614, Loss: 0.367792032842911, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 615, Loss: 0.37113354391288245, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 616, Loss: 0.48292981909437416, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 617, Loss: 0.43438600508681735, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 618, Loss: 0.2088909329578807, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 619, Loss: 0.19340627767915625, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 620, Loss: 0.2498483938982417, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 621, Loss: 0.2959056554597487, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 622, Loss: 0.3778484346076243, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 623, Loss: 0.2708253465908856, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 624, Loss: 0.39691424868949043, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 625, Loss: 0.3108565608657183, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 626, Loss: 0.3122942173983061, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 627, Loss: 0.25012288965467155, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 628, Loss: 0.5485183550848759, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 629, Loss: 0.22360476469401083, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 630, Loss: 0.2896541728189808, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 631, Loss: 0.4436784787908785, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 632, Loss: 0.29786293014552745, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 633, Loss: 0.4383452242946914, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 634, Loss: 0.24606371864142323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 635, Loss: 0.3723355482857229, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 636, Loss: 0.21016742341556233, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 637, Loss: 0.2546626536943498, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 638, Loss: 0.24660351539704217, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 639, Loss: 0.3391816331200699, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 640, Loss: 0.28746647343266996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 641, Loss: 0.3818652392780621, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 642, Loss: 0.3062052318976898, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 643, Loss: 0.36443618883996975, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 644, Loss: 0.4070238923564047, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 645, Loss: 0.44647486029319183, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 646, Loss: 0.4403635156228858, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 647, Loss: 0.21319886197471644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 648, Loss: 0.26678886798751056, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 649, Loss: 0.3367355797287417, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 650, Loss: 0.336078395843154, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 651, Loss: 0.40737410315381706, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 652, Loss: 0.21207072751679928, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 653, Loss: 0.30734959779324866, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 654, Loss: 0.20468679747822469, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 655, Loss: 0.25073081694592014, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 656, Loss: 0.5876289260625274, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 657, Loss: 0.25974097895094334, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 658, Loss: 0.6195554008200612, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 659, Loss: 0.5048353619074629, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 660, Loss: 0.3080984873078282, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 661, Loss: 0.41611849819280844, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 662, Loss: 0.21825831577440813, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 663, Loss: 0.2310671832688606, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 664, Loss: 0.31567995917379643, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 665, Loss: 0.3335416590162583, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 666, Loss: 0.4317851582496174, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 667, Loss: 0.4323276490440876, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 668, Loss: 0.3848770750387007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 669, Loss: 0.4005259377343272, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 670, Loss: 0.23448883164444814, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 671, Loss: 0.2658698934513701, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 672, Loss: 0.2717882204499542, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 673, Loss: 0.5437793484556827, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 674, Loss: 0.2582416555563906, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 675, Loss: 0.4043286194026668, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 676, Loss: 0.38111093300454124, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 677, Loss: 0.23812994513638985, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 678, Loss: 0.5607791766669444, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 679, Loss: 0.23687843117203428, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 680, Loss: 0.24212947167642299, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 681, Loss: 0.38495526081209774, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 682, Loss: 0.6751417482278432, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 683, Loss: 0.2591803219992307, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 684, Loss: 0.27300973035614934, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 685, Loss: 0.3143573118107278, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 686, Loss: 0.8195540994052488, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 687, Loss: 0.28200298290802295, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 688, Loss: 0.5228167172820993, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 689, Loss: 0.2999939511830776, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 690, Loss: 0.6864728591408206, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 691, Loss: 0.28757256966869693, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 692, Loss: 0.5889930347147322, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 693, Loss: 0.31129122663745523, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 694, Loss: 0.5819114836553572, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 695, Loss: 0.551692579091985, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 696, Loss: 0.21498939356078034, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 697, Loss: 0.31835547425281496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 698, Loss: 0.2982358581691631, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 699, Loss: 0.37148875527940417, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 700, Loss: 0.3016145750585323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 701, Loss: 0.3075720858181509, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 702, Loss: 0.40984514178456166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 703, Loss: 0.4713380785974046, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 704, Loss: 0.45351899582368693, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 705, Loss: 0.3003319115201675, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 706, Loss: 0.739407029968209, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 707, Loss: 0.19802856444126613, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 708, Loss: 0.35138685487558496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 709, Loss: 0.44057201098304327, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 710, Loss: 0.36725711228369967, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 711, Loss: 0.6520468386119256, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 712, Loss: 0.5204823004732665, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 713, Loss: 0.29100871590780797, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 714, Loss: 0.41601847518056656, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 715, Loss: 0.22799941670438775, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 716, Loss: 0.40301861013127777, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 717, Loss: 0.3876360451941845, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 718, Loss: 0.6998650776963754, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 719, Loss: 0.28181875115741173, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 720, Loss: 0.3620199159914669, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 721, Loss: 0.3966906476442313, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 722, Loss: 0.4091371263915599, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 723, Loss: 0.2765562797441104, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 724, Loss: 0.25076875365952905, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 725, Loss: 0.3872697673429323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 726, Loss: 0.22974115648908353, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 727, Loss: 0.6018898374411121, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 728, Loss: 0.28831248499515083, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 729, Loss: 0.24891567169636294, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 730, Loss: 0.4675301279917535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 731, Loss: 0.30469672368981704, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 732, Loss: 0.2570252664730234, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 733, Loss: 0.33705332163393265, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 734, Loss: 0.5010279183559965, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 735, Loss: 0.2993444623268882, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 736, Loss: 0.20270206378533237, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 737, Loss: 0.2420146580194082, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 738, Loss: 0.4624682917892644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 739, Loss: 0.4598412677641863, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 740, Loss: 0.22572073818173002, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 741, Loss: 0.2846360495879193, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 742, Loss: 0.37062396314563417, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 743, Loss: 0.487559660678758, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 744, Loss: 0.22782020531490257, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 745, Loss: 0.2943267368403643, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 746, Loss: 0.5193979472798187, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 747, Loss: 0.7368901309155087, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 748, Loss: 0.4191644550020453, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 749, Loss: 0.32600864649462247, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 750, Loss: 0.20062431531507996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 751, Loss: 0.338617917974278, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 752, Loss: 0.34068661297896496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 753, Loss: 0.3570992142541208, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 754, Loss: 0.44657578421059063, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 755, Loss: 0.328834932005015, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 756, Loss: 0.48259611057587826, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 757, Loss: 0.25638788462695267, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 758, Loss: 0.5542440397063332, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 759, Loss: 0.4111733079942195, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 760, Loss: 0.2296525697214471, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 761, Loss: 0.27751213759212867, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 762, Loss: 0.45894265826189784, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 763, Loss: 0.2542713247957633, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 764, Loss: 0.21208483550406246, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 765, Loss: 0.3602549445589509, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 766, Loss: 0.3410495638579087, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 767, Loss: 0.31026272967890545, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 768, Loss: 0.36714024366230047, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 769, Loss: 0.45618782199257607, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 770, Loss: 0.30423970087750507, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 771, Loss: 0.4620843646348371, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 772, Loss: 0.24883517431762603, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 773, Loss: 0.23633822852340103, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 774, Loss: 0.2993948029989588, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 775, Loss: 0.2963754112548903, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 776, Loss: 0.3179354529986129, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 777, Loss: 0.19753851804307726, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 778, Loss: 0.2628490338532245, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 779, Loss: 0.4263473378477809, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 780, Loss: 0.2262659244718048, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 781, Loss: 0.8945337929947973, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 782, Loss: 0.5094716342514314, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 783, Loss: 0.2165025837648061, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 784, Loss: 0.26162868420891244, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 785, Loss: 0.4028200944369695, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 786, Loss: 0.7005472932556311, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 787, Loss: 0.39327137004589907, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 788, Loss: 0.25433813219197393, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 789, Loss: 0.5743423374689682, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 790, Loss: 0.3175146122775008, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 791, Loss: 0.24105940489306987, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 792, Loss: 0.28698734773096657, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 793, Loss: 0.49757979344880066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 794, Loss: 0.2959725456193852, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 795, Loss: 0.40341593484024685, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 796, Loss: 0.6592126956408775, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 797, Loss: 0.6353851586423763, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 798, Loss: 0.2100092795884136, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 799, Loss: 0.4261356431979545, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 800, Loss: 0.2332459730628602, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 801, Loss: 0.395893652772251, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 802, Loss: 0.4369865744150652, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 803, Loss: 0.29991696086786596, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 804, Loss: 0.3570786399893429, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 805, Loss: 0.43158569979309047, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 806, Loss: 0.2129641841874953, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 807, Loss: 0.5010940550551292, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 808, Loss: 0.35090588856562777, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 809, Loss: 0.24228907061525667, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 810, Loss: 0.36182860002773554, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 811, Loss: 0.395445005866855, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 812, Loss: 0.34467918279570486, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 813, Loss: 0.485975796125076, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 814, Loss: 0.30676831043435054, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 815, Loss: 0.3983596767178181, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 816, Loss: 0.5571199580059442, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 817, Loss: 0.21503298961257666, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 818, Loss: 0.36338761739900405, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 819, Loss: 0.34646779195113503, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 820, Loss: 0.36396892162807065, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 821, Loss: 0.4223952888573488, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 822, Loss: 0.5720494490504712, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 823, Loss: 0.5671922340631458, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 824, Loss: 0.32300693197571095, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 825, Loss: 0.25835898152699605, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 826, Loss: 0.5015951053523148, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 827, Loss: 0.3422846195550159, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 828, Loss: 0.3334391260789995, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 829, Loss: 0.3230750603373676, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 830, Loss: 0.44006836516909487, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 831, Loss: 0.2847564120258156, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 832, Loss: 0.20096089916216392, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 833, Loss: 0.34667496243613616, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 834, Loss: 0.23793164016936103, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 835, Loss: 0.48647798338856063, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 836, Loss: 0.3929929691138143, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 837, Loss: 0.5113384301557925, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 838, Loss: 0.28124558992786153, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 839, Loss: 0.35208065898259855, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 840, Loss: 0.6338076553092045, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 841, Loss: 0.4380025813830159, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 842, Loss: 0.33296209845340474, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 843, Loss: 0.3098130404778756, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 844, Loss: 0.48977833822089956, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 845, Loss: 0.4023957815689878, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 846, Loss: 0.3808484472940148, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 847, Loss: 0.4669226889572805, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 848, Loss: 0.449157667880146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 849, Loss: 0.298973984266656, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 850, Loss: 0.5271981143126463, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 851, Loss: 0.27213457663924107, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 852, Loss: 0.30987873100800345, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 853, Loss: 0.22395707145555774, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 854, Loss: 0.5523901906220092, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 855, Loss: 0.26029532537431704, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 856, Loss: 0.25016583382150076, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 857, Loss: 0.5785505670583617, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 858, Loss: 0.32599306793686345, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 859, Loss: 0.40477937726689717, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 860, Loss: 0.25273548394652423, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 861, Loss: 0.49671419488134705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 862, Loss: 0.25106654233946035, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 863, Loss: 0.3361027978046677, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 864, Loss: 0.3196478320072384, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 865, Loss: 0.35688348011079296, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 866, Loss: 0.2443818685686189, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 867, Loss: 0.4117084239363312, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 868, Loss: 0.37682290961873244, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 869, Loss: 0.27245763259832295, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 870, Loss: 0.42541477352135915, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 871, Loss: 0.3313038881228636, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 872, Loss: 0.4810507728321721, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 873, Loss: 0.29232022645791217, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 874, Loss: 0.21046576541471268, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 875, Loss: 0.3684832093534107, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 876, Loss: 0.2712415740717881, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 877, Loss: 0.22188483541972848, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 878, Loss: 0.28611256275090047, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 879, Loss: 0.31058413229644305, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 880, Loss: 0.27481955602378827, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 881, Loss: 0.3563303162389655, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 882, Loss: 0.23729910012169259, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 883, Loss: 0.29364350272890294, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 884, Loss: 0.2592632011701207, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 885, Loss: 0.3796006895985645, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 886, Loss: 0.35448150976294523, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 887, Loss: 0.42460344860435606, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 888, Loss: 0.5658367117673962, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 889, Loss: 0.2885992648980208, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 890, Loss: 0.2565893753127024, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 891, Loss: 0.2750297030849951, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 892, Loss: 0.3835270119709341, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 893, Loss: 0.2778366416684496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 894, Loss: 0.27850828103286573, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 895, Loss: 0.3298266687437689, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 896, Loss: 0.41325187365647353, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 897, Loss: 0.2703867599415225, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 898, Loss: 0.284412478970847, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 899, Loss: 0.2606738303806843, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 900, Loss: 0.588638036644973, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 901, Loss: 0.443543958119055, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 902, Loss: 0.3214052157301044, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 903, Loss: 0.45076541875629744, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 904, Loss: 0.4221927172797063, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 905, Loss: 0.6863381586374748, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 906, Loss: 0.37940920751228047, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 907, Loss: 0.26748413526194303, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 908, Loss: 0.35033108161791754, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 909, Loss: 0.32699182950086403, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 910, Loss: 0.7289899011102892, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 911, Loss: 0.30148160691034454, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 912, Loss: 0.3329419985830716, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 913, Loss: 0.20688700053706477, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 914, Loss: 0.3221958844646465, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 915, Loss: 0.7541687435224707, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 916, Loss: 0.2479842065256332, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 917, Loss: 0.682184904951403, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 918, Loss: 0.25574764612018064, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 919, Loss: 0.36485371194933636, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 920, Loss: 0.4982331104831014, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 921, Loss: 0.21827023200036763, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 922, Loss: 0.2413446720249026, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 923, Loss: 0.30632756232881325, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 924, Loss: 0.30382791084809535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 925, Loss: 0.638292590692149, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 926, Loss: 0.6957411456738982, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 927, Loss: 0.7343364101358401, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 928, Loss: 0.33666093088192794, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 929, Loss: 0.3171500073380509, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 930, Loss: 0.3747558692669297, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 931, Loss: 0.28962798235259624, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 932, Loss: 0.4859324297120083, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 933, Loss: 0.21507767347816964, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 934, Loss: 0.24099954890686415, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 935, Loss: 0.34023719373870764, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 936, Loss: 0.354358191397141, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 937, Loss: 0.34345186671000894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 938, Loss: 0.49525152462578304, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 939, Loss: 0.25996706811612846, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 940, Loss: 0.609590060820978, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 941, Loss: 0.23427074187951225, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 942, Loss: 0.2427734117809437, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 943, Loss: 0.4979050455387944, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 944, Loss: 0.2769133713241544, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 945, Loss: 0.5873406470115543, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 946, Loss: 0.34169385678327613, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 947, Loss: 0.2545689364976509, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 948, Loss: 0.24867788054603984, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 949, Loss: 0.3976539104581711, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 950, Loss: 0.2614565997693226, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 951, Loss: 0.2795268683784752, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 952, Loss: 0.47570286946572066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 953, Loss: 0.38635152812771784, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 954, Loss: 0.3099896975463547, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 955, Loss: 0.34819871425328686, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 956, Loss: 0.32174394006106877, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 957, Loss: 0.3131135517390029, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 958, Loss: 0.2787370838433191, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 959, Loss: 0.376257384438343, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 960, Loss: 0.2781313439192133, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 961, Loss: 0.34530036408979037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 962, Loss: 0.3027770415934854, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 963, Loss: 0.39052375719158655, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 964, Loss: 0.2596325818186956, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 965, Loss: 0.2904505977154971, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 966, Loss: 0.4247002592175946, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 967, Loss: 0.37120366213705325, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 968, Loss: 0.2795920655207004, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 969, Loss: 0.29244509885434933, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 970, Loss: 0.23341774910344704, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 971, Loss: 0.41174533805914426, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 972, Loss: 0.4589088249161921, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 973, Loss: 0.32555636509826236, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 974, Loss: 0.5512975421704461, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 975, Loss: 0.25315453666017984, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 976, Loss: 0.38345723979258456, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 977, Loss: 0.36220458311524567, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 978, Loss: 0.48377351974730287, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 979, Loss: 0.22237839316217234, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 980, Loss: 0.5152492320023391, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 981, Loss: 0.3010486302254094, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 982, Loss: 0.18900517621591337, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 983, Loss: 0.25761879684742756, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 984, Loss: 0.23991421739136579, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 985, Loss: 0.22714900134448526, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 986, Loss: 0.47060447711622166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 987, Loss: 0.2362138557803214, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 988, Loss: 0.2790966784283284, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 989, Loss: 0.3424670471264938, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 990, Loss: 0.5742795528620084, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 991, Loss: 0.5301577924869844, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 992, Loss: 0.2842074552223147, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 993, Loss: 0.23631992774411992, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 994, Loss: 0.43798484687905037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 995, Loss: 0.37541059498427787, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 996, Loss: 0.5180227424189541, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 997, Loss: 0.3178513729117289, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 998, Loss: 0.2895337779056353, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 999, Loss: 0.23163249213255138, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1000, Loss: 0.3387452401583085, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1001, Loss: 0.42780702136430426, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1002, Loss: 0.479922892731629, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1003, Loss: 0.24860451567349312, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1004, Loss: 0.25583133556412674, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1005, Loss: 0.37493948457273346, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1006, Loss: 0.4390271097409809, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1007, Loss: 0.4271781116331892, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1008, Loss: 0.29530957621897996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1009, Loss: 0.40697070579609995, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1010, Loss: 0.25707138458957823, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1011, Loss: 0.24813941458674568, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1012, Loss: 0.41987789641528694, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1013, Loss: 0.3869333201828102, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1014, Loss: 0.40479536767347146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1015, Loss: 0.3245679816199551, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1016, Loss: 0.31043225544868647, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1017, Loss: 0.4601863756146852, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1018, Loss: 0.27284564359987856, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1019, Loss: 0.3805879185170338, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1020, Loss: 0.3503119685649946, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1021, Loss: 0.5660639876661705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1022, Loss: 0.40890738011329314, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1023, Loss: 0.44852260515656067, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1024, Loss: 0.37986810295747686, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1025, Loss: 0.2940713202229254, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1026, Loss: 0.2338166540749037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1027, Loss: 0.4454145656913878, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1028, Loss: 0.2728477656411907, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1029, Loss: 0.3555153665461131, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1030, Loss: 0.33025532404193847, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1031, Loss: 0.5774373955524745, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1032, Loss: 0.3718218729299354, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1033, Loss: 0.3512229021441632, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1034, Loss: 0.269756842883328, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1035, Loss: 0.35996081458564755, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1036, Loss: 0.30365903720112275, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1037, Loss: 0.45505740967659786, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1038, Loss: 0.28961594168810806, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1039, Loss: 0.27059783392505926, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1040, Loss: 0.48937406986786514, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1041, Loss: 0.22010178974443884, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1042, Loss: 0.19169439297355412, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1043, Loss: 0.30399169969869444, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1044, Loss: 0.3129293706007456, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1045, Loss: 0.32118369585531614, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1046, Loss: 0.3232238420353639, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1047, Loss: 0.4801280417986414, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1048, Loss: 0.188331172991143, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1049, Loss: 0.4727297317591287, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1050, Loss: 0.3581729496805752, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1051, Loss: 0.3105097073485138, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1052, Loss: 0.44755117269078304, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1053, Loss: 0.342951457492086, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1054, Loss: 0.27907016944231267, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1055, Loss: 0.22108950073585237, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1056, Loss: 0.42266974044076555, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1057, Loss: 0.20339041091238697, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1058, Loss: 0.39893057890298467, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1059, Loss: 0.32355357909826743, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1060, Loss: 0.21160561264994165, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1061, Loss: 0.476178541648397, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1062, Loss: 0.5337975672321944, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1063, Loss: 0.2699720076210666, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1064, Loss: 0.45513247709646876, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1065, Loss: 0.44520408984739124, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1066, Loss: 0.2682415156736501, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1067, Loss: 0.22088342476644274, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1068, Loss: 0.2414085389017996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1069, Loss: 0.4158873058303916, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1070, Loss: 0.210351114250653, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1071, Loss: 0.3889793815924798, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1072, Loss: 0.2847391707181867, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1073, Loss: 0.4281534825384433, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1074, Loss: 0.2370286917291905, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1075, Loss: 0.5379111380931166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1076, Loss: 0.39465617795083185, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1077, Loss: 0.23925689688058555, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1078, Loss: 0.2634494570710676, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1079, Loss: 0.4078757472548028, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1080, Loss: 0.32268351375621523, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1081, Loss: 0.28883189130148523, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1082, Loss: 0.2793670487901204, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1083, Loss: 0.2962972353161366, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1084, Loss: 0.22571507059339185, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1085, Loss: 0.4829746215992465, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1086, Loss: 0.6518225335735793, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1087, Loss: 0.45230445752031845, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1088, Loss: 0.5295909967164368, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1089, Loss: 0.24509231815923868, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1090, Loss: 0.3071136352461502, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1091, Loss: 0.4329408483829198, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1092, Loss: 0.42638484473882693, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1093, Loss: 0.4517452457206155, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1094, Loss: 0.5649762470911724, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1095, Loss: 0.2796819005739612, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1096, Loss: 0.40072193205883266, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1097, Loss: 0.35101311430343274, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1098, Loss: 0.4154724217793413, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1099, Loss: 0.31020791202548254, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1100, Loss: 0.36892238643250874, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1101, Loss: 0.5032540133265807, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1102, Loss: 0.5336205827636689, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1103, Loss: 0.357709756602236, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1104, Loss: 0.5320334613241836, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1105, Loss: 0.3023672165156829, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1106, Loss: 0.23515595468088113, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1107, Loss: 0.33302396344169466, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1108, Loss: 0.2710787238184204, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1109, Loss: 0.38771406248766166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1110, Loss: 0.2526538931817418, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1111, Loss: 0.6495650250118377, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1112, Loss: 0.5437596411472373, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1113, Loss: 0.21276423147509277, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1114, Loss: 0.286646806810553, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1115, Loss: 0.22696495585369678, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1116, Loss: 0.5807384858739534, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1117, Loss: 0.3013980598220464, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1118, Loss: 0.30413809128438296, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1119, Loss: 0.5792606582927436, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1120, Loss: 0.3933924939641458, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1121, Loss: 0.42936060285191546, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1122, Loss: 0.22989223327968555, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1123, Loss: 0.2615525670694528, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1124, Loss: 0.37650491895797933, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1125, Loss: 0.3299405734925044, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1126, Loss: 0.4432915295264116, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1127, Loss: 0.5057595065703229, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1128, Loss: 0.4659180924032558, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1129, Loss: 0.3366536818593042, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1130, Loss: 0.2154363198023471, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1131, Loss: 0.7693779947479619, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1132, Loss: 0.24392260055917547, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1133, Loss: 0.5044231937570184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1134, Loss: 0.5375972424794391, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1135, Loss: 0.2914658671013629, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1136, Loss: 0.4039002326340817, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1137, Loss: 0.4910302697626165, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1138, Loss: 0.4734348128210748, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1139, Loss: 0.25478406791423996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1140, Loss: 0.526699737195782, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1141, Loss: 0.6597174589121417, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1142, Loss: 0.3923136853912759, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1143, Loss: 0.5511594319539762, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1144, Loss: 0.3846210029843824, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1145, Loss: 0.4478147567365507, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1146, Loss: 0.229376249006368, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1147, Loss: 0.23975011098189383, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1148, Loss: 0.26343718986530706, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1149, Loss: 0.4712824400069196, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1150, Loss: 0.49511921994336017, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1151, Loss: 0.48185433655482435, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1152, Loss: 0.25542404154598813, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1153, Loss: 0.2072846684990644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1154, Loss: 0.34691410106667264, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1155, Loss: 0.3589513457412384, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1156, Loss: 0.40462353584936783, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1157, Loss: 0.3848667373058543, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1158, Loss: 0.41860846570732324, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1159, Loss: 0.3441752517477352, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1160, Loss: 0.6114992963893955, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1161, Loss: 0.35417547353408635, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1162, Loss: 0.3574773450133677, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1163, Loss: 0.24111639716495775, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1164, Loss: 0.49134498669429794, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1165, Loss: 0.3385011605045598, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1166, Loss: 0.3523814997075925, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1167, Loss: 0.2547120596391785, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1168, Loss: 0.3562685857312141, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1169, Loss: 0.5688963302790593, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1170, Loss: 0.25553401164926864, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1171, Loss: 0.5456280591703173, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1172, Loss: 0.36391663630430005, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1173, Loss: 0.6001533731926637, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1174, Loss: 0.42263526038213745, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1175, Loss: 0.38412230397723973, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1176, Loss: 0.26522837955021084, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1177, Loss: 0.2594210960443596, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1178, Loss: 0.2015152927884227, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1179, Loss: 0.44635618571134894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1180, Loss: 0.30814708237989064, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1181, Loss: 0.6344879945719559, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1182, Loss: 0.6344354354227171, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1183, Loss: 0.24875736861221914, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1184, Loss: 0.3933223124168437, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1185, Loss: 0.22057556195750716, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1186, Loss: 0.20198288006825982, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1187, Loss: 0.3553027095430815, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1188, Loss: 0.551154939393185, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1189, Loss: 0.19504145808464848, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1190, Loss: 0.237322121819652, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1191, Loss: 0.5143425440412959, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1192, Loss: 0.24764947616973054, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1193, Loss: 0.565075152654146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1194, Loss: 0.45370073397039157, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1195, Loss: 0.2891377398838291, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1196, Loss: 0.26526250904922977, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1197, Loss: 0.26709574850794887, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1198, Loss: 0.4494938119430918, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1199, Loss: 0.44067631718729927, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1200, Loss: 0.33315219605883184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1201, Loss: 0.23040812274436034, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1202, Loss: 0.27149008892378185, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1203, Loss: 0.5148807081413985, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1204, Loss: 0.2869672085213874, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1205, Loss: 0.23850794564315586, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1206, Loss: 0.31659770373972657, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1207, Loss: 0.37076124049665243, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1208, Loss: 0.34879587808752943, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1209, Loss: 0.19760506079746346, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1210, Loss: 0.2533418043045007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1211, Loss: 0.49620780801589076, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1212, Loss: 0.4160006173032561, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1213, Loss: 0.3388028648281416, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1214, Loss: 0.23774682204007463, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1215, Loss: 0.30163982591793315, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1216, Loss: 0.27102721726505274, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1217, Loss: 0.2568279074875297, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1218, Loss: 0.5826791741462158, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1219, Loss: 0.20525473928918464, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1220, Loss: 0.37043346290879825, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1221, Loss: 0.7538276143562734, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1222, Loss: 0.46675194350946203, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1223, Loss: 0.40337536928576245, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1224, Loss: 0.23780674048409295, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1225, Loss: 0.33839641248839447, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1226, Loss: 0.4011288327817218, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1227, Loss: 0.316502317741883, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1228, Loss: 0.2228101982844927, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1229, Loss: 0.6746630518005229, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1230, Loss: 0.3361560610302715, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1231, Loss: 0.3979936232902631, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1232, Loss: 0.29274988536295043, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1233, Loss: 0.3459631809359801, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1234, Loss: 0.4203702488130697, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1235, Loss: 0.5315062564363995, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1236, Loss: 0.3943341716712697, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1237, Loss: 0.4024673540788164, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1238, Loss: 0.4521651422159366, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1239, Loss: 0.370279023298534, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1240, Loss: 0.43476801754066485, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1241, Loss: 0.2511909608872919, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1242, Loss: 0.31955124518906963, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1243, Loss: 0.36048682293205003, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1244, Loss: 0.21835939468590124, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1245, Loss: 0.36034811006896844, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1246, Loss: 0.3699061064120406, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1247, Loss: 0.22257362074558812, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1248, Loss: 0.29487845760668396, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1249, Loss: 0.28632378902181344, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1250, Loss: 0.3878344021463228, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1251, Loss: 0.3720929653653323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1252, Loss: 0.39370083388793065, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1253, Loss: 0.3505097538860916, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1254, Loss: 0.37962747245799633, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1255, Loss: 0.3451325092779107, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1256, Loss: 0.469745650578156, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1257, Loss: 0.5748662889083407, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1258, Loss: 0.2252368251354162, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1259, Loss: 0.2876986327099885, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1260, Loss: 0.2377007992191834, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1261, Loss: 0.451658293997381, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1262, Loss: 0.49707784425150076, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1263, Loss: 0.3850459918436774, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1264, Loss: 0.2901569890979012, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1265, Loss: 0.2812936704530801, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1266, Loss: 0.2715571498453272, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1267, Loss: 0.29092411826478876, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1268, Loss: 0.7398489530754542, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1269, Loss: 0.38637738119526394, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1270, Loss: 0.2672535217032932, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1271, Loss: 0.31357190002197033, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1272, Loss: 0.2659454503937298, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1273, Loss: 0.4363097935609299, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1274, Loss: 0.5023767810671337, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1275, Loss: 0.327165740883711, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1276, Loss: 0.45916725929607516, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1277, Loss: 0.24114008763768607, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1278, Loss: 0.4109518645186881, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1279, Loss: 0.2766620762738781, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1280, Loss: 0.42311992286536804, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1281, Loss: 0.24438810819792678, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1282, Loss: 0.2878402313583366, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1283, Loss: 0.2476219771195975, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1284, Loss: 0.2963987228642388, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1285, Loss: 0.2465491933927581, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1286, Loss: 0.3173443179839045, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1287, Loss: 0.3918152152291542, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1288, Loss: 0.563522020215441, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1289, Loss: 0.398982995161382, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1290, Loss: 0.37790731333891925, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1291, Loss: 0.3965057276316737, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1292, Loss: 0.4186971742651868, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1293, Loss: 0.5863733875698836, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1294, Loss: 0.3105138165722753, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1295, Loss: 0.45088188032712384, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1296, Loss: 0.33751212241291084, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1297, Loss: 0.4133201153123296, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1298, Loss: 0.5862565308723748, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1299, Loss: 0.475002038544487, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1300, Loss: 0.2935625623481418, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1301, Loss: 0.35265903733224646, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1302, Loss: 0.34447925573032984, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1303, Loss: 0.3981475347521196, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1304, Loss: 0.24428933246930928, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1305, Loss: 0.3610548632090177, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1306, Loss: 0.2843605208476465, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1307, Loss: 0.46982806760378854, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1308, Loss: 0.4570370845781693, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1309, Loss: 0.43093315159977874, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1310, Loss: 0.32712913137264943, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1311, Loss: 0.32973771260376694, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1312, Loss: 0.40360545074196585, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1313, Loss: 0.3771576602561737, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1314, Loss: 0.519221501946081, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1315, Loss: 0.4077458157271019, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1316, Loss: 0.3150303013430782, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1317, Loss: 0.20947798578668353, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1318, Loss: 0.2130023497618315, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1319, Loss: 0.32589458422472517, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1320, Loss: 0.2814925013307713, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1321, Loss: 0.3756373816618359, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1322, Loss: 0.38615557323151967, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1323, Loss: 0.3685279595333707, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1324, Loss: 0.5000258598419426, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1325, Loss: 0.4502375803742316, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1326, Loss: 0.38808191082413507, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1327, Loss: 0.2875243543395643, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1328, Loss: 0.5286059751826819, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1329, Loss: 0.38750777044092827, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1330, Loss: 0.277213544872173, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1331, Loss: 0.23144664346604812, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1332, Loss: 0.3824733883583537, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1333, Loss: 0.4942495843362069, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1334, Loss: 0.5195270958484957, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1335, Loss: 0.3800891040926067, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1336, Loss: 0.8350775910321425, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1337, Loss: 0.2509681423948801, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1338, Loss: 0.2919986088600059, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1339, Loss: 0.5840576651499764, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1340, Loss: 0.3639182994923039, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1341, Loss: 0.4816160113654514, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1342, Loss: 0.4177363113408653, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1343, Loss: 0.2313878934228229, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1344, Loss: 0.27289188383645274, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1345, Loss: 0.28724813146915085, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1346, Loss: 0.272304360604837, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1347, Loss: 0.380932185773094, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1348, Loss: 0.39327292161638017, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1349, Loss: 0.5678915187692626, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1350, Loss: 0.20488744738447562, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1351, Loss: 0.28104231407342595, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1352, Loss: 0.4601629247995964, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1353, Loss: 0.4556459930548237, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1354, Loss: 0.3578892397925829, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1355, Loss: 0.40873115244029007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1356, Loss: 0.28950492416171375, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1357, Loss: 0.40088903152659633, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1358, Loss: 0.27870436269709525, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1359, Loss: 0.37403581424808596, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1360, Loss: 0.28404007798356107, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1361, Loss: 0.3561736546687617, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1362, Loss: 0.337128319650841, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1363, Loss: 0.5264652676446222, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1364, Loss: 0.4208132749729482, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1365, Loss: 0.35998406976381925, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1366, Loss: 0.38382833864656896, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1367, Loss: 0.30301267825984723, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1368, Loss: 0.5335697365003614, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1369, Loss: 0.35472542492561066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1370, Loss: 0.4767406963234459, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1371, Loss: 0.8251627737220036, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1372, Loss: 0.46255140224246916, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1373, Loss: 0.43752138585748845, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1374, Loss: 0.5714443869590728, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1375, Loss: 0.2386412967253313, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1376, Loss: 0.3937577854086477, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1377, Loss: 0.6465129737311327, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1378, Loss: 0.2565629738060743, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1379, Loss: 0.24856127557845845, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1380, Loss: 0.5216548512088512, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1381, Loss: 0.42245003458539265, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1382, Loss: 0.29904108399233004, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1383, Loss: 0.4398677673229555, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1384, Loss: 0.22777534234741176, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1385, Loss: 0.35130418726794, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1386, Loss: 0.39359609663697487, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1387, Loss: 0.24343084657050243, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1388, Loss: 0.3022038996205517, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1389, Loss: 0.5479697270385896, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1390, Loss: 0.4600975394667246, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1391, Loss: 0.4614513345584492, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1392, Loss: 0.34545693244191367, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1393, Loss: 0.40210590675712166, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1394, Loss: 0.37314321891319424, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1395, Loss: 0.30666911421491516, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1396, Loss: 0.2466330648099675, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1397, Loss: 0.2907660563349224, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1398, Loss: 0.4317208868157154, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1399, Loss: 0.455021230638448, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1400, Loss: 0.323127332798257, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1401, Loss: 0.28166983014600416, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1402, Loss: 0.33084012261712015, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1403, Loss: 0.2860237170425169, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1404, Loss: 0.4084927466503402, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1405, Loss: 0.20705519684792711, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1406, Loss: 0.38065192508314055, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1407, Loss: 0.37113636663128324, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1408, Loss: 0.43734799903737087, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1409, Loss: 0.30478859830482685, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1410, Loss: 0.6661611111940475, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1411, Loss: 0.3102247620633869, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1412, Loss: 0.6383858335766014, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1413, Loss: 0.2892377108828689, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1414, Loss: 0.38246272888105814, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1415, Loss: 0.48760288113741934, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1416, Loss: 0.3934074727190344, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1417, Loss: 0.22440856674039913, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1418, Loss: 0.2204840071326934, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1419, Loss: 0.31054632609156846, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1420, Loss: 0.28600396775373244, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1421, Loss: 0.2797044623930308, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1422, Loss: 0.45597934642004634, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1423, Loss: 0.4410756319651863, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1424, Loss: 0.5649198205518537, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1425, Loss: 0.5162741906697301, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1426, Loss: 0.23979848437202542, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1427, Loss: 0.363115512894865, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1428, Loss: 0.40966668057743694, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1429, Loss: 0.39438112224638866, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1430, Loss: 0.4105358335901511, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1431, Loss: 0.2756695707696765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1432, Loss: 0.5387980561268554, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1433, Loss: 0.31692377727146526, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1434, Loss: 0.2903586650468246, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1435, Loss: 0.28167604019827563, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1436, Loss: 0.20112286747378832, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1437, Loss: 0.2436119608640568, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1438, Loss: 0.36250811485762696, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1439, Loss: 0.24105885253667503, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1440, Loss: 0.4347491167874552, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1441, Loss: 0.4756744646360812, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1442, Loss: 0.4074710862779378, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1443, Loss: 0.23095540755249136, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1444, Loss: 0.3436838952606958, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1445, Loss: 0.44073747014129966, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1446, Loss: 0.2832685977406765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1447, Loss: 0.24511584655487273, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1448, Loss: 0.2561924668018156, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1449, Loss: 0.4007385133733351, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1450, Loss: 0.25441615032051956, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1451, Loss: 0.3285953788106344, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1452, Loss: 0.27853246285266464, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1453, Loss: 0.2268199941811297, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1454, Loss: 0.28424600000904887, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1455, Loss: 0.45414172483450066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1456, Loss: 0.32075469518692323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1457, Loss: 0.2653977451641202, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1458, Loss: 0.33455102585266705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1459, Loss: 0.5091373306485403, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1460, Loss: 0.24277522157034975, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1461, Loss: 0.4082448417786981, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1462, Loss: 0.36364229617285126, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1463, Loss: 0.2676949677776964, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1464, Loss: 0.28605691915255355, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1465, Loss: 0.24618368758514125, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1466, Loss: 0.39295296784469147, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1467, Loss: 0.3918258058057009, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1468, Loss: 0.39540010041584595, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1469, Loss: 0.42692604260914174, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1470, Loss: 0.7736858504336903, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1471, Loss: 0.29286687794628036, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1472, Loss: 0.3514277465723196, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1473, Loss: 0.23345465111117433, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1474, Loss: 0.21662545526348337, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1475, Loss: 0.42974756228154315, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1476, Loss: 0.4655974743783833, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1477, Loss: 0.494982130391597, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1478, Loss: 0.48580496378345406, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1479, Loss: 0.3113743955508068, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1480, Loss: 0.4617713344620216, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1481, Loss: 0.5511518670462912, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1482, Loss: 0.3392326100829387, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1483, Loss: 0.2194963615870229, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1484, Loss: 0.3956279349753138, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1485, Loss: 0.5001847217629918, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1486, Loss: 0.21832196511907437, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1487, Loss: 0.2529933622269471, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1488, Loss: 0.35815509249861965, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1489, Loss: 0.3016624702903121, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1490, Loss: 0.3359090888077927, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1491, Loss: 0.2980063182147393, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1492, Loss: 0.2902989003010761, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1493, Loss: 0.22405007097291296, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1494, Loss: 0.40937252905218413, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1495, Loss: 0.3331924848116266, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1496, Loss: 0.3777086529216678, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1497, Loss: 0.5332121071070318, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1498, Loss: 0.29115284468666275, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1499, Loss: 0.35650309523477, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1500, Loss: 0.4461127945661717, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1501, Loss: 0.2349095465168611, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1502, Loss: 0.20368095419072804, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1503, Loss: 0.23665183730896663, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1504, Loss: 0.3206016381027184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1505, Loss: 0.46639903519343395, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1506, Loss: 0.38232839982664746, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1507, Loss: 0.238119035309663, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1508, Loss: 0.4337978248836811, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1509, Loss: 0.3407979030599459, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1510, Loss: 0.2983588441926167, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1511, Loss: 0.39993415579967284, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1512, Loss: 1.1829809096594264, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1513, Loss: 0.31496817631902485, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1514, Loss: 0.2470418698514551, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1515, Loss: 0.4655721721678241, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1516, Loss: 0.3397249123284731, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1517, Loss: 0.3575093076332323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1518, Loss: 0.36099015159485026, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1519, Loss: 0.6388706013296223, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1520, Loss: 0.6392758360903885, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1521, Loss: 0.21618365542604168, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1522, Loss: 0.511036296038863, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1523, Loss: 0.35460309211748253, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1524, Loss: 0.2190931335367704, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1525, Loss: 0.36894312447849287, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1526, Loss: 0.360692261387358, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1527, Loss: 0.5087218323081, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1528, Loss: 0.545013976222604, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1529, Loss: 0.46084404303812676, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1530, Loss: 0.35624146394228673, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1531, Loss: 0.21834410342454488, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1532, Loss: 0.3807651830051644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1533, Loss: 0.533098953778997, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1534, Loss: 0.3066022633665654, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1535, Loss: 0.33062215270231443, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1536, Loss: 0.5227709752684218, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1537, Loss: 0.47416601707860584, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1538, Loss: 0.22434507345663937, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1539, Loss: 0.24574661827654593, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1540, Loss: 0.3424964763138713, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1541, Loss: 0.3690422558406532, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1542, Loss: 0.5088565534032461, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1543, Loss: 0.28984733384729017, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1544, Loss: 0.3720089015704006, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1545, Loss: 0.4630622657119453, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1546, Loss: 0.2458507729615747, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1547, Loss: 0.3883185440850765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1548, Loss: 0.2700163604406884, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1549, Loss: 0.3300815920880247, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1550, Loss: 0.30828351511971785, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1551, Loss: 0.5475556742647931, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1552, Loss: 0.22431221633967313, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1553, Loss: 0.4615619732947652, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1554, Loss: 0.2767597475755965, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1555, Loss: 0.47508302551574944, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1556, Loss: 0.3318607921926894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1557, Loss: 0.26470576349248653, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1558, Loss: 0.20780804979064227, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1559, Loss: 0.38204174561668686, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1560, Loss: 0.5854223841470345, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1561, Loss: 0.29134147022955775, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1562, Loss: 0.8617421875609751, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1563, Loss: 0.2169512654812873, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1564, Loss: 0.2669906796404027, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1565, Loss: 0.23885514831135848, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1566, Loss: 0.25390577711597184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1567, Loss: 0.4035628850949671, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1568, Loss: 0.30648688923094236, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1569, Loss: 0.9606950324944235, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1570, Loss: 0.2757862605476885, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1571, Loss: 0.22959947703586395, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1572, Loss: 0.27641282382186544, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1573, Loss: 0.3338768406181229, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1574, Loss: 0.24757906365785912, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1575, Loss: 0.30860637029305715, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1576, Loss: 0.33889579622034344, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1577, Loss: 0.20362398763011239, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1578, Loss: 0.30921138893891953, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1579, Loss: 0.7000567803371542, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1580, Loss: 0.4698992436958973, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1581, Loss: 0.291369162586672, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1582, Loss: 0.34553027658188895, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1583, Loss: 0.5544657711124918, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1584, Loss: 0.37549466091245776, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1585, Loss: 0.3030229088928765, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1586, Loss: 0.34413541054386043, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1587, Loss: 0.18772585122286378, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1588, Loss: 0.3391954909850873, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1589, Loss: 0.5126553340504485, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1590, Loss: 0.3527013485217554, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1591, Loss: 0.209002700191394, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1592, Loss: 0.2482759164371915, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1593, Loss: 0.26840127551272797, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1594, Loss: 0.36016428291536595, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1595, Loss: 0.25143127535157184, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1596, Loss: 0.26012144503031165, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1597, Loss: 0.2225097827920093, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1598, Loss: 0.3720937191015342, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1599, Loss: 0.2238947479341808, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1600, Loss: 0.32903218843156296, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1601, Loss: 0.44593555405829677, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1602, Loss: 0.20687899371055768, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1603, Loss: 0.5238493594766291, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1604, Loss: 0.570285812252617, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1605, Loss: 0.38457665639754596, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1606, Loss: 0.49536586465156507, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1607, Loss: 0.3477581068085037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1608, Loss: 0.39205155458076474, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1609, Loss: 0.3750295366097106, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1610, Loss: 0.3358185303027756, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1611, Loss: 0.2984664965668421, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1612, Loss: 0.4830270730035824, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1613, Loss: 0.48695769754462126, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1614, Loss: 0.35110992999242785, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1615, Loss: 0.354378278166648, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1616, Loss: 0.5061605956783427, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1617, Loss: 0.32440747196268094, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1618, Loss: 0.4786418323139041, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1619, Loss: 0.2939866385252516, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1620, Loss: 0.43137218881551576, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1621, Loss: 0.29882291412681994, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1622, Loss: 0.2570279523132651, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1623, Loss: 0.32571023296663204, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1624, Loss: 0.25460801816761985, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1625, Loss: 0.30752329874149603, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1626, Loss: 0.20308409129806498, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1627, Loss: 0.33293191442886066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1628, Loss: 0.1927802833470508, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1629, Loss: 0.36289689192038854, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1630, Loss: 0.29154945088750617, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1631, Loss: 0.38864179169078644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1632, Loss: 0.3314424525503492, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1633, Loss: 0.33721925175083606, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1634, Loss: 0.5625152207412458, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1635, Loss: 0.39286752300947625, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1636, Loss: 0.3129027697883563, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1637, Loss: 0.27892406460499763, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1638, Loss: 0.21947158684349996, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1639, Loss: 0.43481163069494067, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1640, Loss: 0.3242524038864598, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1641, Loss: 0.368755359734752, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1642, Loss: 0.44891299797002193, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1643, Loss: 0.36380586375641555, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1644, Loss: 0.5313793851665933, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1645, Loss: 0.42215069758944057, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1646, Loss: 0.29122562986379463, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1647, Loss: 0.2219536591508927, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1648, Loss: 0.2820140629819828, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1649, Loss: 0.4071189557444037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1650, Loss: 0.5480344176370175, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1651, Loss: 0.5911069428523505, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1652, Loss: 0.3113461761333397, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1653, Loss: 0.4508063879831018, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1654, Loss: 0.3930240998455541, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1655, Loss: 0.22448749645281196, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1656, Loss: 0.48555145808373323, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1657, Loss: 0.3034357419235248, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1658, Loss: 0.41335639056297413, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1659, Loss: 0.4327573951605422, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1660, Loss: 0.4244290657971993, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1661, Loss: 0.42736186195319764, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1662, Loss: 0.42282181067737384, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1663, Loss: 0.49386163765526664, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1664, Loss: 0.3956880953742824, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1665, Loss: 0.36313786607354853, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1666, Loss: 0.38165343709856714, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1667, Loss: 0.4014383473613137, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1668, Loss: 0.5522889037288522, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1669, Loss: 0.4318385970013652, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1670, Loss: 0.33928075712411543, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1671, Loss: 0.33443920615688894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1672, Loss: 0.40321855679489016, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1673, Loss: 0.2604489094743143, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1674, Loss: 0.4821186460675386, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1675, Loss: 0.33435559269026194, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1676, Loss: 0.36961064650392506, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1677, Loss: 0.43386782659289647, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1678, Loss: 0.715083108982687, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1679, Loss: 0.23848722667322952, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1680, Loss: 0.36220023355214104, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1681, Loss: 0.30835499472771355, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1682, Loss: 0.31518693615273635, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1683, Loss: 0.31645838993563646, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1684, Loss: 0.3092289122778872, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1685, Loss: 0.3317557767836615, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1686, Loss: 0.512075798716351, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1687, Loss: 0.23519192434939146, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1688, Loss: 0.3769196486388961, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1689, Loss: 0.7256556604083688, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1690, Loss: 0.4090397024738167, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1691, Loss: 0.38968610950623533, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1692, Loss: 0.5176462783524552, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1693, Loss: 0.2693703626095859, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1694, Loss: 0.3180370429936684, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1695, Loss: 0.2133752411244269, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1696, Loss: 0.4963523704455306, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1697, Loss: 0.5733527783314037, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1698, Loss: 0.4520925878344049, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1699, Loss: 0.24175468930382044, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1700, Loss: 0.30776025940552276, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1701, Loss: 0.34146885064109833, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1702, Loss: 0.2578829032905516, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1703, Loss: 0.3935829049098404, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1704, Loss: 0.25501386803007486, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1705, Loss: 0.3017530642969301, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1706, Loss: 0.3309671273141943, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1707, Loss: 0.6160872846717496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1708, Loss: 0.22308875805349998, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1709, Loss: 0.2141747599380926, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1710, Loss: 0.2930121500773726, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1711, Loss: 0.35174317230718044, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1712, Loss: 0.5486527436362626, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1713, Loss: 0.5314101337710091, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1714, Loss: 0.24598245630178678, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1715, Loss: 0.3283041710116539, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1716, Loss: 0.3575845346884562, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1717, Loss: 0.4018701460108397, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1718, Loss: 0.7742128134414576, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1719, Loss: 0.24363950370626133, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1720, Loss: 0.5770456847911565, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1721, Loss: 0.4012121939315656, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1722, Loss: 0.31750763279431704, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1723, Loss: 0.2752439475843296, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1724, Loss: 0.24339907806911906, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1725, Loss: 0.4731438433729257, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1726, Loss: 0.4469008729510593, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1727, Loss: 0.3268897354580054, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1728, Loss: 0.2810678805468367, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1729, Loss: 0.6441144158788705, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1730, Loss: 0.4143511737818242, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1731, Loss: 0.3488371119442042, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1732, Loss: 0.44804320013058524, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1733, Loss: 0.485464413907342, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1734, Loss: 0.33660713745639737, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1735, Loss: 0.226868383370837, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1736, Loss: 0.3249307739299628, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1737, Loss: 0.564821301500449, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1738, Loss: 0.21584255450022713, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1739, Loss: 0.34199436888837303, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1740, Loss: 0.2822733218480028, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1741, Loss: 0.4023800023238916, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1742, Loss: 0.2419179210557356, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1743, Loss: 0.42225573581257214, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1744, Loss: 0.4168409806199503, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1745, Loss: 0.3942106202424346, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1746, Loss: 0.2873310385342762, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1747, Loss: 0.4123399642837561, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1748, Loss: 0.6737035927004102, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1749, Loss: 0.2309222823599894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1750, Loss: 0.8112594127921083, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1751, Loss: 0.3186867323207489, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1752, Loss: 0.31437070681543045, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1753, Loss: 0.2668034336051862, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1754, Loss: 0.34764176719760254, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1755, Loss: 0.3395478693694072, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1756, Loss: 0.2892742140701472, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1757, Loss: 0.5332461232973503, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1758, Loss: 0.38807856075794356, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1759, Loss: 0.29256325860379284, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1760, Loss: 0.34928433722983565, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1761, Loss: 0.26187321996185414, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1762, Loss: 0.3922241532257224, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1763, Loss: 0.6740258666667621, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1764, Loss: 0.3033030285770654, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1765, Loss: 0.38855553028607387, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1766, Loss: 0.31482694764252395, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1767, Loss: 0.2709284139100414, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1768, Loss: 0.31809411679099564, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1769, Loss: 0.6148728498466242, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1770, Loss: 0.4139134837546592, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1771, Loss: 0.4395754128811879, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1772, Loss: 0.35875224320086907, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1773, Loss: 0.495087879901169, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1774, Loss: 0.2701147573649548, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1775, Loss: 0.3393043508175911, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1776, Loss: 0.2674715986509978, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1777, Loss: 0.3840686254525604, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1778, Loss: 0.30311715399310707, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1779, Loss: 0.21550197073885807, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1780, Loss: 0.2664289073367065, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1781, Loss: 0.38742689096841154, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1782, Loss: 0.6213377750967628, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1783, Loss: 0.3922935479455722, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1784, Loss: 0.29745428786632916, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1785, Loss: 0.3165647221903449, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1786, Loss: 0.5089393028679863, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1787, Loss: 0.3340460722400133, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1788, Loss: 0.39473215842647424, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1789, Loss: 0.471040865250434, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1790, Loss: 0.544609798243528, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1791, Loss: 0.44427628504226185, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1792, Loss: 0.42878162163200934, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1793, Loss: 0.4688660112435991, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1794, Loss: 0.39085797601872685, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1795, Loss: 0.20361693807114659, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1796, Loss: 0.4372100574763157, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1797, Loss: 0.2565017031804931, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1798, Loss: 0.20622800570049027, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1799, Loss: 0.21805142571782032, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1800, Loss: 0.2606516851394094, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1801, Loss: 0.34154771545363904, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1802, Loss: 0.6702788118397759, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1803, Loss: 0.24069896471610147, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1804, Loss: 0.40461642813569415, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1805, Loss: 0.3516506032665306, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1806, Loss: 0.3214063262183735, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1807, Loss: 0.4531886754931793, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1808, Loss: 0.4800774060410743, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1809, Loss: 0.28986609939459285, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1810, Loss: 0.2448120628256938, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1811, Loss: 0.28421052236305255, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1812, Loss: 0.5461146421219193, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1813, Loss: 0.4225501142309458, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1814, Loss: 0.3189567906278345, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1815, Loss: 0.31369427054506593, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1816, Loss: 0.31018232884187447, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1817, Loss: 0.392718259441315, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1818, Loss: 0.37457398867499986, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1819, Loss: 0.24572170163471824, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1820, Loss: 0.4780955579541222, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1821, Loss: 0.35431551639856895, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1822, Loss: 0.2952661090732578, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1823, Loss: 0.45470362280412335, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1824, Loss: 0.3536438635858337, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1825, Loss: 0.42527734634636427, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1826, Loss: 0.7454623463644192, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1827, Loss: 0.2314089463536123, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1828, Loss: 0.40672163517774873, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1829, Loss: 0.5115764696947419, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1830, Loss: 0.39299403888703066, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1831, Loss: 0.38609317405382443, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1832, Loss: 0.37886860990563087, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1833, Loss: 0.2116902533705535, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1834, Loss: 0.36769518323717926, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1835, Loss: 0.34351191480807863, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1836, Loss: 0.24433813954154185, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1837, Loss: 0.28059496303193204, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1838, Loss: 0.45918678793164075, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1839, Loss: 0.33561969039890777, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1840, Loss: 0.45107034651907807, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1841, Loss: 0.3541168622722335, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1842, Loss: 0.27437325789201295, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1843, Loss: 0.31002738210407754, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1844, Loss: 0.33208941687993165, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1845, Loss: 0.23347194118227668, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1846, Loss: 0.28688816666957173, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1847, Loss: 0.26671661178898565, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1848, Loss: 0.24059455994122358, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1849, Loss: 0.5092398099361606, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1850, Loss: 0.24411543274059302, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1851, Loss: 0.3176521974906744, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1852, Loss: 0.638960291861931, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1853, Loss: 0.32888122089579935, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1854, Loss: 0.341868997314441, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1855, Loss: 0.2393168049244834, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1856, Loss: 0.55201740546191, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1857, Loss: 0.351826504196644, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1858, Loss: 0.3060327436772088, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1859, Loss: 0.7364199655736464, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1860, Loss: 0.6444891979537715, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1861, Loss: 0.5489135459989007, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1862, Loss: 0.3436112053416119, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1863, Loss: 0.28070042456383615, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1864, Loss: 0.5062384220290755, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1865, Loss: 0.43360115227807017, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1866, Loss: 0.37161449886316616, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1867, Loss: 0.3937071576631107, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1868, Loss: 0.36788589073078104, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1869, Loss: 0.5881912217185652, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1870, Loss: 0.21468543818094496, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1871, Loss: 0.31556582650075077, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1872, Loss: 0.45634617264234634, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1873, Loss: 0.33130922907853894, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1874, Loss: 0.3554039649084669, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Batch 1875, Loss: 0.6500976839432778, Batch Size: 32, Learning Rate: 0.00021674999999999996\n",
      "Epoch 4, Updated Learning Rate: 0.00018423749999999997\n",
      "Epoch 4, Average Loss: 0.3707778330141478, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1, Loss: 1.0683254192249012, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 2, Loss: 0.35616600950606914, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 3, Loss: 0.42271941345370334, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 4, Loss: 0.7471279865966416, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 5, Loss: 0.3039785698088019, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 6, Loss: 0.2716988510413038, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 7, Loss: 0.2632957526152069, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 8, Loss: 0.5546704202414705, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 9, Loss: 0.2556490695992983, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 10, Loss: 0.6677699713880945, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 11, Loss: 0.2524126646186972, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 12, Loss: 0.6890003482036058, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 13, Loss: 0.28437991255269773, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 14, Loss: 0.3265154147050139, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 15, Loss: 0.32340437740686456, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 16, Loss: 0.438481027528568, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 17, Loss: 0.3617531063467165, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 18, Loss: 0.26576843816860785, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 19, Loss: 0.3189667905129363, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 20, Loss: 0.29417855609166504, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 21, Loss: 0.5906557341838363, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 22, Loss: 0.33244594700241337, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 23, Loss: 0.3749200242416847, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 24, Loss: 0.596307838920125, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 25, Loss: 0.5434796664271393, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 26, Loss: 0.2575219429899406, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 27, Loss: 0.30596295694305997, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 28, Loss: 0.27534307026749577, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 29, Loss: 0.8361384502095508, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 30, Loss: 0.24569047962750723, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 31, Loss: 0.4033012146432443, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 32, Loss: 0.2723723609081474, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 33, Loss: 0.4249872110339119, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 34, Loss: 0.29070642781261724, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 35, Loss: 0.40267635262768764, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 36, Loss: 0.32302668255690126, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 37, Loss: 0.26440128656616013, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 38, Loss: 0.24349101648237445, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 39, Loss: 0.3786553044150658, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 40, Loss: 0.44739228809956344, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 41, Loss: 0.32990766617559014, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 42, Loss: 0.20997048009840008, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 43, Loss: 0.5928174609664316, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 44, Loss: 0.26423390048923856, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 45, Loss: 0.28758217236341643, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 46, Loss: 0.36839809674370794, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 47, Loss: 0.28812087212701876, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 48, Loss: 0.36584637344516874, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 49, Loss: 0.3307360183228574, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 50, Loss: 0.3651418241285451, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 51, Loss: 0.5273331042705101, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 52, Loss: 0.32303253050950376, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 53, Loss: 0.2995988114725629, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 54, Loss: 0.3902241141201671, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 55, Loss: 0.2580189577511561, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 56, Loss: 0.4148847658320415, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 57, Loss: 0.3181884566946098, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 58, Loss: 0.3434999725733219, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 59, Loss: 0.28823439073850066, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 60, Loss: 0.5244104219823664, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 61, Loss: 0.3090380966993435, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 62, Loss: 0.31198889736041097, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 63, Loss: 0.32219299577564786, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 64, Loss: 0.3336098047377182, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 65, Loss: 0.3360972980225865, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 66, Loss: 0.31799264047147835, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 67, Loss: 0.42045291179534067, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 68, Loss: 0.2460117374884943, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 69, Loss: 0.4282947666763537, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 70, Loss: 0.45809710579804985, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 71, Loss: 0.4828394225983414, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 72, Loss: 0.2194461688559588, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 73, Loss: 0.37016706271174094, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 74, Loss: 0.42805910302350203, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 75, Loss: 0.4113675622311387, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 76, Loss: 0.2341948178660969, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 77, Loss: 0.5267742223583644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 78, Loss: 0.3367811752995066, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 79, Loss: 0.32858533748210594, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 80, Loss: 0.4303932870909057, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 81, Loss: 0.38930802114491825, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 82, Loss: 0.568603873568263, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 83, Loss: 0.29917925223671726, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 84, Loss: 0.23382531622387334, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 85, Loss: 0.5426943361158688, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 86, Loss: 0.3179396672335665, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 87, Loss: 0.2589985985017236, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 88, Loss: 0.31823960652926286, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 89, Loss: 0.5425224404135431, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 90, Loss: 0.30554691256715416, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 91, Loss: 0.30692644591323454, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 92, Loss: 0.31201687019750735, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 93, Loss: 0.3620664249536938, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 94, Loss: 0.2509989768025832, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 95, Loss: 0.5387606665157356, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 96, Loss: 0.4322119481754727, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 97, Loss: 0.26360954228171785, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 98, Loss: 0.2636659990441619, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 99, Loss: 0.24069287472097695, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 100, Loss: 0.2669057059714067, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 101, Loss: 0.36784417758360044, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 102, Loss: 0.31603953641136234, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 103, Loss: 0.36418074012301305, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 104, Loss: 0.2467758830240041, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 105, Loss: 0.2678390324670704, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 106, Loss: 0.261518184446969, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 107, Loss: 0.24942646408707647, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 108, Loss: 0.29682080133794014, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 109, Loss: 0.3119027541652917, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 110, Loss: 0.294896220622906, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 111, Loss: 0.2657215692826642, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 112, Loss: 0.3368745484507384, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 113, Loss: 0.2760350675610342, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 114, Loss: 0.44551654097142646, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 115, Loss: 0.4054830132545816, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 116, Loss: 0.28387676568585407, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 117, Loss: 0.4941673626575679, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 118, Loss: 0.34775463477558943, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 119, Loss: 0.2881687107143841, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 120, Loss: 0.3662481513167825, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 121, Loss: 0.22201486281156554, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 122, Loss: 0.28082953799307847, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 123, Loss: 0.47860075321687456, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 124, Loss: 0.291824430351463, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 125, Loss: 0.34065087513914427, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 126, Loss: 0.3413294802000644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 127, Loss: 0.27386791709075675, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 128, Loss: 0.2547036480783339, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 129, Loss: 0.6047916199538859, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 130, Loss: 0.26534077178960125, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 131, Loss: 0.19721183893758001, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 132, Loss: 0.376061153891718, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 133, Loss: 0.39005180661627015, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 134, Loss: 0.3605182631479571, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 135, Loss: 0.372746804610603, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 136, Loss: 0.5081797912641137, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 137, Loss: 0.30321682564837815, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 138, Loss: 0.3003717896835826, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 139, Loss: 0.25666139432253315, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 140, Loss: 0.36945352146195953, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 141, Loss: 0.29898390873821684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 142, Loss: 0.2983197364555864, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 143, Loss: 0.31265364933236034, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 144, Loss: 0.49224249657224006, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 145, Loss: 0.522258115540476, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 146, Loss: 0.3642128491064582, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 147, Loss: 0.4706955381787516, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 148, Loss: 0.34513004573279216, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 149, Loss: 0.2813634045354358, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 150, Loss: 0.4474731661241008, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 151, Loss: 0.30351795802409276, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 152, Loss: 0.26931749912620695, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 153, Loss: 0.3216414876396345, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 154, Loss: 0.3090673295669464, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 155, Loss: 0.2168293912808302, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 156, Loss: 0.2672549441065819, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 157, Loss: 0.20053078410275646, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 158, Loss: 0.31248693748670453, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 159, Loss: 0.2750288632663064, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 160, Loss: 0.37661120021888833, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 161, Loss: 0.3807345505318033, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 162, Loss: 0.2927488612083485, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 163, Loss: 0.23081129367274858, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 164, Loss: 0.4514297851105228, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 165, Loss: 0.46764606530745806, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 166, Loss: 0.3579023597199644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 167, Loss: 0.3183639178916875, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 168, Loss: 0.357304806452398, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 169, Loss: 0.3783214351600125, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 170, Loss: 0.22216014764253272, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 171, Loss: 0.2651919685143529, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 172, Loss: 0.42272922756857967, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 173, Loss: 0.3827906660373931, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 174, Loss: 0.3482816712289756, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 175, Loss: 0.24258998398944115, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 176, Loss: 0.2464214954560207, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 177, Loss: 0.3827366410710754, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 178, Loss: 0.2767767691512978, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 179, Loss: 0.3644624145915235, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 180, Loss: 0.543849421571977, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 181, Loss: 0.45535850280228496, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 182, Loss: 0.4710280122810885, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 183, Loss: 0.28860116275579806, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 184, Loss: 0.4622205477536707, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 185, Loss: 0.3837616627497671, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 186, Loss: 0.3078581978286377, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 187, Loss: 0.45125256153584203, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 188, Loss: 0.36094556737188976, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 189, Loss: 0.4688925798990375, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 190, Loss: 0.22191464731708635, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 191, Loss: 0.37190463458427925, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 192, Loss: 0.3446386634636607, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 193, Loss: 0.29276597323840203, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 194, Loss: 0.3280018236458081, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 195, Loss: 0.2926958077166681, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 196, Loss: 0.30110431472591814, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 197, Loss: 0.46002110264427365, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 198, Loss: 0.42023464849678605, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 199, Loss: 0.5400279127486726, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 200, Loss: 0.29316347285541533, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 201, Loss: 0.31316812835714947, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 202, Loss: 0.26317316063968044, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 203, Loss: 0.2506341148823193, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 204, Loss: 0.4154558769404363, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 205, Loss: 0.3020864667803494, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 206, Loss: 0.2747323193255703, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 207, Loss: 0.49515612728221403, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 208, Loss: 0.5783922547310565, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 209, Loss: 0.6629515179209305, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 210, Loss: 0.2088765913427868, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 211, Loss: 0.31412725592367485, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 212, Loss: 0.562360941835319, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 213, Loss: 0.35995153225218846, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 214, Loss: 0.384099705224489, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 215, Loss: 0.3728274536234198, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 216, Loss: 0.24437992739829317, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 217, Loss: 0.38060457586333624, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 218, Loss: 0.36368673250555894, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 219, Loss: 0.27761365235849345, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 220, Loss: 0.2767180168942157, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 221, Loss: 0.2409480192028401, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 222, Loss: 0.2578536297511519, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 223, Loss: 0.31164804280508696, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 224, Loss: 0.24591054120297015, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 225, Loss: 0.43152237153149386, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 226, Loss: 0.6316376746968898, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 227, Loss: 0.5132751697740934, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 228, Loss: 0.4431529113107885, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 229, Loss: 0.3588996986275802, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 230, Loss: 0.2968086501361342, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 231, Loss: 0.2593721556612129, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 232, Loss: 0.525584848056523, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 233, Loss: 0.3830666611198162, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 234, Loss: 0.4502744403240189, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 235, Loss: 0.5040987261928893, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 236, Loss: 0.30249417844335524, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 237, Loss: 0.4807174538795826, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 238, Loss: 0.22966791919689006, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 239, Loss: 0.2399787452624884, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 240, Loss: 0.2807999788588918, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 241, Loss: 0.23216725406787825, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 242, Loss: 0.25876099012641546, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 243, Loss: 0.22744131312426455, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 244, Loss: 0.3750435896210996, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 245, Loss: 0.3499528615716837, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 246, Loss: 0.3135491636559915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 247, Loss: 0.3616706849606408, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 248, Loss: 0.28067561206617453, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 249, Loss: 0.46371128702046616, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 250, Loss: 0.556010187175193, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 251, Loss: 0.33799539196445616, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 252, Loss: 0.41729409683353, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 253, Loss: 0.21665212438838657, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 254, Loss: 0.2544962505411337, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 255, Loss: 0.39218557310609314, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 256, Loss: 0.3728498101581677, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 257, Loss: 0.6331471311638908, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 258, Loss: 0.2798996530380611, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 259, Loss: 0.2089282953205718, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 260, Loss: 0.3418386986682974, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 261, Loss: 0.29114617047175023, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 262, Loss: 0.3196050810959995, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 263, Loss: 0.3021249514841844, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 264, Loss: 0.32349232003047906, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 265, Loss: 0.31941599666138726, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 266, Loss: 0.39035617835663866, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 267, Loss: 0.2813411748132186, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 268, Loss: 0.40062535444106906, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 269, Loss: 0.37354649747043295, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 270, Loss: 0.2669730819339016, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 271, Loss: 0.31656483206416397, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 272, Loss: 0.4026874162078825, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 273, Loss: 0.4140681472908685, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 274, Loss: 0.3562852329733342, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 275, Loss: 0.2865856497881974, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 276, Loss: 0.5008552321121779, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 277, Loss: 0.20749120530840595, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 278, Loss: 0.2776681820637865, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 279, Loss: 0.2554635942775071, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 280, Loss: 0.21583933809735167, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 281, Loss: 0.20505666981384235, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 282, Loss: 0.290774604422873, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 283, Loss: 0.2425528146515491, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 284, Loss: 0.568521456914328, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 285, Loss: 0.2142375794706815, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 286, Loss: 0.2874521921856862, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 287, Loss: 0.40653175402388964, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 288, Loss: 0.5509835256797249, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 289, Loss: 0.2920360589628402, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 290, Loss: 0.3955522421747957, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 291, Loss: 0.454396449167226, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 292, Loss: 0.35809873079380783, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 293, Loss: 0.2599697512842446, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 294, Loss: 0.2419660922209853, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 295, Loss: 0.23397117200750514, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 296, Loss: 0.37238680345010844, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 297, Loss: 0.2820580919815437, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 298, Loss: 0.31477573469382875, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 299, Loss: 0.2830591190646537, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 300, Loss: 0.27759277450540043, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 301, Loss: 0.26048173811643366, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 302, Loss: 0.33163078211273384, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 303, Loss: 0.50093860688787, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 304, Loss: 0.4126205483081471, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 305, Loss: 0.20516505625078327, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 306, Loss: 0.34819049238315225, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 307, Loss: 0.39504345593746754, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 308, Loss: 0.24261599000093478, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 309, Loss: 0.2506875993705343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 310, Loss: 0.2629395711390446, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 311, Loss: 0.24751829590209373, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 312, Loss: 0.19459618483400806, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 313, Loss: 0.4085648840058769, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 314, Loss: 0.2435410529496116, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 315, Loss: 0.3278475797699679, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 316, Loss: 0.24148914795214876, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 317, Loss: 0.736981508221382, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 318, Loss: 0.3978278684455796, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 319, Loss: 0.35045711344278946, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 320, Loss: 0.3136194872677093, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 321, Loss: 0.4791220898257154, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 322, Loss: 0.3832079792673214, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 323, Loss: 0.3719966932634296, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 324, Loss: 0.2576209999148361, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 325, Loss: 0.36305226867855217, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 326, Loss: 0.3183192345691911, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 327, Loss: 0.27049889119789744, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 328, Loss: 0.23603253966417276, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 329, Loss: 0.34929170556230005, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 330, Loss: 0.25884472715888995, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 331, Loss: 0.5076501750488793, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 332, Loss: 0.4821800963526143, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 333, Loss: 0.43299470365754855, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 334, Loss: 0.2732272362075252, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 335, Loss: 0.2909389981607083, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 336, Loss: 0.39907674592340264, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 337, Loss: 0.24669676283396474, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 338, Loss: 0.3360311833383798, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 339, Loss: 0.39705346366896554, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 340, Loss: 0.2060252682033482, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 341, Loss: 0.6091002348254082, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 342, Loss: 0.4005585797185295, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 343, Loss: 0.40042877315841924, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 344, Loss: 0.23611912266005897, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 345, Loss: 0.3049950037702781, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 346, Loss: 0.3252369323487775, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 347, Loss: 0.26142150652575286, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 348, Loss: 0.3156390012586767, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 349, Loss: 0.32522189445033767, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 350, Loss: 0.5109257413453283, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 351, Loss: 0.36561938196505217, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 352, Loss: 0.24204558688870564, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 353, Loss: 0.39874111407774415, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 354, Loss: 0.2811070059784938, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 355, Loss: 0.26960888780993975, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 356, Loss: 0.5081374544809856, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 357, Loss: 0.3303172538370893, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 358, Loss: 0.39926205350010807, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 359, Loss: 0.4113200382775748, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 360, Loss: 0.27242608341955743, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 361, Loss: 0.22299285026770163, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 362, Loss: 0.2935230142038942, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 363, Loss: 0.4231327486145837, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 364, Loss: 0.28488625220665886, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 365, Loss: 0.36730359364387544, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 366, Loss: 0.21359012352564988, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 367, Loss: 0.26794264324614137, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 368, Loss: 0.2835879370647465, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 369, Loss: 0.38218933455503545, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 370, Loss: 0.2758032353052174, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 371, Loss: 0.31879284465797453, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 372, Loss: 0.357002255340199, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 373, Loss: 0.40884206803387113, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 374, Loss: 0.3032154590187449, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 375, Loss: 0.3246321288094602, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 376, Loss: 0.27570596147847315, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 377, Loss: 0.9438229929852486, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 378, Loss: 0.23338476764291047, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 379, Loss: 0.20921121829842404, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 380, Loss: 0.31962026086933487, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 381, Loss: 0.24788984698720834, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 382, Loss: 0.4382021982826446, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 383, Loss: 0.4660282693435256, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 384, Loss: 0.37365222857152963, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 385, Loss: 0.4704661737237528, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 386, Loss: 0.24682040862289295, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 387, Loss: 0.37320056469883167, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 388, Loss: 0.37272432775881215, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 389, Loss: 0.38256446995083093, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 390, Loss: 0.19509547638371566, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 391, Loss: 0.31085148502342513, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 392, Loss: 0.35633198790342796, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 393, Loss: 0.32373833772074134, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 394, Loss: 0.26992673682325463, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 395, Loss: 0.2578182267754204, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 396, Loss: 0.45484181997614737, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 397, Loss: 0.2588843047536244, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 398, Loss: 0.26769942233451777, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 399, Loss: 0.2856392769559766, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 400, Loss: 0.272495887073575, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 401, Loss: 0.2937155983005251, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 402, Loss: 0.32136052970894613, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 403, Loss: 0.3524618627260574, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 404, Loss: 0.4083787316966654, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 405, Loss: 0.4081488907793218, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 406, Loss: 0.35067371355104066, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 407, Loss: 0.2928281107994179, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 408, Loss: 0.3642659817386673, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 409, Loss: 0.32139230346756786, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 410, Loss: 0.30245737008391527, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 411, Loss: 0.35491543175846796, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 412, Loss: 0.2287489662064945, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 413, Loss: 0.5093890752944383, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 414, Loss: 0.8095422454638812, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 415, Loss: 0.4247931536078675, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 416, Loss: 0.361051930317094, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 417, Loss: 0.299117010338554, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 418, Loss: 0.257611318306984, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 419, Loss: 0.36372931836533773, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 420, Loss: 0.4032895431989314, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 421, Loss: 0.32606835112001065, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 422, Loss: 0.34733564310603154, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 423, Loss: 0.5830117794058494, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 424, Loss: 0.2422216514376026, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 425, Loss: 0.4916680221760007, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 426, Loss: 0.3758262880095552, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 427, Loss: 0.251085231378415, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 428, Loss: 0.23567377378606355, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 429, Loss: 0.29237832253682716, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 430, Loss: 0.35983744675992924, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 431, Loss: 0.2564800573129646, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 432, Loss: 0.4872340799956647, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 433, Loss: 0.32383958979759575, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 434, Loss: 0.24801964468167156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 435, Loss: 0.40833473522265473, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 436, Loss: 0.5129483740349234, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 437, Loss: 0.30909477970202054, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 438, Loss: 0.6374327687182805, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 439, Loss: 0.30484815545525856, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 440, Loss: 0.23403492083812932, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 441, Loss: 0.39833647405828365, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 442, Loss: 0.5510470890893249, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 443, Loss: 0.32663647456176886, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 444, Loss: 0.2774590764994662, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 445, Loss: 0.24290962460844784, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 446, Loss: 0.5212430365881475, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 447, Loss: 0.33169154567041736, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 448, Loss: 0.2688075964186823, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 449, Loss: 0.3199011522538252, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 450, Loss: 0.4535910282257317, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 451, Loss: 0.21383229471348367, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 452, Loss: 0.5090919867794322, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 453, Loss: 0.23597139545853152, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 454, Loss: 0.2957275401519458, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 455, Loss: 0.3755705802750777, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 456, Loss: 0.38381279957861686, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 457, Loss: 0.30905818430992027, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 458, Loss: 0.43081205954946566, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 459, Loss: 0.31275926363230866, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 460, Loss: 0.2641430077398487, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 461, Loss: 0.20710341828751658, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 462, Loss: 0.2915172024865979, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 463, Loss: 0.31084548561780456, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 464, Loss: 0.27815702613834437, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 465, Loss: 0.5244000426427506, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 466, Loss: 0.22300190378221652, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 467, Loss: 0.3646358133889594, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 468, Loss: 0.2754150746232472, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 469, Loss: 0.48955399071795647, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 470, Loss: 0.6946381662773574, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 471, Loss: 0.2650326888847052, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 472, Loss: 0.3909962343212503, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 473, Loss: 0.4324176228134068, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 474, Loss: 0.6485493746734208, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 475, Loss: 0.26859713487831405, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 476, Loss: 0.3624210566667126, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 477, Loss: 0.5476467203870353, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 478, Loss: 0.3752010871476541, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 479, Loss: 0.8136642979002459, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 480, Loss: 0.34749823650361056, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 481, Loss: 0.22316634314921208, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 482, Loss: 0.24509863803137316, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 483, Loss: 0.5265170348940085, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 484, Loss: 0.21997565331527824, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 485, Loss: 0.8999774962102652, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 486, Loss: 0.280204933697049, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 487, Loss: 0.30738851983163973, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 488, Loss: 0.6499177224027131, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 489, Loss: 0.5197735969719035, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 490, Loss: 0.3406902166675325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 491, Loss: 0.2280803463720669, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 492, Loss: 0.2960760212503132, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 493, Loss: 0.5255018097963008, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 494, Loss: 0.4649303594421689, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 495, Loss: 0.41079496086422596, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 496, Loss: 0.2915082394085232, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 497, Loss: 0.3200317358704244, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 498, Loss: 0.3148796231905495, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 499, Loss: 0.36084062686312146, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 500, Loss: 0.3982358344573845, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 501, Loss: 0.2544622050377591, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 502, Loss: 0.32923849194836735, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 503, Loss: 0.5337077878405516, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 504, Loss: 0.43864356181677305, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 505, Loss: 0.30253572885376123, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 506, Loss: 0.35428252865957627, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 507, Loss: 0.4354589581667047, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 508, Loss: 0.3196912578907441, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 509, Loss: 0.2784617574692356, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 510, Loss: 0.3680177070524276, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 511, Loss: 0.5045329140645997, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 512, Loss: 0.3528498030580991, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 513, Loss: 0.32157260522725306, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 514, Loss: 0.47913405662738784, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 515, Loss: 0.31461672480559266, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 516, Loss: 0.2330066085303579, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 517, Loss: 0.3710900491859888, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 518, Loss: 0.26304716592992894, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 519, Loss: 0.4581612602225754, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 520, Loss: 0.3409149822895815, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 521, Loss: 0.3327973590884732, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 522, Loss: 0.4725009280876915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 523, Loss: 0.35232094693059063, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 524, Loss: 0.4078514935227697, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 525, Loss: 0.2539422609903127, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 526, Loss: 0.4003533087138273, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 527, Loss: 0.3344301323878302, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 528, Loss: 0.1993874047404853, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 529, Loss: 0.37445371770360536, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 530, Loss: 0.4560077498758942, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 531, Loss: 0.3614311244859655, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 532, Loss: 0.22465115028385413, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 533, Loss: 0.2892415677450665, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 534, Loss: 0.20739734713820693, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 535, Loss: 0.3228603867950315, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 536, Loss: 0.47568759906138475, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 537, Loss: 0.3712384849224354, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 538, Loss: 0.4999993558977296, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 539, Loss: 0.44376052798153914, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 540, Loss: 0.2255852729382976, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 541, Loss: 0.3286100003577881, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 542, Loss: 0.23345337720778003, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 543, Loss: 0.2789138040374581, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 544, Loss: 0.21654837373104693, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 545, Loss: 0.43787890543418173, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 546, Loss: 0.3445202411072778, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 547, Loss: 0.24950096994420012, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 548, Loss: 0.3017186350951604, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 549, Loss: 0.3135266826792578, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 550, Loss: 0.2892815314024244, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 551, Loss: 0.3477635361387609, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 552, Loss: 0.536416340764438, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 553, Loss: 0.20349457887529437, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 554, Loss: 0.30319029990967217, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 555, Loss: 0.2715240829892398, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 556, Loss: 0.31409002781668066, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 557, Loss: 0.26674470968295616, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 558, Loss: 0.5831509723907874, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 559, Loss: 0.2278960118376599, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 560, Loss: 0.3471557820925145, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 561, Loss: 0.34880057864707764, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 562, Loss: 0.3594909425627466, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 563, Loss: 0.5522311958078807, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 564, Loss: 0.2333164948882978, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 565, Loss: 0.31932854144526734, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 566, Loss: 0.48140823980440306, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 567, Loss: 0.26436366490024593, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 568, Loss: 0.7019279837694642, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 569, Loss: 0.2912753266681754, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 570, Loss: 0.21867940948720005, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 571, Loss: 0.39887256050049724, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 572, Loss: 0.2390001708440324, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 573, Loss: 0.3432276045304515, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 574, Loss: 0.6892888644560269, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 575, Loss: 0.45399833245294974, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 576, Loss: 0.24503266480268615, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 577, Loss: 0.3059157349437693, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 578, Loss: 0.40507948057710075, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 579, Loss: 0.30201581907216996, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 580, Loss: 0.5920578848963876, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 581, Loss: 0.48477939916293467, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 582, Loss: 0.5072877183736845, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 583, Loss: 0.21967893779431408, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 584, Loss: 0.4873206300230612, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 585, Loss: 0.273079473882694, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 586, Loss: 0.29830063938034634, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 587, Loss: 0.37253457717648564, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 588, Loss: 0.4177678460044104, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 589, Loss: 0.4252470813227882, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 590, Loss: 0.2889730034501569, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 591, Loss: 0.5347752038675715, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 592, Loss: 0.4367858572945208, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 593, Loss: 0.2494556891696263, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 594, Loss: 0.33866881345698036, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 595, Loss: 0.361788278718743, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 596, Loss: 0.26539408365888156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 597, Loss: 0.35236476866275285, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 598, Loss: 0.5323077899901068, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 599, Loss: 0.31046036986387326, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 600, Loss: 0.40383653273292414, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 601, Loss: 0.5704349446775828, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 602, Loss: 0.5520654104654692, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 603, Loss: 0.24279992429971062, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 604, Loss: 0.3461004035110199, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 605, Loss: 0.4608667949874344, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 606, Loss: 0.22287339660629507, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 607, Loss: 0.27765873757505977, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 608, Loss: 0.22640254195359577, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 609, Loss: 0.2920720878695211, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 610, Loss: 0.28090658989027156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 611, Loss: 0.5840905583011649, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 612, Loss: 0.26455816312767194, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 613, Loss: 0.2933424477092615, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 614, Loss: 0.5110126545821265, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 615, Loss: 0.3914104409879775, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 616, Loss: 0.7978283301911986, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 617, Loss: 0.34055422561804277, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 618, Loss: 0.21471182954210677, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 619, Loss: 0.3544931441578635, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 620, Loss: 0.3322865712161881, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 621, Loss: 0.2727222348382356, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 622, Loss: 0.3023323712793119, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 623, Loss: 0.31396663487525495, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 624, Loss: 0.4623777275867459, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 625, Loss: 0.29958775841496166, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 626, Loss: 0.2539513766125058, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 627, Loss: 0.2740439628421636, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 628, Loss: 0.32594281229736677, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 629, Loss: 0.2747520004049225, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 630, Loss: 0.31514871626185714, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 631, Loss: 0.5477823352307613, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 632, Loss: 0.23772562550306736, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 633, Loss: 0.3812882456186716, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 634, Loss: 0.23127299363927428, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 635, Loss: 0.3914614403461808, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 636, Loss: 0.44377774573021156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 637, Loss: 0.2619431614613987, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 638, Loss: 0.4046277544746186, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 639, Loss: 0.25654863116558735, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 640, Loss: 0.22677170769878072, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 641, Loss: 0.3151873958136142, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 642, Loss: 0.33875653671069983, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 643, Loss: 0.4322575004057291, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 644, Loss: 0.5451821129505239, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 645, Loss: 0.45370980947321926, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 646, Loss: 0.3169914787758883, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 647, Loss: 0.27539141197974226, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 648, Loss: 0.30933172366405487, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 649, Loss: 0.2845217676874401, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 650, Loss: 0.34817830338529393, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 651, Loss: 0.30682542649507694, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 652, Loss: 0.20873128787992526, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 653, Loss: 0.28805846532182183, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 654, Loss: 0.22623591864153275, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 655, Loss: 0.3924553520087509, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 656, Loss: 0.5481262649266986, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 657, Loss: 0.29906274864624277, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 658, Loss: 0.3994685057939972, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 659, Loss: 0.3596194723250906, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 660, Loss: 0.25690863133520403, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 661, Loss: 0.23168016207043193, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 662, Loss: 0.20992808117583026, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 663, Loss: 0.3591670333065557, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 664, Loss: 0.2652040830193958, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 665, Loss: 0.33190224695647813, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 666, Loss: 0.4297890121165099, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 667, Loss: 0.42230937978162175, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 668, Loss: 0.3857346256744161, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 669, Loss: 0.5265103284010848, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 670, Loss: 0.23440066074399757, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 671, Loss: 0.27455503783442536, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 672, Loss: 0.22961286773708584, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 673, Loss: 0.3898673412206182, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 674, Loss: 0.3715941434281902, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 675, Loss: 0.42850440766667264, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 676, Loss: 0.2825189540698537, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 677, Loss: 0.2565353900190261, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 678, Loss: 0.4720815802916747, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 679, Loss: 0.27909795317955427, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 680, Loss: 0.20721507551981005, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 681, Loss: 0.3688092877222705, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 682, Loss: 0.517510726825973, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 683, Loss: 0.4103436632843797, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 684, Loss: 0.33697000316331643, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 685, Loss: 0.4909966984190534, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 686, Loss: 0.38915188654012567, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 687, Loss: 0.29577966668209116, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 688, Loss: 0.23785195992906516, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 689, Loss: 0.31045960104043446, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 690, Loss: 0.39146463929774733, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 691, Loss: 0.26600875800154405, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 692, Loss: 0.6690899580015962, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 693, Loss: 0.30213923954425737, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 694, Loss: 0.4995277646148333, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 695, Loss: 0.6024062685146274, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 696, Loss: 0.3848401743832976, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 697, Loss: 0.28881820451125867, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 698, Loss: 0.2986768832637674, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 699, Loss: 0.20729687945901606, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 700, Loss: 0.32951439185899156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 701, Loss: 0.4597823502742858, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 702, Loss: 0.2665702484011903, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 703, Loss: 0.42037691800194493, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 704, Loss: 0.32796475360312183, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 705, Loss: 0.2126243889718574, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 706, Loss: 0.5211453985670488, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 707, Loss: 0.2837894795652297, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 708, Loss: 0.41395479925248757, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 709, Loss: 0.4281851725624999, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 710, Loss: 0.2755303668101037, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 711, Loss: 0.6916627877687824, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 712, Loss: 0.5832676038966733, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 713, Loss: 0.22412283479091036, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 714, Loss: 0.5739810952533477, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 715, Loss: 0.2500550697438277, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 716, Loss: 0.43747337180465407, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 717, Loss: 0.3123822876915141, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 718, Loss: 0.5377256492195776, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 719, Loss: 0.32765557955744007, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 720, Loss: 0.29474066094194773, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 721, Loss: 0.6727019096363602, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 722, Loss: 0.386396017650798, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 723, Loss: 0.3812373867010792, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 724, Loss: 0.3033350833129924, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 725, Loss: 0.4165950820663008, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 726, Loss: 0.2605340851902387, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 727, Loss: 0.5421447859262252, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 728, Loss: 0.27756669806283063, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 729, Loss: 0.31529876913319116, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 730, Loss: 0.3489489393548669, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 731, Loss: 0.36303242087337456, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 732, Loss: 0.25613595154867763, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 733, Loss: 0.31075499123183853, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 734, Loss: 0.5853756074087741, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 735, Loss: 0.21183412461401913, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 736, Loss: 0.2754167323990765, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 737, Loss: 0.2997484625003767, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 738, Loss: 0.30711743838950745, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 739, Loss: 0.45696212366912886, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 740, Loss: 0.21252903301760878, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 741, Loss: 0.4192331782341807, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 742, Loss: 0.5114254176184077, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 743, Loss: 0.300192329744729, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 744, Loss: 0.24179592297798477, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 745, Loss: 0.3849633059001795, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 746, Loss: 0.6803031306548848, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 747, Loss: 0.7029707813214241, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 748, Loss: 0.40997801959013047, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 749, Loss: 0.31889351004029487, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 750, Loss: 0.3261443808600559, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 751, Loss: 0.32986929784190117, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 752, Loss: 0.4195773269912816, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 753, Loss: 0.48444162373146904, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 754, Loss: 0.2565372908142943, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 755, Loss: 0.27815643045735966, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 756, Loss: 0.28627433121534535, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 757, Loss: 0.2953565937921905, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 758, Loss: 0.3663396571552684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 759, Loss: 0.5964771823180735, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 760, Loss: 0.24163409965319524, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 761, Loss: 0.4184245967578542, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 762, Loss: 0.30588512199533857, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 763, Loss: 0.20027194412357513, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 764, Loss: 0.20319041363544985, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 765, Loss: 0.35278170539733034, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 766, Loss: 0.25888320943416937, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 767, Loss: 0.5078949970840132, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 768, Loss: 0.3680379106986147, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 769, Loss: 0.5311521694332189, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 770, Loss: 0.3359393066590779, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 771, Loss: 0.3492348679633044, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 772, Loss: 0.3110679307448716, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 773, Loss: 0.26822383354178514, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 774, Loss: 0.3528039328218794, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 775, Loss: 0.3652189915773656, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 776, Loss: 0.3737272004828014, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 777, Loss: 0.2664156733102381, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 778, Loss: 0.322325489512774, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 779, Loss: 0.3820059717173998, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 780, Loss: 0.26143043079294803, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 781, Loss: 0.91199803922947, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 782, Loss: 0.29333921157683457, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 783, Loss: 0.2136478622483375, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 784, Loss: 0.23098787553746145, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 785, Loss: 0.26523198691847166, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 786, Loss: 0.6474695153647213, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 787, Loss: 0.4888345433993304, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 788, Loss: 0.28270314672433294, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 789, Loss: 0.3967025191585913, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 790, Loss: 0.3169441974523698, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 791, Loss: 0.22190922034254162, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 792, Loss: 0.34035802071427834, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 793, Loss: 0.7678113379212943, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 794, Loss: 0.33276303925421363, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 795, Loss: 0.288088371960622, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 796, Loss: 0.7721735255679729, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 797, Loss: 0.3955817168251579, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 798, Loss: 0.23431038230100265, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 799, Loss: 0.27972022579575195, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 800, Loss: 0.28383161865090145, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 801, Loss: 0.3000940917969748, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 802, Loss: 0.4134902240058852, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 803, Loss: 0.4668571486387413, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 804, Loss: 0.4583096307345944, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 805, Loss: 0.5192974194738532, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 806, Loss: 0.29929453242197046, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 807, Loss: 0.5199941326872819, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 808, Loss: 0.24376966055490956, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 809, Loss: 0.4592646149062467, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 810, Loss: 0.34671013453680866, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 811, Loss: 0.5995374118608343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 812, Loss: 0.3837936554152854, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 813, Loss: 0.37120307241619377, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 814, Loss: 0.28368957555929314, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 815, Loss: 0.24296452902649204, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 816, Loss: 0.43981275902209693, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 817, Loss: 0.2747832417492556, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 818, Loss: 0.2481687441465017, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 819, Loss: 0.2923794745128953, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 820, Loss: 0.3875225524920096, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 821, Loss: 0.32774297081458875, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 822, Loss: 0.33744754428284746, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 823, Loss: 0.49720631166090634, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 824, Loss: 0.38432579333476824, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 825, Loss: 0.24779913082625785, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 826, Loss: 0.4510733360628647, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 827, Loss: 0.24456664318783372, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 828, Loss: 0.30435791701604165, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 829, Loss: 0.27635866770958406, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 830, Loss: 0.31996659674989336, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 831, Loss: 0.2975498875107013, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 832, Loss: 0.23695707112086592, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 833, Loss: 0.6688856048703254, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 834, Loss: 0.40376836615301426, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 835, Loss: 0.3535736718435632, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 836, Loss: 0.33207340486907055, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 837, Loss: 0.41271016531309057, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 838, Loss: 0.2126229813515252, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 839, Loss: 0.33711344424146217, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 840, Loss: 0.4664144213237633, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 841, Loss: 0.3606768780241238, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 842, Loss: 0.284727252289828, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 843, Loss: 0.28942702095141853, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 844, Loss: 0.32895615209811857, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 845, Loss: 0.3415568416663408, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 846, Loss: 0.5112921008200846, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 847, Loss: 0.32042886414234684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 848, Loss: 0.43437212789779445, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 849, Loss: 0.3690702653048227, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 850, Loss: 0.5435405497278782, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 851, Loss: 0.43896327268550317, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 852, Loss: 0.39486212340002513, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 853, Loss: 0.40324601653466585, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 854, Loss: 0.3520130991969368, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 855, Loss: 0.2845778689090094, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 856, Loss: 0.335968666807867, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 857, Loss: 0.6365118386948339, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 858, Loss: 0.3932988827376948, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 859, Loss: 0.5199072038250157, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 860, Loss: 0.416016570054225, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 861, Loss: 0.3610517810257311, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 862, Loss: 0.29000378162300056, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 863, Loss: 0.3017750682841012, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 864, Loss: 0.2814230065355683, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 865, Loss: 0.4576423950657671, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 866, Loss: 0.23310714236144153, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 867, Loss: 0.31921246749038756, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 868, Loss: 0.28854717026305104, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 869, Loss: 0.3533401742676971, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 870, Loss: 0.4605261073986604, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 871, Loss: 0.300514550724648, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 872, Loss: 0.32290161999946176, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 873, Loss: 0.4123385497218174, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 874, Loss: 0.367335345903476, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 875, Loss: 0.22541877447770772, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 876, Loss: 0.30243771423792554, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 877, Loss: 0.25184915365045984, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 878, Loss: 0.36958301702148644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 879, Loss: 0.33545127923036977, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 880, Loss: 0.2558296556543799, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 881, Loss: 0.3598099080196816, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 882, Loss: 0.44319183211650093, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 883, Loss: 0.23303597303348414, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 884, Loss: 0.2842641477425638, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 885, Loss: 0.39669421064019394, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 886, Loss: 0.3373863810494584, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 887, Loss: 0.47903491046564783, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 888, Loss: 0.4376021243026017, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 889, Loss: 0.557106126615648, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 890, Loss: 0.2238268634479298, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 891, Loss: 0.3276725613886503, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 892, Loss: 0.4049867851615595, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 893, Loss: 0.24838144807121268, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 894, Loss: 0.28503426459381564, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 895, Loss: 0.40602060074647817, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 896, Loss: 0.4744334620718256, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 897, Loss: 0.25786330321866, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 898, Loss: 0.29962730919185865, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 899, Loss: 0.22738242678283693, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 900, Loss: 0.5467915125228927, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 901, Loss: 0.4962566172453003, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 902, Loss: 0.3995582498711425, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 903, Loss: 0.5224098155209982, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 904, Loss: 0.31136006262260624, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 905, Loss: 0.5712163775423232, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 906, Loss: 0.4443262082492371, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 907, Loss: 0.24532647748598752, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 908, Loss: 0.26949401494492653, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 909, Loss: 0.2909527182430747, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 910, Loss: 0.5004098354602466, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 911, Loss: 0.24254150529207172, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 912, Loss: 0.3736520739759796, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 913, Loss: 0.24012869515359825, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 914, Loss: 0.2820717962012258, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 915, Loss: 0.5693512442951408, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 916, Loss: 0.310719051960522, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 917, Loss: 0.5131063950260998, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 918, Loss: 0.22146091686962416, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 919, Loss: 0.27794305949399684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 920, Loss: 0.3434078795030655, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 921, Loss: 0.2799375759794649, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 922, Loss: 0.2429035317247743, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 923, Loss: 0.372410392578862, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 924, Loss: 0.39752703116401644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 925, Loss: 0.610715158588897, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 926, Loss: 0.6737891193209387, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 927, Loss: 0.5639820294415758, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 928, Loss: 0.3165873466552301, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 929, Loss: 0.22675245104147723, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 930, Loss: 0.5536798018558851, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 931, Loss: 0.6070532424549984, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 932, Loss: 0.3939847318026011, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 933, Loss: 0.20172780155122072, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 934, Loss: 0.2160312900572117, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 935, Loss: 0.562395089926225, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 936, Loss: 0.319748838438584, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 937, Loss: 0.2453343729360668, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 938, Loss: 0.2877020659726109, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 939, Loss: 0.2621399088292399, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 940, Loss: 0.35690479861324953, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 941, Loss: 0.3320657165141403, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 942, Loss: 0.24024389752177633, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 943, Loss: 0.3624638394168725, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 944, Loss: 0.3861142712190123, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 945, Loss: 0.4152872191201178, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 946, Loss: 0.36003648489139406, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 947, Loss: 0.2360306883147678, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 948, Loss: 0.31076726082876055, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 949, Loss: 0.7174599783335527, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 950, Loss: 0.303675690096267, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 951, Loss: 0.24575796075353915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 952, Loss: 0.6065033638530283, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 953, Loss: 0.4083173040922463, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 954, Loss: 0.24079180504217984, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 955, Loss: 0.458095614692995, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 956, Loss: 0.41584655679985394, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 957, Loss: 0.33421847811144284, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 958, Loss: 0.24258763213122342, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 959, Loss: 0.44122332488486143, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 960, Loss: 0.2829618397170276, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 961, Loss: 0.29647546806488306, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 962, Loss: 0.33745633461010777, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 963, Loss: 0.40655091445365443, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 964, Loss: 0.33940342441680854, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 965, Loss: 0.3661675792485348, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 966, Loss: 0.40218673905814706, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 967, Loss: 0.2818321707594237, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 968, Loss: 0.31943392657478775, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 969, Loss: 0.39582203287464035, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 970, Loss: 0.29221998172282826, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 971, Loss: 0.2567747635956246, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 972, Loss: 0.373401690484373, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 973, Loss: 0.32193496688997103, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 974, Loss: 0.8344998053729356, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 975, Loss: 0.3326132174087948, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 976, Loss: 0.27684873836749446, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 977, Loss: 0.27375495569418234, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 978, Loss: 0.41286987834035305, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 979, Loss: 0.21282323843806797, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 980, Loss: 0.33378090976236974, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 981, Loss: 0.41315880719035625, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 982, Loss: 0.2661818963910783, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 983, Loss: 0.24292469627225458, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 984, Loss: 0.38108294628037, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 985, Loss: 0.30617169420062423, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 986, Loss: 0.38947186327890293, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 987, Loss: 0.46967830443772, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 988, Loss: 0.2535553050309881, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 989, Loss: 0.2728288636476628, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 990, Loss: 0.6183472527369926, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 991, Loss: 0.3542203958750988, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 992, Loss: 0.22202428336855384, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 993, Loss: 0.2894496624389343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 994, Loss: 0.31496098874887296, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 995, Loss: 0.3775610655570488, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 996, Loss: 0.3826625996979017, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 997, Loss: 0.33252013765945215, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 998, Loss: 0.5309783487727217, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 999, Loss: 0.25551331261556715, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1000, Loss: 0.39687763183295033, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1001, Loss: 0.4742318423531595, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1002, Loss: 0.6094873258130458, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1003, Loss: 0.2786047438137876, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1004, Loss: 0.2457499154467791, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1005, Loss: 0.46693324595581553, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1006, Loss: 0.5992869315729683, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1007, Loss: 0.5460933678358583, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1008, Loss: 0.21130136273946137, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1009, Loss: 0.6044787089815371, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1010, Loss: 0.24149449596557937, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1011, Loss: 0.2699300191137033, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1012, Loss: 0.2889432584639786, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1013, Loss: 0.4018520023691897, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1014, Loss: 0.36638400878940314, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1015, Loss: 0.3678552984826421, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1016, Loss: 0.2535018769399741, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1017, Loss: 0.4498409295142204, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1018, Loss: 0.4571563705604612, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1019, Loss: 0.45110224746712707, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1020, Loss: 0.3958521579349037, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1021, Loss: 0.4439735952860845, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1022, Loss: 0.40916284941718417, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1023, Loss: 0.2572811439459554, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1024, Loss: 0.35234852471113753, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1025, Loss: 0.23691514962363963, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1026, Loss: 0.42681864790406565, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1027, Loss: 0.3619208163941718, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1028, Loss: 0.3752953398991389, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1029, Loss: 0.44136316846729895, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1030, Loss: 0.30154341615023633, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1031, Loss: 0.3070056443766749, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1032, Loss: 0.3989289582197889, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1033, Loss: 0.4338510225010969, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1034, Loss: 0.38549594414478777, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1035, Loss: 0.5203540378105012, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1036, Loss: 0.29155610028925893, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1037, Loss: 0.5606275279184285, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1038, Loss: 0.3035854764730725, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1039, Loss: 0.30115833065357134, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1040, Loss: 0.7718080465427261, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1041, Loss: 0.2789901385332174, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1042, Loss: 0.22929601393391688, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1043, Loss: 0.3217289005945745, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1044, Loss: 0.3786680274542228, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1045, Loss: 0.49063038252213304, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1046, Loss: 0.3174869442359387, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1047, Loss: 0.38318991614108877, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1048, Loss: 0.29167865676848115, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1049, Loss: 0.6080037408671974, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1050, Loss: 0.37957681497382184, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1051, Loss: 0.5105384822178621, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1052, Loss: 0.578544539523331, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1053, Loss: 0.37414367724223485, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1054, Loss: 0.2712325606654739, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1055, Loss: 0.22962939259092174, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1056, Loss: 0.2517244496171752, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1057, Loss: 0.21866218198375117, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1058, Loss: 0.5152122087116421, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1059, Loss: 0.24606807059294966, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1060, Loss: 0.3967878583523276, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1061, Loss: 0.359141471564958, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1062, Loss: 0.4131999011785513, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1063, Loss: 0.2636857541975201, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1064, Loss: 0.6440266841052977, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1065, Loss: 0.280450820070029, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1066, Loss: 0.38079208537378473, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1067, Loss: 0.26549206697356, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1068, Loss: 0.2574350372247786, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1069, Loss: 0.3865162876585269, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1070, Loss: 0.21749801098695665, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1071, Loss: 0.30267698425972234, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1072, Loss: 0.31296277150886087, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1073, Loss: 0.38546176649905894, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1074, Loss: 0.28610986920613185, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1075, Loss: 0.48627868823805165, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1076, Loss: 0.2972082630223451, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1077, Loss: 0.24238127626933273, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1078, Loss: 0.3300538269942384, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1079, Loss: 0.3492278028050747, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1080, Loss: 0.22673563530508184, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1081, Loss: 0.29671991082303606, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1082, Loss: 0.2390111148526406, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1083, Loss: 0.2895021072335102, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1084, Loss: 0.3155566161571942, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1085, Loss: 0.5929577532357552, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1086, Loss: 0.35980310435560253, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1087, Loss: 0.360854342334323, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1088, Loss: 0.43460855093636874, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1089, Loss: 0.34725042636519987, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1090, Loss: 0.533780398099722, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1091, Loss: 0.33255996798767673, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1092, Loss: 0.6775178328808399, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1093, Loss: 0.4737139660537325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1094, Loss: 0.6715223170533777, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1095, Loss: 0.36242385908333297, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1096, Loss: 0.33158762853949697, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1097, Loss: 0.29968520262716686, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1098, Loss: 0.2520937667260441, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1099, Loss: 0.28187735568587524, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1100, Loss: 0.2997812961236343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1101, Loss: 0.8429686539202063, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1102, Loss: 0.465542984896927, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1103, Loss: 0.46898056592338766, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1104, Loss: 0.8240624651916988, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1105, Loss: 0.32157418950260247, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1106, Loss: 0.2711586796037996, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1107, Loss: 0.39825692102799176, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1108, Loss: 0.394920740770758, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1109, Loss: 0.27616809425547606, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1110, Loss: 0.36269169325610723, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1111, Loss: 0.8203440260664787, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1112, Loss: 0.38890532354924334, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1113, Loss: 0.2540833361407698, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1114, Loss: 0.33832787379263374, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1115, Loss: 0.2634144748723824, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1116, Loss: 0.44546352236325537, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1117, Loss: 0.32500582967067404, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1118, Loss: 0.3877779823471711, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1119, Loss: 0.32943462198596246, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1120, Loss: 0.3746797244030621, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1121, Loss: 0.3090013873251132, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1122, Loss: 0.30500599964600006, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1123, Loss: 0.32172172885995065, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1124, Loss: 0.2338939374517494, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1125, Loss: 0.2867463171745315, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1126, Loss: 0.4391565087013479, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1127, Loss: 0.3189795775982749, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1128, Loss: 0.47742448953178535, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1129, Loss: 0.3458365414484357, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1130, Loss: 0.31484999034371763, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1131, Loss: 0.7315541576933627, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1132, Loss: 0.3061731034562702, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1133, Loss: 0.4639412096753998, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1134, Loss: 0.5410929824821142, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1135, Loss: 0.2742156100265417, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1136, Loss: 0.3008808463798819, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1137, Loss: 0.45838159001389944, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1138, Loss: 0.3467223289893516, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1139, Loss: 0.328145912955795, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1140, Loss: 0.3232460092249685, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1141, Loss: 0.5680871936777145, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1142, Loss: 0.3785639161510712, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1143, Loss: 0.4105297858846356, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1144, Loss: 0.5507030932681789, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1145, Loss: 0.45921660827141453, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1146, Loss: 0.2683140037344419, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1147, Loss: 0.3936351493443522, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1148, Loss: 0.2474915771226093, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1149, Loss: 0.453775357685391, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1150, Loss: 0.5290469418142946, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1151, Loss: 0.5489262913415751, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1152, Loss: 0.30114549011170905, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1153, Loss: 0.20413196025119076, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1154, Loss: 0.3642431816945098, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1155, Loss: 0.45008986932638695, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1156, Loss: 0.36999973806231157, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1157, Loss: 0.2906402247467222, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1158, Loss: 0.3822687725195214, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1159, Loss: 0.3020196800302961, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1160, Loss: 0.5656641842832077, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1161, Loss: 0.5526345261533712, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1162, Loss: 0.36989383044808377, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1163, Loss: 0.2637827179538763, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1164, Loss: 0.35307320736950476, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1165, Loss: 0.3555981801260517, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1166, Loss: 0.37068976600171755, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1167, Loss: 0.2196275832588973, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1168, Loss: 0.28864185682011745, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1169, Loss: 0.28703488999131327, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1170, Loss: 0.30085321717803265, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1171, Loss: 0.4971812495193099, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1172, Loss: 0.3661274930065605, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1173, Loss: 0.3856724617632623, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1174, Loss: 0.3209250963133291, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1175, Loss: 0.2750138678216062, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1176, Loss: 0.22882479140593784, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1177, Loss: 0.29826557314195645, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1178, Loss: 0.2769282260332445, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1179, Loss: 0.293765376115587, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1180, Loss: 0.33423503559808054, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1181, Loss: 0.31329182321529386, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1182, Loss: 0.5514113758358633, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1183, Loss: 0.3101597815436041, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1184, Loss: 0.5842912457822084, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1185, Loss: 0.23554565847097192, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1186, Loss: 0.20445018245999963, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1187, Loss: 0.3497769191853992, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1188, Loss: 0.4904306266148439, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1189, Loss: 0.2566554688634951, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1190, Loss: 0.4514795385320557, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1191, Loss: 0.8521656830075134, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1192, Loss: 0.38371289768963734, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1193, Loss: 0.37771197200659945, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1194, Loss: 0.46063886595327597, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1195, Loss: 0.21782445053282845, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1196, Loss: 0.3935579563976628, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1197, Loss: 0.24749088826904664, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1198, Loss: 0.3184831807359496, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1199, Loss: 0.32726005181209883, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1200, Loss: 0.32019052835039474, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1201, Loss: 0.234256595472786, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1202, Loss: 0.2798766490596848, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1203, Loss: 0.5714672417521025, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1204, Loss: 0.23897258626305115, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1205, Loss: 0.19250994838269606, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1206, Loss: 0.23527418195681424, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1207, Loss: 0.4505976782213912, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1208, Loss: 0.3260746964742108, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1209, Loss: 0.26206283562336796, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1210, Loss: 0.3168853289559719, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1211, Loss: 0.7040748221811569, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1212, Loss: 0.37306291857737817, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1213, Loss: 0.42562698658016057, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1214, Loss: 0.3044340488598663, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1215, Loss: 0.2620616900098978, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1216, Loss: 0.3745963560665886, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1217, Loss: 0.35456644500984646, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1218, Loss: 0.3496820683334143, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1219, Loss: 0.24355194445578127, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1220, Loss: 0.3346046444028807, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1221, Loss: 0.6411664579753917, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1222, Loss: 0.504381759530629, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1223, Loss: 0.24552511564769527, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1224, Loss: 0.2558263923474938, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1225, Loss: 0.4184525651325043, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1226, Loss: 0.311665089117095, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1227, Loss: 0.3396759645443848, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1228, Loss: 0.23466862541226857, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1229, Loss: 0.6462180378083932, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1230, Loss: 0.22967534776082432, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1231, Loss: 0.2899867910434118, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1232, Loss: 0.25556640952529264, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1233, Loss: 0.22477347572222217, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1234, Loss: 0.4681626783419707, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1235, Loss: 0.4741581809631167, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1236, Loss: 0.6378755264324995, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1237, Loss: 0.24112396592614704, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1238, Loss: 0.34942259692054556, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1239, Loss: 0.4146142783039904, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1240, Loss: 0.5487068971112137, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1241, Loss: 0.2678619537470559, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1242, Loss: 0.23107475494560853, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1243, Loss: 0.34889243248433854, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1244, Loss: 0.22489814256728063, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1245, Loss: 0.4603034389265357, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1246, Loss: 0.31220794092852755, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1247, Loss: 0.28349669777587716, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1248, Loss: 0.26029522285881285, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1249, Loss: 0.22208300730810823, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1250, Loss: 0.5488672571065824, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1251, Loss: 0.35048531478067935, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1252, Loss: 0.49868438334343596, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1253, Loss: 0.32321866910789, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1254, Loss: 0.4856443023952177, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1255, Loss: 0.2901989597136172, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1256, Loss: 0.40205327229955684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1257, Loss: 0.3478850372209119, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1258, Loss: 0.32079998514875335, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1259, Loss: 0.3103228352261336, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1260, Loss: 0.29005264863724833, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1261, Loss: 0.34917319476038544, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1262, Loss: 0.6101064130016046, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1263, Loss: 0.2909311465540687, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1264, Loss: 0.22508402655617848, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1265, Loss: 0.278447122877335, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1266, Loss: 0.2671326102601582, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1267, Loss: 0.30659873343280203, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1268, Loss: 0.5402406505404734, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1269, Loss: 0.31030855029643106, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1270, Loss: 0.407595488648122, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1271, Loss: 0.3599875847742866, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1272, Loss: 0.24294873219057173, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1273, Loss: 0.2726730433165252, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1274, Loss: 0.5578696277777966, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1275, Loss: 0.32778201168941934, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1276, Loss: 0.4792286265157778, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1277, Loss: 0.3006691643488765, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1278, Loss: 0.3159857645708183, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1279, Loss: 0.3113340053462634, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1280, Loss: 0.34270109129744175, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1281, Loss: 0.4325598970892613, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1282, Loss: 0.31962162932101185, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1283, Loss: 0.2769018602032761, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1284, Loss: 0.30654725266258287, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1285, Loss: 0.43528218556223397, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1286, Loss: 0.41302121314237616, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1287, Loss: 0.3205053992888939, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1288, Loss: 0.46607316987990777, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1289, Loss: 0.2611274731269812, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1290, Loss: 0.39933792681420566, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1291, Loss: 0.3485067587973496, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1292, Loss: 0.3272951781397104, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1293, Loss: 0.4948642021252199, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1294, Loss: 0.2621644636162941, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1295, Loss: 0.3411333631466776, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1296, Loss: 0.3603605037112641, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1297, Loss: 0.47580796263436587, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1298, Loss: 0.5789583308933373, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1299, Loss: 0.4476385855158894, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1300, Loss: 0.3980724186098145, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1301, Loss: 0.3264771662541281, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1302, Loss: 0.2572748402824666, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1303, Loss: 0.4062638266999964, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1304, Loss: 0.23779413388244156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1305, Loss: 0.4321505857965319, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1306, Loss: 0.32144870232017614, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1307, Loss: 0.48437360971117094, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1308, Loss: 0.26278538124139794, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1309, Loss: 0.4930140827315465, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1310, Loss: 0.239150995316108, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1311, Loss: 0.2629765255457507, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1312, Loss: 0.47909829172781915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1313, Loss: 0.29181252322386825, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1314, Loss: 0.7335682408284048, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1315, Loss: 0.36013856169716174, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1316, Loss: 0.27011419027442807, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1317, Loss: 0.2819571113155508, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1318, Loss: 0.20038314730040285, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1319, Loss: 0.3770626159938957, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1320, Loss: 0.23831518874766036, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1321, Loss: 0.42375292097710837, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1322, Loss: 0.36629762958960277, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1323, Loss: 0.4239546471478056, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1324, Loss: 0.39663072669549726, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1325, Loss: 0.48563131336364634, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1326, Loss: 0.33588661669171227, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1327, Loss: 0.36738625006346626, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1328, Loss: 0.44235003693074365, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1329, Loss: 0.3469479272175488, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1330, Loss: 0.28234226591023365, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1331, Loss: 0.21790514526706503, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1332, Loss: 0.40905821261261044, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1333, Loss: 0.29424485702918457, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1334, Loss: 0.4278887542579759, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1335, Loss: 0.34795144276075957, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1336, Loss: 0.6245143435523619, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1337, Loss: 0.21747962792099407, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1338, Loss: 0.3528889297307306, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1339, Loss: 0.4155055313363611, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1340, Loss: 0.5548869603048916, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1341, Loss: 0.44925571916596485, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1342, Loss: 0.342709654039731, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1343, Loss: 0.3156460599447523, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1344, Loss: 0.2691896219928698, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1345, Loss: 0.20860540376597564, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1346, Loss: 0.255496374730512, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1347, Loss: 0.5965587880126053, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1348, Loss: 0.5108971490279473, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1349, Loss: 0.5757372406203584, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1350, Loss: 0.2852445103485617, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1351, Loss: 0.3200948407397091, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1352, Loss: 0.42905746417919094, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1353, Loss: 0.3704957533115142, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1354, Loss: 0.3703850469439593, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1355, Loss: 0.3998909253393345, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1356, Loss: 0.33478254866939505, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1357, Loss: 0.3148982538868923, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1358, Loss: 0.2857669365315225, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1359, Loss: 0.3290679931802656, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1360, Loss: 0.3489477633155784, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1361, Loss: 0.41256511511298266, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1362, Loss: 0.38810698588767195, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1363, Loss: 0.5697205253781331, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1364, Loss: 0.3472219538659448, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1365, Loss: 0.27015297464119714, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1366, Loss: 0.2991230408305081, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1367, Loss: 0.3238647692119453, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1368, Loss: 0.48465140850723337, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1369, Loss: 0.3033125772227464, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1370, Loss: 0.37670708097635797, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1371, Loss: 0.33681958573923276, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1372, Loss: 0.35384637979978806, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1373, Loss: 0.4530936663994368, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1374, Loss: 0.4980908087210868, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1375, Loss: 0.21301852134825544, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1376, Loss: 0.3546309874728236, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1377, Loss: 0.6457501506065295, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1378, Loss: 0.4172926757198884, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1379, Loss: 0.4300546204309538, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1380, Loss: 0.6018458621584664, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1381, Loss: 0.49117825409389526, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1382, Loss: 0.24492033831245405, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1383, Loss: 0.3769791608413232, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1384, Loss: 0.24146334339210332, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1385, Loss: 0.36385588874649355, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1386, Loss: 0.32161699905785823, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1387, Loss: 0.3422554234491835, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1388, Loss: 0.33327054180066285, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1389, Loss: 0.4061720061397409, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1390, Loss: 0.42271606639975, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1391, Loss: 0.5385484030133988, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1392, Loss: 0.42450699540215414, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1393, Loss: 0.41146513467913404, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1394, Loss: 0.40822945924239723, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1395, Loss: 0.2896456093333025, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1396, Loss: 0.24528021339457673, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1397, Loss: 0.2639279814954556, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1398, Loss: 0.485319882226126, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1399, Loss: 0.4647651326538218, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1400, Loss: 0.4893539229219461, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1401, Loss: 0.2874142790625361, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1402, Loss: 0.21482322407115814, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1403, Loss: 0.20980292317952126, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1404, Loss: 0.4935683420253343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1405, Loss: 0.2430965978275907, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1406, Loss: 0.3713316871358917, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1407, Loss: 0.2660061103599892, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1408, Loss: 0.30987429593946353, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1409, Loss: 0.24923658809836546, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1410, Loss: 0.32660498317287573, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1411, Loss: 0.2862618092807976, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1412, Loss: 0.3235281790734821, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1413, Loss: 0.43184348297789216, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1414, Loss: 0.49866002732086123, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1415, Loss: 0.5474592702574047, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1416, Loss: 0.6491724714493811, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1417, Loss: 0.27904909361794833, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1418, Loss: 0.3942626469587872, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1419, Loss: 0.3042409003508508, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1420, Loss: 0.20513439733566802, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1421, Loss: 0.21981662515438832, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1422, Loss: 0.359604574069898, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1423, Loss: 0.4067836950344423, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1424, Loss: 0.506599662447728, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1425, Loss: 0.5285751630273194, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1426, Loss: 0.23443712256672933, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1427, Loss: 0.4202011787691774, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1428, Loss: 0.3927541310371506, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1429, Loss: 0.3772909444978908, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1430, Loss: 0.32834549133419577, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1431, Loss: 0.2871327548330425, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1432, Loss: 0.6023392045681626, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1433, Loss: 0.37764264765722644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1434, Loss: 0.24355699007636092, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1435, Loss: 0.23924864137741006, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1436, Loss: 0.45890260850162934, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1437, Loss: 0.29650048479447866, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1438, Loss: 0.237745447738895, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1439, Loss: 0.27126044563022467, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1440, Loss: 0.2562287543454495, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1441, Loss: 0.32237226931591767, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1442, Loss: 0.4386726011602984, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1443, Loss: 0.36234814701713014, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1444, Loss: 0.3646576677876286, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1445, Loss: 0.4826211148616656, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1446, Loss: 0.2136539169664236, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1447, Loss: 0.21248610454553724, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1448, Loss: 0.30563919816326407, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1449, Loss: 0.2290466516192644, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1450, Loss: 0.23559803738181295, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1451, Loss: 0.23649669429813966, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1452, Loss: 0.302039078254564, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1453, Loss: 0.26939423782129535, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1454, Loss: 0.3245450665272902, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1455, Loss: 0.37359237195104505, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1456, Loss: 0.2806618852904519, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1457, Loss: 0.2763539994710487, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1458, Loss: 0.28209570534617584, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1459, Loss: 0.4127243928450879, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1460, Loss: 0.27892824988018183, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1461, Loss: 0.622478322102724, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1462, Loss: 0.3662356273176971, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1463, Loss: 0.25888478491831624, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1464, Loss: 0.3670862803473147, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1465, Loss: 0.21655241733598077, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1466, Loss: 0.3094075557083325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1467, Loss: 0.45013881592118987, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1468, Loss: 0.3222748858936102, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1469, Loss: 0.4405048113870072, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1470, Loss: 0.6145481998536976, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1471, Loss: 0.2335644731794464, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1472, Loss: 0.3435478889453816, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1473, Loss: 0.2573535268650977, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1474, Loss: 0.4249059858748605, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1475, Loss: 0.53378117003313, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1476, Loss: 0.2885395504573313, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1477, Loss: 0.35020508679348666, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1478, Loss: 0.44242139532142016, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1479, Loss: 0.2020825407793126, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1480, Loss: 0.6857922099660603, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1481, Loss: 0.5987788252269403, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1482, Loss: 0.2788658873600255, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1483, Loss: 0.37176558382633434, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1484, Loss: 0.43966274779248604, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1485, Loss: 0.4344618964854957, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1486, Loss: 0.20337977647481612, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1487, Loss: 0.252180848287289, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1488, Loss: 0.39585403437662536, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1489, Loss: 0.3069778722169481, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1490, Loss: 0.2812330200729741, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1491, Loss: 0.3161868416275675, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1492, Loss: 0.3173960378912028, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1493, Loss: 0.2955568835775384, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1494, Loss: 0.32873198549325733, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1495, Loss: 0.22886857318231008, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1496, Loss: 0.2830639709646838, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1497, Loss: 0.35148467940544054, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1498, Loss: 0.2703563563456651, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1499, Loss: 0.5425117534646479, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1500, Loss: 0.3521118387145509, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1501, Loss: 0.3441801298904449, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1502, Loss: 0.30051777589987827, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1503, Loss: 0.28617038618726615, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1504, Loss: 0.23686828288702363, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1505, Loss: 0.4138196787656848, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1506, Loss: 0.2561635569240615, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1507, Loss: 0.23801079356883476, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1508, Loss: 0.33537513148472475, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1509, Loss: 0.39901551838005, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1510, Loss: 0.25710187524743083, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1511, Loss: 0.4092434362298521, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1512, Loss: 0.9845522690933243, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1513, Loss: 0.24118669621234, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1514, Loss: 0.30516106989684955, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1515, Loss: 0.47754520936775535, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1516, Loss: 0.4475127662531029, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1517, Loss: 0.26453128869382253, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1518, Loss: 0.38584140565579295, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1519, Loss: 0.23844683114509915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1520, Loss: 0.32987478058435904, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1521, Loss: 0.28894612218856064, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1522, Loss: 0.3903534507931894, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1523, Loss: 0.33893953119659476, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1524, Loss: 0.1979125471277081, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1525, Loss: 0.312947372168097, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1526, Loss: 0.2683394673023775, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1527, Loss: 0.3808686189973792, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1528, Loss: 0.38583448449857616, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1529, Loss: 0.4917264525106727, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1530, Loss: 0.3123145329848321, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1531, Loss: 0.3118212052705378, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1532, Loss: 0.36671011460720815, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1533, Loss: 0.47189031319560915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1534, Loss: 0.2642979180956934, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1535, Loss: 0.27050272227268574, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1536, Loss: 0.3520307543065979, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1537, Loss: 0.5442571365052883, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1538, Loss: 0.3661189200463803, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1539, Loss: 0.21475710900445455, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1540, Loss: 0.49580876658138917, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1541, Loss: 0.41163562655013464, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1542, Loss: 0.3518268916137774, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1543, Loss: 0.30942308804956975, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1544, Loss: 0.2362650371010841, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1545, Loss: 0.32898244053945713, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1546, Loss: 0.33332493190469176, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1547, Loss: 0.3857966414991123, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1548, Loss: 0.34175041787099925, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1549, Loss: 0.2695680657705952, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1550, Loss: 0.27546548161943385, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1551, Loss: 0.4834770080285731, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1552, Loss: 0.2123131030877284, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1553, Loss: 0.2982161389103959, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1554, Loss: 0.2384439621997937, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1555, Loss: 0.5274232078703621, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1556, Loss: 0.44511869379472313, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1557, Loss: 0.26152966957088203, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1558, Loss: 0.23861334257890043, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1559, Loss: 0.23244180046278742, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1560, Loss: 0.5135005342720019, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1561, Loss: 0.4470498440941306, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1562, Loss: 0.7727952995586922, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1563, Loss: 0.22866881354720858, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1564, Loss: 0.32695197354204353, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1565, Loss: 0.39596422023167077, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1566, Loss: 0.3576083288445293, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1567, Loss: 0.2788416054888927, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1568, Loss: 0.21587270792148758, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1569, Loss: 0.4672860315096141, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1570, Loss: 0.2517950049094112, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1571, Loss: 0.20344065909056816, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1572, Loss: 0.2691167467481298, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1573, Loss: 0.5692356626553792, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1574, Loss: 0.31025473596580655, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1575, Loss: 0.5179431815204604, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1576, Loss: 0.37288431250429144, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1577, Loss: 0.20888880662256396, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1578, Loss: 0.2923627806245436, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1579, Loss: 0.4649311046538136, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1580, Loss: 0.5169032052054243, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1581, Loss: 0.37558347742622905, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1582, Loss: 0.2766403692685001, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1583, Loss: 0.6686506967567574, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1584, Loss: 0.5253350460883118, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1585, Loss: 0.3142765611325143, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1586, Loss: 0.3081379280002331, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1587, Loss: 0.23475589502935734, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1588, Loss: 0.38727242091077363, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1589, Loss: 0.39457140114685163, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1590, Loss: 0.2398971199746177, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1591, Loss: 0.20892695606999068, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1592, Loss: 0.3283495677363155, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1593, Loss: 0.23243756597998275, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1594, Loss: 0.44076734908221915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1595, Loss: 0.2589598590030674, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1596, Loss: 0.24734866340705475, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1597, Loss: 0.32513405648716387, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1598, Loss: 0.31404055072503245, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1599, Loss: 0.3411463839703597, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1600, Loss: 0.3933271453845861, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1601, Loss: 0.3913878489815538, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1602, Loss: 0.2716168617269372, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1603, Loss: 0.5034539941814405, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1604, Loss: 0.444305064299203, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1605, Loss: 0.3526751998269477, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1606, Loss: 0.40522707592997065, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1607, Loss: 0.4260901892692581, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1608, Loss: 0.4920019244057455, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1609, Loss: 0.36403376223649886, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1610, Loss: 0.30261443962169443, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1611, Loss: 0.27055413712127696, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1612, Loss: 0.4981918887790612, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1613, Loss: 0.4125871521849546, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1614, Loss: 0.5709067643960529, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1615, Loss: 0.3098235613184746, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1616, Loss: 0.7728347517131029, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1617, Loss: 0.30902859621628764, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1618, Loss: 0.38155502820107656, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1619, Loss: 0.462032799878347, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1620, Loss: 0.47163742764875416, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1621, Loss: 0.43183457324926733, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1622, Loss: 0.2661957598972326, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1623, Loss: 0.45244960670158685, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1624, Loss: 0.29630557799298474, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1625, Loss: 0.3681628069136059, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1626, Loss: 0.23664556485584798, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1627, Loss: 0.3047384695609323, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1628, Loss: 0.28816831059668757, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1629, Loss: 0.3837452102499652, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1630, Loss: 0.2550720226592186, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1631, Loss: 0.260463889193774, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1632, Loss: 0.4657968170114492, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1633, Loss: 0.37246063477505587, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1634, Loss: 0.4900875465401514, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1635, Loss: 0.5015504418454789, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1636, Loss: 0.2208879671451279, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1637, Loss: 0.2682797429635672, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1638, Loss: 0.29280581238575976, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1639, Loss: 0.26051758661207325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1640, Loss: 0.30173048259201624, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1641, Loss: 0.37568793649304, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1642, Loss: 0.6774286154859721, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1643, Loss: 0.2727391231681, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1644, Loss: 0.5885829236899875, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1645, Loss: 0.43842252954739425, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1646, Loss: 0.29150193230984833, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1647, Loss: 0.2351335744156775, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1648, Loss: 0.29007067075198684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1649, Loss: 0.23644625342692524, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1650, Loss: 0.6047889912507605, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1651, Loss: 0.5620408510966403, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1652, Loss: 0.30382896958671074, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1653, Loss: 0.5262442305476734, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1654, Loss: 0.42390450635143706, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1655, Loss: 0.3640493363802728, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1656, Loss: 0.6065404251712297, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1657, Loss: 0.2525323825261542, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1658, Loss: 0.4472826896868969, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1659, Loss: 0.3991822106946157, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1660, Loss: 0.288051287482291, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1661, Loss: 0.6643071080009862, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1662, Loss: 0.36056615658683366, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1663, Loss: 0.40496203405977527, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1664, Loss: 0.31040845141703016, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1665, Loss: 0.32117185442346674, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1666, Loss: 0.46818128104150064, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1667, Loss: 0.4783444191929226, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1668, Loss: 0.4578167559034364, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1669, Loss: 0.4247709033216688, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1670, Loss: 0.25489446736439453, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1671, Loss: 0.5682769735614251, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1672, Loss: 0.35227939424752175, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1673, Loss: 0.42931837691270286, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1674, Loss: 0.48419173638336616, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1675, Loss: 0.32046710011873325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1676, Loss: 0.6091936490343994, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1677, Loss: 0.44326902320636746, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1678, Loss: 0.6383615625602889, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1679, Loss: 0.23576739325999713, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1680, Loss: 0.4690063251043801, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1681, Loss: 0.24353600608395176, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1682, Loss: 0.2378793732447702, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1683, Loss: 0.2985850411324249, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1684, Loss: 0.33906156550476174, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1685, Loss: 0.4315873485021844, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1686, Loss: 0.9332516448959585, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1687, Loss: 0.2853083291848165, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1688, Loss: 0.26777664104804383, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1689, Loss: 0.46625086531884163, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1690, Loss: 0.25790488560776115, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1691, Loss: 0.3619041517874635, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1692, Loss: 0.4395017989802688, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1693, Loss: 0.28099256358037245, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1694, Loss: 0.41941150666869625, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1695, Loss: 0.2557083659284013, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1696, Loss: 0.5703261563285166, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1697, Loss: 0.4557624416848711, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1698, Loss: 0.5159633317523424, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1699, Loss: 0.25474610706385636, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1700, Loss: 0.2709947928502325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1701, Loss: 0.26984443426451826, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1702, Loss: 0.2685708733276407, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1703, Loss: 0.3434746853687709, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1704, Loss: 0.2111467567659168, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1705, Loss: 0.323770912349439, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1706, Loss: 0.40820091169031325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1707, Loss: 0.670692353969661, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1708, Loss: 0.24012205636063555, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1709, Loss: 0.23719714163031091, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1710, Loss: 0.22536691833326153, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1711, Loss: 0.34834209957438655, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1712, Loss: 0.6431326729342877, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1713, Loss: 0.5064458493786406, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1714, Loss: 0.32356924690337907, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1715, Loss: 0.3659685930387075, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1716, Loss: 0.36795949019706675, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1717, Loss: 0.2582777639623436, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1718, Loss: 0.4110173260095617, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1719, Loss: 0.2614112244760717, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1720, Loss: 0.39147782570039913, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1721, Loss: 0.5213172877386483, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1722, Loss: 0.2673275601823684, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1723, Loss: 0.2174542146965152, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1724, Loss: 0.24724055928659955, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1725, Loss: 0.3794502256285096, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1726, Loss: 0.371816538601488, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1727, Loss: 0.3851338007980264, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1728, Loss: 0.26999882740973735, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1729, Loss: 0.3976863866133681, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1730, Loss: 0.3309919817548749, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1731, Loss: 0.33880299002949343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1732, Loss: 0.31293338870726845, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1733, Loss: 0.7026946251844977, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1734, Loss: 0.38604382999351855, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1735, Loss: 0.34586056427782297, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1736, Loss: 0.3653121102913222, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1737, Loss: 0.33917373366767156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1738, Loss: 0.2323712081980785, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1739, Loss: 0.2786965261230952, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1740, Loss: 0.23606616203836608, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1741, Loss: 0.44776941584884966, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1742, Loss: 0.28865184721086096, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1743, Loss: 0.32744008468374847, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1744, Loss: 0.47043172497577124, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1745, Loss: 0.44090183746484185, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1746, Loss: 0.21586147377719123, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1747, Loss: 0.3388283521977018, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1748, Loss: 0.4062691247055745, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1749, Loss: 0.24518196705549442, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1750, Loss: 0.6525496247615787, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1751, Loss: 0.2554355336895015, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1752, Loss: 0.25504479617154624, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1753, Loss: 0.24846445723888738, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1754, Loss: 0.35194824658437407, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1755, Loss: 0.2546449065197812, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1756, Loss: 0.2474640490984435, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1757, Loss: 0.5067576415290869, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1758, Loss: 0.35688183119156813, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1759, Loss: 0.255885378893264, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1760, Loss: 0.3219850758760077, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1761, Loss: 0.22142824996257596, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1762, Loss: 0.9682033964700589, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1763, Loss: 0.38533707558261787, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1764, Loss: 0.3305773634312912, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1765, Loss: 0.3570701917548881, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1766, Loss: 0.24985365687840633, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1767, Loss: 0.3178468064209734, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1768, Loss: 0.3445463093441947, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1769, Loss: 0.5353293696123249, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1770, Loss: 0.42738703532364886, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1771, Loss: 0.4808577500727341, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1772, Loss: 0.27155447696131085, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1773, Loss: 0.46456414982300953, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1774, Loss: 0.4116079214934187, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1775, Loss: 0.5290257910671526, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1776, Loss: 0.42225392930554795, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1777, Loss: 0.34454023314424065, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1778, Loss: 0.3050810389000293, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1779, Loss: 0.30374800828134585, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1780, Loss: 0.3338928361919931, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1781, Loss: 0.2749042615168642, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1782, Loss: 0.6826766616942943, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1783, Loss: 0.3987539176752345, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1784, Loss: 0.2983645397897785, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1785, Loss: 0.4090290701171698, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1786, Loss: 0.41247668325292763, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1787, Loss: 0.25519971179670514, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1788, Loss: 0.3313073691865729, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1789, Loss: 0.3836614439942122, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1790, Loss: 0.74048041800452, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1791, Loss: 0.47711560299585054, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1792, Loss: 0.28899892457617926, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1793, Loss: 0.3700819975687156, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1794, Loss: 0.23964206208265584, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1795, Loss: 0.29701236513236784, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1796, Loss: 0.4594014709621815, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1797, Loss: 0.2371795229266656, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1798, Loss: 0.25159054796648583, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1799, Loss: 0.3425739395515949, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1800, Loss: 0.41337218048804114, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1801, Loss: 0.4991481685458573, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1802, Loss: 0.31051144391302427, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1803, Loss: 0.3217543019386345, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1804, Loss: 0.459993982819179, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1805, Loss: 0.3582647469261732, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1806, Loss: 0.36808638991525944, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1807, Loss: 0.8109405809530638, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1808, Loss: 0.3516641027972197, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1809, Loss: 0.21196849777989862, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1810, Loss: 0.2981268588811429, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1811, Loss: 0.2590237123520993, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1812, Loss: 0.5226448686865472, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1813, Loss: 0.2656363424349907, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1814, Loss: 0.4468046349104052, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1815, Loss: 0.29380409974244837, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1816, Loss: 0.3731980889089315, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1817, Loss: 0.3609904791342915, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1818, Loss: 0.23980084452293932, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1819, Loss: 0.22025696923512617, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1820, Loss: 0.24467955625739998, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1821, Loss: 0.38784836564843317, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1822, Loss: 0.43747680155275154, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1823, Loss: 0.35946951422417317, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1824, Loss: 0.2398551175619173, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1825, Loss: 0.3647351484396416, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1826, Loss: 0.4930156323458169, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1827, Loss: 0.3382984994802345, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1828, Loss: 0.37216012090201583, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1829, Loss: 0.44273098742438943, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1830, Loss: 0.31841488441308785, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1831, Loss: 0.3565960119302954, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1832, Loss: 0.3732740959034567, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1833, Loss: 0.2824245055288769, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1834, Loss: 0.3931667654320591, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1835, Loss: 0.3869246725471301, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1836, Loss: 0.25642131638551285, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1837, Loss: 0.2781179588407118, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1838, Loss: 0.42196542386857694, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1839, Loss: 0.34376549018946145, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1840, Loss: 0.7233883005354936, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1841, Loss: 0.4549296797748593, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1842, Loss: 0.33865096935035655, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1843, Loss: 0.2389421774411703, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1844, Loss: 0.22938329672145186, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1845, Loss: 0.2434721333187325, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1846, Loss: 0.24087760793600516, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1847, Loss: 0.274909314873309, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1848, Loss: 0.2847962225360159, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1849, Loss: 0.6428413388831224, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1850, Loss: 0.32612264468241603, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1851, Loss: 0.2778316971237614, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1852, Loss: 0.5235891803412205, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1853, Loss: 0.29360346417141214, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1854, Loss: 0.3102730043822637, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1855, Loss: 0.2728303606891343, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1856, Loss: 0.37293866740350406, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1857, Loss: 0.22725675280026464, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1858, Loss: 0.2924596202880231, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1859, Loss: 0.5979832534984185, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1860, Loss: 0.49252359929823497, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1861, Loss: 0.38036850125277655, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1862, Loss: 0.3211014042305732, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1863, Loss: 0.2465768154799488, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1864, Loss: 0.2984418994446342, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1865, Loss: 0.3249125002966143, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1866, Loss: 0.24946195003270027, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1867, Loss: 0.3077075827048724, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1868, Loss: 0.22861036550732175, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1869, Loss: 0.518063906011361, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1870, Loss: 0.6193569828574894, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1871, Loss: 0.27807289779484756, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1872, Loss: 0.27915437138785615, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1873, Loss: 0.4458433866789392, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1874, Loss: 0.3362501793338831, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Batch 1875, Loss: 0.598249747462005, Batch Size: 32, Learning Rate: 0.00018423749999999997\n",
      "Epoch 5, Updated Learning Rate: 0.00015660187499999995\n",
      "Epoch 5, Average Loss: 0.36405908196299896, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1, Loss: 0.5437439664393942, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 2, Loss: 0.2793802759844775, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 3, Loss: 0.27588046002992606, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 4, Loss: 0.696832645457757, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 5, Loss: 0.343204288367317, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 6, Loss: 0.23680267742117317, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 7, Loss: 0.2786075098089578, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 8, Loss: 0.632774215810408, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 9, Loss: 0.22131804903748448, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 10, Loss: 0.5461423736048978, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 11, Loss: 0.23319212139533585, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 12, Loss: 0.5271177960736664, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 13, Loss: 0.3474491504326036, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 14, Loss: 0.2847279581534535, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 15, Loss: 0.40115732594324494, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 16, Loss: 0.6164378672141124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 17, Loss: 0.27587503805477703, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 18, Loss: 0.2760988245456072, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 19, Loss: 0.26335720823537057, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 20, Loss: 0.21653082181504388, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 21, Loss: 0.620822561232311, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 22, Loss: 0.3178393869277638, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 23, Loss: 0.3547646428182193, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 24, Loss: 0.42630285682933033, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 25, Loss: 0.5392472730324582, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 26, Loss: 0.2486816723725075, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 27, Loss: 0.2931940997612981, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 28, Loss: 0.26451344444684777, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 29, Loss: 0.704565483124858, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 30, Loss: 0.40090105903319634, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 31, Loss: 0.38541782129514557, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 32, Loss: 0.2431537075641405, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 33, Loss: 0.5638669046089947, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 34, Loss: 0.28366235125151223, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 35, Loss: 0.2522330242546346, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 36, Loss: 0.34342655825255863, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 37, Loss: 0.23496179991201413, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 38, Loss: 0.25664758260693943, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 39, Loss: 0.3426942007946845, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 40, Loss: 0.4904378762762409, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 41, Loss: 0.3772119870786226, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 42, Loss: 0.2288726849324448, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 43, Loss: 0.6852092518957054, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 44, Loss: 0.31529408798584474, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 45, Loss: 0.2785804948534602, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 46, Loss: 0.2661792974416861, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 47, Loss: 0.37218077108087927, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 48, Loss: 0.3244529622162517, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 49, Loss: 0.5725099043779053, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 50, Loss: 0.3057940182887926, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 51, Loss: 0.4047151810579559, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 52, Loss: 0.3499580003659407, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 53, Loss: 0.2233788818702337, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 54, Loss: 0.5424139607839358, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 55, Loss: 0.24593354166382755, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 56, Loss: 0.5278705684144043, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 57, Loss: 0.3808172515272622, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 58, Loss: 0.2875743445394612, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 59, Loss: 0.2837251359706915, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 60, Loss: 0.3616281519987937, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 61, Loss: 0.2522500242397435, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 62, Loss: 0.28641201570581754, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 63, Loss: 0.281737236249225, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 64, Loss: 0.34959238198883424, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 65, Loss: 0.3211477208075356, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 66, Loss: 0.22833558429093997, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 67, Loss: 0.5292409443532418, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 68, Loss: 0.24137389804278456, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 69, Loss: 0.2704968656632697, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 70, Loss: 0.4596748578855353, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 71, Loss: 0.580668218896406, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 72, Loss: 0.2500512965051223, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 73, Loss: 0.3690012534565298, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 74, Loss: 0.3055291033254217, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 75, Loss: 0.2975558790263323, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 76, Loss: 0.2550348559934024, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 77, Loss: 0.30316100206739904, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 78, Loss: 0.6135068788381286, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 79, Loss: 0.3293572463530533, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 80, Loss: 0.4222394031600153, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 81, Loss: 0.4157170630949675, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 82, Loss: 0.7021942199245137, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 83, Loss: 0.21229045276497963, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 84, Loss: 0.27588361832833097, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 85, Loss: 0.7786685949897804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 86, Loss: 0.48265598539139903, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 87, Loss: 0.2701715615093891, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 88, Loss: 0.23439768496441554, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 89, Loss: 0.46596259495571324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 90, Loss: 0.30770110625343317, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 91, Loss: 0.3594122392080774, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 92, Loss: 0.23505269439142285, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 93, Loss: 0.3848440276163034, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 94, Loss: 0.276734547618389, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 95, Loss: 0.5961666990480723, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 96, Loss: 0.34049195045298053, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 97, Loss: 0.22225169274283563, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 98, Loss: 0.3781459709136309, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 99, Loss: 0.2804042434864079, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 100, Loss: 0.2980932979917199, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 101, Loss: 0.2744722272258697, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 102, Loss: 0.3747913277827008, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 103, Loss: 0.3210934845324408, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 104, Loss: 0.28272541253017675, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 105, Loss: 0.2671525413083616, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 106, Loss: 0.3381734419414681, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 107, Loss: 0.2788355944344162, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 108, Loss: 0.21766740647326127, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 109, Loss: 0.4666132190789877, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 110, Loss: 0.34474696704040286, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 111, Loss: 0.41505389841535134, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 112, Loss: 0.3956264885623748, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 113, Loss: 0.44482824816001987, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 114, Loss: 0.5002795763224124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 115, Loss: 0.2276452078298482, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 116, Loss: 0.25217772905843905, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 117, Loss: 0.6362699611839095, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 118, Loss: 0.5077632128196395, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 119, Loss: 0.3579244090647393, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 120, Loss: 0.4024351573866844, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 121, Loss: 0.2677803864394145, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 122, Loss: 0.2877799320810572, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 123, Loss: 0.3220489028935172, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 124, Loss: 0.3860924288773082, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 125, Loss: 0.36561955932237666, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 126, Loss: 0.3085071481937579, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 127, Loss: 0.35099015586829746, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 128, Loss: 0.324846132558119, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 129, Loss: 0.5794736190428635, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 130, Loss: 0.3129824268458842, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 131, Loss: 0.30663669043062475, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 132, Loss: 0.28930572362401347, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 133, Loss: 0.3127856827265131, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 134, Loss: 0.4317357647200073, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 135, Loss: 0.21795148934743103, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 136, Loss: 0.4303759681562769, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 137, Loss: 0.20244198403734145, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 138, Loss: 0.2331081737048545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 139, Loss: 0.34141317709989605, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 140, Loss: 0.5491761716403527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 141, Loss: 0.35908909943545925, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 142, Loss: 0.25194621848513865, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 143, Loss: 0.3225596818518004, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 144, Loss: 0.4323503223724201, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 145, Loss: 0.2724171120019291, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 146, Loss: 0.2663142266941248, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 147, Loss: 0.5096327068973656, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 148, Loss: 0.2706028398843293, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 149, Loss: 0.27132687263030364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 150, Loss: 0.3605530889314324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 151, Loss: 0.4411489026265968, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 152, Loss: 0.381523277696732, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 153, Loss: 0.30287366724607656, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 154, Loss: 0.4610263843094957, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 155, Loss: 0.306549137615213, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 156, Loss: 0.3964487076514934, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 157, Loss: 0.2587605168364683, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 158, Loss: 0.21522085725013185, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 159, Loss: 0.34237532096796874, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 160, Loss: 0.3591581882087045, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 161, Loss: 0.34702596165927957, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 162, Loss: 0.2790426935556897, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 163, Loss: 0.2539366881700614, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 164, Loss: 0.3219212382928909, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 165, Loss: 0.22792286228720948, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 166, Loss: 0.49601593183679127, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 167, Loss: 0.3760653610815964, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 168, Loss: 0.2955868074621538, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 169, Loss: 0.32311901457652015, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 170, Loss: 0.22473620192668642, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 171, Loss: 0.2515914018102444, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 172, Loss: 0.27105792707765913, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 173, Loss: 0.3421059584632392, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 174, Loss: 0.3544317386783872, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 175, Loss: 0.2760266089161288, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 176, Loss: 0.28757886840751234, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 177, Loss: 0.4041370812471655, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 178, Loss: 0.3154139355809259, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 179, Loss: 0.2619185773426813, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 180, Loss: 0.4897469724835854, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 181, Loss: 0.47273462588815673, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 182, Loss: 0.2919862981534615, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 183, Loss: 0.3000938281712846, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 184, Loss: 0.35400847934465585, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 185, Loss: 0.2664029936509456, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 186, Loss: 0.2952644787800342, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 187, Loss: 0.4263129756263495, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 188, Loss: 0.3771616031936015, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 189, Loss: 0.437718295361216, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 190, Loss: 0.31964590504043733, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 191, Loss: 0.4099493844507422, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 192, Loss: 0.2640329453065183, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 193, Loss: 0.23460392697819424, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 194, Loss: 0.3272030974503522, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 195, Loss: 0.3397892748074314, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 196, Loss: 0.465266288093399, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 197, Loss: 0.4982272119668924, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 198, Loss: 0.30440597184360496, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 199, Loss: 0.4456928887984366, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 200, Loss: 0.4082130212424965, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 201, Loss: 0.2616083195891081, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 202, Loss: 0.33185067283641234, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 203, Loss: 0.3253058166104373, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 204, Loss: 0.4282304768780343, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 205, Loss: 0.21729596140189766, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 206, Loss: 0.2700693122817649, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 207, Loss: 0.38922063162611265, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 208, Loss: 0.46614781084885193, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 209, Loss: 0.8896247516192851, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 210, Loss: 0.2697876572621209, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 211, Loss: 0.37614057003029355, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 212, Loss: 0.6259355200074004, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 213, Loss: 0.4127062438245508, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 214, Loss: 0.35063177175034976, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 215, Loss: 0.28479675934083304, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 216, Loss: 0.27360941556900903, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 217, Loss: 0.2850503933611227, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 218, Loss: 0.38332517150455064, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 219, Loss: 0.21583333294131593, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 220, Loss: 0.24719053815952513, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 221, Loss: 0.22477011259092403, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 222, Loss: 0.32964642753507556, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 223, Loss: 0.41872268952335356, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 224, Loss: 0.2773527810587981, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 225, Loss: 0.5362254616002804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 226, Loss: 0.7798231780394285, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 227, Loss: 0.36074254632827985, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 228, Loss: 0.5019740020518005, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 229, Loss: 0.3773078505707699, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 230, Loss: 0.25204565936567214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 231, Loss: 0.3415399940532816, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 232, Loss: 0.6397714258682939, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 233, Loss: 0.39987236097725687, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 234, Loss: 0.43953985050163197, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 235, Loss: 0.51046298948798, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 236, Loss: 0.2705946193686743, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 237, Loss: 0.3020307846040106, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 238, Loss: 0.24122635206399093, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 239, Loss: 0.2649033588391363, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 240, Loss: 0.3786725025901986, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 241, Loss: 0.2608561525324352, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 242, Loss: 0.24071407966272015, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 243, Loss: 0.28068575438413124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 244, Loss: 0.4460401107433388, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 245, Loss: 0.39737074300882214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 246, Loss: 0.3607393756857028, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 247, Loss: 0.3716132355944226, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 248, Loss: 0.5984839176400796, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 249, Loss: 0.2819969290243121, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 250, Loss: 0.559746798865568, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 251, Loss: 0.3262733593742607, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 252, Loss: 0.403482092902677, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 253, Loss: 0.22539321900906473, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 254, Loss: 0.3022285785343796, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 255, Loss: 0.29304863751589477, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 256, Loss: 0.3834725510354539, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 257, Loss: 0.46661373838278686, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 258, Loss: 0.5403572861870024, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 259, Loss: 0.28124987400674195, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 260, Loss: 0.250392637970433, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 261, Loss: 0.29650406404977214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 262, Loss: 0.38185495119489, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 263, Loss: 0.24891722677488204, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 264, Loss: 0.3298669679696008, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 265, Loss: 0.35381800123816864, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 266, Loss: 0.40434029369592633, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 267, Loss: 0.27179413645465333, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 268, Loss: 0.26238076234063457, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 269, Loss: 0.4212348321643934, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 270, Loss: 0.3206656310340371, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 271, Loss: 0.5165148406702246, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 272, Loss: 0.2608220761588367, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 273, Loss: 0.291577785650463, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 274, Loss: 0.3892592682676673, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 275, Loss: 0.3170629676080425, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 276, Loss: 0.5073457148511651, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 277, Loss: 0.24852062759038812, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 278, Loss: 0.3495415824378274, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 279, Loss: 0.29986426970771307, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 280, Loss: 0.22758646644120883, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 281, Loss: 0.23810950752896298, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 282, Loss: 0.2642791484308086, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 283, Loss: 0.4140744807494322, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 284, Loss: 0.4783059876187844, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 285, Loss: 0.3152252041317175, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 286, Loss: 0.32143230117918614, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 287, Loss: 0.4301487748991313, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 288, Loss: 0.42335929541158746, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 289, Loss: 0.3391363090835088, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 290, Loss: 0.3077723078473653, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 291, Loss: 0.3714258726010668, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 292, Loss: 0.45650840295169187, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 293, Loss: 0.2353575269501629, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 294, Loss: 0.23386193296603552, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 295, Loss: 0.22774346058265005, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 296, Loss: 0.3301778833399892, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 297, Loss: 0.24341173255952642, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 298, Loss: 0.46197781355609346, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 299, Loss: 0.37471087110868373, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 300, Loss: 0.5025297000358431, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 301, Loss: 0.29751607998336027, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 302, Loss: 0.33139309083416113, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 303, Loss: 0.5270173081611091, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 304, Loss: 0.3259593639017523, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 305, Loss: 0.22071030940463107, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 306, Loss: 0.36638391380129637, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 307, Loss: 0.3051449194532502, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 308, Loss: 0.2374912891472581, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 309, Loss: 0.2815018909599807, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 310, Loss: 0.32083904823261056, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 311, Loss: 0.26362407592238324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 312, Loss: 0.22042695301980364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 313, Loss: 0.4647974899648982, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 314, Loss: 0.25349199777560705, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 315, Loss: 0.31479947256618623, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 316, Loss: 0.22608819628645324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 317, Loss: 0.3737506315913958, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 318, Loss: 0.44882733092692906, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 319, Loss: 0.4019839144678408, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 320, Loss: 0.41146919617534966, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 321, Loss: 0.5079207482635322, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 322, Loss: 0.3082681929349871, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 323, Loss: 0.44972200156696773, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 324, Loss: 0.379423348487174, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 325, Loss: 0.3106679585495312, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 326, Loss: 0.25171157687818246, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 327, Loss: 0.31226854379424196, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 328, Loss: 0.22621822999464683, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 329, Loss: 0.40947304900144543, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 330, Loss: 0.5039833910048626, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 331, Loss: 0.49012487287150647, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 332, Loss: 0.27622743429716284, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 333, Loss: 0.27426543948127824, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 334, Loss: 0.38364051841069585, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 335, Loss: 0.2883328483750818, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 336, Loss: 0.31895830304700007, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 337, Loss: 0.2561564357778793, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 338, Loss: 0.32781092656932387, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 339, Loss: 0.4198616206617185, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 340, Loss: 0.2116422009936514, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 341, Loss: 0.5917482546731065, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 342, Loss: 0.3798664485343776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 343, Loss: 0.3870542815059115, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 344, Loss: 0.24612445806929165, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 345, Loss: 0.22333757390037526, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 346, Loss: 0.3441915334009402, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 347, Loss: 0.2915572197811736, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 348, Loss: 0.26220203524535335, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 349, Loss: 0.30378524957407826, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 350, Loss: 0.49685082454465823, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 351, Loss: 0.5354707791565692, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 352, Loss: 0.2641303393124447, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 353, Loss: 0.3203337195805903, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 354, Loss: 0.222528483387741, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 355, Loss: 0.22905035159532056, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 356, Loss: 0.4705014765737384, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 357, Loss: 0.36739138912224734, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 358, Loss: 0.37270376354695695, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 359, Loss: 0.3942701955903504, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 360, Loss: 0.3056794945953802, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 361, Loss: 0.2634738767001669, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 362, Loss: 0.28127970179030837, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 363, Loss: 0.3845423208663198, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 364, Loss: 0.22904546739755083, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 365, Loss: 0.35387032788336115, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 366, Loss: 0.24268335321738227, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 367, Loss: 0.21596102941046705, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 368, Loss: 0.2775744167374984, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 369, Loss: 0.4672609353759481, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 370, Loss: 0.21893371466794054, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 371, Loss: 0.44882836550860206, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 372, Loss: 0.40073749405465897, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 373, Loss: 0.41928166811179834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 374, Loss: 0.2776949987973075, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 375, Loss: 0.3545280016107403, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 376, Loss: 0.25783604052606857, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 377, Loss: 0.6503706356295733, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 378, Loss: 0.29632161305043236, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 379, Loss: 0.2583254427094536, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 380, Loss: 0.5154912946378767, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 381, Loss: 0.22286713288942908, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 382, Loss: 0.615964321199024, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 383, Loss: 0.4763507621032087, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 384, Loss: 0.5307437846446219, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 385, Loss: 0.41362028879639184, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 386, Loss: 0.2540621728127085, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 387, Loss: 0.2425086829875405, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 388, Loss: 0.3378903132020105, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 389, Loss: 0.3968673784926957, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 390, Loss: 0.2285783593633532, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 391, Loss: 0.3679037627030408, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 392, Loss: 0.38393522346422737, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 393, Loss: 0.28935881755380405, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 394, Loss: 0.40380279091991755, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 395, Loss: 0.2185879556023109, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 396, Loss: 0.4563500779960923, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 397, Loss: 0.2487243056956514, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 398, Loss: 0.3468115207232664, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 399, Loss: 0.3037905767544683, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 400, Loss: 0.3199433025351582, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 401, Loss: 0.24352270347090155, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 402, Loss: 0.33211290122897485, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 403, Loss: 0.32201865420527553, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 404, Loss: 0.35394018362886254, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 405, Loss: 0.4201797720875944, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 406, Loss: 0.558379190670673, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 407, Loss: 0.5297136783122726, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 408, Loss: 0.2624399456733946, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 409, Loss: 0.318416863599426, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 410, Loss: 0.31626530406040765, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 411, Loss: 0.30992315211814514, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 412, Loss: 0.30818956641961187, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 413, Loss: 0.2642973981909526, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 414, Loss: 0.7963430704931413, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 415, Loss: 0.49677204399041797, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 416, Loss: 0.40249559825510817, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 417, Loss: 0.22403415567399154, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 418, Loss: 0.22659355368759893, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 419, Loss: 0.342446630105709, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 420, Loss: 0.40273384052363137, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 421, Loss: 0.2849560604474569, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 422, Loss: 0.4127056184664123, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 423, Loss: 0.3475845939542547, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 424, Loss: 0.2641262267040274, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 425, Loss: 0.30048986262893645, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 426, Loss: 0.2770537387904909, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 427, Loss: 0.33380869411820996, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 428, Loss: 0.3923955800054161, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 429, Loss: 0.3306635406604995, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 430, Loss: 0.4142652753269602, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 431, Loss: 0.343060355680273, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 432, Loss: 0.534010244203455, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 433, Loss: 0.33351192228970966, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 434, Loss: 0.28044322280027534, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 435, Loss: 0.34836015510580126, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 436, Loss: 0.41131977092333827, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 437, Loss: 0.27550479339284056, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 438, Loss: 0.4407051483941731, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 439, Loss: 0.4420942166720251, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 440, Loss: 0.24004191941020964, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 441, Loss: 0.5394991155146907, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 442, Loss: 0.3148441961194454, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 443, Loss: 0.41602907675185163, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 444, Loss: 0.30062733368953043, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 445, Loss: 0.25652445238702093, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 446, Loss: 0.4431359704979113, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 447, Loss: 0.36203642723920176, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 448, Loss: 0.2840230561532799, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 449, Loss: 0.32158710488983566, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 450, Loss: 0.5422550445082512, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 451, Loss: 0.23140114626821973, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 452, Loss: 0.2545736231981725, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 453, Loss: 0.22684555996691996, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 454, Loss: 0.34540127191718817, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 455, Loss: 0.43702138950101244, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 456, Loss: 0.3357645663726756, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 457, Loss: 0.23123133748705213, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 458, Loss: 0.27742291021750864, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 459, Loss: 0.27165281226646515, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 460, Loss: 0.48255018762469815, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 461, Loss: 0.21249867785682755, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 462, Loss: 0.23301629856602488, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 463, Loss: 0.22549175814936964, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 464, Loss: 0.22939351614231396, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 465, Loss: 0.49000198924880645, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 466, Loss: 0.3353732923204096, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 467, Loss: 0.3619501005708631, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 468, Loss: 0.43241950694970754, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 469, Loss: 0.23263808206636807, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 470, Loss: 0.4737558772392155, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 471, Loss: 0.2771578486888009, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 472, Loss: 0.4191368657329454, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 473, Loss: 0.38607360726353424, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 474, Loss: 0.40958430733241025, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 475, Loss: 0.21351877492129892, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 476, Loss: 0.238701323360621, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 477, Loss: 0.3318996810913005, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 478, Loss: 0.27847895784694876, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 479, Loss: 0.38495039478098425, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 480, Loss: 0.6899317250623475, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 481, Loss: 0.4008850479203484, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 482, Loss: 0.24475021075038075, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 483, Loss: 0.5175356586981639, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 484, Loss: 0.20612541915833635, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 485, Loss: 0.4588283164303695, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 486, Loss: 0.24659605144994245, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 487, Loss: 0.39027376555675153, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 488, Loss: 0.4716808208966967, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 489, Loss: 0.4287322418071776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 490, Loss: 0.2099998906473915, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 491, Loss: 0.28530356605077223, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 492, Loss: 0.2735793352157919, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 493, Loss: 0.6634590346777772, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 494, Loss: 0.6101829818199129, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 495, Loss: 0.27544997440937025, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 496, Loss: 0.27890971438830114, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 497, Loss: 0.31023774814868527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 498, Loss: 0.21432485388241584, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 499, Loss: 0.43522879792035307, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 500, Loss: 0.30117668034565503, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 501, Loss: 0.2990042526163611, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 502, Loss: 0.3529903783894947, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 503, Loss: 0.4401973083232814, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 504, Loss: 0.33021076469658345, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 505, Loss: 0.4429360293900881, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 506, Loss: 0.2370827482981913, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 507, Loss: 0.34404483362302685, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 508, Loss: 0.4447685038755922, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 509, Loss: 0.3985453845047317, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 510, Loss: 0.3003621722982052, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 511, Loss: 0.3943836566638248, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 512, Loss: 0.32956869970452685, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 513, Loss: 0.42916992928781816, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 514, Loss: 0.5028468158586035, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 515, Loss: 0.40356411561854677, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 516, Loss: 0.31080418960273287, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 517, Loss: 0.2721652196335906, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 518, Loss: 0.4484877145871948, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 519, Loss: 0.3465926469709847, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 520, Loss: 0.30332659518004856, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 521, Loss: 0.31584531476388134, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 522, Loss: 0.352538384141733, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 523, Loss: 0.3050873132953267, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 524, Loss: 0.39295343793599, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 525, Loss: 0.31542509617231296, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 526, Loss: 0.46863738028324364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 527, Loss: 0.27116083887400116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 528, Loss: 0.2954999774138257, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 529, Loss: 0.2854585617245898, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 530, Loss: 0.3718409009421727, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 531, Loss: 0.41383198575863145, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 532, Loss: 0.213166655116772, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 533, Loss: 0.3629562702948804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 534, Loss: 0.2904461117310902, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 535, Loss: 0.2756587544433182, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 536, Loss: 0.5706839179644355, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 537, Loss: 0.37444466177069197, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 538, Loss: 0.6086196173262405, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 539, Loss: 0.580309722868068, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 540, Loss: 0.2997725257230514, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 541, Loss: 0.4106169295266543, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 542, Loss: 0.27317288026752545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 543, Loss: 0.4036119890982451, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 544, Loss: 0.24413770959332617, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 545, Loss: 0.2502516357921061, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 546, Loss: 0.40761950257704244, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 547, Loss: 0.2076351266901351, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 548, Loss: 0.4224678309498178, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 549, Loss: 0.3087334643903929, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 550, Loss: 0.33433041766651744, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 551, Loss: 0.44569491878485457, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 552, Loss: 0.43631683842789526, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 553, Loss: 0.21919167159276637, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 554, Loss: 0.3267950523233777, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 555, Loss: 0.31876655308063273, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 556, Loss: 0.6340147350932775, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 557, Loss: 0.5332846634844367, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 558, Loss: 0.43428582416299544, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 559, Loss: 0.3300084913266428, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 560, Loss: 0.37934424250723264, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 561, Loss: 0.3387716014334343, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 562, Loss: 0.22655211276210335, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 563, Loss: 0.5522122140846044, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 564, Loss: 0.2740713313939784, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 565, Loss: 0.27972421051264873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 566, Loss: 0.4552718342857634, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 567, Loss: 0.27240823325580876, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 568, Loss: 0.4261114020484721, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 569, Loss: 0.3269859202462889, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 570, Loss: 0.2414571233277086, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 571, Loss: 0.3242664721130755, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 572, Loss: 0.22095638170332121, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 573, Loss: 0.19976662645217133, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 574, Loss: 0.5795966382245743, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 575, Loss: 0.41444730184302503, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 576, Loss: 0.2082626365339031, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 577, Loss: 0.3904896536359155, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 578, Loss: 0.31480106277941605, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 579, Loss: 0.34630434560534024, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 580, Loss: 0.8282518672090158, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 581, Loss: 0.45131002126300945, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 582, Loss: 0.4345418895729718, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 583, Loss: 0.4431294080143088, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 584, Loss: 0.5984318516309062, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 585, Loss: 0.22256953384643255, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 586, Loss: 0.3099181151010732, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 587, Loss: 0.29104553722877907, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 588, Loss: 0.46136880911807926, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 589, Loss: 0.33620648385478635, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 590, Loss: 0.361896316151109, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 591, Loss: 0.5437667782049639, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 592, Loss: 0.41269124520174183, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 593, Loss: 0.24940737437018792, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 594, Loss: 0.34904317772227106, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 595, Loss: 0.3357389284629837, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 596, Loss: 0.318756678575313, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 597, Loss: 0.27161577710089274, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 598, Loss: 0.3599342186939176, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 599, Loss: 0.26865440970152354, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 600, Loss: 0.30094751182482116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 601, Loss: 0.47552386001596614, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 602, Loss: 0.4845755251298426, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 603, Loss: 0.27690717498657114, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 604, Loss: 0.36211719427387445, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 605, Loss: 0.583609116495933, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 606, Loss: 0.21731746842482386, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 607, Loss: 0.2797622640411123, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 608, Loss: 0.2329629932814988, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 609, Loss: 0.37187330314603273, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 610, Loss: 0.44865158004938427, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 611, Loss: 0.25913714144398664, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 612, Loss: 0.2860000982171996, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 613, Loss: 0.22841855142555903, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 614, Loss: 0.367063281848677, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 615, Loss: 0.4212744432595381, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 616, Loss: 0.49241574620839135, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 617, Loss: 0.29407627127160496, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 618, Loss: 0.2606888058581949, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 619, Loss: 0.25640062096852073, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 620, Loss: 0.34430563722561625, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 621, Loss: 0.25649510366203626, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 622, Loss: 0.3030282527749994, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 623, Loss: 0.2872617651572059, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 624, Loss: 0.29788626841446086, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 625, Loss: 0.3839695321500882, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 626, Loss: 0.263427620651079, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 627, Loss: 0.291777027404002, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 628, Loss: 0.5131348053925754, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 629, Loss: 0.2358855652596068, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 630, Loss: 0.32956855995681467, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 631, Loss: 0.4468977165211633, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 632, Loss: 0.3568706972489167, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 633, Loss: 0.49831226694040043, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 634, Loss: 0.30288849650669086, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 635, Loss: 0.5537974575248732, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 636, Loss: 0.27793334476583403, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 637, Loss: 0.2775422408103149, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 638, Loss: 0.31344738607197165, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 639, Loss: 0.30325993742556956, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 640, Loss: 0.2946374324088389, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 641, Loss: 0.4341938786509545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 642, Loss: 0.3071980320426721, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 643, Loss: 0.3015598336482868, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 644, Loss: 0.5983939079148284, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 645, Loss: 0.31177224981966706, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 646, Loss: 0.2642082446758838, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 647, Loss: 0.21650709515003594, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 648, Loss: 0.30514637483350027, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 649, Loss: 0.32435253677280773, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 650, Loss: 0.3548852469127558, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 651, Loss: 0.45188338702362574, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 652, Loss: 0.22697106520889612, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 653, Loss: 0.29394354579125065, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 654, Loss: 0.2415390558392616, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 655, Loss: 0.2754228215759068, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 656, Loss: 0.7104749872152348, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 657, Loss: 0.2672881673899494, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 658, Loss: 0.595590601768704, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 659, Loss: 0.45844401522611555, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 660, Loss: 0.4025395449188659, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 661, Loss: 0.3435975419073287, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 662, Loss: 0.248251181673652, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 663, Loss: 0.21656320463221565, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 664, Loss: 0.31856680380057023, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 665, Loss: 0.30110795590288303, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 666, Loss: 0.45094614265166205, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 667, Loss: 0.40164050362911774, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 668, Loss: 0.32141343129895067, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 669, Loss: 0.3791876808644573, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 670, Loss: 0.24947394445343396, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 671, Loss: 0.32807035754141933, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 672, Loss: 0.20611692345563037, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 673, Loss: 0.42681953228244507, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 674, Loss: 0.27374881312761656, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 675, Loss: 0.2656785987964555, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 676, Loss: 0.28212376814408086, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 677, Loss: 0.2672040443613313, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 678, Loss: 0.440255321447915, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 679, Loss: 0.38547199649913366, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 680, Loss: 0.27006655259304707, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 681, Loss: 0.4250441882309114, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 682, Loss: 0.4303293691138397, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 683, Loss: 0.38761276867037214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 684, Loss: 0.3468064702110865, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 685, Loss: 0.653277832861709, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 686, Loss: 0.518610057937215, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 687, Loss: 0.31541345150915356, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 688, Loss: 0.40752486028126755, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 689, Loss: 0.22013068134220845, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 690, Loss: 0.48185300521306296, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 691, Loss: 0.3341499061898426, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 692, Loss: 0.6992322592527868, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 693, Loss: 0.33394968607111963, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 694, Loss: 0.48451377929235273, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 695, Loss: 0.4782865835671027, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 696, Loss: 0.27237532068711545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 697, Loss: 0.2700892465167665, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 698, Loss: 0.28429137473542215, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 699, Loss: 0.3700405098475886, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 700, Loss: 0.3338291547511518, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 701, Loss: 0.39357472702234053, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 702, Loss: 0.28620208265340474, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 703, Loss: 0.35233024525747036, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 704, Loss: 0.5081458409280734, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 705, Loss: 0.2561857142245838, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 706, Loss: 0.7401344819938676, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 707, Loss: 0.26611654986843053, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 708, Loss: 0.43861847218760674, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 709, Loss: 0.5051730733710527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 710, Loss: 0.2262754037487211, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 711, Loss: 0.495626080091392, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 712, Loss: 0.8318963058732032, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 713, Loss: 0.2780846417501671, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 714, Loss: 0.3403198915573984, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 715, Loss: 0.22382102858689928, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 716, Loss: 0.3403454506624899, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 717, Loss: 0.41473024843751916, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 718, Loss: 0.6798769244195978, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 719, Loss: 0.3036072214355149, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 720, Loss: 0.26184305690815124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 721, Loss: 0.7280947773758633, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 722, Loss: 0.3513917542371061, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 723, Loss: 0.4204335100304968, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 724, Loss: 0.3841817993406593, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 725, Loss: 0.25958322847506415, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 726, Loss: 0.3074600514709868, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 727, Loss: 0.2863026474496751, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 728, Loss: 0.22763469592482288, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 729, Loss: 0.4945709241341628, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 730, Loss: 0.5066584861943657, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 731, Loss: 0.25292510665425894, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 732, Loss: 0.2626665087279236, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 733, Loss: 0.39901401229739253, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 734, Loss: 0.4723842609962656, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 735, Loss: 0.4058988805209411, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 736, Loss: 0.25269640105297714, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 737, Loss: 0.3369042798271692, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 738, Loss: 0.3615036077820263, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 739, Loss: 0.654529492669926, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 740, Loss: 0.22573366662977476, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 741, Loss: 0.26237019525768873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 742, Loss: 0.3506230439346616, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 743, Loss: 0.523310161508292, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 744, Loss: 0.2435029145612014, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 745, Loss: 0.4395373966909468, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 746, Loss: 0.5961459140436575, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 747, Loss: 0.7337483922364069, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 748, Loss: 0.46088030204636476, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 749, Loss: 0.3084485831147699, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 750, Loss: 0.22570091096390443, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 751, Loss: 0.38996239617319983, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 752, Loss: 0.30022591339234794, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 753, Loss: 0.5159367417889524, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 754, Loss: 0.2989255575208395, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 755, Loss: 0.29137986516115477, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 756, Loss: 0.34382379701186483, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 757, Loss: 0.32731893359235076, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 758, Loss: 0.4912484949626785, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 759, Loss: 0.4975009803356708, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 760, Loss: 0.31649294857220067, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 761, Loss: 0.3118657324965169, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 762, Loss: 0.3511900275897272, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 763, Loss: 0.2718609641425011, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 764, Loss: 0.22196420823679547, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 765, Loss: 0.3381922241354526, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 766, Loss: 0.2560014020936078, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 767, Loss: 0.47043177967118144, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 768, Loss: 0.6278346820789548, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 769, Loss: 0.46786530824725425, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 770, Loss: 0.27732990119462153, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 771, Loss: 0.4626630817491496, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 772, Loss: 0.2569354030804625, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 773, Loss: 0.239800390582108, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 774, Loss: 0.2791459361887887, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 775, Loss: 0.3199834999264699, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 776, Loss: 0.2745024009387692, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 777, Loss: 0.4525524041153858, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 778, Loss: 0.21725562002197568, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 779, Loss: 0.43832912307558597, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 780, Loss: 0.22691801821329416, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 781, Loss: 0.7371241437442011, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 782, Loss: 0.34599594492524366, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 783, Loss: 0.2973723826771366, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 784, Loss: 0.30926309954916775, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 785, Loss: 0.21201511518005753, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 786, Loss: 0.4945311565594458, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 787, Loss: 0.37620089064508455, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 788, Loss: 0.21052981873443577, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 789, Loss: 0.44157274736875063, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 790, Loss: 0.2716329272220293, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 791, Loss: 0.20936220907538042, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 792, Loss: 0.30609134428014656, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 793, Loss: 0.5406899460106239, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 794, Loss: 0.43185513494863376, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 795, Loss: 0.28113958459414784, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 796, Loss: 0.5848923354031133, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 797, Loss: 0.36118355971641236, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 798, Loss: 0.23563742934564708, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 799, Loss: 0.5103341857413581, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 800, Loss: 0.2420040824509302, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 801, Loss: 0.3725085668575112, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 802, Loss: 0.35574935172007777, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 803, Loss: 0.26417681459145026, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 804, Loss: 0.35560031088317146, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 805, Loss: 0.7722243088598221, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 806, Loss: 0.23404719249610922, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 807, Loss: 0.30656047368478745, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 808, Loss: 0.24033271948311177, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 809, Loss: 0.26473085330088925, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 810, Loss: 0.3117329130097565, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 811, Loss: 1.0184665967111255, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 812, Loss: 0.4444842448375701, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 813, Loss: 0.46235428324358374, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 814, Loss: 0.26765681044791784, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 815, Loss: 0.22506222423003316, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 816, Loss: 0.3877387466681021, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 817, Loss: 0.3659564042917198, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 818, Loss: 0.38113678147156804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 819, Loss: 0.33580242803870913, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 820, Loss: 0.2577403185275395, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 821, Loss: 0.3395153861055813, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 822, Loss: 0.25794935758314513, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 823, Loss: 0.48314414183996257, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 824, Loss: 0.28505600052957936, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 825, Loss: 0.35320831505502115, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 826, Loss: 0.33796268704899834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 827, Loss: 0.3045816472354852, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 828, Loss: 0.23152396745232953, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 829, Loss: 0.2937079542667453, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 830, Loss: 0.3764616624771106, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 831, Loss: 0.2540780793936671, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 832, Loss: 0.2270112574992376, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 833, Loss: 0.5412088651179106, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 834, Loss: 0.47242404180227804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 835, Loss: 0.5847686541004197, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 836, Loss: 0.3603427459680082, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 837, Loss: 0.6337625977565149, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 838, Loss: 0.28476662598709, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 839, Loss: 0.27691025342555586, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 840, Loss: 0.3188681381921909, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 841, Loss: 0.4966958525824694, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 842, Loss: 0.5124955877371592, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 843, Loss: 0.46365018149690373, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 844, Loss: 0.5676149135312287, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 845, Loss: 0.4990606684944041, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 846, Loss: 0.45615728592203486, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 847, Loss: 0.3465417705292223, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 848, Loss: 0.3515006192594756, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 849, Loss: 0.2356879700273321, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 850, Loss: 0.5912676926274092, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 851, Loss: 0.29944765518077476, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 852, Loss: 0.2969994880096702, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 853, Loss: 0.3323557836432103, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 854, Loss: 0.4165862615288697, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 855, Loss: 0.2775602107270132, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 856, Loss: 0.31582092662919786, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 857, Loss: 0.6456532007846347, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 858, Loss: 0.321274396980374, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 859, Loss: 0.25572543169160383, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 860, Loss: 0.3365601739947356, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 861, Loss: 0.5737039755485932, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 862, Loss: 0.30738215179076034, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 863, Loss: 0.23880043741490153, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 864, Loss: 0.3819810123211448, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 865, Loss: 0.40628488085976033, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 866, Loss: 0.2582190738723191, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 867, Loss: 0.3050594545442804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 868, Loss: 0.23027209423813627, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 869, Loss: 0.36397651006562537, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 870, Loss: 0.5412445721792141, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 871, Loss: 0.2816018892485224, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 872, Loss: 0.34922506526300057, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 873, Loss: 0.404830116536231, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 874, Loss: 0.2878578252806991, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 875, Loss: 0.4453423357950183, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 876, Loss: 0.2806028339585656, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 877, Loss: 0.31580115338165127, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 878, Loss: 0.327876949872005, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 879, Loss: 0.2789056643017759, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 880, Loss: 0.3436345295463508, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 881, Loss: 0.34312436411465175, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 882, Loss: 0.2663913619499991, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 883, Loss: 0.3483896348363113, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 884, Loss: 0.24911366625579245, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 885, Loss: 0.276128101146232, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 886, Loss: 0.38377438673722963, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 887, Loss: 0.4706819931972678, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 888, Loss: 0.4772111070922008, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 889, Loss: 0.3581788962890716, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 890, Loss: 0.21125034024454253, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 891, Loss: 0.3670389618866946, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 892, Loss: 0.34919822574849796, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 893, Loss: 0.23019157591947187, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 894, Loss: 0.24556716130935186, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 895, Loss: 0.48916860637739734, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 896, Loss: 0.3870249231725468, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 897, Loss: 0.21782705207520653, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 898, Loss: 0.22576750488720881, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 899, Loss: 0.267529745704561, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 900, Loss: 0.448754340351569, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 901, Loss: 0.5490702352117987, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 902, Loss: 0.4530595783981206, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 903, Loss: 0.5634469314997422, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 904, Loss: 0.35328120888659753, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 905, Loss: 0.44462216621300976, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 906, Loss: 0.4601317174263089, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 907, Loss: 0.2965283449359314, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 908, Loss: 0.2271387560698197, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 909, Loss: 0.3504507026814666, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 910, Loss: 0.5094451330881694, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 911, Loss: 0.37091980304579186, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 912, Loss: 0.40956776874916934, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 913, Loss: 0.2584085501143325, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 914, Loss: 0.2933307420650441, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 915, Loss: 0.7119772322254174, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 916, Loss: 0.2431090121785014, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 917, Loss: 0.5657599962512335, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 918, Loss: 0.29748851797401116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 919, Loss: 0.46774410197955646, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 920, Loss: 0.3667427722844359, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 921, Loss: 0.2658634731467189, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 922, Loss: 0.2962573511763759, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 923, Loss: 0.2728392890353055, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 924, Loss: 0.4104532628868154, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 925, Loss: 0.613962555646042, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 926, Loss: 0.5635714984244482, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 927, Loss: 0.45134126233804084, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 928, Loss: 0.3838667183204281, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 929, Loss: 0.24750070769782773, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 930, Loss: 0.37432777074538565, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 931, Loss: 0.321002454075409, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 932, Loss: 0.28208998696045057, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 933, Loss: 0.2867087315486758, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 934, Loss: 0.2974593674871199, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 935, Loss: 0.36439718609417804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 936, Loss: 0.23456731969265035, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 937, Loss: 0.23429035799500778, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 938, Loss: 0.8115812616098411, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 939, Loss: 0.23078120701908555, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 940, Loss: 0.28505922003134654, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 941, Loss: 0.39875979558513464, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 942, Loss: 0.23942303584514804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 943, Loss: 0.42728317641752567, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 944, Loss: 0.40260648149634737, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 945, Loss: 0.32985645211998726, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 946, Loss: 0.2728230450566503, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 947, Loss: 0.3905661908805076, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 948, Loss: 0.3802834027099953, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 949, Loss: 0.41122511776902804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 950, Loss: 0.27011057938983485, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 951, Loss: 0.2117416451209664, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 952, Loss: 0.3116666676291914, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 953, Loss: 0.2975452426860761, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 954, Loss: 0.3566523416850147, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 955, Loss: 0.34086883449951866, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 956, Loss: 0.26535581070080577, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 957, Loss: 0.29764390439156285, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 958, Loss: 0.24789611392395466, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 959, Loss: 0.5485513285124836, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 960, Loss: 0.3217252269667841, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 961, Loss: 0.30887553906535364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 962, Loss: 0.2928199087231411, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 963, Loss: 0.34164564407120884, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 964, Loss: 0.31615412207489973, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 965, Loss: 0.36256114714413745, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 966, Loss: 0.3880365020386484, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 967, Loss: 0.2630476617365709, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 968, Loss: 0.2363778022055399, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 969, Loss: 0.39634867928161044, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 970, Loss: 0.29092973154400625, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 971, Loss: 0.3728159195470476, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 972, Loss: 0.4051870181760675, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 973, Loss: 0.3747969256902066, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 974, Loss: 0.3845082369892582, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 975, Loss: 0.2860311758701713, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 976, Loss: 0.40308185822432485, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 977, Loss: 0.22883622074466692, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 978, Loss: 0.5882904200099859, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 979, Loss: 0.30929013636386204, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 980, Loss: 0.28763020321143307, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 981, Loss: 0.32339315497916477, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 982, Loss: 0.2554587202607111, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 983, Loss: 0.297887454998338, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 984, Loss: 0.3168395844985062, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 985, Loss: 0.2465394407386849, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 986, Loss: 0.4940400352257397, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 987, Loss: 0.3519155223144875, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 988, Loss: 0.3274690196457898, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 989, Loss: 0.3803709854726204, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 990, Loss: 0.633078148619955, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 991, Loss: 0.2800206544282625, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 992, Loss: 0.23991095021232084, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 993, Loss: 0.21106031370668063, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 994, Loss: 0.23118600797331873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 995, Loss: 0.4689262027052573, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 996, Loss: 0.4637155025025198, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 997, Loss: 0.23348283256729688, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 998, Loss: 0.5782676379990852, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 999, Loss: 0.2584302918122001, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1000, Loss: 0.40420579005348567, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1001, Loss: 0.3742586947600709, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1002, Loss: 0.3895968569349042, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1003, Loss: 0.22238440214254185, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1004, Loss: 0.24486991767365512, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1005, Loss: 0.4097824466663835, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1006, Loss: 0.2962899332081206, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1007, Loss: 0.40514818929724294, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1008, Loss: 0.2102059093905439, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1009, Loss: 0.5386479214164333, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1010, Loss: 0.2782780487057136, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1011, Loss: 0.2910013500237741, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1012, Loss: 0.37224509509787534, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1013, Loss: 0.4011573564679137, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1014, Loss: 0.43965182783318124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1015, Loss: 0.5487170151548539, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1016, Loss: 0.26413515892470907, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1017, Loss: 0.43339013061095366, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1018, Loss: 0.3334154669991453, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1019, Loss: 0.31622348615535123, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1020, Loss: 0.2849167300829273, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1021, Loss: 0.4051865598236356, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1022, Loss: 0.6015696472755695, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1023, Loss: 0.4660226552031862, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1024, Loss: 0.2711894995847947, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1025, Loss: 0.31836436295485915, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1026, Loss: 0.4557803450857274, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1027, Loss: 0.394608006055013, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1028, Loss: 0.44710904997398754, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1029, Loss: 0.3506738462877266, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1030, Loss: 0.3042678646979969, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1031, Loss: 0.41894172996073303, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1032, Loss: 0.31002818029829066, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1033, Loss: 0.5037510382864112, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1034, Loss: 0.3106857130516583, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1035, Loss: 0.4637628147830357, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1036, Loss: 0.2971301643061823, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1037, Loss: 0.40905686273092934, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1038, Loss: 0.297673892746153, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1039, Loss: 0.3059016537091653, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1040, Loss: 0.5272374954800474, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1041, Loss: 0.25440487164648495, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1042, Loss: 0.2366551370656056, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1043, Loss: 0.28993424096959364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1044, Loss: 0.27459339940777705, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1045, Loss: 0.4254149098960762, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1046, Loss: 0.3668143049489173, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1047, Loss: 0.4177816958279954, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1048, Loss: 0.2870687266776733, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1049, Loss: 0.582527548101122, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1050, Loss: 0.40213905956519147, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1051, Loss: 0.25077750530728893, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1052, Loss: 0.4525356513316252, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1053, Loss: 0.44003227664235384, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1054, Loss: 0.26689369349142705, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1055, Loss: 0.25845329181858834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1056, Loss: 0.2682357807261029, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1057, Loss: 0.2978591626135395, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1058, Loss: 0.5398802934394948, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1059, Loss: 0.3325921627205103, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1060, Loss: 0.2518184912296935, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1061, Loss: 0.43474903922500024, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1062, Loss: 0.5843159870759148, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1063, Loss: 0.31610772253139563, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1064, Loss: 0.3780439242639213, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1065, Loss: 0.4946290536047132, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1066, Loss: 0.4056858860863223, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1067, Loss: 0.24448032347830567, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1068, Loss: 0.2640551520917618, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1069, Loss: 0.26961204934519867, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1070, Loss: 0.3059063685011027, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1071, Loss: 0.3047833042201429, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1072, Loss: 0.24900442624918004, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1073, Loss: 0.4751615591315945, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1074, Loss: 0.2691826455929557, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1075, Loss: 0.4086223722126883, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1076, Loss: 0.287088399824711, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1077, Loss: 0.293885937894384, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1078, Loss: 0.29081125811770664, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1079, Loss: 0.36977344577075355, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1080, Loss: 0.27659383701006723, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1081, Loss: 0.27106271039982777, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1082, Loss: 0.2368646929078293, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1083, Loss: 0.33479661993202847, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1084, Loss: 0.3025995441903745, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1085, Loss: 0.46627658038239084, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1086, Loss: 0.3975727895365645, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1087, Loss: 0.44093532042274447, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1088, Loss: 0.4726280511478408, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1089, Loss: 0.2284145176803212, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1090, Loss: 0.3552810908553507, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1091, Loss: 0.3335138232493857, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1092, Loss: 0.5234453533413137, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1093, Loss: 0.2529213269450905, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1094, Loss: 0.9482400699734482, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1095, Loss: 0.24396038387628635, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1096, Loss: 0.38573856474731427, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1097, Loss: 0.3903683150885876, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1098, Loss: 0.3038278155392693, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1099, Loss: 0.310052804000969, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1100, Loss: 0.3041746412028802, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1101, Loss: 0.48634455480650735, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1102, Loss: 0.46744141401496875, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1103, Loss: 0.6372631394658832, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1104, Loss: 0.36845351787497227, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1105, Loss: 0.40573455820063414, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1106, Loss: 0.5020278344309952, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1107, Loss: 0.26479320725552624, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1108, Loss: 0.42794362700398286, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1109, Loss: 0.3123634708672762, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1110, Loss: 0.27524978336921535, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1111, Loss: 0.5697837215788041, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1112, Loss: 0.4540369215613747, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1113, Loss: 0.21026470424888982, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1114, Loss: 0.27634261872837695, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1115, Loss: 0.2137298740714632, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1116, Loss: 0.5339411553300503, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1117, Loss: 0.4995168077985855, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1118, Loss: 0.26224917144732474, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1119, Loss: 0.582230380563703, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1120, Loss: 0.3456466869495983, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1121, Loss: 0.4501310980768727, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1122, Loss: 0.3540411288460316, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1123, Loss: 0.35689529903521683, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1124, Loss: 0.2254249692296877, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1125, Loss: 0.32056360804513473, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1126, Loss: 0.47530207985463735, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1127, Loss: 0.5006581448714239, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1128, Loss: 0.33374816072337793, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1129, Loss: 0.2743317099827151, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1130, Loss: 0.2463074546970967, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1131, Loss: 0.5736409047272569, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1132, Loss: 0.2440127597112946, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1133, Loss: 0.38329801258403384, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1134, Loss: 0.4840558040617977, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1135, Loss: 0.33315187539630847, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1136, Loss: 0.3354280302771374, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1137, Loss: 0.5191105841883752, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1138, Loss: 0.5289053430514615, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1139, Loss: 0.3061772336207298, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1140, Loss: 0.34591467858261493, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1141, Loss: 0.6217711227226153, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1142, Loss: 0.439925951853109, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1143, Loss: 0.30254920478884695, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1144, Loss: 0.4390973996194804, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1145, Loss: 0.3087260656569657, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1146, Loss: 0.32669333609039497, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1147, Loss: 0.26020987181163224, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1148, Loss: 0.22707443532004398, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1149, Loss: 0.3169876964471401, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1150, Loss: 0.47277160535084545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1151, Loss: 0.4460347767780509, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1152, Loss: 0.3545241142058648, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1153, Loss: 0.26694726615452485, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1154, Loss: 0.34968887308415386, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1155, Loss: 0.4307642291663908, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1156, Loss: 0.34572024588600814, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1157, Loss: 0.3143342173932154, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1158, Loss: 0.4712046114427788, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1159, Loss: 0.39229693129758325, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1160, Loss: 0.3530105647954668, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1161, Loss: 0.2757208693895714, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1162, Loss: 0.5098204773995073, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1163, Loss: 0.26233805618083794, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1164, Loss: 0.3498331118010899, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1165, Loss: 0.23941620499011532, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1166, Loss: 0.304102357288662, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1167, Loss: 0.22591047053175192, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1168, Loss: 0.2690563184994259, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1169, Loss: 0.346645764617148, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1170, Loss: 0.25343319027619743, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1171, Loss: 0.5365800902086603, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1172, Loss: 0.2872067229656873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1173, Loss: 0.5982927808935901, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1174, Loss: 0.38790688266254114, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1175, Loss: 0.3657009299838323, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1176, Loss: 0.25346220387046176, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1177, Loss: 0.2230280486684123, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1178, Loss: 0.3371666326558873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1179, Loss: 0.2814297176965139, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1180, Loss: 0.36546743762630796, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1181, Loss: 0.26474326970055206, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1182, Loss: 0.6147065348962815, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1183, Loss: 0.26891749646405916, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1184, Loss: 0.5218193287048943, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1185, Loss: 0.22892640932327776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1186, Loss: 0.2070636734085853, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1187, Loss: 0.33842630978678767, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1188, Loss: 0.32334352649316556, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1189, Loss: 0.22960061621105893, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1190, Loss: 0.2946429324856951, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1191, Loss: 0.655972187034809, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1192, Loss: 0.2755561773908597, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1193, Loss: 0.5731143997899912, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1194, Loss: 0.4403632230384873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1195, Loss: 0.2577533196122921, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1196, Loss: 0.4070246538212724, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1197, Loss: 0.24945034305203612, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1198, Loss: 0.3355618207909085, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1199, Loss: 0.46500477914831134, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1200, Loss: 0.28906650081488566, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1201, Loss: 0.30367106218782514, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1202, Loss: 0.2133904164500186, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1203, Loss: 0.45605364331794396, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1204, Loss: 0.29176369302497596, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1205, Loss: 0.27873723897987357, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1206, Loss: 0.2554062896834661, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1207, Loss: 0.3168781400429277, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1208, Loss: 0.36638326840957, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1209, Loss: 0.26119040750410916, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1210, Loss: 0.2484270513662029, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1211, Loss: 0.6745460804962683, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1212, Loss: 0.29685520890729455, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1213, Loss: 0.5348256865926941, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1214, Loss: 0.28803058354242705, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1215, Loss: 0.323435063256712, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1216, Loss: 0.4015421383261868, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1217, Loss: 0.2506297951440308, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1218, Loss: 0.5988912301751272, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1219, Loss: 0.20789204805426337, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1220, Loss: 0.3685724172788702, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1221, Loss: 0.8590230307800966, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1222, Loss: 0.6393298499599447, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1223, Loss: 0.22352885235331463, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1224, Loss: 0.4521393811901748, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1225, Loss: 0.42521013113092837, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1226, Loss: 0.43677666475320753, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1227, Loss: 0.37370642667064125, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1228, Loss: 0.23898648876188416, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1229, Loss: 0.6154483802900529, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1230, Loss: 0.2826026798160039, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1231, Loss: 0.4734832390476901, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1232, Loss: 0.27480390755439743, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1233, Loss: 0.3731909074728921, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1234, Loss: 0.4105991391709013, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1235, Loss: 0.27409373113439844, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1236, Loss: 0.49313177732751834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1237, Loss: 0.24523739946197826, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1238, Loss: 0.4140474699191877, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1239, Loss: 0.38529104814572457, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1240, Loss: 0.40280981292101015, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1241, Loss: 0.28755344602791444, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1242, Loss: 0.30718323369546785, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1243, Loss: 0.3395784109516977, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1244, Loss: 0.2895607034616388, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1245, Loss: 0.4595181269279115, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1246, Loss: 0.33703521983367923, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1247, Loss: 0.29193239600815507, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1248, Loss: 0.2654283022909126, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1249, Loss: 0.2962194052031512, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1250, Loss: 0.26643008207646834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1251, Loss: 0.4650543002669951, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1252, Loss: 0.4540889696174232, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1253, Loss: 0.33005740529120364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1254, Loss: 0.34211579289911853, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1255, Loss: 0.26182561035632834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1256, Loss: 0.4849867831146406, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1257, Loss: 0.4068377249032513, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1258, Loss: 0.2681458684707264, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1259, Loss: 0.2562145691346819, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1260, Loss: 0.2629204125364119, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1261, Loss: 0.3266980415583769, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1262, Loss: 0.566215090351823, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1263, Loss: 0.30216321110338124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1264, Loss: 0.26635576308486975, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1265, Loss: 0.346337840723268, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1266, Loss: 0.3143636760085604, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1267, Loss: 0.2589797192314365, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1268, Loss: 0.45039780938084084, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1269, Loss: 0.24270185349138798, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1270, Loss: 0.3321215341771644, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1271, Loss: 0.3408795311331132, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1272, Loss: 0.3132391333505194, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1273, Loss: 0.3801211898023359, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1274, Loss: 0.5442121976173169, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1275, Loss: 0.21834110352563646, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1276, Loss: 0.40459989044039923, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1277, Loss: 0.3627300801287606, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1278, Loss: 0.35499473597671, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1279, Loss: 0.22610065197253182, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1280, Loss: 0.3648157985547519, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1281, Loss: 0.4010521151373371, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1282, Loss: 0.2532877527207935, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1283, Loss: 0.3418277647674298, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1284, Loss: 0.4802190546658015, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1285, Loss: 0.2615367674069232, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1286, Loss: 0.3072949934023641, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1287, Loss: 0.3027613464544076, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1288, Loss: 0.6600940460297877, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1289, Loss: 0.3097506768107809, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1290, Loss: 0.3203001720617886, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1291, Loss: 0.39509836552796007, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1292, Loss: 0.2934944483536176, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1293, Loss: 0.5600873962038331, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1294, Loss: 0.3274952568410735, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1295, Loss: 0.2682165916703312, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1296, Loss: 0.29413103666931606, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1297, Loss: 0.3054306298535091, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1298, Loss: 0.4784531416658083, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1299, Loss: 0.32679950309450745, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1300, Loss: 0.39574701046537786, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1301, Loss: 0.45100904489921434, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1302, Loss: 0.4032944821079333, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1303, Loss: 0.29933231197655036, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1304, Loss: 0.2931299943317337, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1305, Loss: 0.3622479780171308, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1306, Loss: 0.2742050849134068, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1307, Loss: 0.5530506599908631, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1308, Loss: 0.3743019069692532, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1309, Loss: 0.4568868110309025, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1310, Loss: 0.3425729968918465, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1311, Loss: 0.31508733878234385, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1312, Loss: 0.5008629745365116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1313, Loss: 0.2546644772464266, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1314, Loss: 0.6312634908022858, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1315, Loss: 0.3413173941904416, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1316, Loss: 0.29960947607341415, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1317, Loss: 0.25738945991930806, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1318, Loss: 0.3500410319865484, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1319, Loss: 0.2599337958103194, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1320, Loss: 0.31476639799804584, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1321, Loss: 0.2568413255070378, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1322, Loss: 0.3883411803496122, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1323, Loss: 0.2612315736662359, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1324, Loss: 0.4449118210281613, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1325, Loss: 0.35855430504582686, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1326, Loss: 0.4509427889328682, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1327, Loss: 0.28383192672351865, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1328, Loss: 0.31749750390266074, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1329, Loss: 0.39417539004730734, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1330, Loss: 0.3761709869286063, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1331, Loss: 0.23188889552418515, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1332, Loss: 0.362886811543057, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1333, Loss: 0.40173838820305835, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1334, Loss: 0.5414747213083562, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1335, Loss: 0.4198617893900888, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1336, Loss: 0.7346526091398793, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1337, Loss: 0.2854366082903915, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1338, Loss: 0.24344394225288396, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1339, Loss: 0.564075886407392, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1340, Loss: 0.33182798158875937, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1341, Loss: 0.31958510631398934, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1342, Loss: 0.3313298333202648, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1343, Loss: 0.24977992386961173, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1344, Loss: 0.35960685161725836, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1345, Loss: 0.2650644656648862, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1346, Loss: 0.23860161714718497, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1347, Loss: 0.6009789706750263, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1348, Loss: 0.2731080861506037, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1349, Loss: 0.8268081310278473, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1350, Loss: 0.24973842777850114, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1351, Loss: 0.32614836569543904, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1352, Loss: 0.3835513615347449, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1353, Loss: 0.47472794245820826, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1354, Loss: 0.3419572395476927, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1355, Loss: 0.3618387730560366, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1356, Loss: 0.29629079074563613, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1357, Loss: 0.3938779663378071, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1358, Loss: 0.2681092714446929, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1359, Loss: 0.3127628908765746, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1360, Loss: 0.3792486394592306, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1361, Loss: 0.28955969081608346, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1362, Loss: 0.3129609516492703, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1363, Loss: 0.341662374010851, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1364, Loss: 0.3682437320377977, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1365, Loss: 0.4164137426361325, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1366, Loss: 0.2573099736479207, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1367, Loss: 0.30856603537712335, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1368, Loss: 0.5951002450841205, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1369, Loss: 0.4579635354780527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1370, Loss: 0.5175104007525028, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1371, Loss: 0.4434617676143161, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1372, Loss: 0.4766488877881614, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1373, Loss: 0.48083478979962896, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1374, Loss: 0.544588570758699, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1375, Loss: 0.30512746971359994, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1376, Loss: 0.3055664925547068, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1377, Loss: 0.37333249232109667, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1378, Loss: 0.499985517515677, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1379, Loss: 0.263227460959895, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1380, Loss: 0.6110447934054999, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1381, Loss: 0.5015452168955847, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1382, Loss: 0.2624406383046427, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1383, Loss: 0.3466455808939708, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1384, Loss: 0.2963442030526474, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1385, Loss: 0.3596564250284806, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1386, Loss: 0.3701487137611827, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1387, Loss: 0.2806299939732655, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1388, Loss: 0.22968464487446316, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1389, Loss: 0.48487229155065575, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1390, Loss: 0.4466830464836977, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1391, Loss: 0.3326680384751445, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1392, Loss: 0.3411985734047894, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1393, Loss: 0.3718768119984016, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1394, Loss: 0.4491292241142275, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1395, Loss: 0.2731509196923837, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1396, Loss: 0.22222092946314034, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1397, Loss: 0.2359823924206958, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1398, Loss: 0.3787405348874834, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1399, Loss: 0.40821154829681894, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1400, Loss: 0.540146769167304, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1401, Loss: 0.2664766654843116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1402, Loss: 0.26696156057853154, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1403, Loss: 0.25221042594908377, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1404, Loss: 0.31259776653788973, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1405, Loss: 0.2123710471637903, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1406, Loss: 0.4617031837740081, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1407, Loss: 0.24084822768314001, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1408, Loss: 0.35550336905405333, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1409, Loss: 0.23591499396882604, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1410, Loss: 0.39810060284203086, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1411, Loss: 0.2626972473872071, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1412, Loss: 0.44808454978760653, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1413, Loss: 0.4788961882022088, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1414, Loss: 0.29422713074274043, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1415, Loss: 0.36439884396173233, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1416, Loss: 0.49096597854070134, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1417, Loss: 0.4501361521640297, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1418, Loss: 0.2220278833000032, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1419, Loss: 0.47716368404896703, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1420, Loss: 0.22730535153194736, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1421, Loss: 0.2259246447978761, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1422, Loss: 0.2520768276123972, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1423, Loss: 0.522601824950305, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1424, Loss: 0.5110096894759482, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1425, Loss: 0.5832180648092471, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1426, Loss: 0.35529083212396567, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1427, Loss: 0.4046364147233301, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1428, Loss: 0.5101953930981438, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1429, Loss: 0.366152241103669, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1430, Loss: 0.419204974861717, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1431, Loss: 0.2657349999262163, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1432, Loss: 0.6175988807713663, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1433, Loss: 0.30861885354532625, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1434, Loss: 0.5198526216094187, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1435, Loss: 0.3333253186700465, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1436, Loss: 0.3252866223930857, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1437, Loss: 0.29779016718257334, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1438, Loss: 0.2612109798653937, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1439, Loss: 0.2393077968842204, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1440, Loss: 0.280186425018529, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1441, Loss: 0.4829734116293527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1442, Loss: 0.4625833152879889, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1443, Loss: 0.3794669108129306, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1444, Loss: 0.38265949395612775, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1445, Loss: 0.43604853321163206, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1446, Loss: 0.2326823395997228, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1447, Loss: 0.26509585454015444, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1448, Loss: 0.28614988060038626, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1449, Loss: 0.3951542414396533, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1450, Loss: 0.24080922920669307, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1451, Loss: 0.3202168120885806, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1452, Loss: 0.249009246271233, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1453, Loss: 0.2439094462545734, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1454, Loss: 0.25235597718496394, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1455, Loss: 0.32669143766462283, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1456, Loss: 0.22653050609078496, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1457, Loss: 0.3089403327212498, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1458, Loss: 0.2271830106536622, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1459, Loss: 0.3055223980548869, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1460, Loss: 0.22677455660036241, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1461, Loss: 0.3862517725659874, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1462, Loss: 0.2916076762830446, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1463, Loss: 0.23320317660400713, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1464, Loss: 0.24819003978554893, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1465, Loss: 0.27413244658989977, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1466, Loss: 0.4528656714517195, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1467, Loss: 0.45189693846606593, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1468, Loss: 0.2314415899900412, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1469, Loss: 0.3377666795207257, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1470, Loss: 0.6992489615843929, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1471, Loss: 0.3389827895339605, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1472, Loss: 0.559633981333451, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1473, Loss: 0.2703254314513869, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1474, Loss: 0.27954693134584296, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1475, Loss: 0.4395660811522861, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1476, Loss: 0.29193086289104436, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1477, Loss: 0.44805351869821697, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1478, Loss: 0.3883656661487214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1479, Loss: 0.3121798814593071, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1480, Loss: 0.6046429585889717, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1481, Loss: 0.5109864728984335, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1482, Loss: 0.33190484493118194, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1483, Loss: 0.5098045586170845, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1484, Loss: 0.3748870049845515, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1485, Loss: 0.4655490688097633, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1486, Loss: 0.2857488344437609, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1487, Loss: 0.3745413328402275, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1488, Loss: 0.21598821985659486, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1489, Loss: 0.24538805373414266, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1490, Loss: 0.245017405330192, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1491, Loss: 0.3024916688278026, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1492, Loss: 0.3384557504356577, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1493, Loss: 0.23663672992544552, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1494, Loss: 0.3540897222465247, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1495, Loss: 0.22580771770473534, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1496, Loss: 0.2755288411061443, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1497, Loss: 0.357727547382662, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1498, Loss: 0.4010298046628694, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1499, Loss: 0.275346835116392, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1500, Loss: 0.4998118442140885, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1501, Loss: 0.29886726275544895, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1502, Loss: 0.22500564901084535, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1503, Loss: 0.25325936891367967, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1504, Loss: 0.36451874842144877, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1505, Loss: 0.3054646452385179, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1506, Loss: 0.30018762354950457, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1507, Loss: 0.2780627470380994, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1508, Loss: 0.27493399996894746, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1509, Loss: 0.2943589041445254, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1510, Loss: 0.31697763115842953, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1511, Loss: 0.3024478043138188, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1512, Loss: 0.9014369321803533, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1513, Loss: 0.30142336253707147, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1514, Loss: 0.30002733179552427, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1515, Loss: 0.37877675647926046, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1516, Loss: 0.35784821939257155, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1517, Loss: 0.2894534780424043, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1518, Loss: 0.3423721875941099, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1519, Loss: 0.43004148732351, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1520, Loss: 0.6107275673168372, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1521, Loss: 0.32477241546691993, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1522, Loss: 0.49664901907675435, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1523, Loss: 0.33019478584168604, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1524, Loss: 0.27504750976460873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1525, Loss: 0.33825130364238776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1526, Loss: 0.24492253652604057, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1527, Loss: 0.3527822286943213, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1528, Loss: 0.36701981417347684, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1529, Loss: 0.2891924682580127, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1530, Loss: 0.2564526169517445, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1531, Loss: 0.24700636391438055, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1532, Loss: 0.48108770452879435, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1533, Loss: 0.30245878884960925, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1534, Loss: 0.31641338120710816, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1535, Loss: 0.33129397132648625, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1536, Loss: 0.5419741706343407, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1537, Loss: 0.3772749274091808, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1538, Loss: 0.2510095828305869, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1539, Loss: 0.23557080035800887, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1540, Loss: 0.3374166580033774, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1541, Loss: 0.42426431572135487, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1542, Loss: 0.23649386411259657, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1543, Loss: 0.265300235545483, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1544, Loss: 0.36022710690488763, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1545, Loss: 0.31016626848433565, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1546, Loss: 0.2800931705648979, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1547, Loss: 0.40340565191499395, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1548, Loss: 0.32223279582945324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1549, Loss: 0.3196592941422116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1550, Loss: 0.37709612930293746, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1551, Loss: 0.4163468994015054, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1552, Loss: 0.21063325941029437, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1553, Loss: 0.42196423123694937, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1554, Loss: 0.440179810425788, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1555, Loss: 0.4748918146634289, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1556, Loss: 0.4055336781324595, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1557, Loss: 0.40792584090918116, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1558, Loss: 0.2236242479111133, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1559, Loss: 0.34644857233585813, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1560, Loss: 0.7180094883167796, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1561, Loss: 0.3530631592746215, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1562, Loss: 0.8283535280815267, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1563, Loss: 0.3232967492389551, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1564, Loss: 0.2913769971683544, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1565, Loss: 0.30570979489054856, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1566, Loss: 0.2917355782576002, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1567, Loss: 0.25990448951840844, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1568, Loss: 0.2167591590838255, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1569, Loss: 0.48910774074512386, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1570, Loss: 0.30713236489630713, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1571, Loss: 0.3057723834010263, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1572, Loss: 0.49092354433974456, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1573, Loss: 0.4494028858613305, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1574, Loss: 0.2606486095543797, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1575, Loss: 0.30567373704163686, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1576, Loss: 0.44907328299762117, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1577, Loss: 0.20418506949362092, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1578, Loss: 0.3853539755876534, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1579, Loss: 0.51723145213165, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1580, Loss: 0.6124199499850359, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1581, Loss: 0.3617805287216266, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1582, Loss: 0.4053177022392278, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1583, Loss: 0.3372904861150332, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1584, Loss: 0.44056294760784487, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1585, Loss: 0.27645954628627795, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1586, Loss: 0.3261659465833291, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1587, Loss: 0.24870349094147134, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1588, Loss: 0.23931265970415797, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1589, Loss: 0.27703224657304387, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1590, Loss: 0.2767604490813208, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1591, Loss: 0.21787119069077313, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1592, Loss: 0.3880822732050266, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1593, Loss: 0.2987295947490527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1594, Loss: 0.3167031894499927, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1595, Loss: 0.2372549883183419, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1596, Loss: 0.22372896450646257, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1597, Loss: 0.2963462261795297, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1598, Loss: 0.3156930173741188, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1599, Loss: 0.28861189418480404, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1600, Loss: 0.23491508516414522, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1601, Loss: 0.4074862778133961, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1602, Loss: 0.2804071618684236, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1603, Loss: 0.6245898518836319, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1604, Loss: 0.5476942380746339, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1605, Loss: 0.41171913414358696, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1606, Loss: 0.3415241792354595, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1607, Loss: 0.4130650141019113, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1608, Loss: 0.38103103582909065, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1609, Loss: 0.33481847641268103, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1610, Loss: 0.250892808342459, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1611, Loss: 0.2634352047021945, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1612, Loss: 0.5043925912735445, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1613, Loss: 0.3141438802707699, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1614, Loss: 0.39959278282217936, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1615, Loss: 0.2522518951191053, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1616, Loss: 0.5324905035395826, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1617, Loss: 0.41951914777650645, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1618, Loss: 0.3146875201148863, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1619, Loss: 0.29634232297531965, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1620, Loss: 0.2940564264504468, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1621, Loss: 0.38447499240763566, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1622, Loss: 0.26106362089873125, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1623, Loss: 0.2960025609854274, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1624, Loss: 0.3129362402994659, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1625, Loss: 0.3466380788210349, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1626, Loss: 0.20810260394329053, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1627, Loss: 0.4524805764252694, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1628, Loss: 0.22589495981776916, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1629, Loss: 0.32597504992288606, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1630, Loss: 0.2583565229504341, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1631, Loss: 0.24177300511542424, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1632, Loss: 0.38110343680153824, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1633, Loss: 0.30661265429281387, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1634, Loss: 0.4998993687379616, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1635, Loss: 0.4592230944778348, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1636, Loss: 0.3463386308201173, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1637, Loss: 0.4285390433532992, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1638, Loss: 0.22413536933664546, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1639, Loss: 0.26594142237893975, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1640, Loss: 0.47407442438204106, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1641, Loss: 0.4961576653136198, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1642, Loss: 0.4027361310207631, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1643, Loss: 0.2967675394486518, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1644, Loss: 0.8404654664682919, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1645, Loss: 0.3868347234324318, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1646, Loss: 0.36050105820260214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1647, Loss: 0.2618423747920859, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1648, Loss: 0.314736846908415, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1649, Loss: 0.23580338258271402, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1650, Loss: 0.570663633225138, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1651, Loss: 0.7443542694972118, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1652, Loss: 0.26127168185650185, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1653, Loss: 0.28364343720821017, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1654, Loss: 0.4005381941248277, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1655, Loss: 0.41701260266537, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1656, Loss: 0.42451564808473957, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1657, Loss: 0.31688433432711505, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1658, Loss: 0.4283529152709725, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1659, Loss: 0.3042612665218324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1660, Loss: 0.3692024901250561, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1661, Loss: 0.33572038011167615, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1662, Loss: 0.4231189630542067, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1663, Loss: 0.22946238684794654, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1664, Loss: 0.4492894632479697, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1665, Loss: 0.2946952346604822, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1666, Loss: 0.4333476207979726, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1667, Loss: 0.38158008045214165, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1668, Loss: 0.3910912134135367, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1669, Loss: 0.4051405156843464, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1670, Loss: 0.3072838970866305, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1671, Loss: 0.41664427571431706, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1672, Loss: 0.2903719814958736, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1673, Loss: 0.4467140740062906, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1674, Loss: 0.3604731841665152, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1675, Loss: 0.40584838709776605, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1676, Loss: 0.42389100318778167, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1677, Loss: 0.45104281701855786, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1678, Loss: 0.4399132913266814, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1679, Loss: 0.25091086700326626, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1680, Loss: 0.4885074104013268, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1681, Loss: 0.3537810463159384, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1682, Loss: 0.3035125171947527, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1683, Loss: 0.3623274009115082, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1684, Loss: 0.517760169876218, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1685, Loss: 0.2861601147352334, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1686, Loss: 0.6920268867329247, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1687, Loss: 0.2594612505148801, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1688, Loss: 0.24323128580063025, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1689, Loss: 0.5058903902454899, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1690, Loss: 0.26970102539750507, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1691, Loss: 0.6614383384698197, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1692, Loss: 0.36279351798355275, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1693, Loss: 0.2231854862667545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1694, Loss: 0.292174167695184, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1695, Loss: 0.22658743544512058, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1696, Loss: 0.32940163833501657, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1697, Loss: 0.4830371889786189, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1698, Loss: 0.3299380533651245, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1699, Loss: 0.33722232651556, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1700, Loss: 0.28086116760171287, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1701, Loss: 0.27332733720396357, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1702, Loss: 0.3030315897423565, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1703, Loss: 0.41430803403257516, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1704, Loss: 0.24327273766966978, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1705, Loss: 0.28333128673071056, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1706, Loss: 0.4768652077840194, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1707, Loss: 0.6233675408328827, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1708, Loss: 0.21522096895461731, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1709, Loss: 0.2165880331501212, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1710, Loss: 0.24478830568353993, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1711, Loss: 0.5804319564911312, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1712, Loss: 0.4787403783606432, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1713, Loss: 0.5519535162267839, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1714, Loss: 0.33219252870995564, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1715, Loss: 0.28183930945327884, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1716, Loss: 0.34045491971494873, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1717, Loss: 0.3184370390620849, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1718, Loss: 0.3350332189145333, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1719, Loss: 0.31210232578595487, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1720, Loss: 0.47704993911833815, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1721, Loss: 0.5094065283360186, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1722, Loss: 0.35786579113022776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1723, Loss: 0.29765211440231115, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1724, Loss: 0.2185365354988169, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1725, Loss: 0.37141788819249766, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1726, Loss: 0.24005967138221704, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1727, Loss: 0.5564106662594739, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1728, Loss: 0.34516572350694913, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1729, Loss: 0.4356403987688809, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1730, Loss: 0.4556130953598781, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1731, Loss: 0.3796773949857961, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1732, Loss: 0.31348173811561675, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1733, Loss: 0.5464195558003293, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1734, Loss: 0.25682341450072693, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1735, Loss: 0.2543975694269084, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1736, Loss: 0.28596231179085735, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1737, Loss: 0.37639240594372947, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1738, Loss: 0.2347462088395607, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1739, Loss: 0.4268636879443475, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1740, Loss: 0.2523188618393549, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1741, Loss: 0.42511422526530573, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1742, Loss: 0.379448181784775, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1743, Loss: 0.3266813177094855, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1744, Loss: 0.4738744458159331, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1745, Loss: 0.3423801366872124, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1746, Loss: 0.30735147714454575, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1747, Loss: 0.39418912722831545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1748, Loss: 0.3826781742823608, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1749, Loss: 0.25450885264885104, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1750, Loss: 0.3491798703373865, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1751, Loss: 0.39289344323332787, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1752, Loss: 0.24080436815233505, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1753, Loss: 0.3629751848929995, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1754, Loss: 0.3774741975293781, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1755, Loss: 0.30466358107000235, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1756, Loss: 0.24682095957550007, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1757, Loss: 0.5139111493721495, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1758, Loss: 0.3224729922120259, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1759, Loss: 0.5501481496737884, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1760, Loss: 0.2749332448083218, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1761, Loss: 0.2259789793061783, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1762, Loss: 0.5386876116207717, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1763, Loss: 0.4217313894072737, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1764, Loss: 0.2603517222162921, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1765, Loss: 0.41305698523018364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1766, Loss: 0.23093761707394544, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1767, Loss: 0.29748866892982867, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1768, Loss: 0.35855538194663317, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1769, Loss: 0.3518317740110747, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1770, Loss: 0.36331344241870234, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1771, Loss: 0.35776456451503136, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1772, Loss: 0.30636025649427084, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1773, Loss: 0.4876247370137793, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1774, Loss: 0.4389117349418362, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1775, Loss: 0.4857877184882401, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1776, Loss: 0.2819334773647944, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1777, Loss: 0.46521159271332974, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1778, Loss: 0.535454794504187, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1779, Loss: 0.25453038540332545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1780, Loss: 0.29679009690628977, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1781, Loss: 0.3222410424466244, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1782, Loss: 0.45750739369886895, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1783, Loss: 0.2762781402306237, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1784, Loss: 0.32825964243913786, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1785, Loss: 0.349186270176261, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1786, Loss: 0.602151815053547, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1787, Loss: 0.23305967039417308, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1788, Loss: 0.3333778294114733, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1789, Loss: 0.35194507958208776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1790, Loss: 0.4477102590103301, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1791, Loss: 0.3105438450341863, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1792, Loss: 0.41919211937159906, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1793, Loss: 0.4890471397082138, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1794, Loss: 0.25073355861119556, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1795, Loss: 0.29837464696599736, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1796, Loss: 0.5308484139122296, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1797, Loss: 0.27061808758345496, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1798, Loss: 0.23760074755964508, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1799, Loss: 0.337553967978531, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1800, Loss: 0.280900589162329, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1801, Loss: 0.3802568221448218, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1802, Loss: 0.38973316969932925, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1803, Loss: 0.24893193690085666, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1804, Loss: 0.31592102225377866, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1805, Loss: 0.24629707428437422, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1806, Loss: 0.3538268675720776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1807, Loss: 0.5988208770129568, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1808, Loss: 0.3345088213141067, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1809, Loss: 0.22631653369380145, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1810, Loss: 0.25362820142775433, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1811, Loss: 0.3376749111339741, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1812, Loss: 0.3761538431029936, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1813, Loss: 0.2511720684204155, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1814, Loss: 0.3198158232700371, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1815, Loss: 0.31467104590929545, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1816, Loss: 0.24937495610875865, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1817, Loss: 0.4363952265849447, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1818, Loss: 0.3626364246403653, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1819, Loss: 0.21948421973173307, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1820, Loss: 0.3586910686299285, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1821, Loss: 0.33873708997569524, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1822, Loss: 0.2893887529123762, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1823, Loss: 0.5633234586381853, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1824, Loss: 0.30435628225080763, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1825, Loss: 0.36807160476441747, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1826, Loss: 0.5924217312065315, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1827, Loss: 0.33697967090196956, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1828, Loss: 0.5020559570537293, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1829, Loss: 0.32051877292493697, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1830, Loss: 0.46409972291181933, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1831, Loss: 0.308777864855181, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1832, Loss: 0.4625575601771655, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1833, Loss: 0.25377005938806524, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1834, Loss: 0.3581920111020017, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1835, Loss: 0.41735522852694135, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1836, Loss: 0.2610293581561941, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1837, Loss: 0.2370574820039264, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1838, Loss: 0.37741378284856214, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1839, Loss: 0.30925011226497867, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1840, Loss: 0.49168831037514404, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1841, Loss: 0.30927614154264715, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1842, Loss: 0.34631309173739244, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1843, Loss: 0.3117809607515924, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1844, Loss: 0.2769776404153169, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1845, Loss: 0.27691295924270776, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1846, Loss: 0.43849273582880155, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1847, Loss: 0.3264578469440551, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1848, Loss: 0.30886048721190273, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1849, Loss: 0.29333777196518623, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1850, Loss: 0.3823038925516289, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1851, Loss: 0.30223491257508134, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1852, Loss: 0.6229902593150503, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1853, Loss: 0.24028628024800205, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1854, Loss: 0.43256935897980636, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1855, Loss: 0.22747834162926772, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1856, Loss: 0.4709355118613307, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1857, Loss: 0.34976528211751, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1858, Loss: 0.29701836899207906, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1859, Loss: 0.6327436274448696, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1860, Loss: 0.6276141823223695, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1861, Loss: 0.3218683073946964, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1862, Loss: 0.29405224174025246, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1863, Loss: 0.42083601156947364, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1864, Loss: 0.3990908850685813, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1865, Loss: 0.4888942579818122, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1866, Loss: 0.2610921266421268, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1867, Loss: 0.2601732150421578, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1868, Loss: 0.2566707140114032, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1869, Loss: 0.8219316775823705, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1870, Loss: 0.24395192085793194, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1871, Loss: 0.2716234115332662, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1872, Loss: 0.42109997928644305, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1873, Loss: 0.30405635965490885, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1874, Loss: 0.2719829280089324, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Batch 1875, Loss: 0.5239906509312531, Batch Size: 32, Learning Rate: 0.00015660187499999995\n",
      "Epoch 6, Updated Learning Rate: 0.00013311159374999998\n",
      "Epoch 6, Average Loss: 0.36099065036102534, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1, Loss: 0.637894217778721, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 2, Loss: 0.4291416731021544, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 3, Loss: 0.2860593945348405, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 4, Loss: 0.3901538961199837, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 5, Loss: 0.29210469627373836, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 6, Loss: 0.3024264848667073, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 7, Loss: 0.2734140454442191, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 8, Loss: 0.7717057126740912, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 9, Loss: 0.2369872452118798, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 10, Loss: 0.5581907806885379, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 11, Loss: 0.33537393589975184, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 12, Loss: 0.6747104472619594, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 13, Loss: 0.2365614249159982, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 14, Loss: 0.26103049052372634, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 15, Loss: 0.30132585550630614, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 16, Loss: 0.5651282652042741, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 17, Loss: 0.32301127857740625, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 18, Loss: 0.33424295270934384, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 19, Loss: 0.2566752205356839, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 20, Loss: 0.30608615663280647, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 21, Loss: 0.558832143081254, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 22, Loss: 0.4008719762988565, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 23, Loss: 0.5078254807660276, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 24, Loss: 0.40813550337871973, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 25, Loss: 0.4500760048398861, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 26, Loss: 0.2534815409049892, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 27, Loss: 0.2891595582068497, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 28, Loss: 0.2353816448127787, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 29, Loss: 0.5344486062126744, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 30, Loss: 0.3879231089001105, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 31, Loss: 0.26180091021744756, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 32, Loss: 0.2736255881008711, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 33, Loss: 0.42180100220160577, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 34, Loss: 0.24330725933344388, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 35, Loss: 0.22236216742729303, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 36, Loss: 0.3479226606845194, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 37, Loss: 0.2701998050640461, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 38, Loss: 0.3412623829151188, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 39, Loss: 0.29134833666985216, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 40, Loss: 0.455411103199717, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 41, Loss: 0.3276400296893451, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 42, Loss: 0.2545627278466301, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 43, Loss: 0.5797205274322281, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 44, Loss: 0.25277775024358606, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 45, Loss: 0.3059378348892729, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 46, Loss: 0.5082873592281216, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 47, Loss: 0.3088832843015128, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 48, Loss: 0.27373255782782746, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 49, Loss: 0.3831256282006499, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 50, Loss: 0.3247782277509028, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 51, Loss: 0.43302074193605683, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 52, Loss: 0.26873622863993, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 53, Loss: 0.3480171430029828, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 54, Loss: 0.39981576023371856, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 55, Loss: 0.4048301484884018, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 56, Loss: 0.46048881831843413, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 57, Loss: 0.41759018683641486, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 58, Loss: 0.3000873165477495, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 59, Loss: 0.29541038824403076, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 60, Loss: 0.32236038578239945, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 61, Loss: 0.3326847943110675, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 62, Loss: 0.4288393256680936, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 63, Loss: 0.4677935362735356, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 64, Loss: 0.2828345878470666, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 65, Loss: 0.44681569678885663, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 66, Loss: 0.3025326489340439, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 67, Loss: 0.2782048546329454, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 68, Loss: 0.34851575231368603, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 69, Loss: 0.26012214892952357, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 70, Loss: 0.4102992198930268, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 71, Loss: 0.4412158985821688, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 72, Loss: 0.23969559952186753, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 73, Loss: 0.29747179082601016, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 74, Loss: 0.3184793692408448, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 75, Loss: 0.30799704734994493, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 76, Loss: 0.3011288220927919, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 77, Loss: 0.29830289315072195, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 78, Loss: 0.3743614747010493, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 79, Loss: 0.2769263027985045, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 80, Loss: 0.41922210450608344, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 81, Loss: 0.33669613963224176, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 82, Loss: 0.3926646862926457, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 83, Loss: 0.20818496122824237, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 84, Loss: 0.34344548912728523, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 85, Loss: 0.6305456818796294, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 86, Loss: 0.3449217715034164, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 87, Loss: 0.38595221030493665, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 88, Loss: 0.2259408719734979, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 89, Loss: 0.40947826719914515, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 90, Loss: 0.297595206419854, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 91, Loss: 0.2836033255948454, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 92, Loss: 0.2377980409327687, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 93, Loss: 0.3394655711545867, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 94, Loss: 0.29049239905134927, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 95, Loss: 0.6027122631692035, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 96, Loss: 0.2474260405426184, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 97, Loss: 0.27162571069468555, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 98, Loss: 0.3197595323931404, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 99, Loss: 0.2528276552019328, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 100, Loss: 0.34946679991703583, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 101, Loss: 0.27707010148596134, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 102, Loss: 0.26093202640286267, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 103, Loss: 0.3901141755345729, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 104, Loss: 0.3108620862563458, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 105, Loss: 0.3774696251905782, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 106, Loss: 0.40830087294109907, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 107, Loss: 0.361900796335359, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 108, Loss: 0.28243867853098187, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 109, Loss: 0.31358288105518467, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 110, Loss: 0.38992637567224353, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 111, Loss: 0.41685331017065447, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 112, Loss: 0.364683414998774, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 113, Loss: 0.39744167659964746, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 114, Loss: 0.3513768582471556, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 115, Loss: 0.3375567604425661, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 116, Loss: 0.5817853708975234, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 117, Loss: 0.3337893821939706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 118, Loss: 0.27584263187329966, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 119, Loss: 0.2944975184699717, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 120, Loss: 0.44999454493092705, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 121, Loss: 0.25398528046184843, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 122, Loss: 0.32844091265356756, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 123, Loss: 0.3418198052248531, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 124, Loss: 0.30584536264581785, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 125, Loss: 0.34614132302322637, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 126, Loss: 0.3673550483891261, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 127, Loss: 0.32297639056735383, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 128, Loss: 0.44728861438095213, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 129, Loss: 0.651762089075905, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 130, Loss: 0.4236745005538786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 131, Loss: 0.265817746972725, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 132, Loss: 0.5650402315630594, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 133, Loss: 0.4908984724269579, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 134, Loss: 0.37085340848187265, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 135, Loss: 0.42348249626730955, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 136, Loss: 0.42538043360015976, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 137, Loss: 0.247225856293512, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 138, Loss: 0.2411167181674548, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 139, Loss: 0.23463841228540613, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 140, Loss: 0.43155508913903157, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 141, Loss: 0.3551642289106358, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 142, Loss: 0.2661911523276224, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 143, Loss: 0.4571842886066221, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 144, Loss: 0.4608392780223526, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 145, Loss: 0.3601868395698034, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 146, Loss: 0.31114830363877366, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 147, Loss: 0.42684971185986914, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 148, Loss: 0.4556232872597883, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 149, Loss: 0.2406190974352748, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 150, Loss: 0.4320004921820111, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 151, Loss: 0.3391953959237868, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 152, Loss: 0.4002493532077285, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 153, Loss: 0.26524625622886666, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 154, Loss: 0.37237754712047055, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 155, Loss: 0.3292624431508635, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 156, Loss: 0.2360845332210721, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 157, Loss: 0.2765765991962399, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 158, Loss: 0.40680923546261283, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 159, Loss: 0.2271649630258747, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 160, Loss: 0.31449979613822054, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 161, Loss: 0.29893893792778786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 162, Loss: 0.4208148866032694, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 163, Loss: 0.2439563864418241, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 164, Loss: 0.6192756713229113, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 165, Loss: 0.3101488660520888, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 166, Loss: 0.41710552851125604, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 167, Loss: 0.3218116100109625, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 168, Loss: 0.42993426872335705, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 169, Loss: 0.3566336606221684, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 170, Loss: 0.2431656809696403, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 171, Loss: 0.3237767405623153, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 172, Loss: 0.2868611349233575, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 173, Loss: 0.31867864306251453, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 174, Loss: 0.26772283967875105, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 175, Loss: 0.23578059697723136, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 176, Loss: 0.2745574400061798, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 177, Loss: 0.3226328819827388, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 178, Loss: 0.35038256973429127, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 179, Loss: 0.28196384455730394, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 180, Loss: 0.30574333614774274, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 181, Loss: 0.34661207370457914, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 182, Loss: 0.3340563338090676, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 183, Loss: 0.3238374936884518, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 184, Loss: 0.27140579199097664, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 185, Loss: 0.33849608524090014, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 186, Loss: 0.3508454635432961, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 187, Loss: 0.7948786335026433, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 188, Loss: 0.42351871775827676, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 189, Loss: 0.5372707522183494, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 190, Loss: 0.22787584390662047, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 191, Loss: 0.3533148405128574, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 192, Loss: 0.24070143454061488, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 193, Loss: 0.278956696173141, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 194, Loss: 0.2856788319325959, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 195, Loss: 0.37709973769028965, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 196, Loss: 0.41760308208318253, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 197, Loss: 0.4670481096112671, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 198, Loss: 0.3209257933373853, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 199, Loss: 0.3087570476353789, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 200, Loss: 0.33966214048736865, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 201, Loss: 0.27196162925669864, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 202, Loss: 0.4878153475823036, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 203, Loss: 0.3250073961348804, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 204, Loss: 0.4295785451648913, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 205, Loss: 0.31766849486262855, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 206, Loss: 0.23441313735280744, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 207, Loss: 0.4695134748617202, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 208, Loss: 0.4089235864937505, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 209, Loss: 0.7128472059417243, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 210, Loss: 0.23068443018363782, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 211, Loss: 0.2990998424648278, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 212, Loss: 0.3508706830801688, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 213, Loss: 0.2973213481144583, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 214, Loss: 0.39362409856143177, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 215, Loss: 0.27466374263686955, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 216, Loss: 0.35024393852227315, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 217, Loss: 0.3554201779621853, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 218, Loss: 0.29854656028632537, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 219, Loss: 0.24683485155597182, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 220, Loss: 0.314916724633695, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 221, Loss: 0.23045123994919542, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 222, Loss: 0.45054799283111313, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 223, Loss: 0.3643817749268641, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 224, Loss: 0.2462395032055238, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 225, Loss: 0.3349513879170617, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 226, Loss: 0.5406891088217798, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 227, Loss: 0.5469859076127515, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 228, Loss: 0.39544982926074923, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 229, Loss: 0.2511468437883666, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 230, Loss: 0.22005719434665233, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 231, Loss: 0.3365335681118363, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 232, Loss: 0.6721069889636406, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 233, Loss: 0.3466735225986204, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 234, Loss: 0.5025249368626371, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 235, Loss: 0.40155921123072, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 236, Loss: 0.29821507206512987, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 237, Loss: 0.38911691600348935, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 238, Loss: 0.2514196483787821, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 239, Loss: 0.2699930420319785, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 240, Loss: 0.31658933023434355, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 241, Loss: 0.33754993280393963, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 242, Loss: 0.2584897927358656, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 243, Loss: 0.24288911375149844, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 244, Loss: 0.27375872575514915, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 245, Loss: 0.3086419023582353, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 246, Loss: 0.31364762481160724, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 247, Loss: 0.318729063978805, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 248, Loss: 0.24020559089707683, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 249, Loss: 0.260778453503129, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 250, Loss: 0.6128224115787366, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 251, Loss: 0.25010102395217415, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 252, Loss: 0.5788286662228697, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 253, Loss: 0.26074671836693253, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 254, Loss: 0.3129162400237194, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 255, Loss: 0.48530912328197856, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 256, Loss: 0.28138005703617586, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 257, Loss: 0.38249352751388654, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 258, Loss: 0.4290244694959867, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 259, Loss: 0.2230096453093052, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 260, Loss: 0.5334121812316119, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 261, Loss: 0.4647531419049843, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 262, Loss: 0.3050461915914815, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 263, Loss: 0.32106046691515044, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 264, Loss: 0.3951071896302133, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 265, Loss: 0.23302608672599903, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 266, Loss: 0.24931199360277861, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 267, Loss: 0.32293713390318596, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 268, Loss: 0.3011956403512803, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 269, Loss: 0.3905430660211303, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 270, Loss: 0.28655209143790794, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 271, Loss: 0.5476322653761229, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 272, Loss: 0.31863205309967735, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 273, Loss: 0.34374878001295944, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 274, Loss: 0.3569848676251979, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 275, Loss: 0.2736701629787378, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 276, Loss: 0.3788622217863946, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 277, Loss: 0.27066272344861586, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 278, Loss: 0.23184132705544486, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 279, Loss: 0.33765608393756885, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 280, Loss: 0.24843474046093714, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 281, Loss: 0.21847014347708976, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 282, Loss: 0.3543804500698342, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 283, Loss: 0.3869724275955402, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 284, Loss: 0.36509970765973276, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 285, Loss: 0.3362120664147821, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 286, Loss: 0.3039217023991372, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 287, Loss: 0.2989033298519685, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 288, Loss: 0.3793944363887515, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 289, Loss: 0.3751127488499567, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 290, Loss: 0.3235996190444697, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 291, Loss: 0.6059501919785802, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 292, Loss: 0.2741255694646849, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 293, Loss: 0.3414040182363077, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 294, Loss: 0.22699389242595522, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 295, Loss: 0.32975688531400277, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 296, Loss: 0.31631205440809923, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 297, Loss: 0.32046627597778166, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 298, Loss: 0.31383273761310115, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 299, Loss: 0.29475285714886246, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 300, Loss: 0.3701555218080219, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 301, Loss: 0.3165832768090371, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 302, Loss: 0.24463865011283858, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 303, Loss: 0.43511738574834047, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 304, Loss: 0.3012338554999749, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 305, Loss: 0.2824008374807082, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 306, Loss: 0.3282616831842222, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 307, Loss: 0.4582621246526971, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 308, Loss: 0.4386655750595243, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 309, Loss: 0.24555673670246791, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 310, Loss: 0.285432347092083, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 311, Loss: 0.3077544170384595, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 312, Loss: 0.23373755558426118, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 313, Loss: 0.5507409336550266, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 314, Loss: 0.27949101385293207, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 315, Loss: 0.24198797950806245, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 316, Loss: 0.27672120201274286, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 317, Loss: 0.48418792619065687, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 318, Loss: 0.2873741788244305, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 319, Loss: 0.31322399020699615, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 320, Loss: 0.4315398518927541, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 321, Loss: 0.3827729974465285, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 322, Loss: 0.30784215135094445, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 323, Loss: 0.3732196480931127, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 324, Loss: 0.33853743735807457, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 325, Loss: 0.3059082420166216, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 326, Loss: 0.31796097867027706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 327, Loss: 0.23902640307565476, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 328, Loss: 0.21755284279323214, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 329, Loss: 0.34656675205814846, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 330, Loss: 0.2578596013837198, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 331, Loss: 0.6937710238552492, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 332, Loss: 0.4040890170343902, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 333, Loss: 0.40019365480633884, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 334, Loss: 0.29885163026123523, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 335, Loss: 0.32464982065498726, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 336, Loss: 0.32983458772665597, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 337, Loss: 0.24522858717737445, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 338, Loss: 0.30600908290965784, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 339, Loss: 0.3956909854354719, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 340, Loss: 0.210695831900089, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 341, Loss: 0.6554770778494986, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 342, Loss: 0.39676270685250903, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 343, Loss: 0.3322125870646606, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 344, Loss: 0.24070297079096484, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 345, Loss: 0.3098616008581321, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 346, Loss: 0.30397422841083965, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 347, Loss: 0.41794478350748065, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 348, Loss: 0.3884627409752364, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 349, Loss: 0.23909034890728964, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 350, Loss: 0.3525350601860789, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 351, Loss: 0.3851606052677028, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 352, Loss: 0.26260333990087253, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 353, Loss: 0.31814351709194566, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 354, Loss: 0.22874704996052392, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 355, Loss: 0.2944543446671965, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 356, Loss: 0.5344397701874912, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 357, Loss: 0.3439571246551748, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 358, Loss: 0.347719906521864, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 359, Loss: 0.259514037012598, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 360, Loss: 0.24474922584591954, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 361, Loss: 0.28663858182457114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 362, Loss: 0.521967982805525, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 363, Loss: 0.3544397152712563, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 364, Loss: 0.3385280657882639, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 365, Loss: 0.35592117206024393, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 366, Loss: 0.30017628509400024, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 367, Loss: 0.25876965625427245, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 368, Loss: 0.36581641797568853, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 369, Loss: 0.5544629191631238, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 370, Loss: 0.2974024220004883, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 371, Loss: 0.4448197213682348, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 372, Loss: 0.28229670475827406, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 373, Loss: 0.34036735808621765, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 374, Loss: 0.3113611562754257, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 375, Loss: 0.2713890589776721, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 376, Loss: 0.3359687409278711, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 377, Loss: 0.8566268012285592, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 378, Loss: 0.30560574644108873, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 379, Loss: 0.23385690276894122, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 380, Loss: 0.5731207724460161, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 381, Loss: 0.4152214690783661, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 382, Loss: 0.7808988617747548, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 383, Loss: 0.5362643086133088, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 384, Loss: 0.47571860449707865, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 385, Loss: 0.4049602755898861, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 386, Loss: 0.2936263050053932, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 387, Loss: 0.37209402672554215, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 388, Loss: 0.4924526654623548, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 389, Loss: 0.4953053330045756, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 390, Loss: 0.32802478144176606, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 391, Loss: 0.35034426831033305, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 392, Loss: 0.2630803714866824, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 393, Loss: 0.27964531311886626, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 394, Loss: 0.30244238836434767, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 395, Loss: 0.26837328886574663, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 396, Loss: 0.43746032159102455, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 397, Loss: 0.24115971492569616, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 398, Loss: 0.27071995289231987, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 399, Loss: 0.2769160260706938, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 400, Loss: 0.2955263776356275, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 401, Loss: 0.37213815250484206, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 402, Loss: 0.2792870534159865, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 403, Loss: 0.4091353516743431, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 404, Loss: 0.44229615812498146, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 405, Loss: 0.41384597459051214, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 406, Loss: 0.4626084987073812, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 407, Loss: 0.2956340006300102, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 408, Loss: 0.324272675283068, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 409, Loss: 0.4174661320593295, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 410, Loss: 0.22540445907456869, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 411, Loss: 0.4090446017232398, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 412, Loss: 0.2479953769191761, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 413, Loss: 0.510073379742041, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 414, Loss: 0.7990313968781148, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 415, Loss: 0.5345332240824637, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 416, Loss: 0.40910905377585477, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 417, Loss: 0.43643618881448554, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 418, Loss: 0.2647243373658102, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 419, Loss: 0.25773611515592565, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 420, Loss: 0.31082051211195627, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 421, Loss: 0.32561599330782337, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 422, Loss: 0.297772682847285, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 423, Loss: 0.3282487274520208, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 424, Loss: 0.2766931656251893, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 425, Loss: 0.31601440067485254, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 426, Loss: 0.3794211792575227, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 427, Loss: 0.2759386824784589, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 428, Loss: 0.2977501106397967, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 429, Loss: 0.29394413607410363, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 430, Loss: 0.36294038914358717, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 431, Loss: 0.2975445266616191, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 432, Loss: 0.5689155382937306, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 433, Loss: 0.27739605181315424, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 434, Loss: 0.3659599695776925, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 435, Loss: 0.3818974066280808, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 436, Loss: 0.40369112305501, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 437, Loss: 0.34492918363462943, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 438, Loss: 0.3723102416181625, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 439, Loss: 0.34619692436817706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 440, Loss: 0.30458483773435546, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 441, Loss: 0.3973678007987914, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 442, Loss: 0.29925392147463653, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 443, Loss: 0.3981641196080019, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 444, Loss: 0.3519622676826051, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 445, Loss: 0.3271472986499453, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 446, Loss: 0.42537524627008594, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 447, Loss: 0.3094448849082576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 448, Loss: 0.2506030544038082, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 449, Loss: 0.2639595253337221, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 450, Loss: 0.40362625439597744, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 451, Loss: 0.2525199587845911, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 452, Loss: 0.2875912525184606, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 453, Loss: 0.3798230823266077, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 454, Loss: 0.4291074058079988, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 455, Loss: 0.31846205752442264, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 456, Loss: 0.2501950237071328, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 457, Loss: 0.2276831544517125, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 458, Loss: 0.3900382501706664, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 459, Loss: 0.3186824739622621, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 460, Loss: 0.3957760372845268, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 461, Loss: 0.25217744208888304, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 462, Loss: 0.29341017119852014, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 463, Loss: 0.23325128368180142, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 464, Loss: 0.2911772683559446, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 465, Loss: 0.5072727056220254, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 466, Loss: 0.2513452315818475, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 467, Loss: 0.3333986984101704, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 468, Loss: 0.2560190695645578, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 469, Loss: 0.30662658553687755, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 470, Loss: 0.48915496381205603, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 471, Loss: 0.27665070283623977, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 472, Loss: 0.3851956072198654, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 473, Loss: 0.5908666064759627, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 474, Loss: 0.6393744537104513, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 475, Loss: 0.2415531417856024, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 476, Loss: 0.3614001609054013, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 477, Loss: 0.4512940154967343, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 478, Loss: 0.42373843969237135, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 479, Loss: 0.4299204221742281, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 480, Loss: 0.46143058556837047, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 481, Loss: 0.27235769270043797, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 482, Loss: 0.2842299540332716, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 483, Loss: 0.3130405713704537, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 484, Loss: 0.26686709725569413, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 485, Loss: 0.7165624369115494, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 486, Loss: 0.29629082294905734, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 487, Loss: 0.3388565444921233, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 488, Loss: 0.4509189233786338, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 489, Loss: 0.285425778854097, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 490, Loss: 0.32309731645408135, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 491, Loss: 0.29204731292449704, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 492, Loss: 0.29915222758801385, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 493, Loss: 0.5791969776349613, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 494, Loss: 0.3767072396506669, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 495, Loss: 0.31834551837689506, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 496, Loss: 0.3487153065720534, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 497, Loss: 0.2902988049804464, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 498, Loss: 0.2269766148242225, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 499, Loss: 0.3584872215579319, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 500, Loss: 0.4634350892855722, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 501, Loss: 0.24230412095188208, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 502, Loss: 0.37052079974791163, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 503, Loss: 0.5038174716740577, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 504, Loss: 0.39461838575639313, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 505, Loss: 0.32150217603999276, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 506, Loss: 0.28325469506675727, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 507, Loss: 0.31493075307382357, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 508, Loss: 0.3517801940123585, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 509, Loss: 0.3069873751335631, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 510, Loss: 0.3246191280128221, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 511, Loss: 0.429547113654831, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 512, Loss: 0.35088224565761544, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 513, Loss: 0.30903343046551407, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 514, Loss: 0.520905074426695, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 515, Loss: 0.31083592731133425, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 516, Loss: 0.2610342317505124, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 517, Loss: 0.3398794429378816, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 518, Loss: 0.26322955857447417, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 519, Loss: 0.28162375154595626, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 520, Loss: 0.39412066441187193, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 521, Loss: 0.36145286000217713, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 522, Loss: 0.37498702638743553, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 523, Loss: 0.2703338549553471, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 524, Loss: 0.39502495201350407, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 525, Loss: 0.27910156522110385, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 526, Loss: 0.3538611838380145, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 527, Loss: 0.3904584749162462, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 528, Loss: 0.297828681021739, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 529, Loss: 0.4393917440053976, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 530, Loss: 0.39423029873647875, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 531, Loss: 0.5644008275559114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 532, Loss: 0.25625849913298576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 533, Loss: 0.5606747655975439, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 534, Loss: 0.38426866055134473, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 535, Loss: 0.3055181092414192, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 536, Loss: 0.32963157921930486, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 537, Loss: 0.34353440766891136, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 538, Loss: 0.4654469935554318, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 539, Loss: 0.5533410102736022, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 540, Loss: 0.3084069518062421, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 541, Loss: 0.3304286713996655, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 542, Loss: 0.25084768441247685, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 543, Loss: 0.4702286308498266, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 544, Loss: 0.22223919352154645, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 545, Loss: 0.3436612283501626, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 546, Loss: 0.32998220156331876, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 547, Loss: 0.21626220280409428, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 548, Loss: 0.4143986134944619, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 549, Loss: 0.5097681718098255, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 550, Loss: 0.27444311829347684, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 551, Loss: 0.37878284913061355, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 552, Loss: 0.5286795709346646, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 553, Loss: 0.25386746403438815, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 554, Loss: 0.3441935899287005, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 555, Loss: 0.24637552933321893, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 556, Loss: 0.4739011577205262, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 557, Loss: 0.23864673608490092, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 558, Loss: 0.5281112716122529, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 559, Loss: 0.32464623188925434, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 560, Loss: 0.3248841415772884, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 561, Loss: 0.3802302478772591, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 562, Loss: 0.3494952691158487, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 563, Loss: 0.3683339468771871, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 564, Loss: 0.26362766655068887, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 565, Loss: 0.26476547301386644, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 566, Loss: 0.5226912351472973, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 567, Loss: 0.2108459182981607, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 568, Loss: 0.39527788278522896, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 569, Loss: 0.42285090776618856, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 570, Loss: 0.21645536460297204, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 571, Loss: 0.32537972625019196, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 572, Loss: 0.2672701252939565, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 573, Loss: 0.3718077607140884, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 574, Loss: 0.5396798126581835, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 575, Loss: 0.5088499315630748, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 576, Loss: 0.2448882615878484, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 577, Loss: 0.397303116524128, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 578, Loss: 0.3615272426033655, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 579, Loss: 0.33259954220827004, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 580, Loss: 0.462822316709408, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 581, Loss: 0.47757936340093715, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 582, Loss: 0.5307411585449002, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 583, Loss: 0.2792566531986815, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 584, Loss: 0.5402627684222635, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 585, Loss: 0.2841423186703716, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 586, Loss: 0.27754696065989465, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 587, Loss: 0.36649776719931737, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 588, Loss: 0.590226080245404, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 589, Loss: 0.27332895686743025, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 590, Loss: 0.2990617272876296, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 591, Loss: 0.5003508485590538, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 592, Loss: 0.3212364328885343, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 593, Loss: 0.314766443182785, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 594, Loss: 0.3206616299067923, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 595, Loss: 0.3140995350073218, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 596, Loss: 0.26909673078517016, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 597, Loss: 0.24652969591676174, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 598, Loss: 0.4943774413616471, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 599, Loss: 0.2354202865759079, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 600, Loss: 0.3257322402412995, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 601, Loss: 0.6768459519604432, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 602, Loss: 0.35753969640691285, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 603, Loss: 0.43437106357255745, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 604, Loss: 0.2889724761940332, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 605, Loss: 0.4904251967657842, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 606, Loss: 0.2905338270131661, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 607, Loss: 0.2469234938794559, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 608, Loss: 0.3097769353607267, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 609, Loss: 0.2633678116035851, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 610, Loss: 0.36117391939700105, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 611, Loss: 0.2358858180085476, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 612, Loss: 0.27481437693320554, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 613, Loss: 0.2733001712931042, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 614, Loss: 0.45112985821955065, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 615, Loss: 0.36411032580363134, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 616, Loss: 0.46995599639548463, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 617, Loss: 0.45878525594895625, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 618, Loss: 0.2991279827132786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 619, Loss: 0.23904040318144065, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 620, Loss: 0.25686084944099163, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 621, Loss: 0.28091202464825715, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 622, Loss: 0.28754287810081736, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 623, Loss: 0.381400136047618, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 624, Loss: 0.4119280027110047, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 625, Loss: 0.35062194429639815, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 626, Loss: 0.3792362401993806, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 627, Loss: 0.3869977982270039, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 628, Loss: 0.40775360205778277, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 629, Loss: 0.2280453050642576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 630, Loss: 0.33319678289850596, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 631, Loss: 0.3260470163894442, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 632, Loss: 0.2836839429428928, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 633, Loss: 0.39749142081308586, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 634, Loss: 0.31798928256959114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 635, Loss: 0.5428872682010519, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 636, Loss: 0.2944875025208853, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 637, Loss: 0.37348923707642767, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 638, Loss: 0.3036748288788248, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 639, Loss: 0.30144238052884925, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 640, Loss: 0.3187659304183755, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 641, Loss: 0.44341078048912075, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 642, Loss: 0.3418542782640381, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 643, Loss: 0.3101830783115207, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 644, Loss: 0.5654177623013836, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 645, Loss: 0.3401116487325344, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 646, Loss: 0.44525213032234967, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 647, Loss: 0.23678176400290785, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 648, Loss: 0.3725237935643551, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 649, Loss: 0.22896254522122628, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 650, Loss: 0.2691949505117766, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 651, Loss: 0.34490933116782263, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 652, Loss: 0.29187716802582153, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 653, Loss: 0.22847625049711545, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 654, Loss: 0.3244800464525356, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 655, Loss: 0.34952101758958176, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 656, Loss: 0.43410122416573405, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 657, Loss: 0.22875528697339398, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 658, Loss: 0.5754946404252349, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 659, Loss: 0.4220951007684123, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 660, Loss: 0.35467155430622316, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 661, Loss: 0.3195875854793326, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 662, Loss: 0.322669807486435, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 663, Loss: 0.5900306609684484, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 664, Loss: 0.3211308438149191, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 665, Loss: 0.3433947751878463, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 666, Loss: 0.4172505245283289, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 667, Loss: 0.6211628659685198, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 668, Loss: 0.37341588900899314, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 669, Loss: 0.36795379895845665, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 670, Loss: 0.28005312014912187, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 671, Loss: 0.2885532205248339, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 672, Loss: 0.27996778601908623, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 673, Loss: 0.40146498083145066, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 674, Loss: 0.3201647943354404, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 675, Loss: 0.5116726141635188, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 676, Loss: 0.21850219071127538, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 677, Loss: 0.27248141665760484, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 678, Loss: 0.4573855215885771, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 679, Loss: 0.2403036969875834, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 680, Loss: 0.25222967866848944, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 681, Loss: 0.3140186970930001, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 682, Loss: 0.4130627742986364, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 683, Loss: 0.36171277649129574, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 684, Loss: 0.2559100688262887, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 685, Loss: 0.27577683840409356, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 686, Loss: 0.5256731605153093, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 687, Loss: 0.3424835054590122, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 688, Loss: 0.4990332255611547, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 689, Loss: 0.417435809229213, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 690, Loss: 0.42809454697761934, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 691, Loss: 0.36939041823025404, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 692, Loss: 0.4991445783065191, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 693, Loss: 0.364883417452844, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 694, Loss: 0.4562734227230613, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 695, Loss: 0.6326625603133675, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 696, Loss: 0.3434336254355541, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 697, Loss: 0.4147688630493617, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 698, Loss: 0.2933343239549542, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 699, Loss: 0.3539306418079681, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 700, Loss: 0.29429720706531176, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 701, Loss: 0.4891071404405197, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 702, Loss: 0.3480561514281207, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 703, Loss: 0.49564908808693964, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 704, Loss: 0.4255595180932471, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 705, Loss: 0.21476541817518388, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 706, Loss: 0.520208802642604, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 707, Loss: 0.2523197736362289, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 708, Loss: 0.3816174397628359, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 709, Loss: 0.47554373772393976, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 710, Loss: 0.3390873070677096, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 711, Loss: 0.36030957289110976, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 712, Loss: 0.602768145856713, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 713, Loss: 0.28004339916536364, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 714, Loss: 0.7491025147596266, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 715, Loss: 0.24723505908745344, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 716, Loss: 0.4248969855019722, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 717, Loss: 0.3623495392101182, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 718, Loss: 0.6037696383402852, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 719, Loss: 0.3517513222255034, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 720, Loss: 0.3700410921256158, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 721, Loss: 0.47094725578028274, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 722, Loss: 0.30967181958175577, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 723, Loss: 0.3698884174151702, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 724, Loss: 0.33647712780917227, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 725, Loss: 0.259374618430751, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 726, Loss: 0.2511624967409894, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 727, Loss: 0.33716374048022163, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 728, Loss: 0.22918545855969877, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 729, Loss: 0.3534542646106025, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 730, Loss: 0.4137301846388669, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 731, Loss: 0.25642510147449954, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 732, Loss: 0.29005689176893146, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 733, Loss: 0.45785396769805975, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 734, Loss: 0.5581291889853642, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 735, Loss: 0.2805733926372169, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 736, Loss: 0.225959811013871, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 737, Loss: 0.23963993627692753, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 738, Loss: 0.597651273623922, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 739, Loss: 0.4354940674322011, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 740, Loss: 0.26395839862511883, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 741, Loss: 0.5220308709912944, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 742, Loss: 0.3669285876613243, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 743, Loss: 0.5493006314873873, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 744, Loss: 0.24612925892415924, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 745, Loss: 0.4026750959427583, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 746, Loss: 0.5760603956815666, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 747, Loss: 0.5739667646435076, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 748, Loss: 0.4006952781456072, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 749, Loss: 0.4426512602900389, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 750, Loss: 0.24658782053048445, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 751, Loss: 0.33409933011866, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 752, Loss: 0.32685761892088927, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 753, Loss: 0.4378789577313449, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 754, Loss: 0.3509146788734827, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 755, Loss: 0.3280268532884012, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 756, Loss: 0.5297799597622082, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 757, Loss: 0.31078116285131613, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 758, Loss: 0.37804101954672154, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 759, Loss: 0.3483753272634329, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 760, Loss: 0.2667366826733861, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 761, Loss: 0.23235251690192005, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 762, Loss: 0.3972781204168059, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 763, Loss: 0.3020746926244231, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 764, Loss: 0.2384540220349584, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 765, Loss: 0.309220159008323, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 766, Loss: 0.2655003376478357, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 767, Loss: 0.46273560286553433, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 768, Loss: 0.2741652521369897, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 769, Loss: 0.38007202870042345, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 770, Loss: 0.33583798320830527, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 771, Loss: 0.36423238274760494, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 772, Loss: 0.44016459217856935, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 773, Loss: 0.2257982247221506, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 774, Loss: 0.2791337232378308, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 775, Loss: 0.263595409583162, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 776, Loss: 0.4369764951021706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 777, Loss: 0.23221157871223838, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 778, Loss: 0.29644215128119344, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 779, Loss: 0.3317319336837832, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 780, Loss: 0.24946459579834834, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 781, Loss: 0.4566829730164973, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 782, Loss: 0.5199534267028101, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 783, Loss: 0.375282138581322, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 784, Loss: 0.32362748784722173, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 785, Loss: 0.34619032497408675, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 786, Loss: 0.5533232690976555, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 787, Loss: 0.5184305918242963, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 788, Loss: 0.29924729714445053, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 789, Loss: 0.3214388130483137, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 790, Loss: 0.3736255695372462, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 791, Loss: 0.2327355318648739, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 792, Loss: 0.5602844508383031, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 793, Loss: 0.6825525844866173, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 794, Loss: 0.4301300531212581, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 795, Loss: 0.25584362882731093, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 796, Loss: 0.5028385067626033, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 797, Loss: 0.31280519734775303, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 798, Loss: 0.26995085496952403, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 799, Loss: 0.4263290314339636, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 800, Loss: 0.27739105587219876, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 801, Loss: 0.24745721005760385, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 802, Loss: 0.46714158695883323, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 803, Loss: 0.290239459616892, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 804, Loss: 0.5312253376631182, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 805, Loss: 0.3621091799386877, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 806, Loss: 0.24379909199014715, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 807, Loss: 0.2763886116780504, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 808, Loss: 0.31040212978779436, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 809, Loss: 0.2447695855008014, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 810, Loss: 0.2986276177032201, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 811, Loss: 0.684018164828184, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 812, Loss: 0.31002204366667485, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 813, Loss: 0.26706142282449086, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 814, Loss: 0.35036829324993046, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 815, Loss: 0.32527558582633537, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 816, Loss: 0.37578628085055443, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 817, Loss: 0.3271190337947437, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 818, Loss: 0.2737716790065479, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 819, Loss: 0.3888732481074018, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 820, Loss: 0.3184706504354743, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 821, Loss: 0.36075008726443913, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 822, Loss: 0.3211497489269224, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 823, Loss: 0.6168575000209235, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 824, Loss: 0.4734074459992057, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 825, Loss: 0.24455109910224437, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 826, Loss: 0.4983693920524074, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 827, Loss: 0.27636493192657985, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 828, Loss: 0.26787442949433754, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 829, Loss: 0.3036761874672855, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 830, Loss: 0.312794988928553, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 831, Loss: 0.2502243347104029, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 832, Loss: 0.25899365400030744, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 833, Loss: 0.3114035315111178, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 834, Loss: 0.39683119383009824, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 835, Loss: 0.4551792123372469, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 836, Loss: 0.405629331173296, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 837, Loss: 0.42185926489144976, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 838, Loss: 0.27238647989556064, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 839, Loss: 0.2513490411765617, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 840, Loss: 0.42382547830689926, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 841, Loss: 0.5107469562330407, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 842, Loss: 0.23931009833202954, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 843, Loss: 0.35179768974674797, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 844, Loss: 0.587417684087353, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 845, Loss: 0.40022408670075066, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 846, Loss: 0.3232573042723088, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 847, Loss: 0.46048889618344857, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 848, Loss: 0.3623918552818545, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 849, Loss: 0.24433174702165897, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 850, Loss: 0.4602085332832308, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 851, Loss: 0.2715314793176483, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 852, Loss: 0.28731178646469385, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 853, Loss: 0.2537397491007184, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 854, Loss: 0.30216793475457165, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 855, Loss: 0.24585965614436378, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 856, Loss: 0.46053952206812243, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 857, Loss: 0.4029158115171193, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 858, Loss: 0.38441071993264275, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 859, Loss: 0.33683568484695575, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 860, Loss: 0.26448447097059063, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 861, Loss: 0.5351367031912814, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 862, Loss: 0.26727383567512353, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 863, Loss: 0.30155061833933117, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 864, Loss: 0.3660738982230505, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 865, Loss: 0.4551526098286589, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 866, Loss: 0.3129137918664646, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 867, Loss: 0.4300764399388876, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 868, Loss: 0.25596774373510056, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 869, Loss: 0.23567850832690965, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 870, Loss: 0.47893884718026425, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 871, Loss: 0.3065519556373486, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 872, Loss: 0.2733646088313081, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 873, Loss: 0.420619989829618, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 874, Loss: 0.25193034452267027, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 875, Loss: 0.404008423510017, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 876, Loss: 0.5134785536022582, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 877, Loss: 0.2511984583612421, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 878, Loss: 0.2612972966210922, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 879, Loss: 0.3128370512200971, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 880, Loss: 0.3773706249058923, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 881, Loss: 0.4524084067065476, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 882, Loss: 0.27204184493484834, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 883, Loss: 0.36651463455102684, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 884, Loss: 0.28199749585674233, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 885, Loss: 0.30203921162837877, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 886, Loss: 0.38917328293285103, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 887, Loss: 0.4191277355838158, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 888, Loss: 0.4949761992261854, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 889, Loss: 0.5530305657822836, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 890, Loss: 0.2201075815960628, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 891, Loss: 0.24827212562523387, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 892, Loss: 0.26142886423001854, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 893, Loss: 0.36468966684147536, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 894, Loss: 0.2291546814683067, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 895, Loss: 0.5117545867352127, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 896, Loss: 0.34066902710514746, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 897, Loss: 0.2559743997313759, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 898, Loss: 0.2666703290070162, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 899, Loss: 0.23714131533502947, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 900, Loss: 0.5832716892166223, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 901, Loss: 0.3049730318023757, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 902, Loss: 0.3182483159294146, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 903, Loss: 0.31802424520815936, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 904, Loss: 0.41526966666372833, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 905, Loss: 0.3877491356380978, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 906, Loss: 0.33405803555529767, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 907, Loss: 0.4497175225269673, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 908, Loss: 0.2725191022753197, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 909, Loss: 0.5404768185677824, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 910, Loss: 0.4964250796388324, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 911, Loss: 0.32482871958617476, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 912, Loss: 0.2739385107667844, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 913, Loss: 0.2667330649611164, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 914, Loss: 0.21880680508230077, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 915, Loss: 0.9564207832538945, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 916, Loss: 0.2961120497744966, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 917, Loss: 0.7437971033318713, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 918, Loss: 0.39146254752452786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 919, Loss: 0.28298637801071813, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 920, Loss: 0.2974757472500138, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 921, Loss: 0.2831756089517411, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 922, Loss: 0.28024142881770076, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 923, Loss: 0.3473371637605827, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 924, Loss: 0.41063004024364236, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 925, Loss: 0.4795485044137653, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 926, Loss: 0.49020557912952467, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 927, Loss: 0.520817329016388, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 928, Loss: 0.3378800803932741, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 929, Loss: 0.2898995171614309, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 930, Loss: 0.3362754032077758, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 931, Loss: 0.31121306097447576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 932, Loss: 0.3414086595477758, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 933, Loss: 0.2388052887613391, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 934, Loss: 0.2803712311052693, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 935, Loss: 0.3932721654707052, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 936, Loss: 0.3094882739890381, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 937, Loss: 0.32383248317948654, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 938, Loss: 0.5547741115483453, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 939, Loss: 0.23416277887170156, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 940, Loss: 0.48126217739506866, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 941, Loss: 0.2533166442191952, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 942, Loss: 0.243401086389214, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 943, Loss: 0.4295892120808672, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 944, Loss: 0.3047043494987165, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 945, Loss: 0.2967620228228712, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 946, Loss: 0.40541194335404596, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 947, Loss: 0.32126100066427576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 948, Loss: 0.3733382482382221, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 949, Loss: 0.3957053006037178, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 950, Loss: 0.2786347210933136, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 951, Loss: 0.25673024949114587, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 952, Loss: 0.4606866871504851, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 953, Loss: 0.29802743148686717, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 954, Loss: 0.26564378124547366, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 955, Loss: 0.3305583010722528, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 956, Loss: 0.27269403353137095, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 957, Loss: 0.38620490281922204, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 958, Loss: 0.26110669171775375, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 959, Loss: 0.355521862618175, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 960, Loss: 0.2559899891286293, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 961, Loss: 0.30093774856838057, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 962, Loss: 0.2714851709408393, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 963, Loss: 0.44047960455545887, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 964, Loss: 0.2730108260068727, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 965, Loss: 0.387606783754192, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 966, Loss: 0.5148973361992003, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 967, Loss: 0.33453128904200435, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 968, Loss: 0.2432011972975707, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 969, Loss: 0.4281745758525942, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 970, Loss: 0.29494980289571315, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 971, Loss: 0.26361869964654167, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 972, Loss: 0.48424942267741555, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 973, Loss: 0.32047993244513184, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 974, Loss: 0.6300134432465866, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 975, Loss: 0.2895347027989162, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 976, Loss: 0.22252073971624392, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 977, Loss: 0.2783179137279527, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 978, Loss: 0.36703472468029963, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 979, Loss: 0.2664826584030594, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 980, Loss: 0.37165653971161067, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 981, Loss: 0.2684133576540236, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 982, Loss: 0.24292908576532332, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 983, Loss: 0.23167842335620553, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 984, Loss: 0.24001750110548617, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 985, Loss: 0.24101722785822188, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 986, Loss: 0.5447392339888912, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 987, Loss: 0.27370618167686256, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 988, Loss: 0.23595503322923086, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 989, Loss: 0.31988432437367886, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 990, Loss: 0.5839592565663398, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 991, Loss: 0.321775704020025, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 992, Loss: 0.28411757989569797, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 993, Loss: 0.2809036061682609, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 994, Loss: 0.25927000690029367, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 995, Loss: 0.2593284897831379, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 996, Loss: 0.5761791234130298, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 997, Loss: 0.2733773428350914, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 998, Loss: 0.4112954609131797, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 999, Loss: 0.3160733072454738, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1000, Loss: 0.3453294281914068, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1001, Loss: 0.4084488109744702, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1002, Loss: 0.30415816442883703, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1003, Loss: 0.2359955565306631, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1004, Loss: 0.28654856662719, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1005, Loss: 0.2834252018518747, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1006, Loss: 0.27323582825301707, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1007, Loss: 0.29998030326457326, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1008, Loss: 0.23167897176933494, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1009, Loss: 0.4448267953331816, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1010, Loss: 0.3539708577780638, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1011, Loss: 0.2546654634813954, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1012, Loss: 0.3976993201652521, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1013, Loss: 0.3006438088506648, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1014, Loss: 0.394681393419064, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1015, Loss: 0.46385878857188284, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1016, Loss: 0.2250910962686728, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1017, Loss: 0.4341398166885384, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1018, Loss: 0.3601253453228154, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1019, Loss: 0.40857927874061667, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1020, Loss: 0.2142212344262632, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1021, Loss: 0.6089598687312565, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1022, Loss: 0.3744279911521016, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1023, Loss: 0.23689745069876655, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1024, Loss: 0.2834795408552817, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1025, Loss: 0.2932367144479138, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1026, Loss: 0.5122699292015085, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1027, Loss: 0.44143972856759106, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1028, Loss: 0.48336970135553414, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1029, Loss: 0.3159519322042321, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1030, Loss: 0.3378365129285167, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1031, Loss: 0.30993472705857095, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1032, Loss: 0.32242808168380543, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1033, Loss: 0.4156480835829297, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1034, Loss: 0.40851243174192053, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1035, Loss: 0.4775457565911071, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1036, Loss: 0.31684814248790155, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1037, Loss: 0.47079968493749835, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1038, Loss: 0.3732367316228782, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1039, Loss: 0.27331360286095185, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1040, Loss: 0.39443059959966986, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1041, Loss: 0.22213737116816926, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1042, Loss: 0.3728422868175607, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1043, Loss: 0.35955828568181464, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1044, Loss: 0.25553561081683374, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1045, Loss: 0.5154095840504547, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1046, Loss: 0.36660071284370865, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1047, Loss: 0.48524132770009437, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1048, Loss: 0.2784317753796077, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1049, Loss: 0.39536270832362397, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1050, Loss: 0.3664413455438858, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1051, Loss: 0.3095355130725147, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1052, Loss: 0.6722597711286585, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1053, Loss: 0.38939356975654676, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1054, Loss: 0.2700545760962653, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1055, Loss: 0.27178795846739673, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1056, Loss: 0.2860041283517512, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1057, Loss: 0.28549923692159984, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1058, Loss: 0.41214913570256906, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1059, Loss: 0.4395570850262841, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1060, Loss: 0.27847236152396093, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1061, Loss: 0.23432581503170097, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1062, Loss: 0.609057952706737, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1063, Loss: 0.26551334228720375, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1064, Loss: 0.49317116214913403, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1065, Loss: 0.326554863976903, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1066, Loss: 0.26248073821832807, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1067, Loss: 0.39751047543641715, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1068, Loss: 0.3250898532065517, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1069, Loss: 0.2924627002615619, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1070, Loss: 0.21348609849659786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1071, Loss: 0.321944869778144, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1072, Loss: 0.2148267479615343, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1073, Loss: 0.5142477082173277, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1074, Loss: 0.2383787377151935, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1075, Loss: 0.46902053663623877, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1076, Loss: 0.29237994774138987, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1077, Loss: 0.25870152422895, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1078, Loss: 0.3621310382352072, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1079, Loss: 0.4446733929441824, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1080, Loss: 0.25467820391366114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1081, Loss: 0.32800042704373455, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1082, Loss: 0.25288976811807207, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1083, Loss: 0.32074903154734286, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1084, Loss: 0.5257031459990956, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1085, Loss: 0.41569269481833987, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1086, Loss: 0.35408111670971176, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1087, Loss: 0.4623596850838361, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1088, Loss: 0.6538887501781929, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1089, Loss: 0.25253647906089977, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1090, Loss: 0.3893277831583208, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1091, Loss: 0.4402295067984852, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1092, Loss: 0.6986042722755641, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1093, Loss: 0.2956544811779926, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1094, Loss: 0.5373001510103913, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1095, Loss: 0.23789925258800637, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1096, Loss: 0.3876892726934811, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1097, Loss: 0.29945707145613937, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1098, Loss: 0.32273658607555294, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1099, Loss: 0.30151109256163733, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1100, Loss: 0.23729708431966076, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1101, Loss: 0.6591826859123244, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1102, Loss: 0.44882221002417644, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1103, Loss: 0.6397038232502299, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1104, Loss: 0.4021162313153437, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1105, Loss: 0.21904714966002367, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1106, Loss: 0.4071719571832515, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1107, Loss: 0.25565647018228943, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1108, Loss: 0.2745606706169863, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1109, Loss: 0.29592057678747413, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1110, Loss: 0.331676031277254, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1111, Loss: 0.6823575029648732, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1112, Loss: 0.41017994108327677, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1113, Loss: 0.24070964917364257, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1114, Loss: 0.2683011155950611, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1115, Loss: 0.31086970419277954, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1116, Loss: 0.6135724991075364, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1117, Loss: 0.40173535483387046, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1118, Loss: 0.3732301658911505, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1119, Loss: 0.56223455372965, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1120, Loss: 0.298841671639415, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1121, Loss: 0.3887592749879828, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1122, Loss: 0.3501872460462432, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1123, Loss: 0.23257843360660468, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1124, Loss: 0.2783257093436847, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1125, Loss: 0.3063462874566305, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1126, Loss: 0.31489791970337244, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1127, Loss: 0.35688822628261996, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1128, Loss: 0.436232755456366, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1129, Loss: 0.2995515718749068, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1130, Loss: 0.23125986832977724, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1131, Loss: 0.4191555094907876, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1132, Loss: 0.29368965655500284, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1133, Loss: 0.2746698970873026, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1134, Loss: 0.42074894823428743, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1135, Loss: 0.2880729234155183, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1136, Loss: 0.41453505838397264, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1137, Loss: 0.7330209934368834, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1138, Loss: 0.41506172885775516, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1139, Loss: 0.24433226292265725, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1140, Loss: 0.3625879567268724, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1141, Loss: 0.5505932796466064, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1142, Loss: 0.4406928312587448, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1143, Loss: 0.3809781980977851, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1144, Loss: 0.4001920562885194, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1145, Loss: 0.44192108581611333, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1146, Loss: 0.32821039113687034, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1147, Loss: 0.30042639115874786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1148, Loss: 0.3123449305951358, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1149, Loss: 0.46705070667827553, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1150, Loss: 0.4237153082304774, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1151, Loss: 0.26203730142351755, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1152, Loss: 0.27092503365775883, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1153, Loss: 0.23908304230754795, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1154, Loss: 0.3140549326829462, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1155, Loss: 0.4837210228093512, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1156, Loss: 0.4128730215266294, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1157, Loss: 0.3029316696231105, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1158, Loss: 0.4223357051963471, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1159, Loss: 0.3142450287656672, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1160, Loss: 0.49140733184751906, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1161, Loss: 0.4520581822927111, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1162, Loss: 0.4741652474740766, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1163, Loss: 0.37587029917285353, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1164, Loss: 0.3328643010262714, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1165, Loss: 0.32356830123369507, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1166, Loss: 0.3233483738733618, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1167, Loss: 0.2501526614420528, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1168, Loss: 0.26699561187524673, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1169, Loss: 0.3960083375182881, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1170, Loss: 0.24549541489585158, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1171, Loss: 0.42670031127802666, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1172, Loss: 0.2762869315511831, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1173, Loss: 0.3865427236007095, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1174, Loss: 0.381052300332543, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1175, Loss: 0.3434057120894189, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1176, Loss: 0.2866212906967527, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1177, Loss: 0.22953292753246676, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1178, Loss: 0.3490367523450344, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1179, Loss: 0.35528163636174176, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1180, Loss: 0.37464723126329824, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1181, Loss: 0.3284960455907764, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1182, Loss: 0.6166755911656933, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1183, Loss: 0.23984282530490125, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1184, Loss: 0.6641872366200705, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1185, Loss: 0.22493575269925783, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1186, Loss: 0.3271329975095009, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1187, Loss: 0.38274593008977303, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1188, Loss: 0.3548419608466668, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1189, Loss: 0.29704994958342756, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1190, Loss: 0.22224993895892953, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1191, Loss: 0.5649891847026043, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1192, Loss: 0.323138821850214, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1193, Loss: 0.6768084552437981, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1194, Loss: 0.41227733673350847, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1195, Loss: 0.3428108321080008, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1196, Loss: 0.4186495123680706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1197, Loss: 0.23647873732937674, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1198, Loss: 0.5406262744898811, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1199, Loss: 0.38283011422285707, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1200, Loss: 0.2679571460789899, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1201, Loss: 0.24065385405373058, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1202, Loss: 0.3497251913628162, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1203, Loss: 0.5780527563597626, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1204, Loss: 0.22260472698366532, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1205, Loss: 0.22237279417144862, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1206, Loss: 0.2864236141093511, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1207, Loss: 0.341290634888597, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1208, Loss: 0.33366052398869206, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1209, Loss: 0.21046216519222108, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1210, Loss: 0.302993029933925, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1211, Loss: 0.7564641709657158, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1212, Loss: 0.2785221364859232, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1213, Loss: 0.28441882052375966, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1214, Loss: 0.27368730375595474, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1215, Loss: 0.3644447129511028, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1216, Loss: 0.39242496362385826, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1217, Loss: 0.2791830629483649, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1218, Loss: 0.6070655589137051, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1219, Loss: 0.21977127568452018, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1220, Loss: 0.34473164157124137, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1221, Loss: 0.8109918328962522, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1222, Loss: 0.39971897319653993, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1223, Loss: 0.26786091575531834, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1224, Loss: 0.37001689530376075, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1225, Loss: 0.38210491445202344, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1226, Loss: 0.5102247500506603, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1227, Loss: 0.35852998065653197, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1228, Loss: 0.2972584413673694, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1229, Loss: 0.550493773190025, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1230, Loss: 0.2782780168837068, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1231, Loss: 0.3095323852270077, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1232, Loss: 0.26924988060217014, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1233, Loss: 0.36153289314261194, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1234, Loss: 0.2957366644657978, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1235, Loss: 0.6289313277097583, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1236, Loss: 0.5351041435903047, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1237, Loss: 0.24082060741638492, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1238, Loss: 0.3207909112611055, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1239, Loss: 0.48455237016033703, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1240, Loss: 0.3647706075316395, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1241, Loss: 0.3823524008334154, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1242, Loss: 0.31826910457457774, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1243, Loss: 0.435268099811561, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1244, Loss: 0.22032080361075662, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1245, Loss: 0.4101811572520181, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1246, Loss: 0.3212860377821609, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1247, Loss: 0.3309866703507022, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1248, Loss: 0.4290232414684443, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1249, Loss: 0.3057249479548255, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1250, Loss: 0.5408748399085969, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1251, Loss: 0.33944573486877694, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1252, Loss: 0.30215708040682493, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1253, Loss: 0.3500502223759424, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1254, Loss: 0.30270215591152766, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1255, Loss: 0.2862131444810011, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1256, Loss: 0.6039538121973598, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1257, Loss: 0.4237235618003196, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1258, Loss: 0.23099549014465218, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1259, Loss: 0.33375929065939136, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1260, Loss: 0.24006554634234578, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1261, Loss: 0.2589761157682232, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1262, Loss: 0.44830877977410155, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1263, Loss: 0.3201558207949968, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1264, Loss: 0.26159075626075495, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1265, Loss: 0.31668517649190137, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1266, Loss: 0.2816611782877367, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1267, Loss: 0.44166204407596177, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1268, Loss: 0.41524943757962585, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1269, Loss: 0.2829964476986306, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1270, Loss: 0.33148819247218786, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1271, Loss: 0.31993433216639944, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1272, Loss: 0.31214248333632255, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1273, Loss: 0.3381104434378155, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1274, Loss: 0.4115384579348875, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1275, Loss: 0.27863387861702676, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1276, Loss: 0.45841827224316406, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1277, Loss: 0.29049291692558576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1278, Loss: 0.37178418587178413, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1279, Loss: 0.272577312515322, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1280, Loss: 0.4899789085753121, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1281, Loss: 0.3315338955455446, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1282, Loss: 0.37562103843686256, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1283, Loss: 0.3319976841162094, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1284, Loss: 0.9721074958753801, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1285, Loss: 0.3118858076726707, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1286, Loss: 0.3313376220495224, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1287, Loss: 0.4141857021119603, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1288, Loss: 0.6092137720187557, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1289, Loss: 0.23423545723297778, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1290, Loss: 0.42881445814518004, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1291, Loss: 0.3753686333670836, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1292, Loss: 0.2524588423776192, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1293, Loss: 0.5018871453246021, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1294, Loss: 0.32591817014113733, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1295, Loss: 0.35166608856755766, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1296, Loss: 0.23651464160085828, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1297, Loss: 0.402856746989838, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1298, Loss: 0.6618542165970989, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1299, Loss: 0.4323677029451326, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1300, Loss: 0.5009617483339132, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1301, Loss: 0.5395751241826174, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1302, Loss: 0.220110888742431, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1303, Loss: 0.2978699008706519, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1304, Loss: 0.26275859108721966, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1305, Loss: 0.5169494058235404, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1306, Loss: 0.2964044875658839, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1307, Loss: 0.5793029769658975, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1308, Loss: 0.2906095820686533, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1309, Loss: 0.3699283607770181, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1310, Loss: 0.43364820309679886, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1311, Loss: 0.2755271142399621, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1312, Loss: 0.47810670975677283, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1313, Loss: 0.3507513433371849, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1314, Loss: 0.7012657259070373, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1315, Loss: 0.41774396058514607, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1316, Loss: 0.33352298177518236, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1317, Loss: 0.21932837176501116, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1318, Loss: 0.28745996222165093, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1319, Loss: 0.33059364260809887, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1320, Loss: 0.24457744054997763, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1321, Loss: 0.35838640370969366, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1322, Loss: 0.5210699625120601, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1323, Loss: 0.2571966744844273, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1324, Loss: 0.5594630792656425, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1325, Loss: 0.43512676488229773, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1326, Loss: 0.33449883039283645, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1327, Loss: 0.2784106328046647, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1328, Loss: 0.29833037511649557, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1329, Loss: 0.36730215438177505, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1330, Loss: 0.5329193371858778, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1331, Loss: 0.23646767960017936, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1332, Loss: 0.31360509683529886, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1333, Loss: 0.5340214982281895, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1334, Loss: 0.6885983577787658, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1335, Loss: 0.3323062494338309, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1336, Loss: 0.5889915698198396, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1337, Loss: 0.27576862728664464, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1338, Loss: 0.31757013302153203, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1339, Loss: 0.4411293500152383, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1340, Loss: 0.477867479633443, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1341, Loss: 0.34757295657918463, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1342, Loss: 0.5009717068906477, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1343, Loss: 0.3574563775406798, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1344, Loss: 0.2874266013538885, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1345, Loss: 0.3692014376177101, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1346, Loss: 0.2588972253654772, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1347, Loss: 0.6632311072541744, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1348, Loss: 0.38534781690032416, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1349, Loss: 0.5328293406111483, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1350, Loss: 0.24317413303348023, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1351, Loss: 0.27427241652694606, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1352, Loss: 0.6677863368269561, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1353, Loss: 0.34169484401718114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1354, Loss: 0.5226536898698972, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1355, Loss: 0.35294759592483843, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1356, Loss: 0.2605080964578731, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1357, Loss: 0.3423603573004518, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1358, Loss: 0.3569886364034919, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1359, Loss: 0.3180881766044369, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1360, Loss: 0.3597711962204495, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1361, Loss: 0.35845382994214925, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1362, Loss: 0.3422149671785565, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1363, Loss: 0.4354130139493979, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1364, Loss: 0.3864947247061459, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1365, Loss: 0.4180051871059058, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1366, Loss: 0.2836988682197296, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1367, Loss: 0.3146247331740736, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1368, Loss: 0.7299431209624572, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1369, Loss: 0.3530378354096057, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1370, Loss: 0.4844392907614804, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1371, Loss: 0.37866231368265274, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1372, Loss: 0.3230869700936535, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1373, Loss: 0.40761544190974686, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1374, Loss: 0.42551914333173585, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1375, Loss: 0.2520812192730285, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1376, Loss: 0.2891704819384618, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1377, Loss: 0.5834410295132516, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1378, Loss: 0.31459494218894574, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1379, Loss: 0.24732567490785476, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1380, Loss: 0.5603114622525137, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1381, Loss: 0.470370713783993, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1382, Loss: 0.31523490674603494, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1383, Loss: 0.48204630501400253, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1384, Loss: 0.25947065101183275, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1385, Loss: 0.2561068018911586, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1386, Loss: 0.4144381725525163, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1387, Loss: 0.25744227553159316, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1388, Loss: 0.2519235561117114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1389, Loss: 0.32395655774163495, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1390, Loss: 0.49436339740848956, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1391, Loss: 0.6161204164533224, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1392, Loss: 0.4374316397472034, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1393, Loss: 0.39768782390549196, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1394, Loss: 0.3907709331205645, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1395, Loss: 0.26967354912339336, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1396, Loss: 0.2487333198985485, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1397, Loss: 0.21433390103128871, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1398, Loss: 0.2788532660429341, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1399, Loss: 0.3096373003185894, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1400, Loss: 0.46782672916445345, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1401, Loss: 0.2574590090283638, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1402, Loss: 0.25284169339285284, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1403, Loss: 0.22623611646424502, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1404, Loss: 0.30994709626732697, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1405, Loss: 0.3315123876641969, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1406, Loss: 0.2948358589223725, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1407, Loss: 0.26586756433727154, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1408, Loss: 0.346354298018525, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1409, Loss: 0.2117849060621524, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1410, Loss: 0.36772193450290963, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1411, Loss: 0.41911297452346935, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1412, Loss: 0.6686473186700923, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1413, Loss: 0.39543539926185756, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1414, Loss: 0.27586793241787233, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1415, Loss: 0.373219008801438, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1416, Loss: 0.3627286354595155, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1417, Loss: 0.2991347810092341, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1418, Loss: 0.36094888158459093, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1419, Loss: 0.33463173566471327, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1420, Loss: 0.2899152052504515, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1421, Loss: 0.34184867893635046, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1422, Loss: 0.23392176831290756, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1423, Loss: 0.27700255997183343, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1424, Loss: 0.4815746386716454, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1425, Loss: 0.6052936708804362, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1426, Loss: 0.2662381195175901, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1427, Loss: 0.38148020836775864, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1428, Loss: 0.41204494400123526, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1429, Loss: 0.4089052328011431, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1430, Loss: 0.431075550180095, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1431, Loss: 0.40067398353693195, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1432, Loss: 0.4685918502948307, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1433, Loss: 0.4514463191047336, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1434, Loss: 0.39744038314716795, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1435, Loss: 0.4152906840493951, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1436, Loss: 0.4293380664864417, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1437, Loss: 0.2826093282846531, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1438, Loss: 0.2579484451118458, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1439, Loss: 0.2826292410113777, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1440, Loss: 0.3046257011526552, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1441, Loss: 0.47626947617657256, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1442, Loss: 0.5113130371067616, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1443, Loss: 0.31348393920689116, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1444, Loss: 0.295983695345556, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1445, Loss: 0.2910117231360543, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1446, Loss: 0.2257170780873269, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1447, Loss: 0.2894563319654551, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1448, Loss: 0.24864616706251697, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1449, Loss: 0.32627618919641993, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1450, Loss: 0.24609500844257937, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1451, Loss: 0.3169366872557374, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1452, Loss: 0.37443743539602226, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1453, Loss: 0.24332804424225543, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1454, Loss: 0.24826774357292836, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1455, Loss: 0.37018297279743595, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1456, Loss: 0.21551504547052608, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1457, Loss: 0.23745276398825693, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1458, Loss: 0.33676780528083067, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1459, Loss: 0.397634666833348, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1460, Loss: 0.2292133497892548, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1461, Loss: 0.5163306028565093, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1462, Loss: 0.3799355251862211, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1463, Loss: 0.23706762931249722, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1464, Loss: 0.44234302798812164, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1465, Loss: 0.27802583912424367, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1466, Loss: 0.34518095250458214, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1467, Loss: 0.4191775502135507, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1468, Loss: 0.4209491895541587, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1469, Loss: 0.3353178998848983, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1470, Loss: 0.7077369009772878, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1471, Loss: 0.25651841552057103, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1472, Loss: 0.337788760194509, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1473, Loss: 0.29621959033736706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1474, Loss: 0.24258852881024284, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1475, Loss: 0.45696769579297547, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1476, Loss: 0.2957514857918825, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1477, Loss: 0.40402943339819525, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1478, Loss: 0.31958092934140037, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1479, Loss: 0.36922574707180716, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1480, Loss: 0.42198163612380357, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1481, Loss: 0.6328852803997389, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1482, Loss: 0.27398033439940095, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1483, Loss: 0.39607485907900697, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1484, Loss: 0.32911876036685705, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1485, Loss: 0.4018961028964475, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1486, Loss: 0.3412754020958092, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1487, Loss: 0.31855591225399027, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1488, Loss: 0.33557849457843636, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1489, Loss: 0.23366171423237453, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1490, Loss: 0.31219031485758053, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1491, Loss: 0.3123333718559218, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1492, Loss: 0.2641984913051825, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1493, Loss: 0.27845524481095996, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1494, Loss: 0.318939605545981, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1495, Loss: 0.2991445156866117, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1496, Loss: 0.31466078174077394, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1497, Loss: 0.7549651504606139, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1498, Loss: 0.26413647758355613, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1499, Loss: 0.4031082636539909, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1500, Loss: 0.5868235045389836, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1501, Loss: 0.29034003854160206, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1502, Loss: 0.267327545880521, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1503, Loss: 0.33905256275505946, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1504, Loss: 0.2664659322185997, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1505, Loss: 0.34896742468441416, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1506, Loss: 0.3660736423248748, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1507, Loss: 0.2833316824641183, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1508, Loss: 0.3351963825063725, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1509, Loss: 0.24247148569496021, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1510, Loss: 0.2874337958011205, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1511, Loss: 0.3186149576942749, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1512, Loss: 0.8808653007160352, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1513, Loss: 0.4037055379792195, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1514, Loss: 0.2449167061692037, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1515, Loss: 0.32881013712522855, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1516, Loss: 0.4170815083089266, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1517, Loss: 0.46724939497966456, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1518, Loss: 0.47261380312749635, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1519, Loss: 0.28868384439593525, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1520, Loss: 0.38003783844889255, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1521, Loss: 0.2785675439244267, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1522, Loss: 0.44031062355190603, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1523, Loss: 0.3221244957550331, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1524, Loss: 0.3044593314317759, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1525, Loss: 0.3734789159241326, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1526, Loss: 0.272362498054428, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1527, Loss: 0.3431228249507636, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1528, Loss: 0.41208021760451585, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1529, Loss: 0.2954204014196516, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1530, Loss: 0.2738793215554323, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1531, Loss: 0.31306621601790685, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1532, Loss: 0.3654813613627148, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1533, Loss: 0.43491861551429944, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1534, Loss: 0.30131723327831, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1535, Loss: 0.25212218791234625, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1536, Loss: 0.4713628575901144, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1537, Loss: 0.5778532419366682, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1538, Loss: 0.23464912392813375, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1539, Loss: 0.2613332251492939, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1540, Loss: 0.3644937630330749, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1541, Loss: 0.3043539008018916, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1542, Loss: 0.3107456868646749, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1543, Loss: 0.2809824214496109, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1544, Loss: 0.42848215534875383, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1545, Loss: 0.3889770474201115, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1546, Loss: 0.2724354208898262, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1547, Loss: 0.38259112782975246, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1548, Loss: 0.2980902839929654, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1549, Loss: 0.27804346862441776, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1550, Loss: 0.3127302455050523, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1551, Loss: 0.6363877998658793, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1552, Loss: 0.34945016418010466, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1553, Loss: 0.3847647984991838, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1554, Loss: 0.3817757444867337, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1555, Loss: 0.48958182953758206, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1556, Loss: 0.3298518667663719, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1557, Loss: 0.33852425144886167, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1558, Loss: 0.23513554937144163, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1559, Loss: 0.31333487927698284, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1560, Loss: 0.46570411050537436, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1561, Loss: 0.39140853817840426, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1562, Loss: 0.8037096055664195, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1563, Loss: 0.28595653206724814, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1564, Loss: 0.28268313013997937, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1565, Loss: 0.2465694799275957, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1566, Loss: 0.246711026591009, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1567, Loss: 0.2997909246704045, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1568, Loss: 0.283048193563074, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1569, Loss: 0.4544454983690973, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1570, Loss: 0.26157872202817173, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1571, Loss: 0.23104206677472186, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1572, Loss: 0.3568289669726268, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1573, Loss: 0.39634614480831776, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1574, Loss: 0.2789967651517407, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1575, Loss: 0.30026851034219354, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1576, Loss: 0.36879577288338616, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1577, Loss: 0.23140411103012062, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1578, Loss: 0.40873556065829764, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1579, Loss: 0.6390339842729303, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1580, Loss: 0.5090521727303634, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1581, Loss: 0.5094198098727368, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1582, Loss: 0.4601242752481467, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1583, Loss: 0.47593943566236685, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1584, Loss: 0.35539337255842185, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1585, Loss: 0.25583769660622996, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1586, Loss: 0.2589465535441941, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1587, Loss: 0.2167629741303172, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1588, Loss: 0.3828531216250749, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1589, Loss: 0.3656537433884439, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1590, Loss: 0.3388192217882204, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1591, Loss: 0.22790562317693108, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1592, Loss: 0.31035880262755494, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1593, Loss: 0.22095998631508057, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1594, Loss: 0.25496032730994955, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1595, Loss: 0.28704010224635296, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1596, Loss: 0.25791363226380415, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1597, Loss: 0.23451089771791608, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1598, Loss: 0.4382275460955789, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1599, Loss: 0.32246485614296144, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1600, Loss: 0.28863524366661736, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1601, Loss: 0.326008700088112, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1602, Loss: 0.36360252752139266, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1603, Loss: 0.617512638209597, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1604, Loss: 0.5388804908834604, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1605, Loss: 0.427626768071461, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1606, Loss: 0.47879100165672006, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1607, Loss: 0.343855529610624, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1608, Loss: 0.7063853542695042, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1609, Loss: 0.34572809111012526, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1610, Loss: 0.31752066029329823, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1611, Loss: 0.3040273048555695, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1612, Loss: 0.33320699775803536, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1613, Loss: 0.24995497412914605, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1614, Loss: 0.30417387144097596, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1615, Loss: 0.31541446571967213, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1616, Loss: 0.5529326162787844, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1617, Loss: 0.3121035452477284, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1618, Loss: 0.2687729057362581, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1619, Loss: 0.29213120707328866, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1620, Loss: 0.4572913220311623, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1621, Loss: 0.4699071322589689, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1622, Loss: 0.2425957252345649, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1623, Loss: 0.3019882315579393, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1624, Loss: 0.523147076905203, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1625, Loss: 0.3271265912800929, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1626, Loss: 0.2721224606710799, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1627, Loss: 0.5757633942708893, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1628, Loss: 0.2784614669277491, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1629, Loss: 0.24115871738699826, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1630, Loss: 0.24321257113127348, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1631, Loss: 0.2482515876666106, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1632, Loss: 0.33583311192222254, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1633, Loss: 0.2874887059791477, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1634, Loss: 0.5277931409845664, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1635, Loss: 0.354138010508264, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1636, Loss: 0.32876206379857165, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1637, Loss: 0.251751187562407, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1638, Loss: 0.23017614649188126, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1639, Loss: 0.32581777707059023, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1640, Loss: 0.31956023837519415, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1641, Loss: 0.33851643560969286, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1642, Loss: 0.4263326122432431, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1643, Loss: 0.31309171570667543, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1644, Loss: 0.4209278865932633, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1645, Loss: 0.2606767918561746, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1646, Loss: 0.3627444652665125, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1647, Loss: 0.21905479553651921, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1648, Loss: 0.3442330890926827, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1649, Loss: 0.34160769663741875, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1650, Loss: 0.5574062839402631, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1651, Loss: 0.6085918295191858, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1652, Loss: 0.3408379639272635, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1653, Loss: 0.2759420697853092, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1654, Loss: 0.6079829074443512, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1655, Loss: 0.42102137916994276, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1656, Loss: 0.4803433398010073, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1657, Loss: 0.2742013169055103, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1658, Loss: 0.465377249819348, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1659, Loss: 0.30072533733044227, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1660, Loss: 0.39040244758660186, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1661, Loss: 0.43529113368083994, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1662, Loss: 0.3343364350903769, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1663, Loss: 0.5103732641668735, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1664, Loss: 0.2771194226945765, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1665, Loss: 0.29593609275470467, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1666, Loss: 0.734515199025467, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1667, Loss: 0.3786971402872511, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1668, Loss: 0.4760522632421622, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1669, Loss: 0.46614119393425557, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1670, Loss: 0.26075515023956913, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1671, Loss: 0.4207904629283524, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1672, Loss: 0.3249462691582823, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1673, Loss: 0.5469136255971803, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1674, Loss: 0.38937618241902305, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1675, Loss: 0.3414240140328328, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1676, Loss: 0.35219365480664044, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1677, Loss: 0.6095575641234702, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1678, Loss: 0.404126987842027, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1679, Loss: 0.2886403749079817, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1680, Loss: 0.48911456319375796, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1681, Loss: 0.2670641391254081, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1682, Loss: 0.26388406519871804, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1683, Loss: 0.3512255960837262, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1684, Loss: 0.4700473352253274, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1685, Loss: 0.27962029721306525, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1686, Loss: 0.721372473350332, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1687, Loss: 0.2796836009142019, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1688, Loss: 0.3105066021087235, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1689, Loss: 0.4952899202425163, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1690, Loss: 0.3167452507714719, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1691, Loss: 0.6229878437325029, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1692, Loss: 0.3095407789787694, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1693, Loss: 0.3390407960502647, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1694, Loss: 0.35344899308384903, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1695, Loss: 0.2503729146253033, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1696, Loss: 0.3705188649103411, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1697, Loss: 0.4075041765499709, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1698, Loss: 0.3398696173926147, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1699, Loss: 0.2892863438303313, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1700, Loss: 0.3025606938711732, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1701, Loss: 0.25512876029606535, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1702, Loss: 0.35750236824397286, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1703, Loss: 0.2743707147782689, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1704, Loss: 0.4523695987993971, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1705, Loss: 0.33033727876780833, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1706, Loss: 0.39024312347760354, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1707, Loss: 0.5371606628084041, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1708, Loss: 0.28771759657815965, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1709, Loss: 0.2767552660874597, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1710, Loss: 0.33328883544484345, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1711, Loss: 0.3519717386739295, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1712, Loss: 0.4215313245553178, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1713, Loss: 0.48149080312707937, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1714, Loss: 0.42151301423792986, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1715, Loss: 0.4794711751911602, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1716, Loss: 0.3744377336103225, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1717, Loss: 0.288290548114062, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1718, Loss: 0.9100181379734323, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1719, Loss: 0.31539129817399336, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1720, Loss: 0.4321372787868951, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1721, Loss: 0.3681043055480935, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1722, Loss: 0.32364459630390696, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1723, Loss: 0.3022594597012124, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1724, Loss: 0.29661292445164, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1725, Loss: 0.3881242695792946, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1726, Loss: 0.298532304539995, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1727, Loss: 0.4592514780129999, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1728, Loss: 0.36476465666922825, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1729, Loss: 0.5272220156337634, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1730, Loss: 0.3323745829878699, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1731, Loss: 0.320770347569811, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1732, Loss: 0.4329419017270658, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1733, Loss: 0.5107011263610587, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1734, Loss: 0.40479495029048607, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1735, Loss: 0.31479010684568565, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1736, Loss: 0.35215070799202897, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1737, Loss: 0.48463281981318496, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1738, Loss: 0.27346957156495333, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1739, Loss: 0.38993954538144426, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1740, Loss: 0.4068891488277019, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1741, Loss: 0.31943928727066967, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1742, Loss: 0.2900261838643875, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1743, Loss: 0.4600117493901078, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1744, Loss: 0.5139029868923598, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1745, Loss: 0.34749068123453347, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1746, Loss: 0.23914744909386695, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1747, Loss: 0.360053611509248, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1748, Loss: 0.44316795745639914, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1749, Loss: 0.3135566473383497, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1750, Loss: 0.7133055231235608, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1751, Loss: 0.37399688689843413, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1752, Loss: 0.2972181103962322, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1753, Loss: 0.2568725409877671, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1754, Loss: 0.30020932098467323, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1755, Loss: 0.25924180520046164, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1756, Loss: 0.2255513306998963, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1757, Loss: 0.6293746142044827, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1758, Loss: 0.36753233828760495, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1759, Loss: 0.3180752513534776, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1760, Loss: 0.376020181133806, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1761, Loss: 0.2378419154224044, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1762, Loss: 0.6856817053041492, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1763, Loss: 0.286223795362803, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1764, Loss: 0.49612144106483, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1765, Loss: 0.29725470479211846, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1766, Loss: 0.2372347871541591, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1767, Loss: 0.3118191713545807, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1768, Loss: 0.27454762819495704, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1769, Loss: 0.7012711828086265, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1770, Loss: 0.41884264606267574, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1771, Loss: 0.47089053736052994, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1772, Loss: 0.3172310374926246, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1773, Loss: 0.4665910542800389, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1774, Loss: 0.27602705444560705, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1775, Loss: 0.3457510791609942, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1776, Loss: 0.29185393981264857, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1777, Loss: 0.49445348838894765, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1778, Loss: 0.34715838168780694, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1779, Loss: 0.30877701582619005, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1780, Loss: 0.2184294048957221, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1781, Loss: 0.39457237293257913, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1782, Loss: 0.4892073135392409, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1783, Loss: 0.3449486207192804, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1784, Loss: 0.32421404680423127, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1785, Loss: 0.48737326667639225, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1786, Loss: 0.3852885062257114, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1787, Loss: 0.3004195041444564, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1788, Loss: 0.3582273243148617, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1789, Loss: 0.3065471866521025, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1790, Loss: 0.5241800133063667, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1791, Loss: 0.32441781923102686, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1792, Loss: 0.3138983172289327, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1793, Loss: 0.38667162295653335, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1794, Loss: 0.3237396341496085, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1795, Loss: 0.25930798702357843, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1796, Loss: 0.634843067501937, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1797, Loss: 0.2998296284685873, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1798, Loss: 0.29180800699627585, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1799, Loss: 0.23466790912387803, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1800, Loss: 0.3277994603449424, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1801, Loss: 0.2848272950624733, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1802, Loss: 0.5114210839811055, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1803, Loss: 0.28212497085271576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1804, Loss: 0.39048363565816624, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1805, Loss: 0.4207261161814111, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1806, Loss: 0.32231867242568313, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1807, Loss: 0.547257753049982, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1808, Loss: 0.5548328982660099, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1809, Loss: 0.3596318059026269, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1810, Loss: 0.3314035647683115, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1811, Loss: 0.3450377306763375, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1812, Loss: 0.3525533658904781, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1813, Loss: 0.25439673525856893, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1814, Loss: 0.3870837044909294, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1815, Loss: 0.2846997749282091, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1816, Loss: 0.3372641712103463, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1817, Loss: 0.26968848062621026, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1818, Loss: 0.38506833787520234, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1819, Loss: 0.22873736099651995, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1820, Loss: 0.25244826895884237, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1821, Loss: 0.31292584851187, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1822, Loss: 0.29560223861249635, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1823, Loss: 0.2536003800897283, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1824, Loss: 0.26092637830926024, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1825, Loss: 0.3475549205859857, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1826, Loss: 0.5463190500811679, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1827, Loss: 0.30993084104043267, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1828, Loss: 0.3706111519968579, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1829, Loss: 0.28174576621036956, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1830, Loss: 0.47582772706482834, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1831, Loss: 0.31550881968104105, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1832, Loss: 0.3921033809628516, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1833, Loss: 0.3629175288695531, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1834, Loss: 0.25310367328312044, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1835, Loss: 0.3812489669851923, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1836, Loss: 0.2888937632548596, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1837, Loss: 0.23101862877622634, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1838, Loss: 0.2596641862698078, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1839, Loss: 0.285054338815602, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1840, Loss: 0.3788759497812706, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1841, Loss: 0.4492123276172576, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1842, Loss: 0.3208584723691732, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1843, Loss: 0.27947901375511475, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1844, Loss: 0.2682712328308079, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1845, Loss: 0.28233894614960986, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1846, Loss: 0.3993809420496446, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1847, Loss: 0.33569663436677677, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1848, Loss: 0.2827410053094154, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1849, Loss: 0.40205771363475873, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1850, Loss: 0.2688368629483121, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1851, Loss: 0.27280860373233085, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1852, Loss: 0.7152017504066744, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1853, Loss: 0.2593377360964373, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1854, Loss: 0.2968725044261638, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1855, Loss: 0.21632759661749398, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1856, Loss: 0.2860471054442956, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1857, Loss: 0.31503571882462233, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1858, Loss: 0.45172709633081926, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1859, Loss: 0.4847135638230238, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1860, Loss: 0.5193373836677689, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1861, Loss: 0.35973895783657384, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1862, Loss: 0.2402749950713069, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1863, Loss: 0.412565133189041, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1864, Loss: 0.3721736036124391, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1865, Loss: 0.42575158469702645, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1866, Loss: 0.3323289746480872, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1867, Loss: 0.3864552024721436, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1868, Loss: 0.24352951728852074, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1869, Loss: 0.7703173177735814, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1870, Loss: 0.26851810365710427, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1871, Loss: 0.4183523182625373, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1872, Loss: 0.41762275177551, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1873, Loss: 0.27505065967762005, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1874, Loss: 0.2758917704171816, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Batch 1875, Loss: 0.6102588263641427, Batch Size: 32, Learning Rate: 0.00013311159374999998\n",
      "Epoch 7, Updated Learning Rate: 0.00011314485468749997\n",
      "Epoch 7, Average Loss: 0.36203949917649214, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1, Loss: 0.7914750536883073, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 2, Loss: 0.2897578153342839, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 3, Loss: 0.25795313021386346, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 4, Loss: 0.5366545248661226, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 5, Loss: 0.3080702294987356, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 6, Loss: 0.3021055230228776, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 7, Loss: 0.2558361519501112, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 8, Loss: 0.49290843340144846, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 9, Loss: 0.2620554383226345, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 10, Loss: 0.47172049457424253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 11, Loss: 0.2710320948935243, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 12, Loss: 0.59783865670693, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 13, Loss: 0.26612851841581253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 14, Loss: 0.43593769770358304, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 15, Loss: 0.2587055441936555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 16, Loss: 0.5482942541857017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 17, Loss: 0.3277098819992401, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 18, Loss: 0.34538416236742847, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 19, Loss: 0.30691525441257506, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 20, Loss: 0.3477504165876044, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 21, Loss: 0.42257450892352966, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 22, Loss: 0.3050664613576267, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 23, Loss: 0.3872547676059535, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 24, Loss: 0.561923452688228, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 25, Loss: 0.35601687020909883, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 26, Loss: 0.29164061209234127, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 27, Loss: 0.2779579720857688, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 28, Loss: 0.24728184481410598, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 29, Loss: 0.626600986611314, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 30, Loss: 0.4552047027641222, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 31, Loss: 0.40500461781218905, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 32, Loss: 0.3294355628728255, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 33, Loss: 0.4206415573443183, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 34, Loss: 0.3255760533090679, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 35, Loss: 0.2209384473179713, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 36, Loss: 0.3639302537432757, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 37, Loss: 0.35761393605548014, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 38, Loss: 0.26948773286220423, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 39, Loss: 0.23889811482475481, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 40, Loss: 0.3913305733409222, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 41, Loss: 0.4210015698108746, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 42, Loss: 0.22566371333237417, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 43, Loss: 0.8129999342589089, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 44, Loss: 0.47592635135108696, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 45, Loss: 0.24296728486026448, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 46, Loss: 0.40538577222682176, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 47, Loss: 0.2732793209742668, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 48, Loss: 0.35648256915303156, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 49, Loss: 0.4838975298442162, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 50, Loss: 0.3002001835768692, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 51, Loss: 0.738439911791956, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 52, Loss: 0.2559260967009955, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 53, Loss: 0.30469362864157706, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 54, Loss: 0.34821275049458855, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 55, Loss: 0.26683254808781876, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 56, Loss: 0.34112336896123535, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 57, Loss: 0.24608423490022197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 58, Loss: 0.37253802048113893, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 59, Loss: 0.23820499893470912, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 60, Loss: 0.35211001805212466, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 61, Loss: 0.32721449352834475, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 62, Loss: 0.3514032226184986, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 63, Loss: 0.30067223735740833, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 64, Loss: 0.36424584103400415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 65, Loss: 0.32694382609652517, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 66, Loss: 0.3301342529080055, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 67, Loss: 0.37088418847858395, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 68, Loss: 0.22707151105358644, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 69, Loss: 0.25865186812879587, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 70, Loss: 0.4594558040138057, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 71, Loss: 0.5567718732000146, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 72, Loss: 0.25849912345875514, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 73, Loss: 0.2538806483829405, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 74, Loss: 0.3739453715178115, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 75, Loss: 0.4359088549601212, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 76, Loss: 0.301560588455002, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 77, Loss: 0.27786976448772993, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 78, Loss: 0.36495049027199844, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 79, Loss: 0.30052796366115064, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 80, Loss: 0.31593133582839744, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 81, Loss: 0.38608969294570883, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 82, Loss: 0.5602767505030768, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 83, Loss: 0.2954051577751933, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 84, Loss: 0.2587353933373679, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 85, Loss: 0.6604458536289313, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 86, Loss: 0.27165194579246743, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 87, Loss: 0.39617735811638444, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 88, Loss: 0.25478181801419775, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 89, Loss: 0.2779236679706048, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 90, Loss: 0.331702941819358, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 91, Loss: 0.2791353231735201, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 92, Loss: 0.22965536612810108, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 93, Loss: 0.3062176436421067, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 94, Loss: 0.2338372632731639, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 95, Loss: 0.5687246778170854, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 96, Loss: 0.5819803071067877, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 97, Loss: 0.32518099305999126, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 98, Loss: 0.3462152779205847, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 99, Loss: 0.28374382367341205, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 100, Loss: 0.37933383042974317, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 101, Loss: 0.3060864595771807, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 102, Loss: 0.3484435914091524, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 103, Loss: 0.3139877873608532, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 104, Loss: 0.29802874652545686, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 105, Loss: 0.33722605457723626, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 106, Loss: 0.283774841797393, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 107, Loss: 0.24904811919051734, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 108, Loss: 0.2790476549539884, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 109, Loss: 0.348126031577317, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 110, Loss: 0.3177001621635831, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 111, Loss: 0.40489643040424844, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 112, Loss: 0.3021393063448211, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 113, Loss: 0.3093907520115994, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 114, Loss: 0.5341955149383669, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 115, Loss: 0.25361550466239147, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 116, Loss: 0.49643653521979, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 117, Loss: 0.6230231846327757, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 118, Loss: 0.3852538684956581, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 119, Loss: 0.3931649914884078, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 120, Loss: 0.3433361035081989, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 121, Loss: 0.3593038551416101, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 122, Loss: 0.2818223303101476, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 123, Loss: 0.44709902616789915, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 124, Loss: 0.22125584773384932, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 125, Loss: 0.39181481186278017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 126, Loss: 0.29978186569109616, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 127, Loss: 0.3252706778734254, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 128, Loss: 0.35712620240741555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 129, Loss: 0.7226719975403806, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 130, Loss: 0.4058501749781437, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 131, Loss: 0.2598852278005563, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 132, Loss: 0.35240677548879973, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 133, Loss: 0.364092158377206, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 134, Loss: 0.39643566762244586, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 135, Loss: 0.3144142901803455, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 136, Loss: 0.4791526990891737, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 137, Loss: 0.23707715749170613, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 138, Loss: 0.2693884189014827, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 139, Loss: 0.24630692530390902, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 140, Loss: 0.5299212997150444, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 141, Loss: 0.3036885578711082, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 142, Loss: 0.27115491656801505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 143, Loss: 0.389742055525219, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 144, Loss: 0.4458826509774447, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 145, Loss: 0.3023938293092183, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 146, Loss: 0.378981897199145, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 147, Loss: 0.31123370421425633, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 148, Loss: 0.28998115837008137, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 149, Loss: 0.43877221051118787, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 150, Loss: 0.39559198460321743, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 151, Loss: 0.35816157537489224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 152, Loss: 0.34298421077816554, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 153, Loss: 0.3542763682745352, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 154, Loss: 0.3314817469648701, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 155, Loss: 0.2657726363656452, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 156, Loss: 0.29764742361097907, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 157, Loss: 0.2305053905670624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 158, Loss: 0.2921521239880194, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 159, Loss: 0.2238344314498789, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 160, Loss: 0.5371320328187945, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 161, Loss: 0.3374152835062889, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 162, Loss: 0.2804124804021157, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 163, Loss: 0.2890910363925049, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 164, Loss: 0.3139388620815625, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 165, Loss: 0.3028990683203949, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 166, Loss: 0.3087505288757586, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 167, Loss: 0.2811141065904089, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 168, Loss: 0.45041545789632376, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 169, Loss: 0.2616389472441206, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 170, Loss: 0.21701899781172365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 171, Loss: 0.273119342957772, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 172, Loss: 0.3096418389071522, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 173, Loss: 0.380369677024861, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 174, Loss: 0.42363786959780786, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 175, Loss: 0.2545850532899, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 176, Loss: 0.2718421329887706, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 177, Loss: 0.42719998586851504, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 178, Loss: 0.2386904663022129, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 179, Loss: 0.22399584257344282, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 180, Loss: 0.3723006909633309, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 181, Loss: 0.38340629465349496, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 182, Loss: 0.24807991154924922, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 183, Loss: 0.22237285748230154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 184, Loss: 0.29553103046277807, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 185, Loss: 0.45614276383020963, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 186, Loss: 0.3455865932046143, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 187, Loss: 0.7884011340757232, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 188, Loss: 0.5387368240602577, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 189, Loss: 0.4413938973496363, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 190, Loss: 0.3846167852187973, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 191, Loss: 0.30288214250214285, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 192, Loss: 0.3350633632555743, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 193, Loss: 0.2780511752702268, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 194, Loss: 0.3653621714609969, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 195, Loss: 0.29172683687008216, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 196, Loss: 0.3489292085470762, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 197, Loss: 0.5565016613537352, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 198, Loss: 0.28824196289936477, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 199, Loss: 0.30543411864903536, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 200, Loss: 0.3142660792541958, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 201, Loss: 0.28189936520003533, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 202, Loss: 0.40632446493165164, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 203, Loss: 0.2493062296679564, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 204, Loss: 0.36648485116736673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 205, Loss: 0.2649287557236144, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 206, Loss: 0.2835799929761692, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 207, Loss: 0.4150513620140993, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 208, Loss: 0.5956096035588702, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 209, Loss: 0.4017240185021449, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 210, Loss: 0.2967123094807046, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 211, Loss: 0.3361545820005838, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 212, Loss: 0.3705756902375361, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 213, Loss: 0.3902032984075563, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 214, Loss: 0.5577252572585896, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 215, Loss: 0.3226802072114598, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 216, Loss: 0.2988189104543417, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 217, Loss: 0.29274112546958597, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 218, Loss: 0.3480956944795252, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 219, Loss: 0.35554762248873256, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 220, Loss: 0.25440232262558427, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 221, Loss: 0.24260231861878026, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 222, Loss: 0.2614342153048585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 223, Loss: 0.4890932389206596, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 224, Loss: 0.25487285313638514, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 225, Loss: 0.3825928287471852, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 226, Loss: 0.6441652697484956, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 227, Loss: 0.37450462520344363, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 228, Loss: 0.4112329795829671, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 229, Loss: 0.31803644180703616, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 230, Loss: 0.280036311066624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 231, Loss: 0.2736024663715375, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 232, Loss: 0.45801157863291075, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 233, Loss: 0.23791216682929686, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 234, Loss: 0.5257415008020645, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 235, Loss: 0.39891952389319074, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 236, Loss: 0.2603380652059558, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 237, Loss: 0.30544067405896186, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 238, Loss: 0.2837208382638362, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 239, Loss: 0.2266925786791728, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 240, Loss: 0.23350429805649253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 241, Loss: 0.2675176377627797, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 242, Loss: 0.2172413116822803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 243, Loss: 0.28649278372049647, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 244, Loss: 0.28289769530640596, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 245, Loss: 0.24881636854553407, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 246, Loss: 0.4572225328435757, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 247, Loss: 0.29747284003184965, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 248, Loss: 0.28361385598700223, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 249, Loss: 0.3690899402180484, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 250, Loss: 0.6877967945011894, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 251, Loss: 0.2855403869201813, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 252, Loss: 0.5840107017306311, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 253, Loss: 0.22591195347727708, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 254, Loss: 0.23623371686523756, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 255, Loss: 0.32140499653753496, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 256, Loss: 0.2567469112335367, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 257, Loss: 0.3816950390876505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 258, Loss: 0.33913115343170913, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 259, Loss: 0.25953930735049613, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 260, Loss: 0.2726216674798775, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 261, Loss: 0.43740063271813845, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 262, Loss: 0.281976410056035, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 263, Loss: 0.3948457284683197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 264, Loss: 0.4417699449378163, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 265, Loss: 0.3000117322840269, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 266, Loss: 0.5678559742606583, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 267, Loss: 0.30212133792315576, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 268, Loss: 0.281084721349091, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 269, Loss: 0.42331930092733705, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 270, Loss: 0.25491661117276043, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 271, Loss: 0.40795020731137405, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 272, Loss: 0.36712732590493213, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 273, Loss: 0.30368334125648433, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 274, Loss: 0.2790605733917803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 275, Loss: 0.34459030046581585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 276, Loss: 0.3441068677205009, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 277, Loss: 0.28006029507931207, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 278, Loss: 0.3411053866147048, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 279, Loss: 0.24688904782047014, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 280, Loss: 0.32511714708509476, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 281, Loss: 0.3317452632067847, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 282, Loss: 0.3593168407393913, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 283, Loss: 0.39414250349816815, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 284, Loss: 0.352024288318157, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 285, Loss: 0.2573930596306869, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 286, Loss: 0.37927751046004027, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 287, Loss: 0.3301562340617188, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 288, Loss: 0.589367045140561, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 289, Loss: 0.4797897210151334, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 290, Loss: 0.3610366390369138, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 291, Loss: 0.45856359179541584, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 292, Loss: 0.3741372075686603, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 293, Loss: 0.3565694642392261, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 294, Loss: 0.23515103761940526, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 295, Loss: 0.27608976114313377, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 296, Loss: 0.27927972504480736, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 297, Loss: 0.374131316290867, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 298, Loss: 0.3099118153886038, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 299, Loss: 0.3449504963425999, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 300, Loss: 0.4331051642668261, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 301, Loss: 0.3413137622503315, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 302, Loss: 0.24588149917981264, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 303, Loss: 0.5911453336393238, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 304, Loss: 0.41107833916247194, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 305, Loss: 0.3022915828841627, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 306, Loss: 0.3149094980620518, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 307, Loss: 0.4703157858068578, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 308, Loss: 0.24604441833787377, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 309, Loss: 0.24404838141652804, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 310, Loss: 0.3014650357362915, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 311, Loss: 0.283029402414446, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 312, Loss: 0.2318741366919605, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 313, Loss: 0.389564301288147, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 314, Loss: 0.25131681734313976, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 315, Loss: 0.24961633903069785, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 316, Loss: 0.2554094313158526, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 317, Loss: 0.5796797505521636, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 318, Loss: 0.2603862865745129, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 319, Loss: 0.24042145425900852, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 320, Loss: 0.5182791347938707, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 321, Loss: 0.5660672224797437, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 322, Loss: 0.30764722179666637, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 323, Loss: 0.2635114102508782, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 324, Loss: 0.3038280936639065, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 325, Loss: 0.3734997396275341, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 326, Loss: 0.25811649856081215, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 327, Loss: 0.2939174665478392, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 328, Loss: 0.24618570192293734, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 329, Loss: 0.37820411245838215, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 330, Loss: 0.24487948784144106, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 331, Loss: 0.5643020671799492, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 332, Loss: 0.3305594646973081, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 333, Loss: 0.40629369469897963, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 334, Loss: 0.22673734916412347, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 335, Loss: 0.28546418514157296, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 336, Loss: 0.2929566544144809, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 337, Loss: 0.26998935629120313, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 338, Loss: 0.30416203685970244, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 339, Loss: 0.5178952979438874, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 340, Loss: 0.2219572288412723, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 341, Loss: 0.38366138582850795, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 342, Loss: 0.3405012939987394, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 343, Loss: 0.39861853376890755, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 344, Loss: 0.24143682879166445, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 345, Loss: 0.3065978453942498, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 346, Loss: 0.3219883495350683, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 347, Loss: 0.34180394226614014, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 348, Loss: 0.3560304407400071, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 349, Loss: 0.3190265392729451, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 350, Loss: 0.4874946279620142, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 351, Loss: 0.2666166945497142, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 352, Loss: 0.2549742341803166, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 353, Loss: 0.4154548602881707, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 354, Loss: 0.2816180340100391, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 355, Loss: 0.2634615902196081, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 356, Loss: 0.3501414409514465, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 357, Loss: 0.3361365593775111, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 358, Loss: 0.3841141792825611, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 359, Loss: 0.3501765872559109, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 360, Loss: 0.2378053686830918, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 361, Loss: 0.2962313652761512, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 362, Loss: 0.32665394384552543, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 363, Loss: 0.26787531977526163, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 364, Loss: 0.2836621940277012, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 365, Loss: 0.33985726974699915, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 366, Loss: 0.3309633597938218, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 367, Loss: 0.3578968364098894, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 368, Loss: 0.2601385216680012, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 369, Loss: 0.5181549685819875, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 370, Loss: 0.24606724784770972, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 371, Loss: 0.4635894746849816, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 372, Loss: 0.5310442572229822, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 373, Loss: 0.44145469859589176, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 374, Loss: 0.40767159602019887, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 375, Loss: 0.2628707373885729, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 376, Loss: 0.33527892574228496, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 377, Loss: 0.6644226223495399, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 378, Loss: 0.2849284552713153, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 379, Loss: 0.29378056693238247, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 380, Loss: 0.4182726944064532, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 381, Loss: 0.24805998674467786, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 382, Loss: 0.49291051496636706, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 383, Loss: 0.5238477365362024, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 384, Loss: 0.32008497799911473, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 385, Loss: 0.40246832319731635, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 386, Loss: 0.26130892983412485, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 387, Loss: 0.4146735939081961, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 388, Loss: 0.4578457312420365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 389, Loss: 0.30705521187279705, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 390, Loss: 0.27003451273613754, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 391, Loss: 0.2683455202256197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 392, Loss: 0.2731261188002073, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 393, Loss: 0.3669445613408161, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 394, Loss: 0.32766716899703774, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 395, Loss: 0.28850927359874845, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 396, Loss: 0.4541904642059809, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 397, Loss: 0.3545232324094028, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 398, Loss: 0.2587892627553905, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 399, Loss: 0.2352408597885007, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 400, Loss: 0.32514496559937145, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 401, Loss: 0.3551410416356562, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 402, Loss: 0.3778876802336478, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 403, Loss: 0.4143640535954294, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 404, Loss: 0.3788103339426653, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 405, Loss: 0.5254031027430066, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 406, Loss: 0.4611918526719455, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 407, Loss: 0.3096905772515282, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 408, Loss: 0.3060149927662503, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 409, Loss: 0.2624722448578467, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 410, Loss: 0.2579937596070633, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 411, Loss: 0.364082908758227, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 412, Loss: 0.2630497362763834, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 413, Loss: 0.3802664741261905, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 414, Loss: 0.7006743920145785, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 415, Loss: 0.4528402031736717, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 416, Loss: 0.3627450929500774, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 417, Loss: 0.4763782387384105, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 418, Loss: 0.31203030662308207, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 419, Loss: 0.26873280652367726, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 420, Loss: 0.35550305090842305, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 421, Loss: 0.39127544853785656, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 422, Loss: 0.39962263826836614, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 423, Loss: 0.2858122042838156, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 424, Loss: 0.27426079939636244, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 425, Loss: 0.36363836994242593, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 426, Loss: 0.35168671165827736, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 427, Loss: 0.28097939850301673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 428, Loss: 0.28777699666108186, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 429, Loss: 0.2756210871997372, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 430, Loss: 0.3224686519499446, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 431, Loss: 0.2893906695034363, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 432, Loss: 0.5576590742818883, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 433, Loss: 0.2948102411564572, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 434, Loss: 0.2927954700169548, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 435, Loss: 0.3208941132748136, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 436, Loss: 0.3695031804640253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 437, Loss: 0.3232246555485385, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 438, Loss: 0.3421780243447916, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 439, Loss: 0.24220087634878235, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 440, Loss: 0.25047413145755215, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 441, Loss: 0.5717316671490877, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 442, Loss: 0.31162654780409976, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 443, Loss: 0.5273140862208913, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 444, Loss: 0.24203482835959977, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 445, Loss: 0.25771403064033827, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 446, Loss: 0.42337306114898976, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 447, Loss: 0.2586261112247897, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 448, Loss: 0.29452763234370005, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 449, Loss: 0.24142500472746875, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 450, Loss: 0.36410339940377573, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 451, Loss: 0.2665252050805237, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 452, Loss: 0.33151633787530543, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 453, Loss: 0.3351829761207106, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 454, Loss: 0.5059272398155334, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 455, Loss: 0.2373986514442286, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 456, Loss: 0.2750684548092489, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 457, Loss: 0.24110511805849372, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 458, Loss: 0.3527329097642975, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 459, Loss: 0.4182323691490961, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 460, Loss: 0.33060485278155777, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 461, Loss: 0.2706718928433497, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 462, Loss: 0.32923053408930725, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 463, Loss: 0.25269899247376193, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 464, Loss: 0.2855979821054875, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 465, Loss: 0.3182884210734537, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 466, Loss: 0.2496750353977463, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 467, Loss: 0.3438719392157552, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 468, Loss: 0.27859979141906427, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 469, Loss: 0.2886268407991421, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 470, Loss: 0.38881763448464657, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 471, Loss: 0.4241361134703967, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 472, Loss: 0.5151670220896944, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 473, Loss: 0.3198001128500511, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 474, Loss: 0.4130102471845782, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 475, Loss: 0.32504539936990473, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 476, Loss: 0.6223751374920836, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 477, Loss: 0.4190382416332715, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 478, Loss: 0.50208203178343, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 479, Loss: 0.36133008186667176, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 480, Loss: 0.2872007099242067, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 481, Loss: 0.38182661489156655, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 482, Loss: 0.23426860000060173, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 483, Loss: 0.455287086531809, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 484, Loss: 0.22734589160874688, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 485, Loss: 0.5914357734567636, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 486, Loss: 0.2764379642053566, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 487, Loss: 0.45512719935318424, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 488, Loss: 0.6131425995497182, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 489, Loss: 0.35590635057969333, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 490, Loss: 0.3242877997701148, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 491, Loss: 0.2729464237659532, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 492, Loss: 0.3335224966342145, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 493, Loss: 0.4728991163600371, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 494, Loss: 0.3379527842855765, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 495, Loss: 0.37602772369692505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 496, Loss: 0.2690198701861948, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 497, Loss: 0.33193623009380613, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 498, Loss: 0.2810956425764118, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 499, Loss: 0.3200468806416822, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 500, Loss: 0.3594654177808201, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 501, Loss: 0.3107248680055035, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 502, Loss: 0.395153481437138, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 503, Loss: 0.7667428262493082, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 504, Loss: 0.3743434502827784, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 505, Loss: 0.261509766407242, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 506, Loss: 0.2534220652196293, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 507, Loss: 0.3747508203405713, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 508, Loss: 0.5640010127533364, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 509, Loss: 0.4067990773842042, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 510, Loss: 0.25048315815920696, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 511, Loss: 0.4252148309160829, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 512, Loss: 0.36121237738938317, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 513, Loss: 0.2875551408047401, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 514, Loss: 0.5638313891633361, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 515, Loss: 0.26615665297645213, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 516, Loss: 0.24961916589729585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 517, Loss: 0.29350216529657414, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 518, Loss: 0.319803263614605, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 519, Loss: 0.3070584206041197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 520, Loss: 0.3046973126558631, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 521, Loss: 0.48816767367658487, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 522, Loss: 0.47281285630086023, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 523, Loss: 0.2790346577532751, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 524, Loss: 0.40200913594856325, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 525, Loss: 0.3238117185441185, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 526, Loss: 0.44303913712404364, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 527, Loss: 0.3022243356672787, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 528, Loss: 0.339245397836271, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 529, Loss: 0.36708446145205526, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 530, Loss: 0.34506019123244563, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 531, Loss: 0.6566996749487771, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 532, Loss: 0.2514192578245079, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 533, Loss: 0.44310419245662314, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 534, Loss: 0.44954351657416464, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 535, Loss: 0.2727198371285111, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 536, Loss: 0.38407188275933996, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 537, Loss: 0.4380710963099309, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 538, Loss: 0.4365452777065057, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 539, Loss: 0.31949594865191444, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 540, Loss: 0.28646166762884306, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 541, Loss: 0.25245325614035047, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 542, Loss: 0.2510379834250577, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 543, Loss: 0.48538817534765605, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 544, Loss: 0.25222332555131677, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 545, Loss: 0.3750027968495744, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 546, Loss: 0.3712810199567629, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 547, Loss: 0.22578836072310332, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 548, Loss: 0.46344721717324566, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 549, Loss: 0.4889792228264215, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 550, Loss: 0.3702265124647002, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 551, Loss: 0.3820962127875084, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 552, Loss: 0.40091859634986027, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 553, Loss: 0.24025409701204958, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 554, Loss: 0.5410568784133569, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 555, Loss: 0.3261947386353585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 556, Loss: 0.4690659620105963, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 557, Loss: 0.39078263456881296, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 558, Loss: 0.6372471764358616, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 559, Loss: 0.23837481569609525, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 560, Loss: 0.3113678899757502, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 561, Loss: 0.4290352870187035, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 562, Loss: 0.2457474430839174, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 563, Loss: 0.42183165036696935, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 564, Loss: 0.23979700635229373, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 565, Loss: 0.42093840428721524, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 566, Loss: 0.573712358979749, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 567, Loss: 0.2584919430754846, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 568, Loss: 0.31672547501302956, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 569, Loss: 0.42037771682129776, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 570, Loss: 0.23389435892042712, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 571, Loss: 0.29072577416895157, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 572, Loss: 0.3243613431685476, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 573, Loss: 0.3321202153021833, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 574, Loss: 0.4828677808435282, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 575, Loss: 0.5001766702747625, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 576, Loss: 0.24572215957710064, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 577, Loss: 0.37213520430008845, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 578, Loss: 0.3890167585056624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 579, Loss: 0.29630475850690724, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 580, Loss: 0.6930440652610816, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 581, Loss: 0.38461986199938436, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 582, Loss: 0.3137948715389275, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 583, Loss: 0.28639920778144445, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 584, Loss: 0.520336630189175, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 585, Loss: 0.25608858095645887, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 586, Loss: 0.45148808164215415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 587, Loss: 0.35058197879965775, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 588, Loss: 0.638768316458701, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 589, Loss: 0.24456150648782188, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 590, Loss: 0.4424647786412287, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 591, Loss: 0.5133461333771018, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 592, Loss: 0.4843169170791549, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 593, Loss: 0.2731686323764955, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 594, Loss: 0.3730658482915902, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 595, Loss: 0.31032185594538275, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 596, Loss: 0.30291737905160576, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 597, Loss: 0.3428204861391331, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 598, Loss: 0.5090045790302908, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 599, Loss: 0.2422764642863622, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 600, Loss: 0.3088797340764659, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 601, Loss: 0.6074939898195413, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 602, Loss: 0.3670190098323059, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 603, Loss: 0.256843589159713, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 604, Loss: 0.32721470593291146, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 605, Loss: 0.38818416638067565, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 606, Loss: 0.23688756751558784, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 607, Loss: 0.323419685456253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 608, Loss: 0.2611799772230022, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 609, Loss: 0.3447951982195878, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 610, Loss: 0.3192518515340895, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 611, Loss: 0.23348923848894704, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 612, Loss: 0.30747588785504854, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 613, Loss: 0.2652825832908879, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 614, Loss: 0.2627214805024971, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 615, Loss: 0.40087869315848335, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 616, Loss: 0.5221648892771573, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 617, Loss: 0.5301879153207054, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 618, Loss: 0.25680708289469173, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 619, Loss: 0.2301597149554849, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 620, Loss: 0.26530393800835356, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 621, Loss: 0.4305184303800603, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 622, Loss: 0.2729779874273785, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 623, Loss: 0.3540671122478913, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 624, Loss: 0.3740547509766331, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 625, Loss: 0.28302358801564653, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 626, Loss: 0.24845193837200058, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 627, Loss: 0.28976887244619803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 628, Loss: 0.4517950768200041, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 629, Loss: 0.24701389401242613, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 630, Loss: 0.30546616461328585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 631, Loss: 0.5222975355545273, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 632, Loss: 0.3122804801629004, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 633, Loss: 0.4503669450750337, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 634, Loss: 0.32450777436149447, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 635, Loss: 0.5147204807789109, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 636, Loss: 0.37304097974891964, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 637, Loss: 0.2848372555246711, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 638, Loss: 0.299113898744629, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 639, Loss: 0.2639511254390795, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 640, Loss: 0.32473934669516324, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 641, Loss: 0.2759058211317433, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 642, Loss: 0.2716526353471785, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 643, Loss: 0.34140263024621187, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 644, Loss: 0.49042886319127577, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 645, Loss: 0.3149716028881985, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 646, Loss: 0.36412880799223957, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 647, Loss: 0.22980055420011036, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 648, Loss: 0.2761665210259483, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 649, Loss: 0.27574123177935406, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 650, Loss: 0.30121636809104385, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 651, Loss: 0.2895566157757749, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 652, Loss: 0.2591030260003407, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 653, Loss: 0.2425684662634543, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 654, Loss: 0.23235746786390052, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 655, Loss: 0.35579035287118765, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 656, Loss: 0.7390308241657503, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 657, Loss: 0.23554432388571755, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 658, Loss: 0.5296698560935815, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 659, Loss: 0.486197458440658, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 660, Loss: 0.2834229421509518, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 661, Loss: 0.3129411119964234, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 662, Loss: 0.26723243268830726, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 663, Loss: 0.5468194124223771, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 664, Loss: 0.3059576921181334, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 665, Loss: 0.23299173307314303, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 666, Loss: 0.37256889509025926, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 667, Loss: 0.42307133709552924, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 668, Loss: 0.4149893785036084, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 669, Loss: 0.3135465439810192, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 670, Loss: 0.2292731832846887, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 671, Loss: 0.28718049042582466, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 672, Loss: 0.2621340200496274, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 673, Loss: 0.381848104919423, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 674, Loss: 0.3990402099004707, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 675, Loss: 0.29611067164829036, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 676, Loss: 0.28444608008653405, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 677, Loss: 0.3981633504255394, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 678, Loss: 0.3614889169224903, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 679, Loss: 0.2370383726611449, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 680, Loss: 0.302062142945829, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 681, Loss: 0.3707137331512028, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 682, Loss: 0.37373482763703025, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 683, Loss: 0.35110230527242525, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 684, Loss: 0.5002808301634656, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 685, Loss: 0.32072745221944365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 686, Loss: 0.5784077416631934, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 687, Loss: 0.2858813863753197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 688, Loss: 0.41831152132860616, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 689, Loss: 0.3188415562010031, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 690, Loss: 0.4718206935510098, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 691, Loss: 0.2922429206058308, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 692, Loss: 0.5852275922892427, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 693, Loss: 0.31839123505992206, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 694, Loss: 0.4672440540774415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 695, Loss: 0.5569033325775226, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 696, Loss: 0.29192274828001963, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 697, Loss: 0.30623219295430937, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 698, Loss: 0.2855283690892747, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 699, Loss: 0.2752360422182569, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 700, Loss: 0.3118182876846204, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 701, Loss: 0.5017049179287043, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 702, Loss: 0.34763086097994667, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 703, Loss: 0.3570521634983041, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 704, Loss: 0.45801726895387007, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 705, Loss: 0.24652378642904893, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 706, Loss: 0.5250978227405935, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 707, Loss: 0.2576085402993953, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 708, Loss: 0.3781971754308765, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 709, Loss: 0.35218480157086557, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 710, Loss: 0.2620455685646017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 711, Loss: 0.4854349332563898, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 712, Loss: 0.587684963307132, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 713, Loss: 0.39739106524170753, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 714, Loss: 0.3646406958395138, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 715, Loss: 0.2385126118731971, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 716, Loss: 0.48822813285191513, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 717, Loss: 0.325052900149169, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 718, Loss: 0.6876365058834769, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 719, Loss: 0.276132560043253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 720, Loss: 0.44116994067204246, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 721, Loss: 0.4559992113197555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 722, Loss: 0.28354994445083853, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 723, Loss: 0.32332960244615244, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 724, Loss: 0.2582823658576949, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 725, Loss: 0.21785510481014628, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 726, Loss: 0.26864797315814837, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 727, Loss: 0.45277270407412673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 728, Loss: 0.27460649690330174, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 729, Loss: 0.2789139448248087, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 730, Loss: 0.39547318691307143, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 731, Loss: 0.2718054639897642, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 732, Loss: 0.2855186353304835, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 733, Loss: 0.39644780656877826, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 734, Loss: 0.6540979849951323, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 735, Loss: 0.2658830961099857, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 736, Loss: 0.25694955762857963, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 737, Loss: 0.27144091600864023, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 738, Loss: 0.4794097921617154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 739, Loss: 0.5221862812667482, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 740, Loss: 0.24037087151640973, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 741, Loss: 0.3152207634597935, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 742, Loss: 0.46576511461943154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 743, Loss: 0.4563393793922721, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 744, Loss: 0.24449181281677648, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 745, Loss: 0.37997891105864123, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 746, Loss: 0.5718218024341104, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 747, Loss: 0.5584262622120224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 748, Loss: 0.473058016615324, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 749, Loss: 0.3836359710433499, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 750, Loss: 0.32019412316069285, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 751, Loss: 0.3359237685876951, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 752, Loss: 0.3847280318047237, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 753, Loss: 0.47825625128381805, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 754, Loss: 0.3068746507448922, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 755, Loss: 0.262093802690614, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 756, Loss: 0.42023402060422427, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 757, Loss: 0.27333933211362493, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 758, Loss: 0.5057830611572187, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 759, Loss: 0.4483331476370178, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 760, Loss: 0.229187899805329, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 761, Loss: 0.31414880575923443, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 762, Loss: 0.25916743103703305, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 763, Loss: 0.3080375399880311, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 764, Loss: 0.2255194871190702, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 765, Loss: 0.3610856934923976, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 766, Loss: 0.39892099508364365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 767, Loss: 0.6100637605982855, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 768, Loss: 0.5380315212295538, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 769, Loss: 0.45540745379990566, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 770, Loss: 0.3496226163142524, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 771, Loss: 0.3124221630072372, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 772, Loss: 0.35540091990149636, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 773, Loss: 0.2783863237989319, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 774, Loss: 0.3074380205650117, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 775, Loss: 0.2503810572355834, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 776, Loss: 0.3091785054048306, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 777, Loss: 0.30850962601506104, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 778, Loss: 0.342792548786178, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 779, Loss: 0.38617278535932736, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 780, Loss: 0.3495368345067854, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 781, Loss: 0.8221718863344271, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 782, Loss: 0.41015735534256964, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 783, Loss: 0.29198963152267227, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 784, Loss: 0.27355900367950065, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 785, Loss: 0.2750824229739559, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 786, Loss: 0.42421157777383356, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 787, Loss: 0.4560594820865633, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 788, Loss: 0.24623048582722573, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 789, Loss: 0.2812050222040835, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 790, Loss: 0.30518090768444817, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 791, Loss: 0.2543124566845031, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 792, Loss: 0.3424438005007392, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 793, Loss: 0.6602594168698881, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 794, Loss: 0.43923510354034345, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 795, Loss: 0.2648121761462454, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 796, Loss: 0.5936550460400285, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 797, Loss: 0.4760069116678928, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 798, Loss: 0.27815855974872933, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 799, Loss: 0.45750760632415427, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 800, Loss: 0.2966968628173646, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 801, Loss: 0.31358580273255365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 802, Loss: 0.41335763510607415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 803, Loss: 0.3167719637862356, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 804, Loss: 0.4660637439455073, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 805, Loss: 0.3598255653983469, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 806, Loss: 0.3331970430458703, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 807, Loss: 0.625511893723822, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 808, Loss: 0.4775406265437721, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 809, Loss: 0.2732609033515673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 810, Loss: 0.2591006309461402, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 811, Loss: 0.587124909749341, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 812, Loss: 0.2824153656871561, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 813, Loss: 0.3481740456404455, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 814, Loss: 0.22533044384656833, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 815, Loss: 0.4604181287789971, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 816, Loss: 0.5438349852962474, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 817, Loss: 0.3764969196357707, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 818, Loss: 0.3296923230567341, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 819, Loss: 0.33078106301636273, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 820, Loss: 0.3110810296248663, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 821, Loss: 0.2941306634471728, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 822, Loss: 0.2895486206026488, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 823, Loss: 0.46260298768429753, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 824, Loss: 0.43960278152495824, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 825, Loss: 0.2718087234699881, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 826, Loss: 0.3456096603640286, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 827, Loss: 0.23146389239710302, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 828, Loss: 0.45126895193051586, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 829, Loss: 0.30178511653485907, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 830, Loss: 0.2725997450459532, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 831, Loss: 0.23768180967619068, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 832, Loss: 0.34529834726379255, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 833, Loss: 0.3691316164289562, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 834, Loss: 0.3506384697462803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 835, Loss: 0.571434349215225, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 836, Loss: 0.25503316353556266, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 837, Loss: 0.3354656918905906, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 838, Loss: 0.25522805087853717, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 839, Loss: 0.3188452439955157, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 840, Loss: 0.36851143743330483, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 841, Loss: 0.33895482190415505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 842, Loss: 0.3789110068689756, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 843, Loss: 0.3936895212052157, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 844, Loss: 0.3630271147458075, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 845, Loss: 0.3251412170000766, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 846, Loss: 0.5406127958398311, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 847, Loss: 0.27476589495774423, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 848, Loss: 0.28995857123215674, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 849, Loss: 0.2878847779494679, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 850, Loss: 0.5854579260771463, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 851, Loss: 0.3067884585749808, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 852, Loss: 0.37173989034991617, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 853, Loss: 0.3109245506449163, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 854, Loss: 0.3098667465735704, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 855, Loss: 0.297598429399927, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 856, Loss: 0.3004120764812946, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 857, Loss: 0.7659464671808234, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 858, Loss: 0.49625592432154153, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 859, Loss: 0.3719078057459118, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 860, Loss: 0.24325037211218725, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 861, Loss: 0.47773044729847425, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 862, Loss: 0.34990028610914087, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 863, Loss: 0.24092317557337242, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 864, Loss: 0.4060971795398993, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 865, Loss: 0.4913130595930729, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 866, Loss: 0.26393644416440976, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 867, Loss: 0.3013051511723308, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 868, Loss: 0.2703100385526242, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 869, Loss: 0.2872369105556622, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 870, Loss: 0.2971653171560351, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 871, Loss: 0.3384276709839522, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 872, Loss: 0.4687751848549066, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 873, Loss: 0.3335911546988468, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 874, Loss: 0.32673021446260253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 875, Loss: 0.31853557349929196, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 876, Loss: 0.2431360581810178, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 877, Loss: 0.2773924433949918, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 878, Loss: 0.3308022521613956, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 879, Loss: 0.293371536610232, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 880, Loss: 0.35010025796645106, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 881, Loss: 0.3561073708504485, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 882, Loss: 0.3172233506582697, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 883, Loss: 0.3638724645963105, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 884, Loss: 0.2982791316702267, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 885, Loss: 0.4302118600992105, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 886, Loss: 0.2876335950784305, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 887, Loss: 0.5622705450047153, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 888, Loss: 0.43669839344227235, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 889, Loss: 0.3148725052995238, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 890, Loss: 0.23236724952625604, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 891, Loss: 0.4239920700477843, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 892, Loss: 0.29714039495418654, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 893, Loss: 0.23100541326171198, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 894, Loss: 0.24220122642073033, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 895, Loss: 0.3525571581841473, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 896, Loss: 0.38746681236761876, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 897, Loss: 0.29970020214980286, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 898, Loss: 0.23674329065738284, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 899, Loss: 0.23903019362183797, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 900, Loss: 0.32473470903061985, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 901, Loss: 0.420398043620108, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 902, Loss: 0.6531775370161836, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 903, Loss: 0.5735905006154699, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 904, Loss: 0.2866236907214731, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 905, Loss: 0.36058713811587983, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 906, Loss: 0.3459974077981153, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 907, Loss: 0.31407857828550095, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 908, Loss: 0.39318259676191014, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 909, Loss: 0.3067888377547192, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 910, Loss: 0.5897700883237406, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 911, Loss: 0.3169281696870124, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 912, Loss: 0.31845052023634607, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 913, Loss: 0.2616015219422803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 914, Loss: 0.2328399192808221, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 915, Loss: 0.6066434560422981, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 916, Loss: 0.2912394795109582, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 917, Loss: 0.4779066058496295, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 918, Loss: 0.3815596408644608, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 919, Loss: 0.2656014947096915, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 920, Loss: 0.2847286439119078, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 921, Loss: 0.38481643692693157, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 922, Loss: 0.29275502038486434, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 923, Loss: 0.360812927960929, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 924, Loss: 0.44633466503024477, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 925, Loss: 0.6336111757547216, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 926, Loss: 0.4880291148861673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 927, Loss: 0.4547476471639184, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 928, Loss: 0.44520051511273107, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 929, Loss: 0.2289134701684155, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 930, Loss: 0.2882221247764236, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 931, Loss: 0.2814217822693815, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 932, Loss: 0.5500143833253698, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 933, Loss: 0.22227396961406712, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 934, Loss: 0.3310234879266105, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 935, Loss: 0.5187907087548428, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 936, Loss: 0.3498529884988699, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 937, Loss: 0.3087988911730683, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 938, Loss: 0.469743082741476, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 939, Loss: 0.2753104827854734, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 940, Loss: 0.3907700030089173, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 941, Loss: 0.2875141193429268, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 942, Loss: 0.25598255952741233, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 943, Loss: 0.507375384760737, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 944, Loss: 0.2595515212121485, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 945, Loss: 0.346678001794249, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 946, Loss: 0.25458997959705676, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 947, Loss: 0.27765458429223017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 948, Loss: 0.39485735478339323, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 949, Loss: 0.41576499999178235, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 950, Loss: 0.2872513596325219, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 951, Loss: 0.2652043164993352, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 952, Loss: 0.35345557748363987, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 953, Loss: 0.4342450677921481, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 954, Loss: 0.23864355894353517, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 955, Loss: 0.33956410922536856, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 956, Loss: 0.2406593960523653, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 957, Loss: 0.2767309078890706, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 958, Loss: 0.2557046167794265, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 959, Loss: 0.49247193908503206, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 960, Loss: 0.31628643098715464, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 961, Loss: 0.35064260342216824, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 962, Loss: 0.31754421020924817, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 963, Loss: 0.27596868502949584, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 964, Loss: 0.2666910699815607, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 965, Loss: 0.29832266211598146, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 966, Loss: 0.35328967994771415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 967, Loss: 0.3578323815784413, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 968, Loss: 0.29994681556778224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 969, Loss: 0.3615325013812759, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 970, Loss: 0.23598620898313996, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 971, Loss: 0.2910334232572636, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 972, Loss: 0.2816805985356249, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 973, Loss: 0.326117663026921, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 974, Loss: 0.7464437920056348, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 975, Loss: 0.252968046552392, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 976, Loss: 0.3298877134179375, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 977, Loss: 0.24672617135077285, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 978, Loss: 0.41776809106363766, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 979, Loss: 0.245969277650251, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 980, Loss: 0.3868211184196085, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 981, Loss: 0.30713408961987987, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 982, Loss: 0.32729197432399293, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 983, Loss: 0.3069240021313634, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 984, Loss: 0.32054386301864507, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 985, Loss: 0.22034053210856858, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 986, Loss: 0.3858643957849671, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 987, Loss: 0.33014462761433766, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 988, Loss: 0.26596802074651904, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 989, Loss: 0.27693991365763165, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 990, Loss: 0.6987917524286322, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 991, Loss: 0.2781305450523886, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 992, Loss: 0.3614435704272934, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 993, Loss: 0.2499983042266666, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 994, Loss: 0.23772002862530123, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 995, Loss: 0.37256305537691686, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 996, Loss: 0.395752688884699, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 997, Loss: 0.2727022051812208, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 998, Loss: 0.28091791322760096, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 999, Loss: 0.251616722074425, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1000, Loss: 0.4186228482615224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1001, Loss: 0.2787966005100144, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1002, Loss: 0.395319072283265, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1003, Loss: 0.27654673646708716, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1004, Loss: 0.2581979022594513, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1005, Loss: 0.4887967281017253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1006, Loss: 0.25685972356145503, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1007, Loss: 0.23661898756353675, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1008, Loss: 0.21548751396224278, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1009, Loss: 0.5121132105579634, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1010, Loss: 0.3084614855954434, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1011, Loss: 0.292883442840844, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1012, Loss: 0.3648697027175988, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1013, Loss: 0.3575859121404708, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1014, Loss: 0.34417513389247023, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1015, Loss: 0.4328758366387905, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1016, Loss: 0.21865009453412737, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1017, Loss: 0.3414965769578605, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1018, Loss: 0.46316218050771485, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1019, Loss: 0.4081707020859402, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1020, Loss: 0.22977429964239915, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1021, Loss: 0.40010712069124854, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1022, Loss: 0.5381132691797176, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1023, Loss: 0.5468152456139763, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1024, Loss: 0.414493451649965, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1025, Loss: 0.397377645779289, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1026, Loss: 0.2786989690344533, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1027, Loss: 0.42421996750104396, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1028, Loss: 0.28727281022953416, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1029, Loss: 0.42764508504335585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1030, Loss: 0.25148629436205083, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1031, Loss: 0.6126312383472786, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1032, Loss: 0.3337831469706272, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1033, Loss: 0.3374220338729816, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1034, Loss: 0.29640484459305183, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1035, Loss: 0.5815400240609763, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1036, Loss: 0.28098724376596906, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1037, Loss: 0.5898479653343579, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1038, Loss: 0.28055393257669065, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1039, Loss: 0.4178742874991923, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1040, Loss: 0.27594139267898027, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1041, Loss: 0.3351319015062646, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1042, Loss: 0.2148181718296265, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1043, Loss: 0.44009225490947823, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1044, Loss: 0.29587214906409975, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1045, Loss: 0.418336268061817, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1046, Loss: 0.2514453991137708, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1047, Loss: 0.4877325275310861, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1048, Loss: 0.2489188968399062, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1049, Loss: 0.4189307591326693, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1050, Loss: 0.34034259237689757, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1051, Loss: 0.27159889924346553, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1052, Loss: 0.3893264462260909, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1053, Loss: 0.352176339597706, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1054, Loss: 0.35083599204617155, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1055, Loss: 0.22948446029814243, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1056, Loss: 0.3285533677542677, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1057, Loss: 0.25661341868388493, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1058, Loss: 0.37445242005248847, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1059, Loss: 0.2992063812327257, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1060, Loss: 0.26679262191246583, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1061, Loss: 0.23451372628033365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1062, Loss: 0.5460766922451239, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1063, Loss: 0.24640827658464937, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1064, Loss: 0.43518077624890733, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1065, Loss: 0.45813476630710237, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1066, Loss: 0.23446970574881237, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1067, Loss: 0.31353209629171697, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1068, Loss: 0.3540840091678154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1069, Loss: 0.3910600323330547, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1070, Loss: 0.279163338281142, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1071, Loss: 0.24874962240716875, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1072, Loss: 0.27263542280217523, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1073, Loss: 0.3464641472665544, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1074, Loss: 0.2855570139422742, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1075, Loss: 0.42464429036001106, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1076, Loss: 0.25909870989121064, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1077, Loss: 0.26396613819122233, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1078, Loss: 0.24065998280026482, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1079, Loss: 0.40632006866159337, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1080, Loss: 0.31837216097859977, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1081, Loss: 0.2941822629996143, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1082, Loss: 0.25805358362295505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1083, Loss: 0.25586641896019635, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1084, Loss: 0.2640808689185115, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1085, Loss: 0.5544824569156837, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1086, Loss: 0.4087079985787585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1087, Loss: 0.4384659912701631, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1088, Loss: 0.5574986501750907, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1089, Loss: 0.26111105664427076, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1090, Loss: 0.37239504507021104, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1091, Loss: 0.3357637106151165, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1092, Loss: 0.49252047991797665, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1093, Loss: 0.2853925524030363, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1094, Loss: 0.5502996075224816, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1095, Loss: 0.21584876545880666, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1096, Loss: 0.31545614803374744, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1097, Loss: 0.28552650898870313, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1098, Loss: 0.4751858070770698, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1099, Loss: 0.2289535364583719, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1100, Loss: 0.2667628375274943, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1101, Loss: 0.4713588417770197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1102, Loss: 0.32898316381205417, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1103, Loss: 0.513049168952909, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1104, Loss: 0.4430935155228708, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1105, Loss: 0.24416101219657596, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1106, Loss: 0.34458410746616946, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1107, Loss: 0.4191840399747453, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1108, Loss: 0.31485268194061167, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1109, Loss: 0.2794472382248874, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1110, Loss: 0.2615430518480195, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1111, Loss: 0.85935276623029, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1112, Loss: 0.3541373724756959, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1113, Loss: 0.22227607265432983, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1114, Loss: 0.3658212882621562, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1115, Loss: 0.29792837837497194, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1116, Loss: 0.7572924722857202, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1117, Loss: 0.4303405123392018, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1118, Loss: 0.26563338945824194, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1119, Loss: 0.29602517706438103, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1120, Loss: 0.22719166895188217, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1121, Loss: 0.29170484806124825, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1122, Loss: 0.3054823102276714, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1123, Loss: 0.288016368958807, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1124, Loss: 0.2629329973148386, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1125, Loss: 0.3505094437529971, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1126, Loss: 0.31447407913664277, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1127, Loss: 0.44849463555405744, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1128, Loss: 0.2745393125281428, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1129, Loss: 0.29429972006333527, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1130, Loss: 0.32227834743394557, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1131, Loss: 0.6518764431607117, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1132, Loss: 0.2249620297462897, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1133, Loss: 0.35129600834893615, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1134, Loss: 0.37883069995425894, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1135, Loss: 0.42128307214469085, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1136, Loss: 0.28394437301795505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1137, Loss: 0.9220211899525335, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1138, Loss: 0.5326923980944229, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1139, Loss: 0.3278733128261439, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1140, Loss: 0.2744122873015805, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1141, Loss: 0.4362195461030932, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1142, Loss: 0.37447083499272754, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1143, Loss: 0.32694952147867556, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1144, Loss: 0.39649416592231046, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1145, Loss: 0.3846951185505886, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1146, Loss: 0.26220871355840286, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1147, Loss: 0.49099920739268665, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1148, Loss: 0.27413734612264395, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1149, Loss: 0.4320301544127866, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1150, Loss: 0.4920554306643712, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1151, Loss: 0.43577950807247073, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1152, Loss: 0.322769693349506, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1153, Loss: 0.2577292315228128, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1154, Loss: 0.2508367024607443, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1155, Loss: 0.31520380599840614, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1156, Loss: 0.2760764114329545, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1157, Loss: 0.28269921018011623, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1158, Loss: 0.29964725591112695, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1159, Loss: 0.2925342489245698, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1160, Loss: 0.3407838876178175, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1161, Loss: 0.49889225529760584, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1162, Loss: 0.44351814097289904, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1163, Loss: 0.22721312466166954, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1164, Loss: 0.42419308145460577, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1165, Loss: 0.31311739175458286, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1166, Loss: 0.23495724996602718, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1167, Loss: 0.2774383412533197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1168, Loss: 0.31081484401268533, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1169, Loss: 0.2252158176529411, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1170, Loss: 0.24069950476190535, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1171, Loss: 0.3963682759527407, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1172, Loss: 0.25197836805306123, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1173, Loss: 0.45896047080294405, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1174, Loss: 0.3136648616405797, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1175, Loss: 0.4199264625083361, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1176, Loss: 0.40021694479312847, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1177, Loss: 0.2356906513665374, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1178, Loss: 0.2788225032898571, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1179, Loss: 0.41422178090612716, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1180, Loss: 0.42076360122842754, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1181, Loss: 0.27301227463209404, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1182, Loss: 0.49654842467484095, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1183, Loss: 0.23961576764719028, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1184, Loss: 0.5060480786176125, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1185, Loss: 0.2324428406916599, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1186, Loss: 0.3246950402362099, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1187, Loss: 0.33147130997402463, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1188, Loss: 0.49775243030464406, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1189, Loss: 0.35087778491264504, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1190, Loss: 0.29690261049357725, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1191, Loss: 0.4229856566981233, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1192, Loss: 0.2342350865351637, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1193, Loss: 0.6744679833938073, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1194, Loss: 0.28341488162931083, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1195, Loss: 0.29688317441730017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1196, Loss: 0.46869459900080324, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1197, Loss: 0.23570786369031166, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1198, Loss: 0.37569319318610106, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1199, Loss: 0.3168463707101979, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1200, Loss: 0.2744489730655209, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1201, Loss: 0.24550886302300626, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1202, Loss: 0.27464787968813176, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1203, Loss: 0.37666483594187583, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1204, Loss: 0.22676377245999724, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1205, Loss: 0.22437660474171878, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1206, Loss: 0.2716492556907686, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1207, Loss: 0.42149966807076283, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1208, Loss: 0.44878900717650855, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1209, Loss: 0.24365463087347233, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1210, Loss: 0.5238065148627987, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1211, Loss: 0.5348167414899365, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1212, Loss: 0.38120109660849777, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1213, Loss: 0.3008809672371895, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1214, Loss: 0.23467306550516578, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1215, Loss: 0.40572752729814776, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1216, Loss: 0.29813635259234617, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1217, Loss: 0.26528283564206306, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1218, Loss: 0.4933711381359531, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1219, Loss: 0.2167984221804929, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1220, Loss: 0.3748748358282011, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1221, Loss: 0.5997682775977777, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1222, Loss: 0.7965531371670351, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1223, Loss: 0.2711351863367528, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1224, Loss: 0.2987972962168909, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1225, Loss: 0.3473513527782914, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1226, Loss: 0.3163402561330989, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1227, Loss: 0.5459261318419053, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1228, Loss: 0.255594121662599, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1229, Loss: 0.7161432739498703, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1230, Loss: 0.3501847689280589, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1231, Loss: 0.384396477170201, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1232, Loss: 0.2444982766819577, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1233, Loss: 0.38252297853611117, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1234, Loss: 0.3413233904821098, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1235, Loss: 0.3175160766858829, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1236, Loss: 0.48983214621708326, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1237, Loss: 0.2336109348635611, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1238, Loss: 0.4632762199117153, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1239, Loss: 0.36842054229565635, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1240, Loss: 0.3860720563657789, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1241, Loss: 0.3634729917580116, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1242, Loss: 0.2883015836969207, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1243, Loss: 0.3389167353433902, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1244, Loss: 0.218003875084802, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1245, Loss: 0.632373935928562, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1246, Loss: 0.47550735919497333, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1247, Loss: 0.3269333100506497, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1248, Loss: 0.2853194881585989, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1249, Loss: 0.33020749030825863, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1250, Loss: 0.34546169555869477, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1251, Loss: 0.39187233997758697, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1252, Loss: 0.329005224200116, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1253, Loss: 0.26992673595368805, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1254, Loss: 0.3821832362739621, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1255, Loss: 0.2514074349687811, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1256, Loss: 0.5488604509548264, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1257, Loss: 0.38110029236467396, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1258, Loss: 0.23120186015723104, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1259, Loss: 0.34975744111420154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1260, Loss: 0.26753098166594913, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1261, Loss: 0.4103760554572017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1262, Loss: 0.8014363443296252, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1263, Loss: 0.34095196082108803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1264, Loss: 0.23796406802812856, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1265, Loss: 0.3711008373306141, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1266, Loss: 0.3073069507375776, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1267, Loss: 0.3661800590997237, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1268, Loss: 0.49698833618003524, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1269, Loss: 0.30786028330214676, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1270, Loss: 0.3516284983324998, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1271, Loss: 0.40756886484367894, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1272, Loss: 0.3126268941610018, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1273, Loss: 0.3679996614208474, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1274, Loss: 0.47453293214246484, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1275, Loss: 0.24438915921229198, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1276, Loss: 0.45007884533697995, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1277, Loss: 0.3603955763777478, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1278, Loss: 0.3738236676611948, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1279, Loss: 0.270306536649352, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1280, Loss: 0.4590037971368418, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1281, Loss: 0.36515183048796074, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1282, Loss: 0.30451622977610227, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1283, Loss: 0.38745041212186093, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1284, Loss: 0.4092319225718339, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1285, Loss: 0.2968729258471948, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1286, Loss: 0.3122980537274482, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1287, Loss: 0.29213897878301964, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1288, Loss: 0.5045849233495807, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1289, Loss: 0.274369349002354, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1290, Loss: 0.37949887783354197, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1291, Loss: 0.49605826739851067, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1292, Loss: 0.3364306407296982, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1293, Loss: 0.4628618864182287, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1294, Loss: 0.41781549730000855, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1295, Loss: 0.35417990274887656, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1296, Loss: 0.3134261755308778, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1297, Loss: 0.3260198600063059, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1298, Loss: 0.45498108342662813, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1299, Loss: 0.528709183159624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1300, Loss: 0.2816806617080639, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1301, Loss: 0.40548521717157415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1302, Loss: 0.2742998664010637, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1303, Loss: 0.28798124219061144, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1304, Loss: 0.3013802920593104, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1305, Loss: 0.5038554261852682, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1306, Loss: 0.27231195806420333, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1307, Loss: 0.4411055164496691, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1308, Loss: 0.33158038709814663, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1309, Loss: 0.5984151997186822, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1310, Loss: 0.35634291513563743, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1311, Loss: 0.35280635525288095, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1312, Loss: 0.38768740055172224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1313, Loss: 0.39578743756385754, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1314, Loss: 0.4077734374219416, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1315, Loss: 0.28477090986290576, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1316, Loss: 0.32254039769518594, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1317, Loss: 0.29922223269283343, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1318, Loss: 0.23894090573204171, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1319, Loss: 0.23485094386379438, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1320, Loss: 0.27965964512523556, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1321, Loss: 0.4091998016349247, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1322, Loss: 0.4315641865988534, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1323, Loss: 0.42459138064484603, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1324, Loss: 0.5563875363995399, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1325, Loss: 0.36492171149377384, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1326, Loss: 0.3498693906404118, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1327, Loss: 0.3191163166906607, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1328, Loss: 0.39270959066128086, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1329, Loss: 0.31091266694774133, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1330, Loss: 0.45009089266571545, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1331, Loss: 0.304625165333824, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1332, Loss: 0.25410408137991425, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1333, Loss: 0.36036134642916307, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1334, Loss: 0.41887664401839797, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1335, Loss: 0.3123026426941322, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1336, Loss: 0.5772885189669186, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1337, Loss: 0.23261132457270606, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1338, Loss: 0.2864529708955611, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1339, Loss: 0.530276323714171, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1340, Loss: 0.4645149058957658, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1341, Loss: 0.2940758918362818, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1342, Loss: 0.34095271968582974, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1343, Loss: 0.3323592354563791, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1344, Loss: 0.3449422582364917, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1345, Loss: 0.26840029001149723, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1346, Loss: 0.2929421553114231, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1347, Loss: 0.4443574151734138, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1348, Loss: 0.41815868527420713, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1349, Loss: 0.5632491672664427, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1350, Loss: 0.2622451844380364, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1351, Loss: 0.24456360201904634, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1352, Loss: 0.6003931400146045, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1353, Loss: 0.24451068957520938, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1354, Loss: 0.3984492050369423, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1355, Loss: 0.4002583878942173, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1356, Loss: 0.3609818129122061, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1357, Loss: 0.2639601062572839, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1358, Loss: 0.3869762850890917, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1359, Loss: 0.31358462856366087, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1360, Loss: 0.3288435569444755, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1361, Loss: 0.3839602482894483, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1362, Loss: 0.2964757361913283, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1363, Loss: 0.6282329452112367, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1364, Loss: 0.3418012298341322, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1365, Loss: 0.3879953481542731, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1366, Loss: 0.2480801724065772, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1367, Loss: 0.2486563389818937, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1368, Loss: 0.5212529971384563, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1369, Loss: 0.5145031859780127, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1370, Loss: 0.5337362923850787, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1371, Loss: 0.6598166433365193, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1372, Loss: 0.3669018223727597, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1373, Loss: 0.3289101872301717, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1374, Loss: 0.5740355641291844, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1375, Loss: 0.30455163326288215, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1376, Loss: 0.27745580138875703, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1377, Loss: 0.7308677196050665, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1378, Loss: 0.3461890608223233, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1379, Loss: 0.3302465576874964, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1380, Loss: 0.5578971294900723, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1381, Loss: 0.5710158350682056, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1382, Loss: 0.2891829435218552, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1383, Loss: 0.44010946520490324, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1384, Loss: 0.3346570966074648, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1385, Loss: 0.3251133675903468, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1386, Loss: 0.34684409931862725, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1387, Loss: 0.29481493049190904, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1388, Loss: 0.25260382609820353, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1389, Loss: 0.41754176516714014, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1390, Loss: 0.5893386764779702, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1391, Loss: 0.41168339270294685, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1392, Loss: 0.24122871838222312, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1393, Loss: 0.2396021254375972, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1394, Loss: 0.35797304877744734, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1395, Loss: 0.2892622259597695, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1396, Loss: 0.22710470432406146, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1397, Loss: 0.2968685524751314, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1398, Loss: 0.38579311097371005, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1399, Loss: 0.41739413306950873, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1400, Loss: 0.40506428294146524, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1401, Loss: 0.2565939306207992, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1402, Loss: 0.47149131487161555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1403, Loss: 0.2402196826503245, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1404, Loss: 0.3813886267872158, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1405, Loss: 0.2670665232860665, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1406, Loss: 0.5060084204980976, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1407, Loss: 0.28510128930618045, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1408, Loss: 0.3183928126135664, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1409, Loss: 0.2248584095968276, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1410, Loss: 0.40534646587394985, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1411, Loss: 0.29222995167025984, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1412, Loss: 0.31248109132411483, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1413, Loss: 0.5083849348871069, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1414, Loss: 0.46793303889030957, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1415, Loss: 0.440874765171325, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1416, Loss: 0.5545610310866297, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1417, Loss: 0.249063378958523, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1418, Loss: 0.31433439962940235, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1419, Loss: 0.3441388655927563, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1420, Loss: 0.2914382169218479, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1421, Loss: 0.38179694776354367, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1422, Loss: 0.420780680390329, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1423, Loss: 0.4267511631175953, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1424, Loss: 0.5815284231547866, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1425, Loss: 0.526251725116214, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1426, Loss: 0.24611639566910154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1427, Loss: 0.3120213046910087, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1428, Loss: 0.4224937103127512, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1429, Loss: 0.38105843756694846, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1430, Loss: 0.32887826347454213, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1431, Loss: 0.25082260657050454, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1432, Loss: 0.5627525715685491, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1433, Loss: 0.29181855556597636, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1434, Loss: 0.27078011494983684, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1435, Loss: 0.3482266871315988, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1436, Loss: 0.3633953541768757, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1437, Loss: 0.3100783535973543, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1438, Loss: 0.25042889298315885, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1439, Loss: 0.27861242807943615, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1440, Loss: 0.4729460463110533, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1441, Loss: 0.4632049059249559, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1442, Loss: 0.570980708598638, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1443, Loss: 0.31103370260434904, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1444, Loss: 0.40890169142437266, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1445, Loss: 0.5891414130212833, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1446, Loss: 0.22899643187403626, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1447, Loss: 0.35765787676997035, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1448, Loss: 0.2802703370499088, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1449, Loss: 0.3423999946094039, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1450, Loss: 0.27995423331827307, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1451, Loss: 0.25134910843770253, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1452, Loss: 0.3764759578359772, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1453, Loss: 0.23338143914665155, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1454, Loss: 0.3078083856182382, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1455, Loss: 0.3628920179010406, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1456, Loss: 0.2388889033466456, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1457, Loss: 0.31626037554450415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1458, Loss: 0.3036455040859137, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1459, Loss: 0.25664854538102555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1460, Loss: 0.3426412631474576, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1461, Loss: 0.514441529920126, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1462, Loss: 0.6047117009810522, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1463, Loss: 0.3012133086671658, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1464, Loss: 0.3096507494561414, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1465, Loss: 0.24411662300330123, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1466, Loss: 0.3250719671727939, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1467, Loss: 0.41390980726139537, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1468, Loss: 0.3772363061062828, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1469, Loss: 0.303675995081357, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1470, Loss: 0.37289333059181673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1471, Loss: 0.2700147199916182, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1472, Loss: 0.5091511579951633, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1473, Loss: 0.311960452415324, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1474, Loss: 0.45949874250663303, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1475, Loss: 0.45714952900105243, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1476, Loss: 0.3761845843952499, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1477, Loss: 0.37509969314928093, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1478, Loss: 0.3849235280317574, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1479, Loss: 0.2779388030790155, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1480, Loss: 0.36486585516183967, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1481, Loss: 0.5250976457886332, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1482, Loss: 0.3133971291264051, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1483, Loss: 0.3232196435973337, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1484, Loss: 0.3756887935449712, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1485, Loss: 0.37929510028988234, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1486, Loss: 0.2632645012659624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1487, Loss: 0.2769798539948333, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1488, Loss: 0.2559493518262546, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1489, Loss: 0.30405407724884087, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1490, Loss: 0.259875978161717, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1491, Loss: 0.3225864802565776, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1492, Loss: 0.27225462105798526, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1493, Loss: 0.30631411181478224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1494, Loss: 0.28784580688612327, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1495, Loss: 0.25994865160236025, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1496, Loss: 0.2713230010997765, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1497, Loss: 0.4108044686937964, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1498, Loss: 0.3664602326397032, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1499, Loss: 0.4907454589759857, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1500, Loss: 0.519410235335606, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1501, Loss: 0.3069967139212485, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1502, Loss: 0.334486077954099, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1503, Loss: 0.26636283234990316, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1504, Loss: 0.29162709936059533, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1505, Loss: 0.33483955746451133, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1506, Loss: 0.2572462209110501, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1507, Loss: 0.28015218989604096, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1508, Loss: 0.3859002753349944, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1509, Loss: 0.31668305740782543, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1510, Loss: 0.2997209484859029, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1511, Loss: 0.3890964609217821, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1512, Loss: 1.0720056072340791, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1513, Loss: 0.3268229870203776, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1514, Loss: 0.33755968666568614, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1515, Loss: 0.4211894184175784, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1516, Loss: 0.2828310537545172, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1517, Loss: 0.38634629777836044, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1518, Loss: 0.35997996626934625, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1519, Loss: 0.4362550673820663, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1520, Loss: 0.40198187829643384, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1521, Loss: 0.3332779654410722, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1522, Loss: 0.5251179458328505, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1523, Loss: 0.3785064888571955, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1524, Loss: 0.2366531815728225, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1525, Loss: 0.36854822577426083, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1526, Loss: 0.39203354828546655, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1527, Loss: 0.43630925328791414, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1528, Loss: 0.44494158415616414, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1529, Loss: 0.30291178076312175, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1530, Loss: 0.2635802594718785, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1531, Loss: 0.2609824276268899, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1532, Loss: 0.2692181970318788, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1533, Loss: 0.3681723234917921, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1534, Loss: 0.22452900378267165, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1535, Loss: 0.2463982619273246, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1536, Loss: 0.3636477077607555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1537, Loss: 0.3858834577014984, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1538, Loss: 0.2574645512583912, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1539, Loss: 0.3145021842845618, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1540, Loss: 0.3542438080748921, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1541, Loss: 0.36459864298739847, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1542, Loss: 0.29054827607281875, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1543, Loss: 0.29397453879517094, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1544, Loss: 0.25160394616012854, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1545, Loss: 0.27842618784166084, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1546, Loss: 0.26628793230283554, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1547, Loss: 0.33687410146273555, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1548, Loss: 0.29424871299060373, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1549, Loss: 0.4549081907947585, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1550, Loss: 0.2698130509883443, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1551, Loss: 0.5258615895203892, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1552, Loss: 0.299008898444075, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1553, Loss: 0.5967690574667617, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1554, Loss: 0.48118646591461445, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1555, Loss: 0.4488156555248358, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1556, Loss: 0.3455180578834891, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1557, Loss: 0.2780142271115415, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1558, Loss: 0.23633721954414777, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1559, Loss: 0.2603044792194727, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1560, Loss: 0.6011540990993748, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1561, Loss: 0.30253106919410855, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1562, Loss: 0.8224637112932855, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1563, Loss: 0.3636717927681281, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1564, Loss: 0.24419674195161262, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1565, Loss: 0.299584217031443, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1566, Loss: 0.2261259119242282, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1567, Loss: 0.42722735689974867, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1568, Loss: 0.2453304300016883, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1569, Loss: 0.4121418725900635, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1570, Loss: 0.242397015965927, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1571, Loss: 0.2211456025320214, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1572, Loss: 0.2766358130329729, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1573, Loss: 0.3250504303847885, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1574, Loss: 0.2658938037015475, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1575, Loss: 0.40781869747303395, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1576, Loss: 0.3637033542003381, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1577, Loss: 0.22138904378511978, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1578, Loss: 0.4111983525041356, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1579, Loss: 0.6387832184065096, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1580, Loss: 0.4371839857298309, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1581, Loss: 0.3277946159539185, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1582, Loss: 0.3602257459384768, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1583, Loss: 0.4840315430579051, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1584, Loss: 0.24134836549532498, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1585, Loss: 0.27729643973517504, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1586, Loss: 0.2790243072538085, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1587, Loss: 0.23928763300985495, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1588, Loss: 0.4041313273173941, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1589, Loss: 0.34063725380318266, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1590, Loss: 0.3136029213079168, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1591, Loss: 0.219049305389074, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1592, Loss: 0.27884619132471156, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1593, Loss: 0.2496412222152293, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1594, Loss: 0.2507558588315145, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1595, Loss: 0.3140200309564148, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1596, Loss: 0.2268248916699885, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1597, Loss: 0.2479212312279765, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1598, Loss: 0.3632286805957485, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1599, Loss: 0.3809129833101148, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1600, Loss: 0.24316577054778693, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1601, Loss: 0.36258475338945717, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1602, Loss: 0.32321945447176226, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1603, Loss: 0.5250316079839987, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1604, Loss: 0.3774538904940288, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1605, Loss: 0.4551688803198625, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1606, Loss: 0.4300920972448452, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1607, Loss: 0.37811695165810655, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1608, Loss: 0.589696783435362, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1609, Loss: 0.2557985482478103, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1610, Loss: 0.3762605952492381, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1611, Loss: 0.2876621121673605, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1612, Loss: 0.2828582781553963, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1613, Loss: 0.4206152163906116, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1614, Loss: 0.28207222301691015, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1615, Loss: 0.254220093695009, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1616, Loss: 0.6067504830219417, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1617, Loss: 0.39497090330036644, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1618, Loss: 0.3458940387820082, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1619, Loss: 0.37435136777774436, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1620, Loss: 0.4137313472124765, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1621, Loss: 0.35252766949836944, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1622, Loss: 0.3639944296185982, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1623, Loss: 0.23752231442157112, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1624, Loss: 0.2832509524508906, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1625, Loss: 0.46675282098237714, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1626, Loss: 0.2307790598062264, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1627, Loss: 0.3814596404803118, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1628, Loss: 0.27265835133066774, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1629, Loss: 0.2642369272487539, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1630, Loss: 0.2669860809122006, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1631, Loss: 0.2887247151078364, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1632, Loss: 0.3561689140862774, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1633, Loss: 0.289591926421166, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1634, Loss: 0.47210297963438913, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1635, Loss: 0.5641129071943665, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1636, Loss: 0.3403903268820168, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1637, Loss: 0.2886411304417566, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1638, Loss: 0.2297382227906538, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1639, Loss: 0.33537766973658584, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1640, Loss: 0.2861745996350799, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1641, Loss: 0.3581116006056496, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1642, Loss: 0.4563490493114922, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1643, Loss: 0.24909021898402972, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1644, Loss: 0.6620456089193735, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1645, Loss: 0.3416581407983453, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1646, Loss: 0.3151668693229729, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1647, Loss: 0.23651416473412473, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1648, Loss: 0.30330516396460017, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1649, Loss: 0.2428035424935345, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1650, Loss: 0.6418971867494394, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1651, Loss: 0.6875800105390882, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1652, Loss: 0.32170971881971, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1653, Loss: 0.40516867873323703, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1654, Loss: 0.48205970992130137, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1655, Loss: 0.3497970138506231, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1656, Loss: 0.45145240673838394, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1657, Loss: 0.3393506691467057, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1658, Loss: 0.3141935486131628, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1659, Loss: 0.3735764661714168, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1660, Loss: 0.24911145001567053, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1661, Loss: 0.49430266319308047, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1662, Loss: 0.3774783923281951, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1663, Loss: 0.5159367875093922, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1664, Loss: 0.2787013438088112, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1665, Loss: 0.24799342544382127, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1666, Loss: 0.46086586046277755, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1667, Loss: 0.3129195941369492, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1668, Loss: 0.42693488437361093, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1669, Loss: 0.532198957189693, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1670, Loss: 0.320518730806742, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1671, Loss: 0.3546010083783234, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1672, Loss: 0.4376154139059461, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1673, Loss: 0.3835260233979014, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1674, Loss: 0.4839082906035326, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1675, Loss: 0.4236333373217934, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1676, Loss: 0.4650575239723425, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1677, Loss: 0.4539903036099948, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1678, Loss: 0.6511706430996093, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1679, Loss: 0.22217279183777, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1680, Loss: 0.3042163332060548, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1681, Loss: 0.3368216685222753, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1682, Loss: 0.2728810862367508, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1683, Loss: 0.2805320076268891, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1684, Loss: 0.28073971604790315, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1685, Loss: 0.36980174644701824, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1686, Loss: 0.7985401419797198, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1687, Loss: 0.2760238251611492, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1688, Loss: 0.35482459347368184, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1689, Loss: 0.5389723086887979, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1690, Loss: 0.2797313973758469, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1691, Loss: 0.43093314009733397, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1692, Loss: 0.3604867510309435, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1693, Loss: 0.24468668716813285, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1694, Loss: 0.48495865821647643, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1695, Loss: 0.38399684131279005, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1696, Loss: 0.41496256742496923, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1697, Loss: 0.4737152084406675, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1698, Loss: 0.31099888742861664, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1699, Loss: 0.2944196528427737, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1700, Loss: 0.26884995678988494, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1701, Loss: 0.2425824992115268, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1702, Loss: 0.32061102850365064, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1703, Loss: 0.4234407890489551, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1704, Loss: 0.27250246071032236, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1705, Loss: 0.2839264960806235, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1706, Loss: 0.34153275124176513, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1707, Loss: 0.531551202699651, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1708, Loss: 0.25485980363879945, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1709, Loss: 0.22849897680377243, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1710, Loss: 0.2688421757865256, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1711, Loss: 0.3980712586320696, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1712, Loss: 0.36672957494327435, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1713, Loss: 0.31194129896355266, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1714, Loss: 0.3293904610558534, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1715, Loss: 0.6158320029641201, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1716, Loss: 0.2953861796458491, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1717, Loss: 0.28134154443702564, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1718, Loss: 0.44209692272273515, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1719, Loss: 0.4040115537186373, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1720, Loss: 0.456957058666676, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1721, Loss: 0.38850007644121864, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1722, Loss: 0.3571273737032702, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1723, Loss: 0.36046847077353417, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1724, Loss: 0.2756594660983901, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1725, Loss: 0.3449768869605125, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1726, Loss: 0.2612715172194871, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1727, Loss: 0.3914952674113321, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1728, Loss: 0.37753966905924374, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1729, Loss: 0.4290549977119784, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1730, Loss: 0.33159993079503214, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1731, Loss: 0.34598675066394735, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1732, Loss: 0.3811139823240567, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1733, Loss: 0.650169085314169, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1734, Loss: 0.24900663437160006, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1735, Loss: 0.23866601555820224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1736, Loss: 0.2878887908531762, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1737, Loss: 0.33380931636384387, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1738, Loss: 0.21816150073920734, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1739, Loss: 0.28288519316959726, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1740, Loss: 0.3404347348856036, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1741, Loss: 0.3602232232801251, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1742, Loss: 0.26779971332060193, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1743, Loss: 0.3207775555192885, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1744, Loss: 0.5443704036557793, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1745, Loss: 0.32520461873582074, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1746, Loss: 0.24580892909201835, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1747, Loss: 0.314758599241008, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1748, Loss: 0.46863628582805783, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1749, Loss: 0.323408942962684, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1750, Loss: 0.3957715629945229, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1751, Loss: 0.31065247462290074, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1752, Loss: 0.29556746642635084, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1753, Loss: 0.28937688786777865, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1754, Loss: 0.39870323164208, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1755, Loss: 0.26071371456639036, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1756, Loss: 0.2628260110026838, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1757, Loss: 0.4834768642216012, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1758, Loss: 0.3024136729357103, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1759, Loss: 0.3875369708100553, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1760, Loss: 0.30659274045017154, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1761, Loss: 0.23878711588796891, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1762, Loss: 0.6780527591447593, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1763, Loss: 0.5629347758219665, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1764, Loss: 0.24092994749314534, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1765, Loss: 0.3461219582400633, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1766, Loss: 0.243346688443987, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1767, Loss: 0.3067507246907048, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1768, Loss: 0.3525481484139965, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1769, Loss: 0.3266604991123515, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1770, Loss: 0.3775554203405532, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1771, Loss: 0.3401281945929701, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1772, Loss: 0.27954994791898946, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1773, Loss: 0.4228998523253205, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1774, Loss: 0.37251431379390054, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1775, Loss: 0.2660515981284904, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1776, Loss: 0.33505739430793036, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1777, Loss: 0.40729931913303163, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1778, Loss: 0.2934675942385616, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1779, Loss: 0.410632427865408, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1780, Loss: 0.2655679006197287, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1781, Loss: 0.25714632852836283, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1782, Loss: 0.5600206506372049, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1783, Loss: 0.35566642656987885, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1784, Loss: 0.2847461754032953, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1785, Loss: 0.37260809342015927, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1786, Loss: 0.3555267571620168, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1787, Loss: 0.30796673121710627, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1788, Loss: 0.2632924789764201, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1789, Loss: 0.3710751616890572, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1790, Loss: 0.5630367813249624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1791, Loss: 0.2810757401327288, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1792, Loss: 0.3190880317672795, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1793, Loss: 0.3593338557430037, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1794, Loss: 0.3676795979467773, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1795, Loss: 0.3850881797813986, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1796, Loss: 0.6025525026270859, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1797, Loss: 0.2575088254004302, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1798, Loss: 0.2340860541152448, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1799, Loss: 0.4137076978567462, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1800, Loss: 0.307431299556294, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1801, Loss: 0.26500834910239973, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1802, Loss: 0.37434633967935294, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1803, Loss: 0.23693376816565673, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1804, Loss: 0.4861641241867649, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1805, Loss: 0.37503379537076803, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1806, Loss: 0.33107715434794055, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1807, Loss: 0.46393450442984074, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1808, Loss: 0.4191922081974204, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1809, Loss: 0.2445934133336114, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1810, Loss: 0.34838580252718265, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1811, Loss: 0.3412480434992834, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1812, Loss: 0.36456725301992626, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1813, Loss: 0.2675145124876346, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1814, Loss: 0.3072686018789335, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1815, Loss: 0.3482153860820312, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1816, Loss: 0.275429140081323, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1817, Loss: 0.3385003037647718, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1818, Loss: 0.3187140526312049, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1819, Loss: 0.3048508173202557, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1820, Loss: 0.42875178090103494, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1821, Loss: 0.28263378980565224, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1822, Loss: 0.27135269209892177, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1823, Loss: 0.2666366758701711, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1824, Loss: 0.2378625321051024, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1825, Loss: 0.46674686923620506, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1826, Loss: 0.5882032649105633, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1827, Loss: 0.340060625652876, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1828, Loss: 0.44811647545833644, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1829, Loss: 0.35574135927975364, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1830, Loss: 0.3881817458516999, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1831, Loss: 0.2971413435794759, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1832, Loss: 0.27386658378722645, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1833, Loss: 0.2618301708047298, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1834, Loss: 0.2943407545467115, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1835, Loss: 0.48700741541450965, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1836, Loss: 0.2505649085283564, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1837, Loss: 0.23995930927275919, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1838, Loss: 0.451063632113624, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1839, Loss: 0.36138629841364733, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1840, Loss: 0.5338766345366303, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1841, Loss: 0.2840488237531592, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1842, Loss: 0.37720649287031927, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1843, Loss: 0.26956314686752003, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1844, Loss: 0.37394506640171676, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1845, Loss: 0.2232611107637286, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1846, Loss: 0.23669790536181953, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1847, Loss: 0.2792196171123533, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1848, Loss: 0.2649698314524216, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1849, Loss: 0.4293330438593046, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1850, Loss: 0.30006864623890195, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1851, Loss: 0.2811981730165671, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1852, Loss: 0.7870222705395967, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1853, Loss: 0.36712585118740704, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1854, Loss: 0.31791880584501464, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1855, Loss: 0.2965023520537077, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1856, Loss: 0.353915990285968, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1857, Loss: 0.30086997396178605, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1858, Loss: 0.35182050397102704, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1859, Loss: 0.5518448737589221, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1860, Loss: 0.5233048013439201, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1861, Loss: 0.5540731327045842, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1862, Loss: 0.25303480262503575, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1863, Loss: 0.41311806694704, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1864, Loss: 0.2991431365875649, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1865, Loss: 0.2579190407731795, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1866, Loss: 0.27587465514266807, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1867, Loss: 0.36193550616590775, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1868, Loss: 0.27972371343174196, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1869, Loss: 0.43528838068975423, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1870, Loss: 0.2663947694917703, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1871, Loss: 0.241252293336205, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1872, Loss: 0.3083110816123221, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1873, Loss: 0.42637894618002004, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1874, Loss: 0.3177212634397578, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Batch 1875, Loss: 0.6655312332648231, Batch Size: 32, Learning Rate: 0.00011314485468749997\n",
      "Epoch 8, Updated Learning Rate: 9.617312648437496e-05\n",
      "Epoch 8, Average Loss: 0.35776474764154265, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1, Loss: 0.6232644821578509, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 2, Loss: 0.3299738655107237, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 3, Loss: 0.3246535677410587, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 4, Loss: 0.6094812569860327, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 5, Loss: 0.4236410519328353, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 6, Loss: 0.3721079756224286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 7, Loss: 0.36139652976703274, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 8, Loss: 0.5017596862187103, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 9, Loss: 0.2713074388754435, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 10, Loss: 0.3438840785111634, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 11, Loss: 0.2598085736255733, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 12, Loss: 0.8691579775271323, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 13, Loss: 0.33652562033940725, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 14, Loss: 0.2821381068680229, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 15, Loss: 0.31992211470274534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 16, Loss: 0.5136391779727819, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 17, Loss: 0.3573358176733732, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 18, Loss: 0.3300027990204186, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 19, Loss: 0.26347747741600297, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 20, Loss: 0.24440841101723326, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 21, Loss: 0.4349995195637488, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 22, Loss: 0.30559310045074456, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 23, Loss: 0.28757695947953793, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 24, Loss: 0.6552919079976889, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 25, Loss: 0.5945753389968113, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 26, Loss: 0.29758947144200076, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 27, Loss: 0.27147702735575147, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 28, Loss: 0.26786135823301505, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 29, Loss: 0.6929185718161688, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 30, Loss: 0.3668659387923332, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 31, Loss: 0.470303594423431, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 32, Loss: 0.3012663686008877, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 33, Loss: 0.34229070373469217, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 34, Loss: 0.26347977664340766, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 35, Loss: 0.2600418418670426, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 36, Loss: 0.3718766307253566, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 37, Loss: 0.3501346488833681, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 38, Loss: 0.36858325759292404, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 39, Loss: 0.26496060255524195, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 40, Loss: 0.45372646770541963, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 41, Loss: 0.31772954048169033, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 42, Loss: 0.21824619489294905, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 43, Loss: 0.8001283116756873, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 44, Loss: 0.3023426542243199, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 45, Loss: 0.24679150761846855, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 46, Loss: 0.31801627617955286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 47, Loss: 0.2969159868747051, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 48, Loss: 0.28614152163694107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 49, Loss: 0.5721227756411158, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 50, Loss: 0.2598010719400536, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 51, Loss: 0.3846818953762494, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 52, Loss: 0.31650449593186125, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 53, Loss: 0.33916144748788946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 54, Loss: 0.4457019701598809, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 55, Loss: 0.31268876928569883, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 56, Loss: 0.3857588024225105, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 57, Loss: 0.27761073433418987, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 58, Loss: 0.33494148134047896, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 59, Loss: 0.3950262100260294, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 60, Loss: 0.3036096582814943, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 61, Loss: 0.26257765923493803, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 62, Loss: 0.35788406775571424, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 63, Loss: 0.31587939653572006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 64, Loss: 0.3404682756795539, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 65, Loss: 0.3501319718669098, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 66, Loss: 0.382564087188344, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 67, Loss: 0.41067855232450795, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 68, Loss: 0.29644050649094794, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 69, Loss: 0.2867257269350992, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 70, Loss: 0.40567665414141163, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 71, Loss: 0.5047424592842082, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 72, Loss: 0.25031865301866063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 73, Loss: 0.3059340696180778, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 74, Loss: 0.44243526088953344, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 75, Loss: 0.35701556169336546, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 76, Loss: 0.24730593772854653, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 77, Loss: 0.25449407338602636, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 78, Loss: 0.5827430979210773, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 79, Loss: 0.2971192152038129, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 80, Loss: 0.35025414104625097, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 81, Loss: 0.4306495538900364, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 82, Loss: 0.6301859757981948, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 83, Loss: 0.24893561958924557, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 84, Loss: 0.27503232136081746, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 85, Loss: 0.4780773348305453, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 86, Loss: 0.29621360441688954, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 87, Loss: 0.3462911709115065, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 88, Loss: 0.2554366153319915, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 89, Loss: 0.3730533899981918, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 90, Loss: 0.47856459144140595, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 91, Loss: 0.2761924893319891, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 92, Loss: 0.24774195991981296, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 93, Loss: 0.31993178901583774, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 94, Loss: 0.2955560953086629, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 95, Loss: 0.6041912675885465, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 96, Loss: 0.2760257054690872, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 97, Loss: 0.27147199564467484, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 98, Loss: 0.29410792902620325, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 99, Loss: 0.27631806390443625, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 100, Loss: 0.335945438571729, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 101, Loss: 0.26145987706704216, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 102, Loss: 0.2950304726959768, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 103, Loss: 0.3442930438185632, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 104, Loss: 0.27113396894656105, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 105, Loss: 0.3413872898988075, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 106, Loss: 0.37302785494786783, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 107, Loss: 0.32165382421852007, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 108, Loss: 0.23957878953159362, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 109, Loss: 0.36361740057649883, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 110, Loss: 0.3881341032354083, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 111, Loss: 0.3478216659518084, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 112, Loss: 0.36833834967296014, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 113, Loss: 0.37246295335927004, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 114, Loss: 0.4339490209480983, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 115, Loss: 0.29358984697278184, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 116, Loss: 0.28740550286305644, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 117, Loss: 0.6959973112786872, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 118, Loss: 0.574455189918716, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 119, Loss: 0.307834885113655, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 120, Loss: 0.400828423385703, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 121, Loss: 0.34920700250482156, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 122, Loss: 0.35349357380042495, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 123, Loss: 0.5667828419350116, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 124, Loss: 0.2945295986591033, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 125, Loss: 0.34844628773067055, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 126, Loss: 0.3281264677164057, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 127, Loss: 0.3498053693450152, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 128, Loss: 0.3640057083389094, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 129, Loss: 0.5512545136388927, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 130, Loss: 0.3568295266529392, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 131, Loss: 0.24747786172886937, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 132, Loss: 0.49353817637772135, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 133, Loss: 0.2986607827686518, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 134, Loss: 0.4248775814981409, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 135, Loss: 0.388912015565946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 136, Loss: 0.4298102964103907, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 137, Loss: 0.28163040838782877, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 138, Loss: 0.22772456456468115, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 139, Loss: 0.22519537090261954, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 140, Loss: 0.4201520954588261, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 141, Loss: 0.2796864523464827, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 142, Loss: 0.2884661577418717, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 143, Loss: 0.37953673547984146, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 144, Loss: 0.3558169330096984, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 145, Loss: 0.3170358566741433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 146, Loss: 0.31844561624785467, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 147, Loss: 0.3097895633771151, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 148, Loss: 0.23421131618934227, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 149, Loss: 0.23971953115268468, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 150, Loss: 0.28581336099227983, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 151, Loss: 0.40429963880799114, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 152, Loss: 0.4102535402108102, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 153, Loss: 0.3204889629945412, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 154, Loss: 0.41142027573009354, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 155, Loss: 0.26440912553437296, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 156, Loss: 0.34961952655946993, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 157, Loss: 0.22505902846831233, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 158, Loss: 0.27412298216420755, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 159, Loss: 0.30592487071618424, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 160, Loss: 0.38355778975001004, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 161, Loss: 0.3869819517664581, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 162, Loss: 0.2583631060779006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 163, Loss: 0.2717428714246403, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 164, Loss: 0.39400054076914004, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 165, Loss: 0.26107335930530784, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 166, Loss: 0.33546747762382345, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 167, Loss: 0.29803932637538966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 168, Loss: 0.26981884053251093, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 169, Loss: 0.2902526103203071, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 170, Loss: 0.30149005463628453, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 171, Loss: 0.2490532745902994, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 172, Loss: 0.3034442462264241, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 173, Loss: 0.38031314607189637, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 174, Loss: 0.3677790681863692, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 175, Loss: 0.27095988068382776, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 176, Loss: 0.23802135301519767, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 177, Loss: 0.41640603647357943, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 178, Loss: 0.3277844843441423, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 179, Loss: 0.255736739979448, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 180, Loss: 0.3900352757103811, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 181, Loss: 0.4691886672480203, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 182, Loss: 0.4567838557393816, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 183, Loss: 0.34476062400703433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 184, Loss: 0.2960805181470009, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 185, Loss: 0.26415527701954694, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 186, Loss: 0.2846971865925343, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 187, Loss: 0.6040650571728214, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 188, Loss: 0.42185724281494974, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 189, Loss: 0.5158050405472888, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 190, Loss: 0.26000802789009514, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 191, Loss: 0.42478704615132123, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 192, Loss: 0.33246322543646195, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 193, Loss: 0.27952181729466424, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 194, Loss: 0.4296338773218681, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 195, Loss: 0.3453851655758963, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 196, Loss: 0.39359603485018313, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 197, Loss: 0.459711479421545, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 198, Loss: 0.3437968444867957, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 199, Loss: 0.5340312405293011, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 200, Loss: 0.3571332932631872, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 201, Loss: 0.37633241611141266, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 202, Loss: 0.2907491655260051, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 203, Loss: 0.4118038743973202, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 204, Loss: 0.49788486045359925, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 205, Loss: 0.22722743182348096, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 206, Loss: 0.40138643022399617, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 207, Loss: 0.6757862277393852, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 208, Loss: 0.6410539284931446, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 209, Loss: 0.4478050138952008, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 210, Loss: 0.299155399019074, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 211, Loss: 0.3133514248181902, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 212, Loss: 0.34665175238397905, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 213, Loss: 0.3307332590333358, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 214, Loss: 0.329222916783829, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 215, Loss: 0.34458113896296233, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 216, Loss: 0.438322206922092, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 217, Loss: 0.2504774099752408, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 218, Loss: 0.35867881136316015, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 219, Loss: 0.25914913346896984, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 220, Loss: 0.2876522610118919, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 221, Loss: 0.26494402578844223, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 222, Loss: 0.3582157807803934, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 223, Loss: 0.31755999543107105, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 224, Loss: 0.24536448976812553, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 225, Loss: 0.3832282654175029, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 226, Loss: 0.6090783766449166, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 227, Loss: 0.3921186154974911, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 228, Loss: 0.42866943195861024, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 229, Loss: 0.27218871329155486, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 230, Loss: 0.2987364001459761, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 231, Loss: 0.40906616510504784, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 232, Loss: 0.7796822567982713, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 233, Loss: 0.3379266788447059, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 234, Loss: 0.4766749025779853, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 235, Loss: 0.6631865841003495, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 236, Loss: 0.3643603570480997, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 237, Loss: 0.2952578988071239, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 238, Loss: 0.23506268874081226, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 239, Loss: 0.28674501449933737, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 240, Loss: 0.24074576816197468, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 241, Loss: 0.2616907376716553, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 242, Loss: 0.2623916538669241, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 243, Loss: 0.27234990270002507, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 244, Loss: 0.3100669031399645, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 245, Loss: 0.4438381958484312, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 246, Loss: 0.42710309101560945, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 247, Loss: 0.3628031055489578, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 248, Loss: 0.3786089952571414, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 249, Loss: 0.24895240658192944, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 250, Loss: 0.41921258021016095, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 251, Loss: 0.44557980469456326, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 252, Loss: 0.4403067288941931, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 253, Loss: 0.296025130434135, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 254, Loss: 0.2679972270776201, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 255, Loss: 0.42130041888210906, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 256, Loss: 0.38137722766092585, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 257, Loss: 0.32115672090539304, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 258, Loss: 0.39762188879944876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 259, Loss: 0.33551834352449456, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 260, Loss: 0.2717303930900054, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 261, Loss: 0.2716270436514936, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 262, Loss: 0.278181248755346, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 263, Loss: 0.2646017205989286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 264, Loss: 0.40432914003259995, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 265, Loss: 0.2676970250379549, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 266, Loss: 0.31389240738172125, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 267, Loss: 0.2980900758311599, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 268, Loss: 0.26325219019359813, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 269, Loss: 0.4486448096025054, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 270, Loss: 0.29508590820102326, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 271, Loss: 0.32570322141612984, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 272, Loss: 0.32355799590775347, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 273, Loss: 0.32301062731102326, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 274, Loss: 0.3648815503559187, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 275, Loss: 0.32504316575289477, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 276, Loss: 0.4214402721795928, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 277, Loss: 0.2546457869426739, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 278, Loss: 0.3062271364768269, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 279, Loss: 0.275873676109242, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 280, Loss: 0.33201445372588106, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 281, Loss: 0.25788313108317334, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 282, Loss: 0.3230285761310834, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 283, Loss: 0.3518589041004647, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 284, Loss: 0.28089079824753205, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 285, Loss: 0.4186213701771203, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 286, Loss: 0.3724135929232947, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 287, Loss: 0.4918000735831649, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 288, Loss: 0.6126346064566951, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 289, Loss: 0.2579517574974815, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 290, Loss: 0.2840070321852107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 291, Loss: 0.37382296098068646, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 292, Loss: 0.2820818738273077, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 293, Loss: 0.2754177356378419, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 294, Loss: 0.27414445150712063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 295, Loss: 0.24465597297648733, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 296, Loss: 0.3072763972560971, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 297, Loss: 0.3330303345310626, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 298, Loss: 0.36457047159064393, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 299, Loss: 0.39448662969901643, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 300, Loss: 0.457495600785504, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 301, Loss: 0.30464305060631036, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 302, Loss: 0.25313492199596554, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 303, Loss: 0.5469986871796503, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 304, Loss: 0.31527895137130335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 305, Loss: 0.23214172468903355, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 306, Loss: 0.35265636238495623, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 307, Loss: 0.386867884740533, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 308, Loss: 0.3313882810235517, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 309, Loss: 0.26166460988527174, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 310, Loss: 0.29099966358722984, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 311, Loss: 0.24027688050836052, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 312, Loss: 0.2445335079133329, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 313, Loss: 0.3311877904687296, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 314, Loss: 0.263879395144887, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 315, Loss: 0.28916670745131656, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 316, Loss: 0.23532704956217898, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 317, Loss: 0.46938463962681054, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 318, Loss: 0.3337858630030412, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 319, Loss: 0.2841481077017324, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 320, Loss: 0.46051959121683916, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 321, Loss: 0.5457728271240564, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 322, Loss: 0.31890612145294805, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 323, Loss: 0.2801932545413491, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 324, Loss: 0.22058246179131918, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 325, Loss: 0.29013383276070287, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 326, Loss: 0.39610142908880214, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 327, Loss: 0.2569116796531038, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 328, Loss: 0.21604925768883781, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 329, Loss: 0.4614123811212891, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 330, Loss: 0.28035570352553685, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 331, Loss: 0.6239520244946469, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 332, Loss: 0.3573750447409205, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 333, Loss: 0.35115442573990024, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 334, Loss: 0.25760754662844393, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 335, Loss: 0.31668312371045104, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 336, Loss: 0.39561306859860185, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 337, Loss: 0.3039901697306465, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 338, Loss: 0.3296576705085834, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 339, Loss: 0.4111838762054605, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 340, Loss: 0.23620239454774342, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 341, Loss: 0.5395833479115366, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 342, Loss: 0.5140666561120523, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 343, Loss: 0.3072575437787213, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 344, Loss: 0.23303605074729455, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 345, Loss: 0.3321072591304786, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 346, Loss: 0.39926982531190075, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 347, Loss: 0.2517433072080664, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 348, Loss: 0.3484957905165924, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 349, Loss: 0.29813333565226724, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 350, Loss: 0.3847853517480297, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 351, Loss: 0.2881687232555084, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 352, Loss: 0.24407814508807552, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 353, Loss: 0.27951260696665925, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 354, Loss: 0.31397181031064403, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 355, Loss: 0.3290565249937475, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 356, Loss: 0.5683245284939642, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 357, Loss: 0.23014254759282482, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 358, Loss: 0.419588319239875, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 359, Loss: 0.4028985104590302, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 360, Loss: 0.30079699939757126, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 361, Loss: 0.3306077230686461, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 362, Loss: 0.28646696693198115, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 363, Loss: 0.36484408663762335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 364, Loss: 0.24335571827860308, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 365, Loss: 0.29015351184097127, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 366, Loss: 0.33308901036727645, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 367, Loss: 0.26594211758966857, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 368, Loss: 0.25522959630390085, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 369, Loss: 0.4400362415533007, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 370, Loss: 0.2833818400245473, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 371, Loss: 0.5283730787384107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 372, Loss: 0.41917814588207003, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 373, Loss: 0.42139134205728446, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 374, Loss: 0.23682728050791407, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 375, Loss: 0.34696391543553945, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 376, Loss: 0.3037284328380875, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 377, Loss: 0.6291815733249551, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 378, Loss: 0.3266910209847006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 379, Loss: 0.2784897375700418, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 380, Loss: 0.3199718126265488, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 381, Loss: 0.2963524134517201, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 382, Loss: 0.6052830022793994, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 383, Loss: 0.5960970864029437, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 384, Loss: 0.5712538727759742, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 385, Loss: 0.30295913925707507, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 386, Loss: 0.29308509719626186, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 387, Loss: 0.3620229262760326, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 388, Loss: 0.346875861843868, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 389, Loss: 0.41612199425275065, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 390, Loss: 0.2907174056333966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 391, Loss: 0.3720640053161347, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 392, Loss: 0.25124512548425504, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 393, Loss: 0.33539643346520454, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 394, Loss: 0.35293881233893565, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 395, Loss: 0.2288963588305361, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 396, Loss: 0.41169525086080017, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 397, Loss: 0.2614755876972229, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 398, Loss: 0.2552548366126801, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 399, Loss: 0.3527462296652247, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 400, Loss: 0.2932566045671938, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 401, Loss: 0.40545707247103274, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 402, Loss: 0.2889303070759755, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 403, Loss: 0.2211309488377452, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 404, Loss: 0.3535617187287533, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 405, Loss: 0.35921168402458364, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 406, Loss: 0.639780808903544, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 407, Loss: 0.27096552228406795, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 408, Loss: 0.2790623709939303, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 409, Loss: 0.3498301097152019, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 410, Loss: 0.3225234807407241, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 411, Loss: 0.38206939366367865, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 412, Loss: 0.22485167661770172, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 413, Loss: 0.4016366635108786, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 414, Loss: 0.6511101983003832, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 415, Loss: 0.41060092925247516, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 416, Loss: 0.43680345907096285, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 417, Loss: 0.27590131405356794, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 418, Loss: 0.28600622882876187, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 419, Loss: 0.34326207187545443, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 420, Loss: 0.3632042881158632, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 421, Loss: 0.31309574032590465, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 422, Loss: 0.30035151038731367, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 423, Loss: 0.3809287262063986, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 424, Loss: 0.2576067952275213, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 425, Loss: 0.294261336709437, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 426, Loss: 0.3894824504697406, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 427, Loss: 0.24262969828425793, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 428, Loss: 0.260782162709527, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 429, Loss: 0.272834938100742, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 430, Loss: 0.3333010763500297, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 431, Loss: 0.2763900346100774, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 432, Loss: 0.48261482676367473, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 433, Loss: 0.29972530444300083, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 434, Loss: 0.4518396725235023, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 435, Loss: 0.4017355124758405, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 436, Loss: 0.5381279844183778, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 437, Loss: 0.2831057913463614, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 438, Loss: 0.31301519580504633, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 439, Loss: 0.39447582493313016, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 440, Loss: 0.31924566176173513, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 441, Loss: 0.4570988744097495, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 442, Loss: 0.32747394810413843, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 443, Loss: 0.3685293788486472, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 444, Loss: 0.26887233070569905, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 445, Loss: 0.3016632538317996, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 446, Loss: 0.5674930644014626, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 447, Loss: 0.3205024640299512, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 448, Loss: 0.26713120574440835, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 449, Loss: 0.2554967556936609, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 450, Loss: 0.33828085142465114, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 451, Loss: 0.24289957312532656, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 452, Loss: 0.44200346349536396, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 453, Loss: 0.28420132491113037, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 454, Loss: 0.3734869915014749, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 455, Loss: 0.3197426350317123, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 456, Loss: 0.2461209241539641, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 457, Loss: 0.27808975348592513, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 458, Loss: 0.43893609494501373, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 459, Loss: 0.29330612512482696, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 460, Loss: 0.38641434831855914, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 461, Loss: 0.28740985680584513, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 462, Loss: 0.3159280321149926, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 463, Loss: 0.28892636315815423, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 464, Loss: 0.3256403131469021, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 465, Loss: 0.430926724817682, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 466, Loss: 0.28232474787068884, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 467, Loss: 0.44570549523132663, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 468, Loss: 0.29631922996202587, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 469, Loss: 0.27285114488702805, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 470, Loss: 0.5190748910669263, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 471, Loss: 0.3402027066137447, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 472, Loss: 0.484650128297779, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 473, Loss: 0.31941096108394823, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 474, Loss: 0.4097221201922553, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 475, Loss: 0.26504737937788536, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 476, Loss: 0.25879854017823023, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 477, Loss: 0.5402073464023919, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 478, Loss: 0.35225314780222394, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 479, Loss: 0.5128907249421973, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 480, Loss: 0.4372633348937839, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 481, Loss: 0.27578470719302467, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 482, Loss: 0.2938683607813029, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 483, Loss: 0.33854293860904416, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 484, Loss: 0.24362255008591627, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 485, Loss: 0.7708445754942064, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 486, Loss: 0.3895969816571031, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 487, Loss: 0.5047160024523367, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 488, Loss: 0.5820807325570421, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 489, Loss: 0.4060387647834723, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 490, Loss: 0.23706419269040402, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 491, Loss: 0.31606092955438647, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 492, Loss: 0.3095142706553503, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 493, Loss: 0.4772473696463057, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 494, Loss: 0.30251742939734266, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 495, Loss: 0.48125219882360637, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 496, Loss: 0.2386606273209929, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 497, Loss: 0.24236950377130037, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 498, Loss: 0.26342696199689014, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 499, Loss: 0.3679470265952318, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 500, Loss: 0.2975892293959002, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 501, Loss: 0.2628233157928538, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 502, Loss: 0.3828502818496671, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 503, Loss: 0.5217577785873966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 504, Loss: 0.4392134091866255, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 505, Loss: 0.2709929341869609, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 506, Loss: 0.28051556867398275, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 507, Loss: 0.31789609876069314, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 508, Loss: 0.4218921341687085, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 509, Loss: 0.3945357246568796, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 510, Loss: 0.331991917584635, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 511, Loss: 0.31357287894053004, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 512, Loss: 0.2708942870926503, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 513, Loss: 0.3183285143232846, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 514, Loss: 0.37145790052274813, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 515, Loss: 0.2623201086797867, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 516, Loss: 0.30201705734959206, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 517, Loss: 0.3584008685390868, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 518, Loss: 0.3853094475079059, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 519, Loss: 0.40623042090671935, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 520, Loss: 0.32368887428994747, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 521, Loss: 0.3924389407535589, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 522, Loss: 0.30107357063332185, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 523, Loss: 0.30773405383542796, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 524, Loss: 0.40537193705943975, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 525, Loss: 0.27896519696858385, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 526, Loss: 0.38306417915916186, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 527, Loss: 0.38157275643591243, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 528, Loss: 0.31227994139417026, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 529, Loss: 0.26858386842974946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 530, Loss: 0.34389443227365235, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 531, Loss: 0.41744870412211144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 532, Loss: 0.243439315064567, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 533, Loss: 0.355101742340619, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 534, Loss: 0.3073689045616043, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 535, Loss: 0.2474793921849876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 536, Loss: 0.41819995302473223, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 537, Loss: 0.3735468019687752, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 538, Loss: 0.6098095323539748, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 539, Loss: 0.3284602341041162, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 540, Loss: 0.3368970655668272, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 541, Loss: 0.29406845276634846, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 542, Loss: 0.28002177715255494, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 543, Loss: 0.5919899183539867, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 544, Loss: 0.2342003903064668, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 545, Loss: 0.34258689814374643, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 546, Loss: 0.3846389030317794, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 547, Loss: 0.21747581781817807, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 548, Loss: 0.35771506356933436, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 549, Loss: 0.3850005484702211, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 550, Loss: 0.4253287147061613, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 551, Loss: 0.3717339592257096, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 552, Loss: 0.34722167845701113, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 553, Loss: 0.22913542055650193, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 554, Loss: 0.3851081266335399, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 555, Loss: 0.25604157567424757, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 556, Loss: 0.6621885301706415, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 557, Loss: 0.3557635414131133, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 558, Loss: 0.48851542192036435, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 559, Loss: 0.2966296358332496, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 560, Loss: 0.2727346138249254, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 561, Loss: 0.46127453063708557, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 562, Loss: 0.2592372838709176, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 563, Loss: 0.48737527895405, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 564, Loss: 0.27620240985605415, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 565, Loss: 0.37972059486490406, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 566, Loss: 0.32526075273966637, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 567, Loss: 0.2416499723234054, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 568, Loss: 0.31328258057544534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 569, Loss: 0.4270524991436566, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 570, Loss: 0.31010557696362967, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 571, Loss: 0.2391494124186171, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 572, Loss: 0.2276024170135003, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 573, Loss: 0.2355937574579765, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 574, Loss: 0.5569672215399235, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 575, Loss: 0.3766209695725254, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 576, Loss: 0.2185942554429188, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 577, Loss: 0.29613659320557056, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 578, Loss: 0.2229127398634417, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 579, Loss: 0.35568545709976807, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 580, Loss: 0.513143534621703, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 581, Loss: 0.3310835157518577, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 582, Loss: 0.6394928383491407, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 583, Loss: 0.37172381343471195, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 584, Loss: 0.5623872919418323, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 585, Loss: 0.22876066170840006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 586, Loss: 0.30633491459243134, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 587, Loss: 0.3009371320252159, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 588, Loss: 0.48970534902514723, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 589, Loss: 0.28650208431652413, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 590, Loss: 0.37790146709479727, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 591, Loss: 0.5356057959183138, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 592, Loss: 0.4922429343539113, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 593, Loss: 0.5048370289281796, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 594, Loss: 0.443941028218268, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 595, Loss: 0.3530938651907298, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 596, Loss: 0.34412368430103135, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 597, Loss: 0.527503371082324, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 598, Loss: 0.6320965206231284, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 599, Loss: 0.24325885871522757, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 600, Loss: 0.3860520310781535, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 601, Loss: 0.5570255415462774, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 602, Loss: 0.3265659772541455, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 603, Loss: 0.26865440225429116, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 604, Loss: 0.3042353298784735, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 605, Loss: 0.40410898964895103, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 606, Loss: 0.3075172749335976, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 607, Loss: 0.3461311665467143, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 608, Loss: 0.29459444995984624, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 609, Loss: 0.4264649090068719, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 610, Loss: 0.3943677128790528, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 611, Loss: 0.30111960772759283, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 612, Loss: 0.25232499334325753, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 613, Loss: 0.2739702620504321, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 614, Loss: 0.4793093480333518, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 615, Loss: 0.35601156173605897, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 616, Loss: 0.6795202459251717, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 617, Loss: 0.3359115434445282, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 618, Loss: 0.33011237604386734, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 619, Loss: 0.25044781728433607, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 620, Loss: 0.25970152804838176, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 621, Loss: 0.3230381083213931, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 622, Loss: 0.2980051359227801, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 623, Loss: 0.24201930749687875, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 624, Loss: 0.3451224976437048, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 625, Loss: 0.2430939657714075, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 626, Loss: 0.2534532116423243, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 627, Loss: 0.27201471563956053, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 628, Loss: 0.5390859746847555, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 629, Loss: 0.27520896224431257, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 630, Loss: 0.4703178320695063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 631, Loss: 0.2984337853315128, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 632, Loss: 0.26831334819995506, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 633, Loss: 0.3383552900162218, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 634, Loss: 0.3666105520311408, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 635, Loss: 0.49923106825786534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 636, Loss: 0.265098589630292, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 637, Loss: 0.39127692432733696, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 638, Loss: 0.28859100154259965, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 639, Loss: 0.39061475202787077, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 640, Loss: 0.24443452982761177, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 641, Loss: 0.2709304577978977, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 642, Loss: 0.3720215356371884, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 643, Loss: 0.35317526434467805, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 644, Loss: 0.4496554167094384, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 645, Loss: 0.28917363800076534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 646, Loss: 0.35796703565973864, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 647, Loss: 0.24492141427033853, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 648, Loss: 0.30990572790938276, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 649, Loss: 0.2515573602005192, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 650, Loss: 0.330587856846292, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 651, Loss: 0.3516373808747343, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 652, Loss: 0.23818007902634786, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 653, Loss: 0.2849821910789443, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 654, Loss: 0.25165034843940465, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 655, Loss: 0.27455074727446116, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 656, Loss: 0.46120309325042796, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 657, Loss: 0.27776465523945115, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 658, Loss: 0.3968166889458033, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 659, Loss: 0.30218578683813124, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 660, Loss: 0.26032657198493175, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 661, Loss: 0.27066730353721585, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 662, Loss: 0.23610449209825707, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 663, Loss: 0.3606486807756576, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 664, Loss: 0.30523818488621385, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 665, Loss: 0.27928623234269745, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 666, Loss: 0.35370133516005764, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 667, Loss: 0.6294462984980536, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 668, Loss: 0.334228631013504, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 669, Loss: 0.3021908067868845, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 670, Loss: 0.2492734967224373, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 671, Loss: 0.2723046624740962, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 672, Loss: 0.232303864221095, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 673, Loss: 0.3433200120856415, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 674, Loss: 0.4210621080866376, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 675, Loss: 0.38501539222794967, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 676, Loss: 0.3321392522563646, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 677, Loss: 0.3576561679296, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 678, Loss: 0.32320457434295763, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 679, Loss: 0.3166130512265434, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 680, Loss: 0.3104015221017016, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 681, Loss: 0.3275542859169688, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 682, Loss: 0.5334988425591493, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 683, Loss: 0.41804450275577176, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 684, Loss: 0.299348555904485, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 685, Loss: 0.6359132062314107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 686, Loss: 0.29296495460215005, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 687, Loss: 0.26767613884058294, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 688, Loss: 0.38440837698869607, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 689, Loss: 0.31229418118347574, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 690, Loss: 0.3942062133309704, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 691, Loss: 0.3419248865463099, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 692, Loss: 0.43867904042099715, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 693, Loss: 0.3583788631905191, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 694, Loss: 0.39998377357541637, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 695, Loss: 0.5733231588348567, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 696, Loss: 0.29662479592891816, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 697, Loss: 0.3394548274318966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 698, Loss: 0.31494502387622336, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 699, Loss: 0.28208284231984515, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 700, Loss: 0.27530214008605214, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 701, Loss: 0.34602683904126885, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 702, Loss: 0.2531371718186353, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 703, Loss: 0.37644931069143023, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 704, Loss: 0.6116124177342359, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 705, Loss: 0.3197697780877178, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 706, Loss: 0.6016589532356242, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 707, Loss: 0.23288023722675552, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 708, Loss: 0.27099723163566874, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 709, Loss: 0.3552194979919405, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 710, Loss: 0.23942114940805845, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 711, Loss: 0.26933391989736144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 712, Loss: 0.3955853392497656, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 713, Loss: 0.24989590432225242, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 714, Loss: 0.6717688371125019, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 715, Loss: 0.2664973421900739, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 716, Loss: 0.3274438998313341, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 717, Loss: 0.33931359237999303, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 718, Loss: 0.5950104584967355, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 719, Loss: 0.2672894007961432, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 720, Loss: 0.37251457115333086, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 721, Loss: 0.46470267684404715, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 722, Loss: 0.343657956282606, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 723, Loss: 0.2566940164420445, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 724, Loss: 0.2358928623681348, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 725, Loss: 0.4328060732698107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 726, Loss: 0.23177628232491773, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 727, Loss: 0.40149176271736453, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 728, Loss: 0.2475848077270712, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 729, Loss: 0.31621634970659257, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 730, Loss: 0.3839298832405392, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 731, Loss: 0.2512483149713181, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 732, Loss: 0.2398515067036518, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 733, Loss: 0.31204400581911557, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 734, Loss: 0.5767911123501555, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 735, Loss: 0.2886871459128915, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 736, Loss: 0.22482071086853417, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 737, Loss: 0.27922193390185857, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 738, Loss: 0.34298318863028876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 739, Loss: 0.6288601981078181, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 740, Loss: 0.23691710573336086, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 741, Loss: 0.395878370439743, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 742, Loss: 0.4717526165892284, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 743, Loss: 0.3690960949816807, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 744, Loss: 0.24624129772535294, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 745, Loss: 0.40686411046954535, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 746, Loss: 0.4976803400886747, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 747, Loss: 0.3244380624480063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 748, Loss: 0.4075544415198311, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 749, Loss: 0.3803624069596896, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 750, Loss: 0.29034368712186237, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 751, Loss: 0.4144685093744852, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 752, Loss: 0.3185986666161541, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 753, Loss: 0.44542185874700324, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 754, Loss: 0.36368212669954314, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 755, Loss: 0.2506373796794626, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 756, Loss: 0.4541198705840086, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 757, Loss: 0.36257216234477563, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 758, Loss: 0.32302811182346713, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 759, Loss: 0.5025952544414972, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 760, Loss: 0.24874474712414188, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 761, Loss: 0.2410334159675066, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 762, Loss: 0.3156528676227657, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 763, Loss: 0.30442665349603526, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 764, Loss: 0.28532317468398183, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 765, Loss: 0.3221705284302533, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 766, Loss: 0.2717416150877207, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 767, Loss: 0.5241978525867488, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 768, Loss: 0.5512454187502993, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 769, Loss: 0.4583275127535812, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 770, Loss: 0.27976066904297026, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 771, Loss: 0.4853121621272908, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 772, Loss: 0.2600527295855744, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 773, Loss: 0.28543376530211284, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 774, Loss: 0.2662332111726092, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 775, Loss: 0.4320573802673207, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 776, Loss: 0.2775374690688533, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 777, Loss: 0.36600550135576915, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 778, Loss: 0.45572182132625755, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 779, Loss: 0.3774111459084756, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 780, Loss: 0.23674890772129684, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 781, Loss: 0.6691661728548514, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 782, Loss: 0.35560066817942715, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 783, Loss: 0.2741234487277959, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 784, Loss: 0.2343541882710919, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 785, Loss: 0.25144254328200966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 786, Loss: 0.5405845597637821, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 787, Loss: 0.48581882920816755, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 788, Loss: 0.2607918556620967, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 789, Loss: 0.40474203412604015, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 790, Loss: 0.32979513044987213, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 791, Loss: 0.26959437952920995, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 792, Loss: 0.31799207580531086, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 793, Loss: 0.4368423872450946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 794, Loss: 0.31339243467374767, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 795, Loss: 0.25264474703155126, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 796, Loss: 0.5018964506344803, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 797, Loss: 0.5234758279041727, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 798, Loss: 0.32885287464815005, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 799, Loss: 0.4851787428328626, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 800, Loss: 0.2694451656630835, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 801, Loss: 0.39157600621841604, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 802, Loss: 0.47280920190751513, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 803, Loss: 0.3140157048489971, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 804, Loss: 0.35193681297043644, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 805, Loss: 0.30233402929183173, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 806, Loss: 0.30435164073668397, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 807, Loss: 0.5507992387361327, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 808, Loss: 0.2671091910320547, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 809, Loss: 0.2505935614350239, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 810, Loss: 0.30892562340049773, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 811, Loss: 0.5416506723527743, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 812, Loss: 0.573912908165756, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 813, Loss: 0.3161509744010592, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 814, Loss: 0.33585005613665286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 815, Loss: 0.27130996425141063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 816, Loss: 0.4579389763649685, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 817, Loss: 0.32513853795960745, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 818, Loss: 0.46145249738168975, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 819, Loss: 0.44529032275710256, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 820, Loss: 0.3456365926001881, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 821, Loss: 0.45223561846641747, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 822, Loss: 0.27435486782445007, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 823, Loss: 0.456218845425438, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 824, Loss: 0.29639081535993106, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 825, Loss: 0.2712469276363424, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 826, Loss: 0.4068900626833287, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 827, Loss: 0.46225408592421435, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 828, Loss: 0.4495366445651815, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 829, Loss: 0.2545057080256473, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 830, Loss: 0.3701813893280511, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 831, Loss: 0.2477243932485175, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 832, Loss: 0.26397334059638894, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 833, Loss: 0.33190135482308764, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 834, Loss: 0.5680404708310534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 835, Loss: 0.6784875215134727, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 836, Loss: 0.3169758049735667, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 837, Loss: 0.4414849060485875, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 838, Loss: 0.2711913516004335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 839, Loss: 0.34791757521498584, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 840, Loss: 0.42794036230878946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 841, Loss: 0.3472166957593354, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 842, Loss: 0.34761299248973365, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 843, Loss: 0.7158949345282827, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 844, Loss: 0.3535796043121541, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 845, Loss: 0.41118375299463994, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 846, Loss: 0.36896531560584955, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 847, Loss: 0.3820897422213422, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 848, Loss: 0.3479929312299175, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 849, Loss: 0.4273709499387925, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 850, Loss: 0.5093716067524406, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 851, Loss: 0.29445558612751643, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 852, Loss: 0.3349836350807679, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 853, Loss: 0.3095420462493167, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 854, Loss: 0.38065704597514466, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 855, Loss: 0.2826146598601739, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 856, Loss: 0.3756746629962787, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 857, Loss: 0.5913938054724742, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 858, Loss: 0.3508716088504629, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 859, Loss: 0.35058438783735474, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 860, Loss: 0.2670542408499006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 861, Loss: 0.6718223544105886, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 862, Loss: 0.3037592677914426, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 863, Loss: 0.23742145481299826, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 864, Loss: 0.3350440211898298, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 865, Loss: 0.45900981479419534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 866, Loss: 0.28242055453915554, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 867, Loss: 0.2917321156885638, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 868, Loss: 0.2571223030005515, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 869, Loss: 0.23523681222667125, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 870, Loss: 0.3519588196784005, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 871, Loss: 0.2703680476184239, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 872, Loss: 0.4432684309694791, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 873, Loss: 0.3139322021759433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 874, Loss: 0.2409738147325765, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 875, Loss: 0.3753492970747443, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 876, Loss: 0.2840470758589698, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 877, Loss: 0.2312948958677647, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 878, Loss: 0.3123709219445825, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 879, Loss: 0.2922907172168331, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 880, Loss: 0.315919281579022, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 881, Loss: 0.5758555503302378, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 882, Loss: 0.3492008153198767, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 883, Loss: 0.2655538963062382, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 884, Loss: 0.3416834842712725, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 885, Loss: 0.35903105868196905, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 886, Loss: 0.25947413815479536, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 887, Loss: 0.27371185127883946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 888, Loss: 0.3903873077690586, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 889, Loss: 0.37416322496085813, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 890, Loss: 0.26890055176535727, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 891, Loss: 0.30259927463775915, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 892, Loss: 0.3505198458843489, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 893, Loss: 0.24019874328726146, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 894, Loss: 0.26742759798501947, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 895, Loss: 0.4644045475210808, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 896, Loss: 0.3440679127937525, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 897, Loss: 0.28793736189817243, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 898, Loss: 0.26824283117064324, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 899, Loss: 0.2789329618370754, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 900, Loss: 0.44426762192030045, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 901, Loss: 0.3748796346269959, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 902, Loss: 0.36870170806594105, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 903, Loss: 0.35621504217307287, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 904, Loss: 0.2897771117938125, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 905, Loss: 0.3118762433927974, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 906, Loss: 0.3844433666986614, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 907, Loss: 0.42730570144216595, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 908, Loss: 0.2512812183866333, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 909, Loss: 0.43341322627619294, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 910, Loss: 0.46537435200056676, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 911, Loss: 0.3533754066997334, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 912, Loss: 0.34027648253049103, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 913, Loss: 0.2537017281266678, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 914, Loss: 0.22127926040098708, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 915, Loss: 0.5694973971062238, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 916, Loss: 0.3325525261409959, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 917, Loss: 0.6169312681359703, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 918, Loss: 0.24149390920203623, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 919, Loss: 0.3784375102212426, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 920, Loss: 0.2390290007062775, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 921, Loss: 0.3057279471110433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 922, Loss: 0.24586199339976034, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 923, Loss: 0.28791930795903, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 924, Loss: 0.41939905260351196, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 925, Loss: 0.5058446881797742, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 926, Loss: 0.5116015704006638, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 927, Loss: 0.3771054740124123, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 928, Loss: 0.3406989552738042, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 929, Loss: 0.2839943545751945, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 930, Loss: 0.3124020211825262, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 931, Loss: 0.3396869305350684, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 932, Loss: 0.3020209996291617, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 933, Loss: 0.2720349141434996, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 934, Loss: 0.27435555591260946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 935, Loss: 0.3998642662345635, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 936, Loss: 0.2944545026860554, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 937, Loss: 0.28681709805336214, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 938, Loss: 0.5348527819466721, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 939, Loss: 0.29575575491870953, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 940, Loss: 0.38468562921005506, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 941, Loss: 0.37145475971796904, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 942, Loss: 0.2548479497965769, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 943, Loss: 0.4923670775579093, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 944, Loss: 0.41618039377880467, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 945, Loss: 0.3778873090801782, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 946, Loss: 0.41635993036430463, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 947, Loss: 0.25121143195505824, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 948, Loss: 0.31018299248105896, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 949, Loss: 0.41420372689763985, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 950, Loss: 0.4307907917326601, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 951, Loss: 0.40231937863228406, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 952, Loss: 0.28982040377148316, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 953, Loss: 0.388024317430137, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 954, Loss: 0.3999163918266827, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 955, Loss: 0.3145816759968767, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 956, Loss: 0.26657130986033517, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 957, Loss: 0.2957382927606572, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 958, Loss: 0.2734779488683469, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 959, Loss: 0.24138965644899416, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 960, Loss: 0.24947286714471417, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 961, Loss: 0.3631767403774221, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 962, Loss: 0.36223792428139723, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 963, Loss: 0.43320177343896765, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 964, Loss: 0.2959580529799034, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 965, Loss: 0.2857095727266286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 966, Loss: 0.3983693790728466, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 967, Loss: 0.2830761147152573, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 968, Loss: 0.22506210484610276, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 969, Loss: 0.2963528097595865, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 970, Loss: 0.30020030885985516, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 971, Loss: 0.3668486848386828, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 972, Loss: 0.37016833810843974, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 973, Loss: 0.26409930954295996, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 974, Loss: 0.6769828995479668, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 975, Loss: 0.2411094684616735, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 976, Loss: 0.5026547406687258, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 977, Loss: 0.2249015968418477, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 978, Loss: 0.28626741559374946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 979, Loss: 0.333845875006263, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 980, Loss: 0.2598119218562085, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 981, Loss: 0.3493856575069735, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 982, Loss: 0.24640503316159088, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 983, Loss: 0.27537898016466544, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 984, Loss: 0.2809494924896833, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 985, Loss: 0.22705953577298557, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 986, Loss: 0.35335757733456646, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 987, Loss: 0.29504997628304447, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 988, Loss: 0.32066441532493006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 989, Loss: 0.33152254409990617, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 990, Loss: 0.6608538112198481, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 991, Loss: 0.40548230264966667, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 992, Loss: 0.24842383880100485, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 993, Loss: 0.22281362331008037, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 994, Loss: 0.3285772049418685, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 995, Loss: 0.41804556922256153, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 996, Loss: 0.47926834731665946, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 997, Loss: 0.28260318067720436, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 998, Loss: 0.37066597351994024, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 999, Loss: 0.27884139240942024, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1000, Loss: 0.29354847729638106, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1001, Loss: 0.47440278921198464, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1002, Loss: 0.3679914267014797, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1003, Loss: 0.23961322883047995, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1004, Loss: 0.2465689861657011, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1005, Loss: 0.3019019291022473, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1006, Loss: 0.3788840600774799, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1007, Loss: 0.33699050902939465, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1008, Loss: 0.22191816223709693, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1009, Loss: 0.36222318710054335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1010, Loss: 0.24712736596724796, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1011, Loss: 0.32199846026172646, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1012, Loss: 0.3555837519027508, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1013, Loss: 0.32845010514936374, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1014, Loss: 0.4607096860262613, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1015, Loss: 0.5196568375920756, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1016, Loss: 0.24915302654570698, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1017, Loss: 0.3039249059583928, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1018, Loss: 0.3256133298750087, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1019, Loss: 0.48888739278955456, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1020, Loss: 0.23855704378075152, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1021, Loss: 0.6005570917076595, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1022, Loss: 0.40138806093610147, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1023, Loss: 0.5686579836756465, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1024, Loss: 0.29463756794637486, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1025, Loss: 0.29174477073963834, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1026, Loss: 0.33895720767077386, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1027, Loss: 0.34521511595365717, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1028, Loss: 0.36011370007762733, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1029, Loss: 0.43099982595530245, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1030, Loss: 0.42648956755062173, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1031, Loss: 0.678367844245415, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1032, Loss: 0.3245722777672601, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1033, Loss: 0.5937198720492204, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1034, Loss: 0.2568605862396847, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1035, Loss: 0.43539110197885034, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1036, Loss: 0.36738935529457417, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1037, Loss: 0.40092160363975365, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1038, Loss: 0.2530110284402296, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1039, Loss: 0.3273157002136556, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1040, Loss: 0.4123228943304936, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1041, Loss: 0.26375335776648245, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1042, Loss: 0.28110550125591627, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1043, Loss: 0.30952761652649147, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1044, Loss: 0.25258364541574546, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1045, Loss: 0.43159866698549565, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1046, Loss: 0.3825044047522442, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1047, Loss: 0.4343728927551508, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1048, Loss: 0.23495164841104815, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1049, Loss: 0.5662657748120443, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1050, Loss: 0.48404373288446345, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1051, Loss: 0.43860241996258226, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1052, Loss: 0.56658644070111, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1053, Loss: 0.3418527955793227, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1054, Loss: 0.2641783473433277, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1055, Loss: 0.3037642185762347, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1056, Loss: 0.3274509142294464, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1057, Loss: 0.3567655767016405, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1058, Loss: 0.4166378342870257, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1059, Loss: 0.27722052159783445, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1060, Loss: 0.39095812552419335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1061, Loss: 0.27614839828585624, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1062, Loss: 0.46936086333647037, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1063, Loss: 0.3244959994093024, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1064, Loss: 0.396812422263622, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1065, Loss: 0.42421070445463804, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1066, Loss: 0.36436258554351, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1067, Loss: 0.29426126310106065, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1068, Loss: 0.35071246807678347, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1069, Loss: 0.254136842106025, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1070, Loss: 0.25429230792155816, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1071, Loss: 0.31084301015434646, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1072, Loss: 0.22881026858026593, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1073, Loss: 0.4643523659051633, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1074, Loss: 0.37387907693210554, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1075, Loss: 0.47394046139530666, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1076, Loss: 0.3771139307954384, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1077, Loss: 0.2546581768053825, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1078, Loss: 0.3805716473817268, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1079, Loss: 0.38498483484995166, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1080, Loss: 0.25763671594230736, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1081, Loss: 0.30412486709170694, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1082, Loss: 0.2946284398243175, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1083, Loss: 0.35860925955870776, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1084, Loss: 0.386152274309873, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1085, Loss: 0.6119093287823097, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1086, Loss: 0.3941265246136738, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1087, Loss: 0.4003531411866653, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1088, Loss: 0.5933004552798673, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1089, Loss: 0.31683625164795426, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1090, Loss: 0.4297098789909187, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1091, Loss: 0.3426542671649172, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1092, Loss: 0.4738380346445645, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1093, Loss: 0.34714842408718993, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1094, Loss: 0.583509336506603, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1095, Loss: 0.2666020852545641, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1096, Loss: 0.4692539169702027, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1097, Loss: 0.26452469037292387, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1098, Loss: 0.2634461314184655, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1099, Loss: 0.2576917700714473, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1100, Loss: 0.25250543604629505, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1101, Loss: 0.6866638534334958, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1102, Loss: 0.24622786288528195, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1103, Loss: 0.37983418565572913, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1104, Loss: 0.5750175758613637, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1105, Loss: 0.25014068546803453, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1106, Loss: 0.40687586199003745, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1107, Loss: 0.3927667843844035, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1108, Loss: 0.29012017409587565, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1109, Loss: 0.28134802265418846, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1110, Loss: 0.2810690106925914, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1111, Loss: 0.6990030925348358, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1112, Loss: 0.3135301374954386, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1113, Loss: 0.2634870133451432, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1114, Loss: 0.3606748910440258, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1115, Loss: 0.2934205480511042, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1116, Loss: 0.4265055476528171, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1117, Loss: 0.4073398062682164, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1118, Loss: 0.31386007354199047, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1119, Loss: 0.45788292780678985, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1120, Loss: 0.2930118351196908, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1121, Loss: 0.3811025389984309, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1122, Loss: 0.2660478434135201, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1123, Loss: 0.41709875960023735, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1124, Loss: 0.30093345583454806, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1125, Loss: 0.2547524572148726, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1126, Loss: 0.2907099490873549, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1127, Loss: 0.4445808064150194, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1128, Loss: 0.2458207801841966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1129, Loss: 0.27428251038493434, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1130, Loss: 0.26409347962393925, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1131, Loss: 0.6575010593561137, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1132, Loss: 0.3215496505254604, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1133, Loss: 0.3508091069314775, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1134, Loss: 0.4028944492088187, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1135, Loss: 0.4237498191558411, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1136, Loss: 0.41355109523309164, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1137, Loss: 0.6577312363460766, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1138, Loss: 0.45718678482231156, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1139, Loss: 0.2677691944904585, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1140, Loss: 0.30728153568937433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1141, Loss: 0.6775441878644957, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1142, Loss: 0.3379613887512022, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1143, Loss: 0.3082288616331673, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1144, Loss: 0.3185180960386766, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1145, Loss: 0.3823142683066968, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1146, Loss: 0.22955163828361658, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1147, Loss: 0.463429448041911, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1148, Loss: 0.26954045537513843, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1149, Loss: 0.48501824170174934, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1150, Loss: 0.5621576669948833, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1151, Loss: 0.42783305122808457, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1152, Loss: 0.24915422750279886, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1153, Loss: 0.250636352732998, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1154, Loss: 0.33337776892253695, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1155, Loss: 0.33226962658392223, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1156, Loss: 0.3134365865237614, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1157, Loss: 0.28180613567734314, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1158, Loss: 0.2792513382877332, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1159, Loss: 0.36111674300343677, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1160, Loss: 0.46213346803069755, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1161, Loss: 0.3725057637458722, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1162, Loss: 0.3789337119918188, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1163, Loss: 0.3428093585502159, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1164, Loss: 0.4003277407835493, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1165, Loss: 0.3397183928870193, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1166, Loss: 0.39327298215675377, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1167, Loss: 0.24086902945005517, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1168, Loss: 0.280071299275071, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1169, Loss: 0.3619464679217198, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1170, Loss: 0.30761932895804833, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1171, Loss: 0.5605222245566885, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1172, Loss: 0.3095437322592854, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1173, Loss: 0.2645268792891424, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1174, Loss: 0.3658894272385652, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1175, Loss: 0.4329645957952226, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1176, Loss: 0.2658161827424368, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1177, Loss: 0.2522362070219984, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1178, Loss: 0.24369709668268882, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1179, Loss: 0.3302303448842434, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1180, Loss: 0.370497289701351, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1181, Loss: 0.28525145009769487, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1182, Loss: 0.40269037759044196, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1183, Loss: 0.3743129722685764, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1184, Loss: 0.4930813055778408, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1185, Loss: 0.22340702633559228, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1186, Loss: 0.25387701678911667, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1187, Loss: 0.3584169783324031, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1188, Loss: 0.4254332344758923, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1189, Loss: 0.2634720033989636, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1190, Loss: 0.33137287568777457, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1191, Loss: 0.6235783711773353, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1192, Loss: 0.309058835239192, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1193, Loss: 0.4257878885223839, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1194, Loss: 0.420270124161409, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1195, Loss: 0.3208314505794479, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1196, Loss: 0.3536941375387888, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1197, Loss: 0.26186761867491043, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1198, Loss: 0.3576322662206005, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1199, Loss: 0.37441368059638713, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1200, Loss: 0.2599630379549667, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1201, Loss: 0.2940099860658991, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1202, Loss: 0.2660560707303983, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1203, Loss: 0.3981829997501609, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1204, Loss: 0.31374213019219255, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1205, Loss: 0.26819796454361144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1206, Loss: 0.329808864383748, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1207, Loss: 0.3940026199360571, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1208, Loss: 0.29846626998669534, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1209, Loss: 0.22785421863546249, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1210, Loss: 0.27628666722669515, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1211, Loss: 0.47241795109911683, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1212, Loss: 0.2772350231217583, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1213, Loss: 0.31938524415208736, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1214, Loss: 0.27059064330826144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1215, Loss: 0.36520673407219817, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1216, Loss: 0.3155001105726688, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1217, Loss: 0.2499402930627431, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1218, Loss: 0.4012911487925787, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1219, Loss: 0.28924158515459975, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1220, Loss: 0.2914454488413133, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1221, Loss: 0.45751202850215317, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1222, Loss: 0.5431138401426775, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1223, Loss: 0.24579132238176493, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1224, Loss: 0.3294970363932094, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1225, Loss: 0.4519977487954462, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1226, Loss: 0.448974893194045, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1227, Loss: 0.5103753060217231, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1228, Loss: 0.27549296522333366, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1229, Loss: 0.46432477580876197, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1230, Loss: 0.2491443620153535, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1231, Loss: 0.45645786432132696, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1232, Loss: 0.32509515061184974, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1233, Loss: 0.3322727796436771, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1234, Loss: 0.38919262823745415, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1235, Loss: 0.5519312937188655, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1236, Loss: 0.5288151592622903, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1237, Loss: 0.23333004173962715, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1238, Loss: 0.6668103896832847, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1239, Loss: 0.39361182517098886, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1240, Loss: 0.4002615443429351, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1241, Loss: 0.3238191854828726, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1242, Loss: 0.26469301076245794, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1243, Loss: 0.28922544669021244, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1244, Loss: 0.232242193776536, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1245, Loss: 0.5155469986593967, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1246, Loss: 0.3292860818857966, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1247, Loss: 0.3454128219888167, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1248, Loss: 0.3879989022973335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1249, Loss: 0.3882120887447604, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1250, Loss: 0.4639586736649106, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1251, Loss: 0.3571999464811967, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1252, Loss: 0.2911870521405221, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1253, Loss: 0.34452601926574344, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1254, Loss: 0.26345178400108016, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1255, Loss: 0.2502437276350191, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1256, Loss: 0.452859493877391, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1257, Loss: 0.3027411292509906, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1258, Loss: 0.22563927314848273, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1259, Loss: 0.41238152903330827, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1260, Loss: 0.27282413878734446, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1261, Loss: 0.2737960919064662, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1262, Loss: 0.46643694949149495, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1263, Loss: 0.41136293299695725, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1264, Loss: 0.27311871619366446, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1265, Loss: 0.27730876863551657, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1266, Loss: 0.28392215404769605, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1267, Loss: 0.2936653295817034, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1268, Loss: 0.407409015029681, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1269, Loss: 0.34234954322769495, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1270, Loss: 0.28870830794991775, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1271, Loss: 0.4001242692805627, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1272, Loss: 0.3343092562647564, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1273, Loss: 0.33096719265219926, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1274, Loss: 0.40251025238764504, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1275, Loss: 0.31083905898648606, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1276, Loss: 0.42792760159733156, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1277, Loss: 0.33231681032297533, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1278, Loss: 0.34839028947768724, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1279, Loss: 0.231002207254071, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1280, Loss: 0.4844950005843941, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1281, Loss: 0.5716634261029483, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1282, Loss: 0.24029310154530759, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1283, Loss: 0.3627439087641616, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1284, Loss: 0.3747847016996052, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1285, Loss: 0.25379302894396333, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1286, Loss: 0.27885183799703217, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1287, Loss: 0.3292823924394009, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1288, Loss: 0.532068277139567, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1289, Loss: 0.32242323939616857, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1290, Loss: 0.33398774265432624, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1291, Loss: 0.5439164121771861, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1292, Loss: 0.28065682744319115, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1293, Loss: 0.5334521283398344, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1294, Loss: 0.27316364036865337, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1295, Loss: 0.30525212103400373, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1296, Loss: 0.3859568119780474, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1297, Loss: 0.3751971600839953, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1298, Loss: 0.48222113974991965, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1299, Loss: 0.4959356858929128, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1300, Loss: 0.38853714830631514, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1301, Loss: 0.2745228014346619, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1302, Loss: 0.2774360593412377, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1303, Loss: 0.3654738487373168, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1304, Loss: 0.31204783592037844, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1305, Loss: 0.3219110230686987, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1306, Loss: 0.3591373233426632, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1307, Loss: 0.45346727172880097, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1308, Loss: 0.29834023354660605, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1309, Loss: 0.42638617988313643, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1310, Loss: 0.3210480313417313, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1311, Loss: 0.2589152034333485, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1312, Loss: 0.390112526267386, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1313, Loss: 0.3640206485382329, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1314, Loss: 0.5520349329346644, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1315, Loss: 0.28237424784364845, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1316, Loss: 0.2765310282471551, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1317, Loss: 0.25969740472175246, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1318, Loss: 0.30314548891397697, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1319, Loss: 0.28909046617563433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1320, Loss: 0.2695018056082168, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1321, Loss: 0.43608898238589605, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1322, Loss: 0.43504606823182124, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1323, Loss: 0.3088446554999138, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1324, Loss: 0.4222213153055877, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1325, Loss: 0.5643776655708392, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1326, Loss: 0.3647484286979815, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1327, Loss: 0.3224706322388011, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1328, Loss: 0.40677939818411435, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1329, Loss: 0.29970042103906336, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1330, Loss: 0.2992589643029058, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1331, Loss: 0.21963321482741555, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1332, Loss: 0.2565179444411872, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1333, Loss: 0.5100096459442968, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1334, Loss: 0.38990988869769544, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1335, Loss: 0.36878311882061954, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1336, Loss: 1.0486364257642276, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1337, Loss: 0.27790958145877703, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1338, Loss: 0.35729733935830943, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1339, Loss: 0.6555745380171323, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1340, Loss: 0.3695930299752429, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1341, Loss: 0.30377803065659725, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1342, Loss: 0.33398346010078295, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1343, Loss: 0.35273444097017703, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1344, Loss: 0.33955859762254476, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1345, Loss: 0.31035690486388995, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1346, Loss: 0.23883906859726226, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1347, Loss: 0.505317311952741, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1348, Loss: 0.4094631148619013, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1349, Loss: 0.543132906387132, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1350, Loss: 0.30455983395008657, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1351, Loss: 0.30201175147469833, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1352, Loss: 0.3754236565783923, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1353, Loss: 0.31329721311985853, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1354, Loss: 0.4163854075723065, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1355, Loss: 0.3154564901237167, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1356, Loss: 0.31994118725189347, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1357, Loss: 0.33272800014114506, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1358, Loss: 0.3126361076189138, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1359, Loss: 0.29715745256883264, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1360, Loss: 0.25322347945701773, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1361, Loss: 0.28019160689085554, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1362, Loss: 0.2503557953390255, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1363, Loss: 0.36728801120735305, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1364, Loss: 0.2822551527328569, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1365, Loss: 0.24900057523470198, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1366, Loss: 0.30812716646184135, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1367, Loss: 0.324963063331602, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1368, Loss: 0.4077480432263102, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1369, Loss: 0.3584549328457421, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1370, Loss: 0.4584492163038321, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1371, Loss: 0.6102834760368139, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1372, Loss: 0.5355570428236651, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1373, Loss: 0.2844798520331563, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1374, Loss: 0.42551331217927824, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1375, Loss: 0.23269105006568513, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1376, Loss: 0.3626101919628618, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1377, Loss: 0.4890837982378776, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1378, Loss: 0.4080012976463971, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1379, Loss: 0.2821166099666914, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1380, Loss: 0.6927292020910991, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1381, Loss: 0.3842092746659127, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1382, Loss: 0.29729445881505356, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1383, Loss: 0.35343672051976177, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1384, Loss: 0.271481529418959, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1385, Loss: 0.22845270083326394, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1386, Loss: 0.447050274537668, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1387, Loss: 0.24305355955641111, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1388, Loss: 0.2807933827384322, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1389, Loss: 0.3422994182173004, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1390, Loss: 0.5183055809905879, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1391, Loss: 0.4625182135654291, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1392, Loss: 0.2758968539610237, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1393, Loss: 0.34822224377476707, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1394, Loss: 0.2940141059338133, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1395, Loss: 0.3528557740045792, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1396, Loss: 0.27863165346857804, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1397, Loss: 0.22388633692785365, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1398, Loss: 0.28024763453832763, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1399, Loss: 0.3990771111961774, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1400, Loss: 0.33123614104432864, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1401, Loss: 0.28678544231349457, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1402, Loss: 0.3177842920906667, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1403, Loss: 0.24482042458742126, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1404, Loss: 0.51040510690497, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1405, Loss: 0.23296436964744882, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1406, Loss: 0.36696847908549, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1407, Loss: 0.4483960226204139, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1408, Loss: 0.3048639696194406, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1409, Loss: 0.2314542962210443, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1410, Loss: 0.3336141967910357, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1411, Loss: 0.4251074394013379, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1412, Loss: 0.42240168526002486, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1413, Loss: 0.5350524324320184, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1414, Loss: 0.3010072914150387, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1415, Loss: 0.40543620435736927, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1416, Loss: 0.5224949075794365, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1417, Loss: 0.3111729938535418, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1418, Loss: 0.3489784386884601, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1419, Loss: 0.3705658185920476, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1420, Loss: 0.27096967375790093, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1421, Loss: 0.2225400272039917, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1422, Loss: 0.35950192088704747, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1423, Loss: 0.4250379370425962, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1424, Loss: 0.3394736073242742, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1425, Loss: 0.49973558217630776, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1426, Loss: 0.3265999016402193, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1427, Loss: 0.27767454878015907, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1428, Loss: 0.5320004045726259, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1429, Loss: 0.3357178290316505, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1430, Loss: 0.4472147620011063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1431, Loss: 0.2409580634168411, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1432, Loss: 0.4562889042848855, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1433, Loss: 0.30828932883614923, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1434, Loss: 0.374636607422253, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1435, Loss: 0.3325552829005612, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1436, Loss: 0.2613985579500756, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1437, Loss: 0.32051446691268176, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1438, Loss: 0.308843768212748, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1439, Loss: 0.2499704365686373, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1440, Loss: 0.37685491523919074, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1441, Loss: 0.3650956698141182, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1442, Loss: 0.30620339736870983, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1443, Loss: 0.3323907472186898, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1444, Loss: 0.3657116138743701, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1445, Loss: 0.506661116026706, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1446, Loss: 0.23996473964827916, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1447, Loss: 0.22222092569299307, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1448, Loss: 0.25661450170574013, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1449, Loss: 0.44610813083892276, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1450, Loss: 0.25071728135043614, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1451, Loss: 0.28349377555247535, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1452, Loss: 0.3077881071498926, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1453, Loss: 0.2609254152098372, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1454, Loss: 0.2248029914091876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1455, Loss: 0.30977695182185705, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1456, Loss: 0.2466652745972178, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1457, Loss: 0.2960904166978512, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1458, Loss: 0.325704240278904, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1459, Loss: 0.412872735534493, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1460, Loss: 0.2640771440723835, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1461, Loss: 0.379227760686538, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1462, Loss: 0.295205889019223, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1463, Loss: 0.28084489124480944, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1464, Loss: 0.2909874080390288, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1465, Loss: 0.3172692567613139, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1466, Loss: 0.2655811935760385, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1467, Loss: 0.34338341763051095, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1468, Loss: 0.3649947557866876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1469, Loss: 0.41809253034862126, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1470, Loss: 0.4428901002970489, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1471, Loss: 0.2775503502893484, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1472, Loss: 0.41594431825585915, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1473, Loss: 0.2682468269407663, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1474, Loss: 0.25510019008736984, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1475, Loss: 0.5272259686160262, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1476, Loss: 0.3541491143316426, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1477, Loss: 0.33534286520140855, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1478, Loss: 0.32575059028522396, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1479, Loss: 0.24915843737869736, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1480, Loss: 0.45727489388782144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1481, Loss: 0.30198366524980114, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1482, Loss: 0.2764246569684625, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1483, Loss: 0.28939816820155645, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1484, Loss: 0.39646053170075746, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1485, Loss: 0.4599301641149053, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1486, Loss: 0.23498188488199587, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1487, Loss: 0.27572323605006066, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1488, Loss: 0.26454632221703606, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1489, Loss: 0.2551041936001316, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1490, Loss: 0.29528384791800544, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1491, Loss: 0.24609382623344422, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1492, Loss: 0.3757417942588879, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1493, Loss: 0.23025754587559247, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1494, Loss: 0.3417259419678682, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1495, Loss: 0.2953351536683422, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1496, Loss: 0.29493780556535876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1497, Loss: 0.3509111037517192, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1498, Loss: 0.3421714196526996, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1499, Loss: 0.5012263662552229, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1500, Loss: 0.42236238682666083, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1501, Loss: 0.2995984400278107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1502, Loss: 0.30497272006944315, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1503, Loss: 0.31299215639752564, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1504, Loss: 0.2747764262965678, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1505, Loss: 0.3578322195602376, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1506, Loss: 0.26228673749844356, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1507, Loss: 0.2885116297673201, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1508, Loss: 0.3368725372285367, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1509, Loss: 0.3154874410667676, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1510, Loss: 0.25990377932284286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1511, Loss: 0.29335742570882395, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1512, Loss: 1.1478268383155692, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1513, Loss: 0.4011509016178665, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1514, Loss: 0.28138520303872827, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1515, Loss: 0.5235954161291436, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1516, Loss: 0.38277011053507193, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1517, Loss: 0.30257527022675307, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1518, Loss: 0.2999003448164695, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1519, Loss: 0.4071413362171251, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1520, Loss: 0.3988745782788142, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1521, Loss: 0.2452804695533254, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1522, Loss: 0.5002736176506779, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1523, Loss: 0.29541268474542154, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1524, Loss: 0.27887715526155477, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1525, Loss: 0.3466877886731242, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1526, Loss: 0.2892897111517357, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1527, Loss: 0.5245872276313123, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1528, Loss: 0.35920763735939276, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1529, Loss: 0.3366211565413008, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1530, Loss: 0.2824278131500275, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1531, Loss: 0.3043660428745778, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1532, Loss: 0.3615134423434043, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1533, Loss: 0.3484947152510789, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1534, Loss: 0.23461687924675964, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1535, Loss: 0.28231390040829235, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1536, Loss: 0.4630409044249466, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1537, Loss: 0.4192684254954274, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1538, Loss: 0.26615433629787416, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1539, Loss: 0.25790500099535063, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1540, Loss: 0.34870137220097963, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1541, Loss: 0.3099265555037517, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1542, Loss: 0.30127895895134793, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1543, Loss: 0.3554545814518198, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1544, Loss: 0.2977821561861511, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1545, Loss: 0.3430110994209875, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1546, Loss: 0.2368085288346922, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1547, Loss: 0.3679658005530896, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1548, Loss: 0.3166199930619279, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1549, Loss: 0.3503471424612047, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1550, Loss: 0.3236767214637784, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1551, Loss: 0.4678337820070348, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1552, Loss: 0.2210096353492378, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1553, Loss: 0.3703143565412491, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1554, Loss: 0.5777476647292101, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1555, Loss: 0.45860516270391116, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1556, Loss: 0.4379158451633527, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1557, Loss: 0.33516923331239895, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1558, Loss: 0.23654641330495157, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1559, Loss: 0.26300322665187126, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1560, Loss: 0.4597727387707808, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1561, Loss: 0.3292065076125854, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1562, Loss: 0.7449983682577619, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1563, Loss: 0.26754057656627317, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1564, Loss: 0.30944680141946107, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1565, Loss: 0.2813991057542079, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1566, Loss: 0.3326176961193804, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1567, Loss: 0.26658409687002643, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1568, Loss: 0.28800975791135364, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1569, Loss: 0.511950664293069, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1570, Loss: 0.27426723589081853, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1571, Loss: 0.24927283971510336, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1572, Loss: 0.32052540636055665, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1573, Loss: 0.38331065882634807, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1574, Loss: 0.22739952986181933, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1575, Loss: 0.38466774223352895, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1576, Loss: 0.3019287294803713, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1577, Loss: 0.2293223505209045, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1578, Loss: 0.3536989540124922, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1579, Loss: 0.6428914268967196, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1580, Loss: 0.5315391792722901, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1581, Loss: 0.38121015884583875, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1582, Loss: 0.4232872629948336, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1583, Loss: 0.3877134493602906, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1584, Loss: 0.3293861191589543, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1585, Loss: 0.2363613667675659, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1586, Loss: 0.3077908497473175, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1587, Loss: 0.2501265117426273, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1588, Loss: 0.37628882597067925, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1589, Loss: 0.31578899038826286, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1590, Loss: 0.3714065967647106, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1591, Loss: 0.2229560398229026, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1592, Loss: 0.37479755795782577, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1593, Loss: 0.25069778808808596, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1594, Loss: 0.451887452248749, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1595, Loss: 0.2541632914639337, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1596, Loss: 0.283229673443624, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1597, Loss: 0.2782346729281784, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1598, Loss: 0.3457204606891337, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1599, Loss: 0.313789403778216, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1600, Loss: 0.46317658240961396, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1601, Loss: 0.47809672027546685, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1602, Loss: 0.2962964973083057, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1603, Loss: 0.4244854885047165, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1604, Loss: 0.43719770456937085, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1605, Loss: 0.4201701652978384, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1606, Loss: 0.5116325526583185, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1607, Loss: 0.27519762752765936, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1608, Loss: 0.48912908026442614, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1609, Loss: 0.26789426579325404, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1610, Loss: 0.2860229280083906, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1611, Loss: 0.5112939588471275, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1612, Loss: 0.30150547115784765, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1613, Loss: 0.28717796421051334, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1614, Loss: 0.37184441452542416, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1615, Loss: 0.2920824268025295, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1616, Loss: 0.5772906358263866, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1617, Loss: 0.2861593040360341, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1618, Loss: 0.3703217017129315, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1619, Loss: 0.2473394820862953, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1620, Loss: 0.47385889526999236, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1621, Loss: 0.26960285918462, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1622, Loss: 0.24686678162190367, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1623, Loss: 0.2922436462879794, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1624, Loss: 0.3957847242377615, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1625, Loss: 0.5189438269963641, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1626, Loss: 0.2336097675787528, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1627, Loss: 0.31006573449271657, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1628, Loss: 0.42421908610259007, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1629, Loss: 0.2293938813745959, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1630, Loss: 0.2758785484946051, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1631, Loss: 0.2515021588831724, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1632, Loss: 0.28116758100599204, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1633, Loss: 0.3068031355142621, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1634, Loss: 0.4525540386648246, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1635, Loss: 0.2583355814980717, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1636, Loss: 0.2604811089990555, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1637, Loss: 0.2796386795382731, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1638, Loss: 0.2192629536757748, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1639, Loss: 0.25308064821859827, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1640, Loss: 0.41528977930906796, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1641, Loss: 0.3685322776221457, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1642, Loss: 0.3331133449592267, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1643, Loss: 0.293470428970123, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1644, Loss: 0.5414866625373321, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1645, Loss: 0.37017152025832656, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1646, Loss: 0.3459318990579132, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1647, Loss: 0.23083177121280352, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1648, Loss: 0.3504322458922581, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1649, Loss: 0.3029853570603673, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1650, Loss: 0.49437527336049947, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1651, Loss: 0.38088137355732476, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1652, Loss: 0.2952789279657174, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1653, Loss: 0.30025425280890844, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1654, Loss: 0.2914326617336109, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1655, Loss: 0.27366788755555704, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1656, Loss: 0.3611045247392358, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1657, Loss: 0.24113467485351744, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1658, Loss: 0.33239046912034487, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1659, Loss: 0.3078459472330657, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1660, Loss: 0.3261925650991221, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1661, Loss: 0.43823892314661617, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1662, Loss: 0.49766846236579576, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1663, Loss: 0.41328505758880413, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1664, Loss: 0.3222162062937026, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1665, Loss: 0.25979137706025696, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1666, Loss: 0.6087401467177556, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1667, Loss: 0.325752947138818, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1668, Loss: 0.6101959954757794, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1669, Loss: 0.35308574001427245, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1670, Loss: 0.24529789197176954, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1671, Loss: 0.326302907238868, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1672, Loss: 0.3652108074072064, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1673, Loss: 0.2681070140256108, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1674, Loss: 0.44732281603111546, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1675, Loss: 0.3808751768175744, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1676, Loss: 0.394931819210005, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1677, Loss: 0.43689250043833994, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1678, Loss: 0.5564124736858278, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1679, Loss: 0.2270189566128272, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1680, Loss: 0.3498974453005991, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1681, Loss: 0.2762665619162969, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1682, Loss: 0.3187426568879963, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1683, Loss: 0.3836154802699876, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1684, Loss: 0.3678460877093121, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1685, Loss: 0.2914955481534862, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1686, Loss: 0.6721611504849437, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1687, Loss: 0.300254435845039, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1688, Loss: 0.2929069531402443, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1689, Loss: 0.4777590364966851, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1690, Loss: 0.27643961337711076, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1691, Loss: 0.5413425300431104, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1692, Loss: 0.3933993979323152, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1693, Loss: 0.28288115913093786, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1694, Loss: 0.5109132962379628, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1695, Loss: 0.25180909690411857, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1696, Loss: 0.35325752702803126, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1697, Loss: 0.4996091565159815, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1698, Loss: 0.31234936688026627, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1699, Loss: 0.3654872872814088, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1700, Loss: 0.35799562819767144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1701, Loss: 0.27714996641205664, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1702, Loss: 0.25722226336117476, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1703, Loss: 0.4155535955534755, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1704, Loss: 0.29996301147044657, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1705, Loss: 0.3263169226285305, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1706, Loss: 0.3210209660633746, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1707, Loss: 0.38101252384009887, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1708, Loss: 0.24043568715547006, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1709, Loss: 0.2576135748129834, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1710, Loss: 0.29816598402474026, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1711, Loss: 0.33504803208034994, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1712, Loss: 0.6609469580298213, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1713, Loss: 0.47995608374461374, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1714, Loss: 0.25549693188072176, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1715, Loss: 0.3855612934862279, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1716, Loss: 0.41028178201697635, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1717, Loss: 0.24281378609692253, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1718, Loss: 0.4135303099590343, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1719, Loss: 0.25662422547404806, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1720, Loss: 0.3355867531300513, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1721, Loss: 0.35522230708535557, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1722, Loss: 0.3253906417220644, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1723, Loss: 0.24351870200502962, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1724, Loss: 0.221528910164225, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1725, Loss: 0.25371281144960867, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1726, Loss: 0.2492250406679867, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1727, Loss: 0.33383798994033653, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1728, Loss: 0.3138963283995218, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1729, Loss: 0.34780050744889757, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1730, Loss: 0.3558107727570568, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1731, Loss: 0.28654760112940825, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1732, Loss: 0.2893430046363997, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1733, Loss: 0.6927498200790365, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1734, Loss: 0.29812311948420717, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1735, Loss: 0.3043788408431863, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1736, Loss: 0.5312109056317824, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1737, Loss: 0.30889978822467473, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1738, Loss: 0.22608387458078888, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1739, Loss: 0.37433503082824493, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1740, Loss: 0.3109579680779433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1741, Loss: 0.4099682120306356, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1742, Loss: 0.3077118103238306, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1743, Loss: 0.34171272821629, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1744, Loss: 0.5972507726038041, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1745, Loss: 0.31759125072667965, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1746, Loss: 0.3264716343988153, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1747, Loss: 0.3531465377555909, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1748, Loss: 0.4293767693281891, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1749, Loss: 0.48677694653964904, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1750, Loss: 0.5055025424978737, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1751, Loss: 0.2782978552025749, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1752, Loss: 0.3549162079262911, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1753, Loss: 0.24842072110067923, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1754, Loss: 0.42114094492328213, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1755, Loss: 0.26908425652531365, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1756, Loss: 0.29375663404181773, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1757, Loss: 0.538339458392842, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1758, Loss: 0.2524962991843212, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1759, Loss: 0.25638515805709483, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1760, Loss: 0.45297700305085037, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1761, Loss: 0.27234126829087335, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1762, Loss: 0.853436165649207, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1763, Loss: 0.695081112416782, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1764, Loss: 0.28177248728574544, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1765, Loss: 0.3356846142752527, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1766, Loss: 0.3134913712757704, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1767, Loss: 0.2872660440717041, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1768, Loss: 0.2865408593283372, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1769, Loss: 0.3838534350040198, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1770, Loss: 0.4231137629964211, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1771, Loss: 0.4499548919847831, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1772, Loss: 0.3025686174894987, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1773, Loss: 0.3694276594764321, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1774, Loss: 0.2898982145631217, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1775, Loss: 0.2600020601663583, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1776, Loss: 0.3746145800145635, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1777, Loss: 0.3511485362136519, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1778, Loss: 0.27923524682443057, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1779, Loss: 0.2961701665973771, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1780, Loss: 0.2822321396284783, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1781, Loss: 0.39459320545550847, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1782, Loss: 0.5870843690435248, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1783, Loss: 0.34766123454560527, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1784, Loss: 0.2871756816253027, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1785, Loss: 0.3646701363101264, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1786, Loss: 0.367486732767195, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1787, Loss: 0.28935534099201043, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1788, Loss: 0.43938392736179116, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1789, Loss: 0.32156546182307605, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1790, Loss: 0.5360663589231149, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1791, Loss: 0.2613608240118115, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1792, Loss: 0.4486255835158488, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1793, Loss: 0.4460164917306011, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1794, Loss: 0.30290346762534837, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1795, Loss: 0.24189229216441419, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1796, Loss: 0.45732594487824496, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1797, Loss: 0.35344469315532834, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1798, Loss: 0.23546349459992483, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1799, Loss: 0.2563427928247252, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1800, Loss: 0.24349997904480725, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1801, Loss: 0.2752512752626033, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1802, Loss: 0.3252565490393938, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1803, Loss: 0.2538382762623163, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1804, Loss: 0.41713326533301665, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1805, Loss: 0.3640680636482308, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1806, Loss: 0.35518927647976317, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1807, Loss: 0.5772458011884356, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1808, Loss: 0.55474523293145, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1809, Loss: 0.2919085741248135, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1810, Loss: 0.29292464067752144, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1811, Loss: 0.2526212744619283, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1812, Loss: 0.3456350421890097, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1813, Loss: 0.29423775593955404, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1814, Loss: 0.5428270348295411, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1815, Loss: 0.24137921411977933, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1816, Loss: 0.2528862600391682, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1817, Loss: 0.6115287964798322, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1818, Loss: 0.38301960289316056, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1819, Loss: 0.2477808091412954, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1820, Loss: 0.37830865427984395, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1821, Loss: 0.35103584336075433, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1822, Loss: 0.35686350032954506, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1823, Loss: 0.23942638242097905, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1824, Loss: 0.22835154707411492, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1825, Loss: 0.48313405880017934, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1826, Loss: 0.45032278354382893, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1827, Loss: 0.3520149054019533, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1828, Loss: 0.2995510107242165, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1829, Loss: 0.3745839854465829, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1830, Loss: 0.4450251306694132, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1831, Loss: 0.338683512933763, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1832, Loss: 0.2876099356527484, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1833, Loss: 0.24623544934208735, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1834, Loss: 0.35548270342136906, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1835, Loss: 0.44246483102218137, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1836, Loss: 0.24270676772941016, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1837, Loss: 0.2436026177162528, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1838, Loss: 0.35836102088405575, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1839, Loss: 0.3343998884885736, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1840, Loss: 0.5575129514645422, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1841, Loss: 0.3353095317420163, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1842, Loss: 0.34100526770942785, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1843, Loss: 0.264626634975366, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1844, Loss: 0.25615829805740564, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1845, Loss: 0.2879354371806382, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1846, Loss: 0.34076244476404405, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1847, Loss: 0.33793173847768776, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1848, Loss: 0.27398865816472295, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1849, Loss: 0.49115505949989613, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1850, Loss: 0.3375732039909003, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1851, Loss: 0.3844024646540765, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1852, Loss: 0.6436816415883686, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1853, Loss: 0.3133259650742337, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1854, Loss: 0.3488099830987683, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1855, Loss: 0.23628288868345731, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1856, Loss: 0.45009396114963, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1857, Loss: 0.26204028134916635, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1858, Loss: 0.41241948024796865, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1859, Loss: 0.629224136521102, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1860, Loss: 0.5427032697211696, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1861, Loss: 0.4555581054199862, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1862, Loss: 0.3768272243883677, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1863, Loss: 0.6071194687127366, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1864, Loss: 0.3461155910735816, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1865, Loss: 0.38080030816870997, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1866, Loss: 0.31402825626555836, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1867, Loss: 0.2761514467405768, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1868, Loss: 0.29908977118282054, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1869, Loss: 0.7441855802282795, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1870, Loss: 0.3976976694993515, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1871, Loss: 0.274395455091222, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1872, Loss: 0.3956487237813655, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1873, Loss: 0.27924880107686606, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1874, Loss: 0.2378539504583871, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Batch 1875, Loss: 0.447504096690183, Batch Size: 32, Learning Rate: 9.617312648437496e-05\n",
      "Epoch 9, Updated Learning Rate: 8.174715751171873e-05\n",
      "Epoch 9, Average Loss: 0.3556499168978891, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1, Loss: 0.7150112762682607, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 2, Loss: 0.3256177414639603, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 3, Loss: 0.29663668827081735, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 4, Loss: 0.43858421166132755, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 5, Loss: 0.3796126345268454, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 6, Loss: 0.37108668713915494, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 7, Loss: 0.45138340850827996, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 8, Loss: 0.5741975812339414, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 9, Loss: 0.25073298897347596, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 10, Loss: 0.477732252491169, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 11, Loss: 0.24546123289481064, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 12, Loss: 0.5907485658307285, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 13, Loss: 0.4120431797680872, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 14, Loss: 0.4245394260680328, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 15, Loss: 0.2812391304886147, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 16, Loss: 0.3656922049710294, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 17, Loss: 0.33956780237311746, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 18, Loss: 0.3370519091537828, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 19, Loss: 0.29406002361419037, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 20, Loss: 0.35545807507745497, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 21, Loss: 0.45894673350056336, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 22, Loss: 0.3289214942946877, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 23, Loss: 0.2761702420868841, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 24, Loss: 0.558928071500426, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 25, Loss: 0.4362252817043382, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 26, Loss: 0.25330245880032537, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 27, Loss: 0.3648514862776656, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 28, Loss: 0.2783635504421629, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 29, Loss: 0.4098975572222028, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 30, Loss: 0.2876681005512124, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 31, Loss: 0.3494875582172898, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 32, Loss: 0.22722939812251214, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 33, Loss: 0.42914847388550015, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 34, Loss: 0.3667295204438161, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 35, Loss: 0.29375900913074193, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 36, Loss: 0.31962263749900277, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 37, Loss: 0.28217305060078535, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 38, Loss: 0.428326443475678, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 39, Loss: 0.26426675987493403, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 40, Loss: 0.5300965314150381, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 41, Loss: 0.3140708366415384, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 42, Loss: 0.22629241227003025, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 43, Loss: 0.6320287682209907, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 44, Loss: 0.260407887038291, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 45, Loss: 0.37809914083232515, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 46, Loss: 0.35075856045579396, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 47, Loss: 0.2572448757477792, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 48, Loss: 0.346106745481295, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 49, Loss: 0.7126206508656918, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 50, Loss: 0.2468820639910654, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 51, Loss: 0.4255850154670842, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 52, Loss: 0.2532056413623691, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 53, Loss: 0.3081046818165884, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 54, Loss: 0.449713016159439, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 55, Loss: 0.2705193902615666, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 56, Loss: 0.4269254441602902, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 57, Loss: 0.3698122706557777, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 58, Loss: 0.353058001560015, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 59, Loss: 0.2969512112651371, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 60, Loss: 0.28834134915774035, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 61, Loss: 0.2787073338969728, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 62, Loss: 0.4289644807683811, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 63, Loss: 0.3863057459381994, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 64, Loss: 0.2519283572848565, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 65, Loss: 0.34093156428285437, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 66, Loss: 0.3162633167562935, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 67, Loss: 0.36947033735945334, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 68, Loss: 0.2836716053150685, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 69, Loss: 0.4303098943944492, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 70, Loss: 0.5136253700212279, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 71, Loss: 0.5207581602824469, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 72, Loss: 0.22058665405004685, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 73, Loss: 0.2808265483919048, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 74, Loss: 0.4049792694004832, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 75, Loss: 0.43012013279439576, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 76, Loss: 0.25071321919008344, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 77, Loss: 0.27754639926435887, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 78, Loss: 0.5903192738945462, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 79, Loss: 0.31439947840477706, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 80, Loss: 0.5147411719117587, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 81, Loss: 0.31695370697413616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 82, Loss: 0.59332943020772, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 83, Loss: 0.26957900542813806, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 84, Loss: 0.3172820109007508, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 85, Loss: 0.3824333762627804, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 86, Loss: 0.3599472512845624, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 87, Loss: 0.3597643000846135, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 88, Loss: 0.22237279983033464, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 89, Loss: 0.28151704751640483, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 90, Loss: 0.29452552807439336, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 91, Loss: 0.28429684921696496, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 92, Loss: 0.36492480530362514, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 93, Loss: 0.3057378789274468, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 94, Loss: 0.24314647686006344, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 95, Loss: 0.48602416147621286, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 96, Loss: 0.40294869502954545, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 97, Loss: 0.29496770167017317, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 98, Loss: 0.25313967672621934, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 99, Loss: 0.2746047055209194, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 100, Loss: 0.32765439027027055, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 101, Loss: 0.333516064361347, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 102, Loss: 0.30117560297343837, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 103, Loss: 0.32137709329040753, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 104, Loss: 0.3032919091912161, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 105, Loss: 0.2875452492480845, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 106, Loss: 0.2543273765418047, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 107, Loss: 0.37097264357880805, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 108, Loss: 0.231089383104839, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 109, Loss: 0.34037414989572506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 110, Loss: 0.2850249753823009, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 111, Loss: 0.3001918920729062, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 112, Loss: 0.2985086894775531, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 113, Loss: 0.3697143637582798, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 114, Loss: 0.37493371608160675, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 115, Loss: 0.2421134472693254, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 116, Loss: 0.28074204565742855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 117, Loss: 0.5502592186931736, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 118, Loss: 0.3163758265943547, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 119, Loss: 0.37299238433499404, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 120, Loss: 0.46496699259470003, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 121, Loss: 0.33405199794398405, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 122, Loss: 0.24994113138689336, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 123, Loss: 0.3815564640174103, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 124, Loss: 0.34980874985964616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 125, Loss: 0.3494211954627655, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 126, Loss: 0.2604265866289052, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 127, Loss: 0.2971524609048117, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 128, Loss: 0.3421667559616994, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 129, Loss: 0.49572320616882837, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 130, Loss: 0.4029192254494471, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 131, Loss: 0.36184657055741243, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 132, Loss: 0.43888241987386356, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 133, Loss: 0.3613143161419118, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 134, Loss: 0.27366913062801773, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 135, Loss: 0.32305589590646716, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 136, Loss: 0.40822615096843606, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 137, Loss: 0.22608324625247744, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 138, Loss: 0.26358102261402777, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 139, Loss: 0.27143819520565426, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 140, Loss: 0.40810985702904995, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 141, Loss: 0.2966566931699677, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 142, Loss: 0.25268860659788694, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 143, Loss: 0.32014346994853515, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 144, Loss: 0.3871981098867584, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 145, Loss: 0.29583316522910397, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 146, Loss: 0.3995468257468994, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 147, Loss: 0.611894217827003, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 148, Loss: 0.28343604981473003, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 149, Loss: 0.3007929811929835, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 150, Loss: 0.394640305915816, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 151, Loss: 0.39166003358464785, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 152, Loss: 0.27229695327790643, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 153, Loss: 0.44591791117045343, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 154, Loss: 0.32205817753896393, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 155, Loss: 0.3136899638526576, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 156, Loss: 0.2901655436287311, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 157, Loss: 0.2835403389441365, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 158, Loss: 0.2826418320280899, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 159, Loss: 0.24072802240061708, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 160, Loss: 0.39370995569392664, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 161, Loss: 0.2697325765839218, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 162, Loss: 0.3297793038884889, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 163, Loss: 0.3091494234026767, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 164, Loss: 0.3694089042039523, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 165, Loss: 0.3106533306544685, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 166, Loss: 0.5126445966260147, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 167, Loss: 0.3776324513099288, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 168, Loss: 0.3056140767459513, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 169, Loss: 0.37193730614970955, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 170, Loss: 0.22548080749354643, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 171, Loss: 0.2622996354617257, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 172, Loss: 0.2750959608772776, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 173, Loss: 0.2818114468517391, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 174, Loss: 0.2702013655495985, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 175, Loss: 0.31604046493977056, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 176, Loss: 0.25399485759977375, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 177, Loss: 0.4099063916508585, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 178, Loss: 0.24218068445690577, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 179, Loss: 0.37342760190306096, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 180, Loss: 0.28045816448810035, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 181, Loss: 0.30556865880397815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 182, Loss: 0.41688732021376507, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 183, Loss: 0.3284272401659503, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 184, Loss: 0.31454761727762254, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 185, Loss: 0.2746851816969647, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 186, Loss: 0.2523849666967891, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 187, Loss: 0.8208885352933235, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 188, Loss: 0.3437770276545471, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 189, Loss: 0.5034573031759694, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 190, Loss: 0.22944960598867048, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 191, Loss: 0.29715482723870457, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 192, Loss: 0.31765842736214955, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 193, Loss: 0.24604251530949062, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 194, Loss: 0.2664194536820553, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 195, Loss: 0.4008254657153088, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 196, Loss: 0.4420601341363077, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 197, Loss: 0.431168598944091, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 198, Loss: 0.2708200619330964, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 199, Loss: 0.3767010198380567, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 200, Loss: 0.27607244225593414, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 201, Loss: 0.36120923783145786, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 202, Loss: 0.2946800721215789, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 203, Loss: 0.3008747681519738, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 204, Loss: 0.34917232964303957, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 205, Loss: 0.23346303697213988, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 206, Loss: 0.34853474485601876, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 207, Loss: 0.5735336973199093, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 208, Loss: 0.35514785307082447, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 209, Loss: 0.502295908054065, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 210, Loss: 0.2662032500835672, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 211, Loss: 0.32911194811038236, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 212, Loss: 0.4073445821017012, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 213, Loss: 0.301691388957257, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 214, Loss: 0.3192786252219117, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 215, Loss: 0.3583710837218631, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 216, Loss: 0.2408377521303601, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 217, Loss: 0.32067761173817066, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 218, Loss: 0.4088719076708693, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 219, Loss: 0.24414888289055967, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 220, Loss: 0.315165537182222, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 221, Loss: 0.23425198497954428, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 222, Loss: 0.238784500201042, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 223, Loss: 0.3321462990200382, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 224, Loss: 0.3586049501368851, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 225, Loss: 0.4311248944883417, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 226, Loss: 0.7826210969097274, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 227, Loss: 0.38354692414269465, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 228, Loss: 0.4779292548211095, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 229, Loss: 0.34459916670801494, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 230, Loss: 0.3606204395960048, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 231, Loss: 0.27155194575334207, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 232, Loss: 0.6594920365223085, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 233, Loss: 0.28219747040341203, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 234, Loss: 0.4140502349411206, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 235, Loss: 0.485435501242139, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 236, Loss: 0.3581997890050032, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 237, Loss: 0.3257254545620618, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 238, Loss: 0.27085109680263253, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 239, Loss: 0.2989947129861108, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 240, Loss: 0.2990631200715406, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 241, Loss: 0.23704154857092702, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 242, Loss: 0.26482072395506057, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 243, Loss: 0.30688762632353495, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 244, Loss: 0.36553416174416026, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 245, Loss: 0.39676928663986544, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 246, Loss: 0.3568573332610796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 247, Loss: 0.3217992746395849, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 248, Loss: 0.29619049543701614, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 249, Loss: 0.3160700504659233, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 250, Loss: 0.5287328167927133, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 251, Loss: 0.2904800293867237, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 252, Loss: 0.47827281821924394, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 253, Loss: 0.30452192193523187, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 254, Loss: 0.26932057432263573, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 255, Loss: 0.3326180048673202, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 256, Loss: 0.2630210147853905, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 257, Loss: 0.4093658982159569, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 258, Loss: 0.382156749351369, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 259, Loss: 0.3375687135811781, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 260, Loss: 0.31032726750262934, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 261, Loss: 0.2657890696326456, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 262, Loss: 0.30734453808792894, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 263, Loss: 0.2885036952148801, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 264, Loss: 0.4075066792747761, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 265, Loss: 0.29256996518792633, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 266, Loss: 0.324361536592272, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 267, Loss: 0.3929980251132207, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 268, Loss: 0.3846423222257156, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 269, Loss: 0.4214450217305951, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 270, Loss: 0.2770881574911335, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 271, Loss: 0.36707407124673763, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 272, Loss: 0.31771069849893796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 273, Loss: 0.29743469878219464, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 274, Loss: 0.4335683669905903, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 275, Loss: 0.29977485013522687, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 276, Loss: 0.47347449735719815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 277, Loss: 0.31202407132620597, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 278, Loss: 0.3043202615810859, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 279, Loss: 0.4079815263123907, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 280, Loss: 0.34266428312961295, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 281, Loss: 0.2601329130046445, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 282, Loss: 0.32407250686058015, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 283, Loss: 0.3479521353909525, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 284, Loss: 0.32888037892958616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 285, Loss: 0.27842628378102413, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 286, Loss: 0.2307859160604295, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 287, Loss: 0.3881108962356157, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 288, Loss: 0.5875528249608581, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 289, Loss: 0.34625477124190185, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 290, Loss: 0.41583596646279236, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 291, Loss: 0.4944228730316092, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 292, Loss: 0.3821068231243407, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 293, Loss: 0.2978000995316274, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 294, Loss: 0.2378835462043024, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 295, Loss: 0.2406468611221022, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 296, Loss: 0.3453170399155068, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 297, Loss: 0.36429504228056564, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 298, Loss: 0.4817295818322054, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 299, Loss: 0.35184049937685524, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 300, Loss: 0.36802454374670235, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 301, Loss: 0.2894288674811185, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 302, Loss: 0.35002567791423406, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 303, Loss: 0.44590470910746616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 304, Loss: 0.3409740708611363, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 305, Loss: 0.25411610097557846, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 306, Loss: 0.39891193946161396, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 307, Loss: 0.42451788021069203, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 308, Loss: 0.2881321396894963, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 309, Loss: 0.23975430383103666, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 310, Loss: 0.33458402085100614, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 311, Loss: 0.29312031180012676, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 312, Loss: 0.23459148002160302, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 313, Loss: 0.4955231756043936, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 314, Loss: 0.2536218879726037, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 315, Loss: 0.4135466571596275, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 316, Loss: 0.235415545524547, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 317, Loss: 0.554596497499726, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 318, Loss: 0.30307949405904183, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 319, Loss: 0.32335190777653267, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 320, Loss: 0.31875521512485105, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 321, Loss: 0.5572658804420458, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 322, Loss: 0.26366713927968877, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 323, Loss: 0.3894339467043047, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 324, Loss: 0.30963568088824506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 325, Loss: 0.3227759064371053, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 326, Loss: 0.32717740820921, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 327, Loss: 0.2553163132155204, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 328, Loss: 0.2679103389011952, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 329, Loss: 0.3328712532175332, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 330, Loss: 0.27908193752002997, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 331, Loss: 0.5132185450632039, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 332, Loss: 0.33103992804037397, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 333, Loss: 0.30962451315897255, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 334, Loss: 0.23692908434471918, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 335, Loss: 0.46950952045894234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 336, Loss: 0.450975051903882, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 337, Loss: 0.3215754012990784, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 338, Loss: 0.2601130787112567, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 339, Loss: 0.28353790322522954, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 340, Loss: 0.22882128682294825, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 341, Loss: 0.4965040933858649, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 342, Loss: 0.4186377456117101, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 343, Loss: 0.3742767407005546, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 344, Loss: 0.3410272568998671, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 345, Loss: 0.3218421563588103, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 346, Loss: 0.3508994235464134, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 347, Loss: 0.3209622872753275, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 348, Loss: 0.3448144948168797, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 349, Loss: 0.4205997548404642, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 350, Loss: 0.6241792721936559, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 351, Loss: 0.3776270702074047, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 352, Loss: 0.23208669448688196, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 353, Loss: 0.2924826373575426, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 354, Loss: 0.2875651354455032, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 355, Loss: 0.26818936475292215, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 356, Loss: 0.5394038393397962, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 357, Loss: 0.3642875219692333, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 358, Loss: 0.3612150940534189, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 359, Loss: 0.3646695705263003, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 360, Loss: 0.25283083692451663, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 361, Loss: 0.24198650514990205, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 362, Loss: 0.3935382101654654, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 363, Loss: 0.4223442391045238, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 364, Loss: 0.3143457300625512, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 365, Loss: 0.27427264185336153, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 366, Loss: 0.3721103241203116, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 367, Loss: 0.3092578869035692, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 368, Loss: 0.29823708737250354, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 369, Loss: 0.6634392813184378, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 370, Loss: 0.3703546436014119, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 371, Loss: 0.35401152393258184, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 372, Loss: 0.3539429548410036, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 373, Loss: 0.3817454798406005, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 374, Loss: 0.26363567675064176, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 375, Loss: 0.34619285010161605, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 376, Loss: 0.27813545987179744, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 377, Loss: 0.7956607248660875, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 378, Loss: 0.3072826444505994, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 379, Loss: 0.259987680842895, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 380, Loss: 0.5179400927504556, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 381, Loss: 0.3584982885030038, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 382, Loss: 0.5125431284106506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 383, Loss: 0.5398060520304697, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 384, Loss: 0.4804202703844537, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 385, Loss: 0.48545367579787446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 386, Loss: 0.47850716890611844, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 387, Loss: 0.2865867412927072, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 388, Loss: 0.38891972045088796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 389, Loss: 0.27477384023680534, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 390, Loss: 0.28836497704590297, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 391, Loss: 0.29825228311065155, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 392, Loss: 0.35087720035483655, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 393, Loss: 0.2897650172506642, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 394, Loss: 0.45454903379027567, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 395, Loss: 0.306077611044658, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 396, Loss: 0.4031409292986638, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 397, Loss: 0.33114878203999043, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 398, Loss: 0.26285929347690823, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 399, Loss: 0.3089101125100603, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 400, Loss: 0.3013908417562305, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 401, Loss: 0.3330233601358164, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 402, Loss: 0.27054112793968405, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 403, Loss: 0.3023275840919771, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 404, Loss: 0.2898925808891484, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 405, Loss: 0.32729047436625625, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 406, Loss: 0.3963486572247401, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 407, Loss: 0.2897264339394842, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 408, Loss: 0.2947927845544568, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 409, Loss: 0.28205692871620996, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 410, Loss: 0.23991445738504996, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 411, Loss: 0.28576897629099873, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 412, Loss: 0.23682870103203096, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 413, Loss: 0.3594826201558866, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 414, Loss: 1.0031271789226819, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 415, Loss: 0.3400869725799854, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 416, Loss: 0.3678827193467803, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 417, Loss: 0.28934507269205534, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 418, Loss: 0.31148632803053616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 419, Loss: 0.41546201097630187, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 420, Loss: 0.32795538746037006, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 421, Loss: 0.33767471113156466, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 422, Loss: 0.3031052604973979, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 423, Loss: 0.31818465835087484, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 424, Loss: 0.266646109504292, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 425, Loss: 0.3379268083604591, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 426, Loss: 0.5061328921279757, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 427, Loss: 0.24337101738646022, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 428, Loss: 0.2604577076141503, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 429, Loss: 0.2737687508531328, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 430, Loss: 0.5440850875634855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 431, Loss: 0.2563810543369073, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 432, Loss: 0.3396100005434153, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 433, Loss: 0.27216496726019623, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 434, Loss: 0.2620235505840735, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 435, Loss: 0.33395602527908663, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 436, Loss: 0.41257201615526096, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 437, Loss: 0.2576798877641795, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 438, Loss: 0.326911857980577, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 439, Loss: 0.28893656020302483, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 440, Loss: 0.32624127113892154, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 441, Loss: 0.5940798431670861, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 442, Loss: 0.29347094554319575, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 443, Loss: 0.32823205694308255, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 444, Loss: 0.25104463293225343, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 445, Loss: 0.25525858851023947, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 446, Loss: 0.5797323182370282, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 447, Loss: 0.3106334200173753, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 448, Loss: 0.41838216609946416, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 449, Loss: 0.23807103643433952, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 450, Loss: 0.3908187004031202, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 451, Loss: 0.2386694315410073, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 452, Loss: 0.28972830883962664, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 453, Loss: 0.37219921664702826, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 454, Loss: 0.41189831483064265, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 455, Loss: 0.2920682628747042, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 456, Loss: 0.258590659375188, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 457, Loss: 0.27198860554135457, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 458, Loss: 0.2679579438218031, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 459, Loss: 0.37230780658073437, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 460, Loss: 0.33942608176170286, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 461, Loss: 0.2542995496087624, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 462, Loss: 0.29378811925064074, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 463, Loss: 0.25949691143491466, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 464, Loss: 0.310066591933051, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 465, Loss: 0.3505227225427068, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 466, Loss: 0.2512042267712291, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 467, Loss: 0.2752815699517425, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 468, Loss: 0.29671468350172514, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 469, Loss: 0.24409026440030426, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 470, Loss: 0.637137397950799, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 471, Loss: 0.2809966984644548, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 472, Loss: 0.4593616868947604, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 473, Loss: 0.3444392101191266, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 474, Loss: 0.595383362873587, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 475, Loss: 0.29262210672493294, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 476, Loss: 0.5261173431691804, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 477, Loss: 0.4964749864139525, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 478, Loss: 0.5049630477363407, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 479, Loss: 0.5183700510090293, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 480, Loss: 0.4039023102868885, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 481, Loss: 0.3546791040652758, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 482, Loss: 0.2784498950262328, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 483, Loss: 0.3029930817500365, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 484, Loss: 0.2312871725818424, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 485, Loss: 0.48168720757528494, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 486, Loss: 0.23860952404539387, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 487, Loss: 0.27256886487492393, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 488, Loss: 0.551836000190113, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 489, Loss: 0.40784823312276014, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 490, Loss: 0.3418505795277256, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 491, Loss: 0.2335556008948123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 492, Loss: 0.24634238673451347, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 493, Loss: 0.4037395372966318, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 494, Loss: 0.37262342030852214, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 495, Loss: 0.24759005985052024, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 496, Loss: 0.2994842363180054, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 497, Loss: 0.24751158462027034, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 498, Loss: 0.3094610734920966, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 499, Loss: 0.4530467631398756, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 500, Loss: 0.3735804744819533, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 501, Loss: 0.3529924067441946, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 502, Loss: 0.3348724453221585, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 503, Loss: 0.5444845835652092, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 504, Loss: 0.4039554572272496, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 505, Loss: 0.2813272427192876, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 506, Loss: 0.29223794087636, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 507, Loss: 0.31609748220747264, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 508, Loss: 0.3595141874893878, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 509, Loss: 0.6820769172279741, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 510, Loss: 0.41517855500653844, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 511, Loss: 0.41565919319730465, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 512, Loss: 0.2577475533646119, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 513, Loss: 0.24667832308343415, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 514, Loss: 0.4537187951599907, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 515, Loss: 0.28482737753505905, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 516, Loss: 0.31871213980705404, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 517, Loss: 0.2851390513473195, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 518, Loss: 0.4744318685130865, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 519, Loss: 0.24986385582174064, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 520, Loss: 0.3053234440092895, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 521, Loss: 0.4614917919222187, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 522, Loss: 0.3633035884055864, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 523, Loss: 0.31535560284609104, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 524, Loss: 0.29648689978253695, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 525, Loss: 0.2417121401849036, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 526, Loss: 0.41641692864862456, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 527, Loss: 0.5420533451453876, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 528, Loss: 0.3340317769845379, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 529, Loss: 0.282913746244102, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 530, Loss: 0.30964353379789655, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 531, Loss: 0.4881979310353991, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 532, Loss: 0.2429417405515837, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 533, Loss: 0.4054964346722054, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 534, Loss: 0.3564390813881585, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 535, Loss: 0.2961766618774164, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 536, Loss: 0.4905950635337204, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 537, Loss: 0.44819521559757014, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 538, Loss: 0.5070749636045229, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 539, Loss: 0.45091407642259856, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 540, Loss: 0.3090905474573088, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 541, Loss: 0.35864714917731777, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 542, Loss: 0.26513734183372556, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 543, Loss: 0.32214682247413257, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 544, Loss: 0.2600036826559954, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 545, Loss: 0.3414225663522758, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 546, Loss: 0.38885420193663833, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 547, Loss: 0.22911763231179463, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 548, Loss: 0.4266145023065627, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 549, Loss: 0.32426317513661573, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 550, Loss: 0.3567308067509405, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 551, Loss: 0.3156714497395272, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 552, Loss: 0.4308169382713493, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 553, Loss: 0.2601774289900207, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 554, Loss: 0.4824650883749686, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 555, Loss: 0.40385194714260875, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 556, Loss: 0.3815184522582065, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 557, Loss: 0.38254658952426124, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 558, Loss: 0.4595686125867721, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 559, Loss: 0.3328246140856276, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 560, Loss: 0.23234471349910485, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 561, Loss: 0.5111357286320946, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 562, Loss: 0.24640197819766022, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 563, Loss: 0.3923543734354695, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 564, Loss: 0.22624082652112654, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 565, Loss: 0.40569543624762416, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 566, Loss: 0.4853053038647419, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 567, Loss: 0.24088299071459138, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 568, Loss: 0.28655433063503666, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 569, Loss: 0.3950860990890578, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 570, Loss: 0.28210047088238427, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 571, Loss: 0.3187281657337364, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 572, Loss: 0.3915931373854553, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 573, Loss: 0.239479075381866, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 574, Loss: 0.5392406018532916, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 575, Loss: 0.42060303641471086, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 576, Loss: 0.24904482009135084, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 577, Loss: 0.2767109324330085, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 578, Loss: 0.32875475030113266, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 579, Loss: 0.3109726251924473, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 580, Loss: 0.46614533674806613, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 581, Loss: 0.34854884890392973, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 582, Loss: 0.3098216717474719, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 583, Loss: 0.3524016278650208, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 584, Loss: 0.5134939819315634, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 585, Loss: 0.24240907742647932, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 586, Loss: 0.3783764019096813, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 587, Loss: 0.3130629988802905, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 588, Loss: 0.46905088160802155, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 589, Loss: 0.4004017984936403, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 590, Loss: 0.2498287893286238, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 591, Loss: 0.44374698284674535, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 592, Loss: 0.46860858408619743, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 593, Loss: 0.41490814812724053, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 594, Loss: 0.43685056591767224, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 595, Loss: 0.2622889384656146, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 596, Loss: 0.2725231030154751, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 597, Loss: 0.4201512037198, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 598, Loss: 0.3208688473142899, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 599, Loss: 0.2935240012403837, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 600, Loss: 0.3384342772530629, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 601, Loss: 0.526233355604573, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 602, Loss: 0.3104556962804047, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 603, Loss: 0.2550914616172717, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 604, Loss: 0.2684495430832297, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 605, Loss: 0.561605185758334, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 606, Loss: 0.2331705031397036, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 607, Loss: 0.2506431301586697, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 608, Loss: 0.3227278010504986, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 609, Loss: 0.2787376360210754, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 610, Loss: 0.3045144681773626, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 611, Loss: 0.27463430261971217, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 612, Loss: 0.33692674793851224, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 613, Loss: 0.29982225848411204, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 614, Loss: 0.3155149437365194, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 615, Loss: 0.2941286795463175, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 616, Loss: 0.4388068340196788, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 617, Loss: 0.28906032031661755, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 618, Loss: 0.24524481260095576, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 619, Loss: 0.28245881126197836, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 620, Loss: 0.32743991483858637, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 621, Loss: 0.2692998096720238, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 622, Loss: 0.2644854356972951, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 623, Loss: 0.2857875969288232, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 624, Loss: 0.3697797142115498, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 625, Loss: 0.2726722622251594, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 626, Loss: 0.23770976983272665, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 627, Loss: 0.44881021147968464, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 628, Loss: 0.3255801540614871, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 629, Loss: 0.2717754178316598, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 630, Loss: 0.30574477124341937, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 631, Loss: 0.3757900466941678, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 632, Loss: 0.3818375487423544, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 633, Loss: 0.44500032581837917, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 634, Loss: 0.3441165175177166, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 635, Loss: 0.4453530264628162, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 636, Loss: 0.387518342186962, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 637, Loss: 0.3503577175121475, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 638, Loss: 0.32172705329194545, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 639, Loss: 0.45584531659677163, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 640, Loss: 0.22758649734686145, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 641, Loss: 0.24838752389325391, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 642, Loss: 0.3405508618081752, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 643, Loss: 0.5346009461865817, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 644, Loss: 0.3907515407452156, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 645, Loss: 0.3236721798520752, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 646, Loss: 0.272731281538721, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 647, Loss: 0.2302479166426176, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 648, Loss: 0.30019125333145824, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 649, Loss: 0.22393024542431034, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 650, Loss: 0.42552857556552204, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 651, Loss: 0.3712370237722735, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 652, Loss: 0.2313490159462576, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 653, Loss: 0.41643515957632987, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 654, Loss: 0.25346901426803964, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 655, Loss: 0.3062013031939271, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 656, Loss: 0.4844873887577018, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 657, Loss: 0.2694744386683731, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 658, Loss: 0.3918015654149381, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 659, Loss: 0.3036115044755643, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 660, Loss: 0.32743475452695037, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 661, Loss: 0.29453452787177786, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 662, Loss: 0.28104915712037454, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 663, Loss: 0.32392601619994427, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 664, Loss: 0.27799254941132134, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 665, Loss: 0.24259285591379104, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 666, Loss: 0.3204512799072761, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 667, Loss: 0.36463939207489693, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 668, Loss: 0.31901949197456836, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 669, Loss: 0.2920428125465202, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 670, Loss: 0.2516895067247529, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 671, Loss: 0.2630279407328039, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 672, Loss: 0.27486654265678856, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 673, Loss: 0.3651162381616728, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 674, Loss: 0.51179833098975, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 675, Loss: 0.46477622704319066, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 676, Loss: 0.3694126176969731, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 677, Loss: 0.2852260277595271, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 678, Loss: 0.3310806976997269, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 679, Loss: 0.2443405513621268, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 680, Loss: 0.24063025642914831, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 681, Loss: 0.47813039934841206, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 682, Loss: 0.6111622875226361, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 683, Loss: 0.44255174854572815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 684, Loss: 0.2828089745042166, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 685, Loss: 0.4164850035361916, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 686, Loss: 0.3976223640392034, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 687, Loss: 0.28973667729088254, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 688, Loss: 0.3569654173746286, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 689, Loss: 0.27329701700662967, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 690, Loss: 0.38432115074338746, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 691, Loss: 0.31419645822461756, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 692, Loss: 0.42779764819608246, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 693, Loss: 0.2669453599178617, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 694, Loss: 0.3824266056421185, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 695, Loss: 0.5984930085309783, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 696, Loss: 0.29270927915812006, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 697, Loss: 0.3311141454431017, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 698, Loss: 0.2785102261600947, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 699, Loss: 0.25764555599263195, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 700, Loss: 0.3223175452756909, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 701, Loss: 0.5441217217963242, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 702, Loss: 0.2663813647537907, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 703, Loss: 0.4209360143123464, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 704, Loss: 0.4430936463181103, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 705, Loss: 0.2691304609695465, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 706, Loss: 0.5663814408201339, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 707, Loss: 0.2390804725935857, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 708, Loss: 0.4210352930609508, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 709, Loss: 0.46972975339763196, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 710, Loss: 0.2603985363282206, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 711, Loss: 0.4121159731512152, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 712, Loss: 0.6052597026263242, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 713, Loss: 0.38159161897927246, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 714, Loss: 0.8760355987694921, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 715, Loss: 0.23941985911142208, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 716, Loss: 0.34138647925893756, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 717, Loss: 0.3584933248622668, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 718, Loss: 0.5371599784667466, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 719, Loss: 0.33826487980857234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 720, Loss: 0.3365309037485276, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 721, Loss: 0.39206826404249817, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 722, Loss: 0.3614145424912524, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 723, Loss: 0.3279604322270569, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 724, Loss: 0.2674765404367908, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 725, Loss: 0.2658148435431369, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 726, Loss: 0.2828912450673487, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 727, Loss: 0.31697011075528425, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 728, Loss: 0.2596952868113346, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 729, Loss: 0.37626760876366405, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 730, Loss: 0.3134701144295896, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 731, Loss: 0.24260247432133542, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 732, Loss: 0.2475933097268023, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 733, Loss: 0.3129612803014545, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 734, Loss: 0.4622090696875635, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 735, Loss: 0.4275033409032346, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 736, Loss: 0.28335539736236004, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 737, Loss: 0.26628319041591764, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 738, Loss: 0.48994624116350977, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 739, Loss: 0.45778439372021007, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 740, Loss: 0.27299620114034273, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 741, Loss: 0.3911880490635772, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 742, Loss: 0.30137275552142345, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 743, Loss: 0.42215613519005296, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 744, Loss: 0.28070974615911837, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 745, Loss: 0.35222905383254854, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 746, Loss: 0.5570718167313483, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 747, Loss: 0.6549202771561262, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 748, Loss: 0.38355388242710986, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 749, Loss: 0.37213236675215505, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 750, Loss: 0.2519932694150411, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 751, Loss: 0.47374831181436633, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 752, Loss: 0.3303432074774981, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 753, Loss: 0.4859660998001455, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 754, Loss: 0.49029469760977873, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 755, Loss: 0.26064723962844394, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 756, Loss: 0.37193736272486555, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 757, Loss: 0.29208138867915684, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 758, Loss: 0.3209520767134263, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 759, Loss: 0.3808540172730982, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 760, Loss: 0.31016823330526616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 761, Loss: 0.2506039008222027, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 762, Loss: 0.2560601488804989, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 763, Loss: 0.28329189054655257, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 764, Loss: 0.2245337470103109, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 765, Loss: 0.373786216641246, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 766, Loss: 0.2947559626554927, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 767, Loss: 0.39341655650525326, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 768, Loss: 0.4430688299743408, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 769, Loss: 0.5607047183173912, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 770, Loss: 0.3549113023465257, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 771, Loss: 0.32426027119891415, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 772, Loss: 0.2658313413601605, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 773, Loss: 0.31411618131466335, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 774, Loss: 0.33717527849993345, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 775, Loss: 0.3209481680211659, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 776, Loss: 0.3555913973629018, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 777, Loss: 0.30588112215520086, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 778, Loss: 0.2882392141816882, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 779, Loss: 0.41177879046283533, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 780, Loss: 0.2371645134588066, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 781, Loss: 0.6061970166873638, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 782, Loss: 0.3143832312455705, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 783, Loss: 0.33669019024877966, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 784, Loss: 0.31559464312725605, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 785, Loss: 0.30776685387446767, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 786, Loss: 0.5789663776675511, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 787, Loss: 0.48070036588419135, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 788, Loss: 0.2557498244498261, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 789, Loss: 0.35816735715045833, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 790, Loss: 0.2737983581229082, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 791, Loss: 0.28152338191927995, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 792, Loss: 0.5609854712014777, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 793, Loss: 0.5837332218866421, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 794, Loss: 0.4107236437078243, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 795, Loss: 0.3195949350518815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 796, Loss: 0.5269931415282301, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 797, Loss: 0.33918472399339716, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 798, Loss: 0.24600593781707106, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 799, Loss: 0.2747478331642578, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 800, Loss: 0.24127925283881435, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 801, Loss: 0.3348449559576796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 802, Loss: 0.45987462609320073, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 803, Loss: 0.3098935576410185, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 804, Loss: 0.33536389666323685, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 805, Loss: 0.37517805977782004, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 806, Loss: 0.2685467951086064, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 807, Loss: 0.6085741807394043, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 808, Loss: 0.24137718007960246, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 809, Loss: 0.3556630597780913, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 810, Loss: 0.2541554226023978, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 811, Loss: 0.6547716844526913, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 812, Loss: 0.37955795602877085, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 813, Loss: 0.37914533152133933, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 814, Loss: 0.395809093179631, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 815, Loss: 0.2911917000216895, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 816, Loss: 0.33558588983786464, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 817, Loss: 0.34431825277999106, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 818, Loss: 0.3628997181497178, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 819, Loss: 0.3598840005203112, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 820, Loss: 0.2745204735040909, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 821, Loss: 0.3087733254230521, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 822, Loss: 0.305382116277212, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 823, Loss: 0.4458390738776039, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 824, Loss: 0.27533236384368565, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 825, Loss: 0.2670027262136535, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 826, Loss: 0.3472177519076085, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 827, Loss: 0.26796395627501124, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 828, Loss: 0.27630652905893477, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 829, Loss: 0.3338807100111266, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 830, Loss: 0.3012548655265375, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 831, Loss: 0.2520321438349449, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 832, Loss: 0.3493112381765161, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 833, Loss: 0.3780792256669555, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 834, Loss: 0.47243625059262934, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 835, Loss: 0.5782227284418706, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 836, Loss: 0.3079565624496018, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 837, Loss: 0.3367668165258082, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 838, Loss: 0.2389015542483394, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 839, Loss: 0.2850832977185145, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 840, Loss: 0.31472677840152974, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 841, Loss: 0.3603027840061582, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 842, Loss: 0.41084463077758465, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 843, Loss: 0.4120624254914731, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 844, Loss: 0.42063186534617353, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 845, Loss: 0.4107436785925056, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 846, Loss: 0.35777482929593973, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 847, Loss: 0.28303290014122506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 848, Loss: 0.43052676467143736, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 849, Loss: 0.26603421653362147, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 850, Loss: 0.48459210315462514, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 851, Loss: 0.34538595031888863, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 852, Loss: 0.27558473067726663, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 853, Loss: 0.2742918488862245, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 854, Loss: 0.4017148819538676, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 855, Loss: 0.3573851002740596, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 856, Loss: 0.5211960494097823, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 857, Loss: 0.6099571386519285, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 858, Loss: 0.3846937233608481, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 859, Loss: 0.29225102821270504, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 860, Loss: 0.2438126879522795, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 861, Loss: 0.43179758432895793, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 862, Loss: 0.29238552944257634, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 863, Loss: 0.2805509333875409, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 864, Loss: 0.2947334398912301, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 865, Loss: 0.32876246958306016, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 866, Loss: 0.26860633981159504, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 867, Loss: 0.3397235373333733, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 868, Loss: 0.24746675654526948, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 869, Loss: 0.28855000361966443, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 870, Loss: 0.391099560833083, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 871, Loss: 0.2781145759237563, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 872, Loss: 0.3497058961495608, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 873, Loss: 0.5212618835837327, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 874, Loss: 0.2541944313220504, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 875, Loss: 0.4075770971018459, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 876, Loss: 0.2794193011000259, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 877, Loss: 0.28806015173592864, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 878, Loss: 0.23916033496421146, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 879, Loss: 0.2712970865614764, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 880, Loss: 0.41406351353923665, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 881, Loss: 0.4351222573605059, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 882, Loss: 0.3687622449714907, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 883, Loss: 0.2935127034082804, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 884, Loss: 0.2539358416449264, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 885, Loss: 0.4678652078644428, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 886, Loss: 0.39201281915052666, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 887, Loss: 0.41967192542399323, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 888, Loss: 0.4261871168811161, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 889, Loss: 0.3291497213443009, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 890, Loss: 0.2873044757619523, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 891, Loss: 0.2873236568773125, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 892, Loss: 0.43603658126304323, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 893, Loss: 0.2427451621045698, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 894, Loss: 0.30888651169430453, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 895, Loss: 0.37451398965024135, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 896, Loss: 0.3406723985434595, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 897, Loss: 0.24613660514667712, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 898, Loss: 0.31394799030542814, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 899, Loss: 0.2502556801771188, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 900, Loss: 0.5265314131582124, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 901, Loss: 0.5576845228677205, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 902, Loss: 0.36909445771498905, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 903, Loss: 0.5123159875619284, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 904, Loss: 0.4122405833636225, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 905, Loss: 0.36269068908579305, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 906, Loss: 0.4179464505946893, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 907, Loss: 0.3171099434129319, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 908, Loss: 0.4156803673081274, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 909, Loss: 0.34224417144457314, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 910, Loss: 0.567446504753727, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 911, Loss: 0.2694055900447879, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 912, Loss: 0.2847628983047221, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 913, Loss: 0.24830268321708235, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 914, Loss: 0.2318919488607333, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 915, Loss: 0.46004423279542683, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 916, Loss: 0.2686965162111502, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 917, Loss: 0.6214602328187441, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 918, Loss: 0.2789395138669739, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 919, Loss: 0.35651455733877035, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 920, Loss: 0.33229947167799734, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 921, Loss: 0.3241363394744202, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 922, Loss: 0.2825322527286556, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 923, Loss: 0.36469007518974517, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 924, Loss: 0.3131359113118142, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 925, Loss: 0.6764534317438423, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 926, Loss: 0.5103066098018112, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 927, Loss: 0.46095097110764977, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 928, Loss: 0.35053072095670956, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 929, Loss: 0.2686655706307748, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 930, Loss: 0.2631697793580964, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 931, Loss: 0.3142306234663816, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 932, Loss: 0.3823761599835549, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 933, Loss: 0.2493704793690103, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 934, Loss: 0.32575544082515373, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 935, Loss: 0.36532977972609515, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 936, Loss: 0.3252424529509913, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 937, Loss: 0.2371562331437133, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 938, Loss: 0.6046733834056125, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 939, Loss: 0.232675623515404, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 940, Loss: 0.33905756633242123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 941, Loss: 0.245233149039602, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 942, Loss: 0.25099654415251194, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 943, Loss: 0.3605454644422953, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 944, Loss: 0.2409734316734795, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 945, Loss: 0.31324450838290974, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 946, Loss: 0.25280346144391025, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 947, Loss: 0.2598610648575849, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 948, Loss: 0.24820681870200234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 949, Loss: 0.34624736177401516, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 950, Loss: 0.23179789194552333, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 951, Loss: 0.25575399286286765, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 952, Loss: 0.24590992489363575, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 953, Loss: 0.38741657100268145, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 954, Loss: 0.31467736420276826, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 955, Loss: 0.25820871172455123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 956, Loss: 0.2618667764253863, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 957, Loss: 0.24895185837439246, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 958, Loss: 0.31654188704460334, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 959, Loss: 0.262873119872802, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 960, Loss: 0.3580760133881534, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 961, Loss: 0.4110783536416959, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 962, Loss: 0.26034213711111376, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 963, Loss: 0.26669298956718135, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 964, Loss: 0.2941311603879544, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 965, Loss: 0.3639907854708897, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 966, Loss: 0.40155449036325963, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 967, Loss: 0.2860619136866514, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 968, Loss: 0.25594979238364257, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 969, Loss: 0.3124731738664789, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 970, Loss: 0.2432823086204025, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 971, Loss: 0.2863235922803185, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 972, Loss: 0.6523270458642949, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 973, Loss: 0.2932196331990786, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 974, Loss: 0.5658349182084144, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 975, Loss: 0.2912331483443856, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 976, Loss: 0.4345446370550068, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 977, Loss: 0.2811431542957628, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 978, Loss: 0.2743792023947961, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 979, Loss: 0.3790890298171274, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 980, Loss: 0.25384976617119515, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 981, Loss: 0.3664012390876987, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 982, Loss: 0.2590661144607759, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 983, Loss: 0.28881677932583383, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 984, Loss: 0.3358939688504658, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 985, Loss: 0.24964895658771677, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 986, Loss: 0.38135397855532227, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 987, Loss: 0.26245751246830296, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 988, Loss: 0.267567751528709, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 989, Loss: 0.3340070622557646, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 990, Loss: 0.4855495550013107, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 991, Loss: 0.35839120522507906, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 992, Loss: 0.23143493246919855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 993, Loss: 0.2962751624596572, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 994, Loss: 0.2432510641913636, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 995, Loss: 0.35044446315634425, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 996, Loss: 0.4435244895087015, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 997, Loss: 0.2562798708688746, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 998, Loss: 0.3633397082535141, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 999, Loss: 0.2998123717926299, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1000, Loss: 0.32990312365976776, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1001, Loss: 0.401541572265192, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1002, Loss: 0.3596020635819863, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1003, Loss: 0.2926631297003535, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1004, Loss: 0.23664934818644795, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1005, Loss: 0.4783036533928656, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1006, Loss: 0.2898716764875056, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1007, Loss: 0.3177080606098918, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1008, Loss: 0.2321542375300315, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1009, Loss: 0.3850551203640122, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1010, Loss: 0.28109433060815336, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1011, Loss: 0.26814375334259055, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1012, Loss: 0.5199344186731316, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1013, Loss: 0.30944121739544705, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1014, Loss: 0.4323703062516179, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1015, Loss: 0.4219483042688687, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1016, Loss: 0.25131238718072263, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1017, Loss: 0.34035886392267006, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1018, Loss: 0.26068265478261554, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1019, Loss: 0.39050299834863517, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1020, Loss: 0.33501810725252495, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1021, Loss: 0.5901601384821716, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1022, Loss: 0.3682606149501223, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1023, Loss: 0.31864208817911505, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1024, Loss: 0.2690630441332882, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1025, Loss: 0.22409427622651212, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1026, Loss: 0.6007722595247872, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1027, Loss: 0.3607757771080353, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1028, Loss: 0.2940785091613234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1029, Loss: 0.3710549231581186, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1030, Loss: 0.2483099425819572, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1031, Loss: 0.393456580884614, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1032, Loss: 0.395517424097799, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1033, Loss: 0.38819022378561985, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1034, Loss: 0.24114303518108388, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1035, Loss: 0.6451079117215364, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1036, Loss: 0.37466597743824015, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1037, Loss: 0.40976473815141023, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1038, Loss: 0.29251525226275543, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1039, Loss: 0.2387038541300287, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1040, Loss: 0.25664317382594315, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1041, Loss: 0.28158548938254085, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1042, Loss: 0.2716035516449407, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1043, Loss: 0.4430110525833763, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1044, Loss: 0.31785419817063326, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1045, Loss: 0.38341132174646, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1046, Loss: 0.25176690448625333, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1047, Loss: 0.539792863333518, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1048, Loss: 0.25838535724110784, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1049, Loss: 0.34863703329648643, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1050, Loss: 0.43536907729754704, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1051, Loss: 0.315871992987417, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1052, Loss: 0.5383345480109889, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1053, Loss: 0.27674925033298575, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1054, Loss: 0.3240173828506955, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1055, Loss: 0.22625435070788794, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1056, Loss: 0.27262135148526123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1057, Loss: 0.25349751463170855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1058, Loss: 0.39751512564679536, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1059, Loss: 0.24511367739962625, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1060, Loss: 0.2574634164304668, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1061, Loss: 0.30117775351435516, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1062, Loss: 0.5129677036635159, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1063, Loss: 0.27936239430171034, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1064, Loss: 0.3060673782074492, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1065, Loss: 0.43694627778674544, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1066, Loss: 0.26698287670142107, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1067, Loss: 0.294317746637081, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1068, Loss: 0.3535770537713051, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1069, Loss: 0.40210756041497747, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1070, Loss: 0.22389075425925375, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1071, Loss: 0.3543581959669656, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1072, Loss: 0.22444793256519427, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1073, Loss: 0.34609373641415375, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1074, Loss: 0.26626081530192125, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1075, Loss: 0.2928933682295794, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1076, Loss: 0.33381192065406945, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1077, Loss: 0.2526711686820042, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1078, Loss: 0.3513674495363004, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1079, Loss: 0.4219667052898123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1080, Loss: 0.2716256375465909, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1081, Loss: 0.3240681689643028, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1082, Loss: 0.2601218791494998, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1083, Loss: 0.24862370389149824, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1084, Loss: 0.27290818869083694, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1085, Loss: 0.5809412641741404, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1086, Loss: 0.39197584978242006, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1087, Loss: 0.4485532256460433, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1088, Loss: 0.4857298874624019, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1089, Loss: 0.3169281123524844, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1090, Loss: 0.3462638936436249, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1091, Loss: 0.3845483188740283, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1092, Loss: 0.553909553887903, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1093, Loss: 0.32397074195259434, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1094, Loss: 0.6860126322228088, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1095, Loss: 0.23282374458448857, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1096, Loss: 0.5413532848199076, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1097, Loss: 0.3469232738813277, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1098, Loss: 0.4607387601864618, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1099, Loss: 0.2972750690549647, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1100, Loss: 0.3217231934956196, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1101, Loss: 0.5662554165585202, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1102, Loss: 0.3343199015363738, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1103, Loss: 0.3472717364633585, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1104, Loss: 0.6122787105799516, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1105, Loss: 0.31457661676015214, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1106, Loss: 0.2690450960561824, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1107, Loss: 0.4376644385170446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1108, Loss: 0.29108767459422685, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1109, Loss: 0.3101007523214795, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1110, Loss: 0.23771511652880814, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1111, Loss: 0.7332582824436972, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1112, Loss: 0.2855810440793839, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1113, Loss: 0.23686656118685512, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1114, Loss: 0.2780085742040281, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1115, Loss: 0.3466264483015593, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1116, Loss: 0.4936542499187166, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1117, Loss: 0.5749232194847809, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1118, Loss: 0.46097899561205097, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1119, Loss: 0.32920318655577613, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1120, Loss: 0.4038193154268328, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1121, Loss: 0.35812591002149385, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1122, Loss: 0.29987303776904634, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1123, Loss: 0.3415963522391712, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1124, Loss: 0.3798440523359137, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1125, Loss: 0.30081838462313126, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1126, Loss: 0.45824133743227535, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1127, Loss: 0.4901911052006028, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1128, Loss: 0.3399118855841013, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1129, Loss: 0.24786735319500053, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1130, Loss: 0.26861477445119736, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1131, Loss: 0.5474029285253187, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1132, Loss: 0.2418178672991223, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1133, Loss: 0.250004680118381, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1134, Loss: 0.4329032772339756, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1135, Loss: 0.3238443524164902, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1136, Loss: 0.3630720717263599, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1137, Loss: 0.6986216975846984, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1138, Loss: 0.38472274731976897, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1139, Loss: 0.2677491694796479, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1140, Loss: 0.29857334879412667, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1141, Loss: 0.4546732392528521, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1142, Loss: 0.31066870993995965, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1143, Loss: 0.4755494589761343, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1144, Loss: 0.42642606060033234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1145, Loss: 0.37001513625149907, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1146, Loss: 0.3499210225257562, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1147, Loss: 0.3848898638128131, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1148, Loss: 0.23812827647739523, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1149, Loss: 0.31293471321352273, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1150, Loss: 0.4016721017149327, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1151, Loss: 0.5569312103276246, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1152, Loss: 0.28980635841573876, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1153, Loss: 0.2297244702866953, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1154, Loss: 0.2913302675650377, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1155, Loss: 0.289450533386251, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1156, Loss: 0.3268378184622317, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1157, Loss: 0.3150263567524237, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1158, Loss: 0.3452554420274814, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1159, Loss: 0.3801859197353758, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1160, Loss: 0.42388043186784796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1161, Loss: 0.45086073526978576, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1162, Loss: 0.44579080815975547, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1163, Loss: 0.24619987439132446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1164, Loss: 0.3825243430068951, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1165, Loss: 0.292160558734026, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1166, Loss: 0.3092495450115247, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1167, Loss: 0.25387362090805154, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1168, Loss: 0.3579565976516895, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1169, Loss: 0.30391708304457243, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1170, Loss: 0.2664431407867975, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1171, Loss: 0.4333255703862896, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1172, Loss: 0.24987043136833725, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1173, Loss: 0.41735189895117, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1174, Loss: 0.35583603641163025, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1175, Loss: 0.25351787856057123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1176, Loss: 0.28168670048686945, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1177, Loss: 0.22910939374966835, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1178, Loss: 0.3449826043050597, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1179, Loss: 0.29750996243105043, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1180, Loss: 0.37608683681389377, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1181, Loss: 0.3919552858989108, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1182, Loss: 0.4763397987636162, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1183, Loss: 0.32621447067077103, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1184, Loss: 0.5176825363845915, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1185, Loss: 0.30165550245067096, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1186, Loss: 0.2508433232877606, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1187, Loss: 0.5012729452115164, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1188, Loss: 0.5012702383014677, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1189, Loss: 0.26740722074928225, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1190, Loss: 0.2914197892184222, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1191, Loss: 0.6451608371903198, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1192, Loss: 0.24103589041998774, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1193, Loss: 0.35723372071697534, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1194, Loss: 0.3269378603529348, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1195, Loss: 0.2407767297907035, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1196, Loss: 0.34840997000729446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1197, Loss: 0.33293859568103923, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1198, Loss: 0.35768778275207636, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1199, Loss: 0.38473033309916055, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1200, Loss: 0.27378528388852424, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1201, Loss: 0.3189333752643313, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1202, Loss: 0.23932131522403344, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1203, Loss: 0.3499418747238069, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1204, Loss: 0.2526764350950909, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1205, Loss: 0.237755663647784, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1206, Loss: 0.2789290584082127, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1207, Loss: 0.3570147351509605, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1208, Loss: 0.2541655259463333, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1209, Loss: 0.2356687694037927, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1210, Loss: 0.40320983792310494, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1211, Loss: 0.46850992063463714, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1212, Loss: 0.3043530617820092, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1213, Loss: 0.335820567849164, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1214, Loss: 0.2724437354100284, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1215, Loss: 0.3738909268372661, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1216, Loss: 0.28430501806146524, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1217, Loss: 0.2548327048095382, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1218, Loss: 0.502055714487166, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1219, Loss: 0.294158290716072, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1220, Loss: 0.30686620504911466, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1221, Loss: 0.48925638538053834, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1222, Loss: 0.4995456986492393, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1223, Loss: 0.33855259636397916, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1224, Loss: 0.3223887490188594, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1225, Loss: 0.3016412775237075, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1226, Loss: 0.3335170474105225, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1227, Loss: 0.3305738218236081, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1228, Loss: 0.2846847852637334, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1229, Loss: 0.5914121971734816, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1230, Loss: 0.2404044964129643, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1231, Loss: 0.3179176539223821, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1232, Loss: 0.3433792522303435, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1233, Loss: 0.3467906201198264, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1234, Loss: 0.3906136322143634, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1235, Loss: 0.28235833066960825, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1236, Loss: 0.37445864453658123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1237, Loss: 0.2939364435649642, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1238, Loss: 0.44393232164552, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1239, Loss: 0.4677362544727194, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1240, Loss: 0.39095836905774817, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1241, Loss: 0.3350133569612045, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1242, Loss: 0.30146069062291664, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1243, Loss: 0.40556224674119346, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1244, Loss: 0.22470248138841456, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1245, Loss: 0.3870071445447721, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1246, Loss: 0.2820322137187058, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1247, Loss: 0.2821858517488267, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1248, Loss: 0.2911647114012976, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1249, Loss: 0.31200548072249024, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1250, Loss: 0.40848912652815794, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1251, Loss: 0.3518472857735978, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1252, Loss: 0.2851647735857214, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1253, Loss: 0.33342111980727823, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1254, Loss: 0.3506821209928867, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1255, Loss: 0.2787007613710217, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1256, Loss: 0.4925123604793029, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1257, Loss: 0.29801072239487125, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1258, Loss: 0.26485531508084226, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1259, Loss: 0.2800763246832353, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1260, Loss: 0.3152255699910129, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1261, Loss: 0.27068832483404426, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1262, Loss: 0.4130625905809048, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1263, Loss: 0.39653122899556215, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1264, Loss: 0.3264141725999272, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1265, Loss: 0.4452675582520266, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1266, Loss: 0.3206380517051082, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1267, Loss: 0.2803598415430758, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1268, Loss: 0.34420357114910227, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1269, Loss: 0.2716020911390849, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1270, Loss: 0.3955307115595511, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1271, Loss: 0.31124824032070664, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1272, Loss: 0.282280775512718, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1273, Loss: 0.3307482145454182, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1274, Loss: 0.4609407336143644, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1275, Loss: 0.3140813431032511, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1276, Loss: 0.3994088212377717, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1277, Loss: 0.41071890524470533, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1278, Loss: 0.3550026780091099, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1279, Loss: 0.2798164443896714, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1280, Loss: 0.378636851715932, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1281, Loss: 0.3706397346452585, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1282, Loss: 0.3038077701584696, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1283, Loss: 0.26055957501524807, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1284, Loss: 0.4062367579388032, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1285, Loss: 0.25119179359502, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1286, Loss: 0.30698161073414254, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1287, Loss: 0.30619083565440053, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1288, Loss: 0.49784292718329903, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1289, Loss: 0.23869115338271735, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1290, Loss: 0.3098735665838423, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1291, Loss: 0.45808422786057335, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1292, Loss: 0.3333313273311595, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1293, Loss: 0.5919867526670978, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1294, Loss: 0.33609682043132755, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1295, Loss: 0.35269814072013284, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1296, Loss: 0.23986814245063737, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1297, Loss: 0.399729099175534, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1298, Loss: 0.6412153108532914, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1299, Loss: 0.29592855851306177, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1300, Loss: 0.41243175045517433, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1301, Loss: 0.378857428098165, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1302, Loss: 0.2982045180366977, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1303, Loss: 0.35788063739427856, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1304, Loss: 0.24917184808727838, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1305, Loss: 0.53057661639629, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1306, Loss: 0.3154995820248249, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1307, Loss: 0.3835884276203274, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1308, Loss: 0.24606191911410918, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1309, Loss: 0.4725825594110446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1310, Loss: 0.310694905037194, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1311, Loss: 0.2842800611305633, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1312, Loss: 0.368264323227282, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1313, Loss: 0.32362790142018016, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1314, Loss: 0.4049627003876273, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1315, Loss: 0.3363412632545832, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1316, Loss: 0.30002764502045254, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1317, Loss: 0.2473040776466279, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1318, Loss: 0.24697333317457298, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1319, Loss: 0.2594943319726768, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1320, Loss: 0.2686057420418303, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1321, Loss: 0.2847114599531504, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1322, Loss: 0.4712599894895594, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1323, Loss: 0.2911512870025706, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1324, Loss: 0.44489417974899303, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1325, Loss: 0.28160627495894247, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1326, Loss: 0.44496977426119644, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1327, Loss: 0.35732172752047026, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1328, Loss: 0.4033918234579588, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1329, Loss: 0.3311888232392003, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1330, Loss: 0.3036392968140068, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1331, Loss: 0.3164566298363808, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1332, Loss: 0.31391150802845047, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1333, Loss: 0.4967372615084235, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1334, Loss: 0.43226371933749796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1335, Loss: 0.3311638136353942, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1336, Loss: 0.43545480389081337, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1337, Loss: 0.2435462017043456, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1338, Loss: 0.2645507465658685, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1339, Loss: 0.4488864302073669, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1340, Loss: 0.4686627158697828, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1341, Loss: 0.3666022443711603, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1342, Loss: 0.3762285596411781, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1343, Loss: 0.22807008861066985, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1344, Loss: 0.24788492405705745, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1345, Loss: 0.2782812523475284, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1346, Loss: 0.2576905758346592, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1347, Loss: 0.3793131237874673, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1348, Loss: 0.4877579613570068, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1349, Loss: 0.6236102584799937, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1350, Loss: 0.24188917003918456, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1351, Loss: 0.2304903854587357, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1352, Loss: 0.3398965448373611, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1353, Loss: 0.33765854552684693, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1354, Loss: 0.383896604625673, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1355, Loss: 0.28573171771601646, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1356, Loss: 0.29542577365284384, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1357, Loss: 0.3964286791022631, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1358, Loss: 0.2912460463938739, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1359, Loss: 0.3948872363179811, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1360, Loss: 0.3052880065167868, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1361, Loss: 0.2522007822320287, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1362, Loss: 0.3593091201265211, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1363, Loss: 0.40667148219201665, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1364, Loss: 0.3397887170334587, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1365, Loss: 0.3126967584333673, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1366, Loss: 0.24298588359933262, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1367, Loss: 0.25968801508377326, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1368, Loss: 0.39273471725102505, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1369, Loss: 0.2813093680373604, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1370, Loss: 0.355362791865734, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1371, Loss: 0.45266248371862194, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1372, Loss: 0.3324903711788544, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1373, Loss: 0.3553326787172382, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1374, Loss: 0.5313840454881633, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1375, Loss: 0.2695715240805714, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1376, Loss: 0.3237662374559373, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1377, Loss: 0.35308903737854297, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1378, Loss: 0.4014150715201392, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1379, Loss: 0.2608637857461698, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1380, Loss: 0.48625312111595076, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1381, Loss: 0.5289159185401664, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1382, Loss: 0.33307020458784037, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1383, Loss: 0.3619438830792854, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1384, Loss: 0.2596552703402461, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1385, Loss: 0.34564412029826874, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1386, Loss: 0.2865443904963739, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1387, Loss: 0.25873383182748777, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1388, Loss: 0.2602524778450633, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1389, Loss: 0.42028126647615016, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1390, Loss: 0.517846136680442, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1391, Loss: 0.4520468852173987, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1392, Loss: 0.25455977223760606, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1393, Loss: 0.339114933868307, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1394, Loss: 0.45791528014121063, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1395, Loss: 0.34478173355189906, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1396, Loss: 0.2649638081136693, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1397, Loss: 0.24673920879436007, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1398, Loss: 0.2987018137480339, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1399, Loss: 0.4479931276139234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1400, Loss: 0.4060701096953042, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1401, Loss: 0.2348979855396042, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1402, Loss: 0.5082553272199115, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1403, Loss: 0.22650092218761195, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1404, Loss: 0.35528206303873755, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1405, Loss: 0.2699876910607431, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1406, Loss: 0.24624922302983107, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1407, Loss: 0.42299097756460635, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1408, Loss: 0.3051949979916798, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1409, Loss: 0.2321471008925727, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1410, Loss: 0.38378864650985023, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1411, Loss: 0.2731858414896184, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1412, Loss: 0.49605318969932466, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1413, Loss: 0.4489625139619043, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1414, Loss: 0.32552020123878844, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1415, Loss: 0.32632956837385385, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1416, Loss: 0.5179108686504643, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1417, Loss: 0.30858232293630444, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1418, Loss: 0.2665198801009316, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1419, Loss: 0.39926730419720274, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1420, Loss: 0.281472273842799, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1421, Loss: 0.3164724685773222, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1422, Loss: 0.28441042503169517, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1423, Loss: 0.3178445727072045, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1424, Loss: 0.4697276682818145, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1425, Loss: 0.4374685710709083, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1426, Loss: 0.28329308580732504, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1427, Loss: 0.28490869939297647, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1428, Loss: 0.53106349855914, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1429, Loss: 0.31494808520263406, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1430, Loss: 0.3301586746272268, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1431, Loss: 0.2501943188735974, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1432, Loss: 0.5025960440176107, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1433, Loss: 0.32883027367600554, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1434, Loss: 0.4804671249170358, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1435, Loss: 0.24391480305120947, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1436, Loss: 0.31728251958699727, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1437, Loss: 0.30087608978242597, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1438, Loss: 0.3510218660416472, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1439, Loss: 0.251564070078306, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1440, Loss: 0.39160611441603627, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1441, Loss: 0.42188944333010814, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1442, Loss: 0.45784435474460794, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1443, Loss: 0.3429202545814424, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1444, Loss: 0.38583066893265555, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1445, Loss: 0.3105499926700165, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1446, Loss: 0.31844721515989544, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1447, Loss: 0.24292194793760996, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1448, Loss: 0.27325383584897106, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1449, Loss: 0.33150753025572655, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1450, Loss: 0.25793348586581094, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1451, Loss: 0.26855889184104204, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1452, Loss: 0.3198244236087796, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1453, Loss: 0.24938113241691118, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1454, Loss: 0.3206847002291442, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1455, Loss: 0.30941169955718706, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1456, Loss: 0.23405614248004353, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1457, Loss: 0.2708872132108829, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1458, Loss: 0.3346004895128858, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1459, Loss: 0.3170906788560894, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1460, Loss: 0.3147325739167506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1461, Loss: 0.49566284904330427, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1462, Loss: 0.3969853945897578, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1463, Loss: 0.2646931202716636, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1464, Loss: 0.3094726933971381, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1465, Loss: 0.2980616505401187, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1466, Loss: 0.32556488556775337, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1467, Loss: 0.4287505478999348, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1468, Loss: 0.30013064248506993, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1469, Loss: 0.3919651833571858, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1470, Loss: 0.4894975383151592, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1471, Loss: 0.33393997262317, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1472, Loss: 0.28423473099946295, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1473, Loss: 0.285222413186506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1474, Loss: 0.27529164516667676, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1475, Loss: 0.3874260144780613, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1476, Loss: 0.29871952761418696, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1477, Loss: 0.37759710731132234, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1478, Loss: 0.2711513958542791, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1479, Loss: 0.36832303250289966, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1480, Loss: 0.29938564688066915, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1481, Loss: 0.4657585106728821, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1482, Loss: 0.33858100702133653, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1483, Loss: 0.3236988463388815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1484, Loss: 0.4837857660006748, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1485, Loss: 0.5785023370263402, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1486, Loss: 0.2591531111267361, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1487, Loss: 0.3758263463833503, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1488, Loss: 0.2580429031268422, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1489, Loss: 0.2961070575196044, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1490, Loss: 0.29448247656005855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1491, Loss: 0.29807417833228556, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1492, Loss: 0.29891078214247574, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1493, Loss: 0.27232740354303553, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1494, Loss: 0.29317785046070105, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1495, Loss: 0.3268549894991677, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1496, Loss: 0.25140504020169785, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1497, Loss: 0.4872267584185343, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1498, Loss: 0.2818158014630955, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1499, Loss: 0.36693449669042855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1500, Loss: 0.4178248795619518, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1501, Loss: 0.25524040244864754, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1502, Loss: 0.30247191487095304, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1503, Loss: 0.44664044097092415, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1504, Loss: 0.2574361833984079, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1505, Loss: 0.3615149996887422, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1506, Loss: 0.2658506937021154, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1507, Loss: 0.26966429026040123, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1508, Loss: 0.3451973405674016, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1509, Loss: 0.2851411358112904, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1510, Loss: 0.34231109488279254, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1511, Loss: 0.28190240280565076, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1512, Loss: 1.213844551192255, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1513, Loss: 0.3296870781914485, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1514, Loss: 0.3519741617175489, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1515, Loss: 0.3251218136316517, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1516, Loss: 0.3262528579636072, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1517, Loss: 0.3182391393701511, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1518, Loss: 0.3295545637722932, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1519, Loss: 0.3119038823637515, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1520, Loss: 0.41290121512268174, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1521, Loss: 0.28063592578230706, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1522, Loss: 0.6312515729810916, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1523, Loss: 0.2948134709802042, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1524, Loss: 0.2843259620151705, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1525, Loss: 0.5231430452506669, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1526, Loss: 0.2918836798501359, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1527, Loss: 0.516296062817293, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1528, Loss: 0.3202454602802396, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1529, Loss: 0.2954515038669703, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1530, Loss: 0.3181854461743985, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1531, Loss: 0.26477852918595235, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1532, Loss: 0.3288544801262715, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1533, Loss: 0.4045800238972188, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1534, Loss: 0.28416452862050506, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1535, Loss: 0.3517472869927893, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1536, Loss: 0.4045545145320988, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1537, Loss: 0.43534561517979214, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1538, Loss: 0.2601608782524294, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1539, Loss: 0.2731961659380478, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1540, Loss: 0.3708524921020917, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1541, Loss: 0.3167858391768798, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1542, Loss: 0.37125669849365106, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1543, Loss: 0.2886873564367748, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1544, Loss: 0.2903394200474106, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1545, Loss: 0.4005388307734575, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1546, Loss: 0.27182458831455114, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1547, Loss: 0.3669655620626139, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1548, Loss: 0.2878196094594862, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1549, Loss: 0.33755239265245435, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1550, Loss: 0.35691867467500915, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1551, Loss: 0.565438298125247, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1552, Loss: 0.22412814963740493, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1553, Loss: 0.45986079726249096, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1554, Loss: 0.41264240578788836, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1555, Loss: 0.44720117056539677, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1556, Loss: 0.3462956496438768, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1557, Loss: 0.27830518628121625, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1558, Loss: 0.23972735567534867, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1559, Loss: 0.30533672572864134, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1560, Loss: 0.4547890762807, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1561, Loss: 0.28030173938152525, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1562, Loss: 0.5671334491107674, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1563, Loss: 0.30410199876127014, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1564, Loss: 0.28571975783808296, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1565, Loss: 0.340419764164313, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1566, Loss: 0.23655721677154465, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1567, Loss: 0.42636301396035725, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1568, Loss: 0.251246808754621, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1569, Loss: 0.4274199824957586, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1570, Loss: 0.2904455610835645, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1571, Loss: 0.23782182654878975, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1572, Loss: 0.3006187771004757, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1573, Loss: 0.32654654757394563, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1574, Loss: 0.2553356693199932, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1575, Loss: 0.5038918877225552, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1576, Loss: 0.4350216248717308, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1577, Loss: 0.22321766467279566, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1578, Loss: 0.36341672095645033, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1579, Loss: 0.5608673045005294, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1580, Loss: 0.4375881328636875, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1581, Loss: 0.30336631956344057, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1582, Loss: 0.39771054973248054, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1583, Loss: 0.47991152468356935, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1584, Loss: 0.2604297493555446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1585, Loss: 0.2794839356831253, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1586, Loss: 0.27495350321551293, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1587, Loss: 0.23851838403304515, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1588, Loss: 0.30085072675537317, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1589, Loss: 0.5294866544085568, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1590, Loss: 0.3797188763473698, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1591, Loss: 0.22692715138739142, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1592, Loss: 0.3038647939673442, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1593, Loss: 0.273418323074362, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1594, Loss: 0.4651877694452996, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1595, Loss: 0.38418383470648965, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1596, Loss: 0.2579759643519312, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1597, Loss: 0.37299916268022537, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1598, Loss: 0.26542657119820384, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1599, Loss: 0.25993552772417805, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1600, Loss: 0.3637095673047657, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1601, Loss: 0.3477154736774716, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1602, Loss: 0.27276381644792913, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1603, Loss: 0.4302144306159979, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1604, Loss: 0.48265533908223746, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1605, Loss: 0.3491877031118178, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1606, Loss: 0.3263243285708974, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1607, Loss: 0.37793464140235256, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1608, Loss: 0.373363059402566, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1609, Loss: 0.3078496770258662, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1610, Loss: 0.3617475440718642, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1611, Loss: 0.28319778309121507, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1612, Loss: 0.2730857438086109, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1613, Loss: 0.4163518915736219, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1614, Loss: 0.4957147227120968, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1615, Loss: 0.34400080385850407, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1616, Loss: 0.7840412158687985, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1617, Loss: 0.2434756925229612, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1618, Loss: 0.4330629956736577, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1619, Loss: 0.39082211677893997, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1620, Loss: 0.5195162663813132, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1621, Loss: 0.26281742778793127, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1622, Loss: 0.26933906373031186, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1623, Loss: 0.2779920271307803, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1624, Loss: 0.34490424886454607, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1625, Loss: 0.40980756629275483, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1626, Loss: 0.26767268920915377, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1627, Loss: 0.3118920589268909, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1628, Loss: 0.31197166602761317, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1629, Loss: 0.27360136671920815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1630, Loss: 0.2847979716290765, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1631, Loss: 0.26413926480896804, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1632, Loss: 0.3393409425157911, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1633, Loss: 0.27168819099428837, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1634, Loss: 0.39221215435479073, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1635, Loss: 0.2693216314708513, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1636, Loss: 0.271103015704717, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1637, Loss: 0.2663579886507569, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1638, Loss: 0.26179750329893003, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1639, Loss: 0.36992487867730794, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1640, Loss: 0.37298170614585446, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1641, Loss: 0.3938396051669283, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1642, Loss: 0.4765521387280701, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1643, Loss: 0.23890977550643797, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1644, Loss: 0.48100977055151295, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1645, Loss: 0.4028858937125738, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1646, Loss: 0.3416805062310976, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1647, Loss: 0.3030025949941037, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1648, Loss: 0.26513877820829673, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1649, Loss: 0.37838488435856144, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1650, Loss: 0.5433265980499059, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1651, Loss: 0.7969845610463501, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1652, Loss: 0.2887389677401717, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1653, Loss: 0.3510703752788543, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1654, Loss: 0.39418072609328353, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1655, Loss: 0.3325707195278079, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1656, Loss: 0.421024629754129, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1657, Loss: 0.3729143768080908, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1658, Loss: 0.40318425630274746, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1659, Loss: 0.4178826956759477, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1660, Loss: 0.28989367533347177, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1661, Loss: 0.6745641860858764, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1662, Loss: 0.37382618356883857, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1663, Loss: 0.26399164253257706, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1664, Loss: 0.3080076711408184, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1665, Loss: 0.27256293534711873, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1666, Loss: 0.600252873230672, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1667, Loss: 0.3688715142442198, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1668, Loss: 0.4440355587388707, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1669, Loss: 0.45247232902024304, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1670, Loss: 0.28459457837402014, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1671, Loss: 0.3499143082417232, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1672, Loss: 0.3119801852258824, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1673, Loss: 0.4379911115851715, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1674, Loss: 0.4344504334333954, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1675, Loss: 0.29905699026337135, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1676, Loss: 0.40668773701681016, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1677, Loss: 0.33378315082050863, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1678, Loss: 0.5352441737032341, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1679, Loss: 0.27414545227761605, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1680, Loss: 0.35120625114842646, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1681, Loss: 0.32183884305202315, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1682, Loss: 0.28559365066012454, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1683, Loss: 0.3479323175187552, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1684, Loss: 0.3368193484000649, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1685, Loss: 0.36508348999084694, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1686, Loss: 0.5526474357769913, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1687, Loss: 0.2785015204674661, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1688, Loss: 0.2485659472455395, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1689, Loss: 0.3935488614774312, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1690, Loss: 0.2535908689692849, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1691, Loss: 0.48367702197461715, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1692, Loss: 0.39754770227690034, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1693, Loss: 0.23721464219345617, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1694, Loss: 0.4232474265159058, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1695, Loss: 0.23614807327704265, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1696, Loss: 0.4243521686608047, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1697, Loss: 0.6836737986657674, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1698, Loss: 0.286854747783325, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1699, Loss: 0.3089540632890953, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1700, Loss: 0.32658009889827855, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1701, Loss: 0.23716259741303292, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1702, Loss: 0.2794431062736108, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1703, Loss: 0.35049826237593196, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1704, Loss: 0.4538213600570823, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1705, Loss: 0.3681811271924934, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1706, Loss: 0.27486033319202074, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1707, Loss: 0.35774128959843765, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1708, Loss: 0.3555669566309012, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1709, Loss: 0.26231966445601945, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1710, Loss: 0.29048796221006445, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1711, Loss: 0.4194917364241608, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1712, Loss: 0.49431959815831616, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1713, Loss: 0.2789573690719365, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1714, Loss: 0.3589329781995198, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1715, Loss: 0.3857069336983162, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1716, Loss: 0.38518841144699206, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1717, Loss: 0.2515470711840457, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1718, Loss: 0.5619601969243815, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1719, Loss: 0.34325128827687057, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1720, Loss: 0.43311606870079056, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1721, Loss: 0.30041037971586326, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1722, Loss: 0.4731884298287799, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1723, Loss: 0.24885003325100824, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1724, Loss: 0.2945262117582564, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1725, Loss: 0.3496641598092092, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1726, Loss: 0.26716562094808183, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1727, Loss: 0.5774250461095802, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1728, Loss: 0.3710858446261095, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1729, Loss: 0.4082078657940088, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1730, Loss: 0.44126394619145765, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1731, Loss: 0.33612471631427165, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1732, Loss: 0.28909856197852124, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1733, Loss: 0.6264104528731513, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1734, Loss: 0.4055084055511152, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1735, Loss: 0.3963090846553665, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1736, Loss: 0.35070708578469006, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1737, Loss: 0.33595825417045627, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1738, Loss: 0.23495162461489313, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1739, Loss: 0.3506535610679749, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1740, Loss: 0.3133654767596824, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1741, Loss: 0.2944922131858373, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1742, Loss: 0.23446980555901575, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1743, Loss: 0.3642187355272314, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1744, Loss: 0.5216363908089335, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1745, Loss: 0.35380223838221125, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1746, Loss: 0.2618679326534915, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1747, Loss: 0.24189109067567469, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1748, Loss: 0.46727603114022565, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1749, Loss: 0.3778278309247425, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1750, Loss: 0.5387165117995165, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1751, Loss: 0.25463876291052534, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1752, Loss: 0.2428174432499359, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1753, Loss: 0.290537303271046, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1754, Loss: 0.3409044556710857, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1755, Loss: 0.2510930008452375, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1756, Loss: 0.284403846383029, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1757, Loss: 0.47117329980512856, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1758, Loss: 0.2628422106280498, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1759, Loss: 0.3278415099311721, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1760, Loss: 0.3023071422709095, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1761, Loss: 0.30709145433352036, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1762, Loss: 0.4613941286341169, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1763, Loss: 0.5337727245621434, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1764, Loss: 0.3455887691629479, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1765, Loss: 0.34722224270577906, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1766, Loss: 0.267732873177374, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1767, Loss: 0.33919420824609225, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1768, Loss: 0.33338366006343645, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1769, Loss: 0.4304127296746934, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1770, Loss: 0.37591749266765084, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1771, Loss: 0.33886258184575946, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1772, Loss: 0.3681268912273476, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1773, Loss: 0.3782594976967178, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1774, Loss: 0.303134813628804, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1775, Loss: 0.31274904345892934, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1776, Loss: 0.2863322014012798, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1777, Loss: 0.2498894117501428, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1778, Loss: 0.23411379513003217, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1779, Loss: 0.4105045692214767, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1780, Loss: 0.22977811806539217, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1781, Loss: 0.416565426294032, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1782, Loss: 0.5081393342641248, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1783, Loss: 0.31293584937785096, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1784, Loss: 0.27795306914008716, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1785, Loss: 0.3727237990636273, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1786, Loss: 0.42455932103446703, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1787, Loss: 0.282985246148311, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1788, Loss: 0.2703880609252086, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1789, Loss: 0.2977442479164922, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1790, Loss: 0.39456204112291227, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1791, Loss: 0.3339744525260203, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1792, Loss: 0.23643693212446754, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1793, Loss: 0.5087443776187964, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1794, Loss: 0.2798578297706134, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1795, Loss: 0.2655856537789165, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1796, Loss: 0.4225052517609005, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1797, Loss: 0.26787702152947795, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1798, Loss: 0.24250771711554578, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1799, Loss: 0.2677129813678898, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1800, Loss: 0.26199247320380586, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1801, Loss: 0.307955426340468, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1802, Loss: 0.3778130074760844, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1803, Loss: 0.2700336212639501, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1804, Loss: 0.39868243480417176, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1805, Loss: 0.41566414873701385, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1806, Loss: 0.3422856202259327, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1807, Loss: 0.38380484857766006, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1808, Loss: 0.6692702857140201, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1809, Loss: 0.2504714722197289, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1810, Loss: 0.3214177003022241, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1811, Loss: 0.29911939803981386, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1812, Loss: 0.32578908372502474, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1813, Loss: 0.26529376815763617, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1814, Loss: 0.3056352816232845, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1815, Loss: 0.26564791620198763, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1816, Loss: 0.25406738964211584, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1817, Loss: 0.3731471904695477, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1818, Loss: 0.4316988734084979, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1819, Loss: 0.26947665639458085, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1820, Loss: 0.3227036745764007, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1821, Loss: 0.3183090023470363, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1822, Loss: 0.39033376903805106, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1823, Loss: 0.2522936497440679, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1824, Loss: 0.28640516982516073, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1825, Loss: 0.403525625333618, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1826, Loss: 0.8378852340128681, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1827, Loss: 0.2889930624145381, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1828, Loss: 0.5236774376468456, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1829, Loss: 0.35018889242745793, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1830, Loss: 0.2938782845053457, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1831, Loss: 0.27241725244750825, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1832, Loss: 0.25279256202678346, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1833, Loss: 0.3247499857490668, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1834, Loss: 0.3194841183851037, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1835, Loss: 0.3547442061636793, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1836, Loss: 0.3565081117886735, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1837, Loss: 0.28027029724275293, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1838, Loss: 0.3031150269884583, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1839, Loss: 0.25364496181271873, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1840, Loss: 0.34045914030645585, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1841, Loss: 0.33770656857057635, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1842, Loss: 0.3063929422677878, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1843, Loss: 0.260385326008829, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1844, Loss: 0.23961907237888497, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1845, Loss: 0.2341939695631961, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1846, Loss: 0.2798966103282876, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1847, Loss: 0.2848705297814819, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1848, Loss: 0.2480076352969268, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1849, Loss: 0.45662177014395144, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1850, Loss: 0.29371024283025987, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1851, Loss: 0.24842024852743264, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1852, Loss: 0.6786731732935675, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1853, Loss: 0.2649188188356255, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1854, Loss: 0.23228476475775392, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1855, Loss: 0.36472515486495216, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1856, Loss: 0.4649800698425327, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1857, Loss: 0.26960935295502997, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1858, Loss: 0.3211350489848239, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1859, Loss: 0.6300639257576606, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1860, Loss: 0.607272152549645, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1861, Loss: 0.5307086491795199, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1862, Loss: 0.2709057599077913, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1863, Loss: 0.40204228583054785, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1864, Loss: 0.3207494720938187, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1865, Loss: 0.34164974380256097, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1866, Loss: 0.35824245936958, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1867, Loss: 0.3239796803888649, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1868, Loss: 0.23867067915724688, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1869, Loss: 0.9531773214726752, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1870, Loss: 0.27227982195017997, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1871, Loss: 0.24474833679354263, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1872, Loss: 0.32780624856091695, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1873, Loss: 0.284837281182388, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1874, Loss: 0.2510321449164494, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Batch 1875, Loss: 0.47317418111119736, Batch Size: 32, Learning Rate: 8.174715751171873e-05\n",
      "Epoch 10, Updated Learning Rate: 6.948508388496091e-05\n",
      "Epoch 10, Average Loss: 0.35094546637163343, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1, Loss: 0.7691837311172606, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 2, Loss: 0.3765098484333176, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 3, Loss: 0.26954013331934884, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 4, Loss: 0.3920984144217313, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 5, Loss: 0.27184019315979874, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 6, Loss: 0.24866568639360204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 7, Loss: 0.34642237285440125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 8, Loss: 0.5006523029457423, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 9, Loss: 0.3026036971126719, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 10, Loss: 0.6121003085652383, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 11, Loss: 0.3805478606626757, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 12, Loss: 0.5985638823542933, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 13, Loss: 0.2660620053884101, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 14, Loss: 0.30540288409352295, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 15, Loss: 0.3116082156562123, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 16, Loss: 0.40967269898596403, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 17, Loss: 0.3387180575397174, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 18, Loss: 0.2473910000574964, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 19, Loss: 0.23144745442842135, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 20, Loss: 0.3312173573682332, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 21, Loss: 0.5631230739972928, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 22, Loss: 0.4392691428759702, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 23, Loss: 0.3877203573210215, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 24, Loss: 0.5328646794374865, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 25, Loss: 0.4553393421792191, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 26, Loss: 0.35164715260753715, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 27, Loss: 0.3995393378623846, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 28, Loss: 0.32117331532860915, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 29, Loss: 0.5586036175342028, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 30, Loss: 0.28264628587957136, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 31, Loss: 0.33981402237700453, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 32, Loss: 0.23644668338328353, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 33, Loss: 0.3139853851299878, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 34, Loss: 0.28138452301247574, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 35, Loss: 0.2655351126134583, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 36, Loss: 0.26273140037524645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 37, Loss: 0.27831705565967924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 38, Loss: 0.36018797965735755, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 39, Loss: 0.27375754232382193, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 40, Loss: 0.3673389851909825, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 41, Loss: 0.24307293708175068, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 42, Loss: 0.28634795955713577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 43, Loss: 0.5002288051994818, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 44, Loss: 0.2898162321620924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 45, Loss: 0.28493114916443507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 46, Loss: 0.39859827909945755, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 47, Loss: 0.24040017373606204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 48, Loss: 0.4029827230907083, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 49, Loss: 0.49828561190848303, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 50, Loss: 0.2923934888951564, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 51, Loss: 0.49946637406782823, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 52, Loss: 0.25305492274660474, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 53, Loss: 0.31008338184475887, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 54, Loss: 0.3934224414680859, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 55, Loss: 0.30767526263343226, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 56, Loss: 0.3991210528695337, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 57, Loss: 0.3053647607312681, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 58, Loss: 0.3415241025190297, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 59, Loss: 0.24683697621282769, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 60, Loss: 0.383883799204382, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 61, Loss: 0.3394527391266359, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 62, Loss: 0.35423075836846263, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 63, Loss: 0.4185132658314534, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 64, Loss: 0.35705383241497196, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 65, Loss: 0.3237440663535696, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 66, Loss: 0.31768645063825685, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 67, Loss: 0.3385023259405072, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 68, Loss: 0.2865375282864602, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 69, Loss: 0.3460360304628018, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 70, Loss: 0.49441909823439295, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 71, Loss: 0.503482790219492, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 72, Loss: 0.27478950443717975, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 73, Loss: 0.24219131934158353, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 74, Loss: 0.2812035008896957, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 75, Loss: 0.46772180217301335, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 76, Loss: 0.26835827157837516, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 77, Loss: 0.23643653640639398, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 78, Loss: 0.33392986382453416, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 79, Loss: 0.2696184562113796, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 80, Loss: 0.4882685370142454, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 81, Loss: 0.3602472263640901, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 82, Loss: 0.6785501407435562, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 83, Loss: 0.3159589454183369, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 84, Loss: 0.24767503694086507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 85, Loss: 0.5721018240426314, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 86, Loss: 0.37049752400401303, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 87, Loss: 0.31564254471198216, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 88, Loss: 0.23756277907431134, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 89, Loss: 0.24410510761693838, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 90, Loss: 0.276997031367175, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 91, Loss: 0.23346802041846176, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 92, Loss: 0.23926313022811543, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 93, Loss: 0.3507127369530457, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 94, Loss: 0.26171780202963035, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 95, Loss: 0.4873911331108247, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 96, Loss: 0.30942847870371204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 97, Loss: 0.2632144732105625, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 98, Loss: 0.3012633916201258, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 99, Loss: 0.2760404519486806, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 100, Loss: 0.4315287110291796, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 101, Loss: 0.2895593261310181, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 102, Loss: 0.24508703596668063, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 103, Loss: 0.3930676115907703, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 104, Loss: 0.32059279567340837, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 105, Loss: 0.35484862124435984, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 106, Loss: 0.2970544598005835, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 107, Loss: 0.2987558336594205, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 108, Loss: 0.24608410748901052, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 109, Loss: 0.42883150916887086, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 110, Loss: 0.323114181669017, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 111, Loss: 0.4473755641513296, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 112, Loss: 0.3328208178202085, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 113, Loss: 0.3962124766301145, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 114, Loss: 0.3533186405020314, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 115, Loss: 0.2593916821468737, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 116, Loss: 0.2559229312502797, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 117, Loss: 0.40265158255938244, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 118, Loss: 0.30545269317260537, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 119, Loss: 0.39761927389883756, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 120, Loss: 0.30647605212510387, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 121, Loss: 0.35317290357581577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 122, Loss: 0.3158666409242147, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 123, Loss: 0.43703991601586534, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 124, Loss: 0.242852557215054, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 125, Loss: 0.4058298292555319, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 126, Loss: 0.3058579791402808, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 127, Loss: 0.3056167825201299, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 128, Loss: 0.27223899632997656, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 129, Loss: 0.599753694507571, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 130, Loss: 0.3853388759522956, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 131, Loss: 0.28735043966439866, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 132, Loss: 0.48044413236135985, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 133, Loss: 0.36308133252129454, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 134, Loss: 0.4527908996491258, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 135, Loss: 0.5122750727017615, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 136, Loss: 0.47176428992314645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 137, Loss: 0.29916463375655034, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 138, Loss: 0.28852934703927013, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 139, Loss: 0.3347282992085632, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 140, Loss: 0.44226467911616163, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 141, Loss: 0.29202364812058645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 142, Loss: 0.26109979631091285, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 143, Loss: 0.3382330767018535, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 144, Loss: 0.280709993749842, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 145, Loss: 0.2635852748271407, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 146, Loss: 0.3691989874712577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 147, Loss: 0.3682822145330703, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 148, Loss: 0.29021051370749934, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 149, Loss: 0.26349166474671193, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 150, Loss: 0.4196071192504959, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 151, Loss: 0.4029572408966565, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 152, Loss: 0.3733328064205349, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 153, Loss: 0.2533253645116893, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 154, Loss: 0.44089672826394, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 155, Loss: 0.22836498581329512, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 156, Loss: 0.3920328059982361, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 157, Loss: 0.23322467797759758, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 158, Loss: 0.2944815007870113, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 159, Loss: 0.2851840645808995, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 160, Loss: 0.38305348029264175, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 161, Loss: 0.3398205946250832, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 162, Loss: 0.2583513426284165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 163, Loss: 0.2680767420534859, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 164, Loss: 0.4885249360035199, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 165, Loss: 0.2853131116073969, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 166, Loss: 0.2820258112885043, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 167, Loss: 0.286533441161136, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 168, Loss: 0.311878301850785, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 169, Loss: 0.31226043636966505, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 170, Loss: 0.23812053192492064, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 171, Loss: 0.36706097060085263, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 172, Loss: 0.27078610609761933, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 173, Loss: 0.24209672859862777, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 174, Loss: 0.39651599788101577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 175, Loss: 0.2706788516080417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 176, Loss: 0.2995311126447141, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 177, Loss: 0.4038196645507849, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 178, Loss: 0.2716560712963809, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 179, Loss: 0.3318740915204682, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 180, Loss: 0.3149398801447109, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 181, Loss: 0.521691502091525, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 182, Loss: 0.39649114695607723, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 183, Loss: 0.6968426887173251, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 184, Loss: 0.4193963161879474, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 185, Loss: 0.2792738368617171, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 186, Loss: 0.26462184188946486, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 187, Loss: 0.6156036055071332, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 188, Loss: 0.41725720733057237, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 189, Loss: 0.48003547128594837, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 190, Loss: 0.2728108624737745, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 191, Loss: 0.40693632959381165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 192, Loss: 0.3587122932048441, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 193, Loss: 0.28719425885086336, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 194, Loss: 0.4569851514334867, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 195, Loss: 0.3215487648081663, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 196, Loss: 0.5064397306092452, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 197, Loss: 0.33859059769959987, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 198, Loss: 0.24204256107819463, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 199, Loss: 0.4244488817484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 200, Loss: 0.3459261590503263, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 201, Loss: 0.2705796462663266, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 202, Loss: 0.24453024455512637, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 203, Loss: 0.40698892037226375, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 204, Loss: 0.25022902214992665, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 205, Loss: 0.3398210520044668, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 206, Loss: 0.3646506155727164, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 207, Loss: 0.6087244805624399, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 208, Loss: 0.4571768014591665, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 209, Loss: 0.5146610262851509, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 210, Loss: 0.3133096920579812, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 211, Loss: 0.4886305604553697, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 212, Loss: 0.3712139468810228, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 213, Loss: 0.3923134753534621, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 214, Loss: 0.3708331370279301, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 215, Loss: 0.36026098719083166, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 216, Loss: 0.33073207978610497, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 217, Loss: 0.3278299380094236, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 218, Loss: 0.3226143052442697, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 219, Loss: 0.27135599949017025, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 220, Loss: 0.46371973932560984, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 221, Loss: 0.24654117466691874, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 222, Loss: 0.31974935790138165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 223, Loss: 0.38546057653065685, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 224, Loss: 0.2859286759357357, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 225, Loss: 0.48403036399664795, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 226, Loss: 0.4752565614258637, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 227, Loss: 0.33747974415758697, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 228, Loss: 0.47526627585442693, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 229, Loss: 0.38951637483669005, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 230, Loss: 0.46204589383244177, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 231, Loss: 0.28647260462172686, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 232, Loss: 0.6466025423014119, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 233, Loss: 0.3649572114543529, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 234, Loss: 0.33551430061736764, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 235, Loss: 0.398146909523607, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 236, Loss: 0.3455800529835982, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 237, Loss: 0.337809231390696, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 238, Loss: 0.23319735647902384, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 239, Loss: 0.3239614292477353, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 240, Loss: 0.24109210709035747, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 241, Loss: 0.26033539816851775, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 242, Loss: 0.3024634849805486, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 243, Loss: 0.23654546988605785, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 244, Loss: 0.46660079595816506, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 245, Loss: 0.2664315540859438, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 246, Loss: 0.4013228226914804, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 247, Loss: 0.3064897592553896, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 248, Loss: 0.2760160690011422, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 249, Loss: 0.2952077123474638, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 250, Loss: 0.7334100465295639, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 251, Loss: 0.26567177979290146, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 252, Loss: 0.3138285801117857, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 253, Loss: 0.2511208643003181, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 254, Loss: 0.2301038422399172, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 255, Loss: 0.2976949081009176, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 256, Loss: 0.38420276456724234, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 257, Loss: 0.3748209174727784, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 258, Loss: 0.36948291091067775, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 259, Loss: 0.29022683853939335, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 260, Loss: 0.4116568110217148, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 261, Loss: 0.4441305288712425, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 262, Loss: 0.262643050634496, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 263, Loss: 0.28178942343008123, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 264, Loss: 0.38982874640404164, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 265, Loss: 0.2962124409899741, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 266, Loss: 0.26340069936166977, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 267, Loss: 0.3763122598097165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 268, Loss: 0.27881896335689277, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 269, Loss: 0.3603911210701635, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 270, Loss: 0.46524291106656457, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 271, Loss: 0.5404815716914049, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 272, Loss: 0.260961838406818, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 273, Loss: 0.2690485092448074, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 274, Loss: 0.357668666683851, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 275, Loss: 0.5170362916587395, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 276, Loss: 0.39187298222380085, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 277, Loss: 0.30445213727313014, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 278, Loss: 0.3505998350856886, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 279, Loss: 0.3912858413410667, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 280, Loss: 0.3478346574085435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 281, Loss: 0.22687641256871996, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 282, Loss: 0.2517123749557662, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 283, Loss: 0.34615051395460683, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 284, Loss: 0.33564345630401937, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 285, Loss: 0.24371239214675805, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 286, Loss: 0.3062083807917274, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 287, Loss: 0.399158029151957, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 288, Loss: 0.49213114626387533, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 289, Loss: 0.2783410206103733, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 290, Loss: 0.3139968035465338, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 291, Loss: 0.3980062280625008, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 292, Loss: 0.39682153736713466, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 293, Loss: 0.32167513831808603, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 294, Loss: 0.2359751502772227, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 295, Loss: 0.2899863547845396, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 296, Loss: 0.30205225883523623, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 297, Loss: 0.32973142039917125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 298, Loss: 0.385790707324989, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 299, Loss: 0.2636337108570509, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 300, Loss: 0.40151235877527613, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 301, Loss: 0.33328660847033825, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 302, Loss: 0.288393537762458, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 303, Loss: 0.483247332759413, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 304, Loss: 0.41724264691196883, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 305, Loss: 0.2339623749695939, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 306, Loss: 0.36855304758184426, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 307, Loss: 0.38808968874866645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 308, Loss: 0.29595214087146826, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 309, Loss: 0.30819006071765626, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 310, Loss: 0.24773798664198557, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 311, Loss: 0.35161758007582056, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 312, Loss: 0.25422249296004773, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 313, Loss: 0.33735588053693866, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 314, Loss: 0.26040913270249405, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 315, Loss: 0.250378530839126, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 316, Loss: 0.31233626593145236, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 317, Loss: 0.46933295968876243, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 318, Loss: 0.26109249062404805, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 319, Loss: 0.31741181788317724, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 320, Loss: 0.2853624906192701, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 321, Loss: 0.4441159262103449, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 322, Loss: 0.33377051335066327, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 323, Loss: 0.2944279301725608, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 324, Loss: 0.27867429117707465, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 325, Loss: 0.2781029411393216, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 326, Loss: 0.30197055952112567, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 327, Loss: 0.27295187459936393, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 328, Loss: 0.2744143363077651, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 329, Loss: 0.3171368572057694, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 330, Loss: 0.28873133700058007, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 331, Loss: 0.516660839967954, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 332, Loss: 0.3565096147599195, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 333, Loss: 0.4125226642840104, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 334, Loss: 0.24990555007700535, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 335, Loss: 0.3265098410737961, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 336, Loss: 0.49071568477750044, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 337, Loss: 0.3087067361904096, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 338, Loss: 0.32296080050448256, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 339, Loss: 0.42545864733453376, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 340, Loss: 0.2537736153742112, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 341, Loss: 0.6342323951915911, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 342, Loss: 0.24447331493970267, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 343, Loss: 0.3703335419574867, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 344, Loss: 0.2955034247338241, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 345, Loss: 0.41856504812866024, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 346, Loss: 0.44333712259659397, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 347, Loss: 0.27570624353698636, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 348, Loss: 0.2829156006823266, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 349, Loss: 0.3481725544317398, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 350, Loss: 0.5222934212290308, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 351, Loss: 0.4632455936988997, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 352, Loss: 0.25079164871740456, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 353, Loss: 0.2589942035518337, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 354, Loss: 0.30354651293868656, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 355, Loss: 0.25244338584064946, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 356, Loss: 0.40905430520619335, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 357, Loss: 0.2697643672001439, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 358, Loss: 0.4204515293979602, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 359, Loss: 0.3375203341360831, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 360, Loss: 0.2528108449939771, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 361, Loss: 0.2873128168814571, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 362, Loss: 0.26095674283057596, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 363, Loss: 0.3891056827579569, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 364, Loss: 0.3727886512738779, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 365, Loss: 0.3806381571056707, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 366, Loss: 0.29397431407223906, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 367, Loss: 0.31947003824051057, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 368, Loss: 0.3798602653154507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 369, Loss: 0.5276903988217064, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 370, Loss: 0.32082032794297094, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 371, Loss: 0.37357799166999917, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 372, Loss: 0.35412249735104073, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 373, Loss: 0.538713533176537, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 374, Loss: 0.35403590413608893, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 375, Loss: 0.2693129756749483, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 376, Loss: 0.32151136016223303, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 377, Loss: 0.5484384118738403, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 378, Loss: 0.31489534900783644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 379, Loss: 0.23324103650778805, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 380, Loss: 0.4244020527935005, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 381, Loss: 0.38971579434660586, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 382, Loss: 0.4602894100600189, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 383, Loss: 0.7712866123773066, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 384, Loss: 0.34577225357998437, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 385, Loss: 0.4091967254076262, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 386, Loss: 0.4291827192534027, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 387, Loss: 0.342750358443704, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 388, Loss: 0.4751973514595801, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 389, Loss: 0.3968855123422731, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 390, Loss: 0.47945806856621515, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 391, Loss: 0.34181697852128234, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 392, Loss: 0.23245086025172704, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 393, Loss: 0.26662056524055666, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 394, Loss: 0.32203396050364297, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 395, Loss: 0.3173542013020433, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 396, Loss: 0.42027237249065175, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 397, Loss: 0.31064552404700757, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 398, Loss: 0.29602540080002104, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 399, Loss: 0.31271657769548394, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 400, Loss: 0.3515796613170185, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 401, Loss: 0.2667141682627107, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 402, Loss: 0.339316332488917, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 403, Loss: 0.29656877794894293, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 404, Loss: 0.26691011796985936, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 405, Loss: 0.39839342121738275, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 406, Loss: 0.4243873233043902, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 407, Loss: 0.32093243103956837, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 408, Loss: 0.35729266383717984, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 409, Loss: 0.29418051541127577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 410, Loss: 0.3501677783366719, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 411, Loss: 0.435244208110926, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 412, Loss: 0.22452597697678559, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 413, Loss: 0.5049921433091169, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 414, Loss: 0.6909498333851678, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 415, Loss: 0.47368205302943744, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 416, Loss: 0.44963400546247073, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 417, Loss: 0.29670853483125825, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 418, Loss: 0.28648987229533296, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 419, Loss: 0.3693840029751947, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 420, Loss: 0.460798837010704, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 421, Loss: 0.3439645028049093, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 422, Loss: 0.34630070582492434, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 423, Loss: 0.2784804869576829, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 424, Loss: 0.2288943123878689, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 425, Loss: 0.3268913531670373, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 426, Loss: 0.2981747812284854, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 427, Loss: 0.32702570594422803, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 428, Loss: 0.24102140776015618, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 429, Loss: 0.27274842058474863, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 430, Loss: 0.35275296939173456, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 431, Loss: 0.2360965919547435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 432, Loss: 0.47520811954486264, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 433, Loss: 0.31320155215736467, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 434, Loss: 0.31873434854826915, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 435, Loss: 0.3284549076807039, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 436, Loss: 0.3914413383432489, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 437, Loss: 0.27214402171738417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 438, Loss: 0.30779081541533954, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 439, Loss: 0.3224548802979581, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 440, Loss: 0.3036355935739752, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 441, Loss: 0.43731624034121663, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 442, Loss: 0.2585990383445221, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 443, Loss: 0.34418489828726473, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 444, Loss: 0.2544050044376362, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 445, Loss: 0.40711584325734435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 446, Loss: 0.5244487104834598, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 447, Loss: 0.30590331517834507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 448, Loss: 0.5608121589387424, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 449, Loss: 0.30081053079495335, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 450, Loss: 0.39261153704710566, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 451, Loss: 0.23094307533253644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 452, Loss: 0.3045785538001847, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 453, Loss: 0.35064556647896494, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 454, Loss: 0.39018984385818367, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 455, Loss: 0.29912996795945357, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 456, Loss: 0.3176975821887749, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 457, Loss: 0.28968403842912627, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 458, Loss: 0.3073964131789294, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 459, Loss: 0.30985984542245204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 460, Loss: 0.38950209520053325, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 461, Loss: 0.2623490440421331, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 462, Loss: 0.2885745015093141, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 463, Loss: 0.26885807945432205, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 464, Loss: 0.2760367963028342, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 465, Loss: 0.42639333120953915, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 466, Loss: 0.291184366192618, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 467, Loss: 0.2948610003046599, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 468, Loss: 0.3985654882403703, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 469, Loss: 0.23722915524799731, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 470, Loss: 0.31444587484476083, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 471, Loss: 0.3264984706273221, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 472, Loss: 0.4967678373554295, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 473, Loss: 0.39132993196792376, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 474, Loss: 0.4663788238284724, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 475, Loss: 0.2556671352264733, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 476, Loss: 0.4043019175653566, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 477, Loss: 0.41175321771030043, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 478, Loss: 0.4572209651364373, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 479, Loss: 0.35208486343126144, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 480, Loss: 0.3183503515084825, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 481, Loss: 0.25075015084013597, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 482, Loss: 0.27766957151937705, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 483, Loss: 0.30985358841138655, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 484, Loss: 0.23306952972831205, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 485, Loss: 0.4627500514808081, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 486, Loss: 0.22701012931831782, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 487, Loss: 0.40252295441985875, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 488, Loss: 0.7126924637901249, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 489, Loss: 0.42507941632168117, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 490, Loss: 0.2395841281340562, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 491, Loss: 0.23319344106070955, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 492, Loss: 0.44290034639085096, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 493, Loss: 0.541681320984181, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 494, Loss: 0.4488525812811461, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 495, Loss: 0.2684383394538864, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 496, Loss: 0.2217617524796462, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 497, Loss: 0.42234262474762174, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 498, Loss: 0.2587360705507632, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 499, Loss: 0.47452437688955396, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 500, Loss: 0.49953053694759625, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 501, Loss: 0.2854940436755435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 502, Loss: 0.35498636266880484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 503, Loss: 0.5397900042626871, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 504, Loss: 0.3176809251698387, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 505, Loss: 0.3390425481208078, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 506, Loss: 0.299348501381778, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 507, Loss: 0.3589129455786372, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 508, Loss: 0.3544112969454566, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 509, Loss: 0.4228784393535708, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 510, Loss: 0.3943390828657077, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 511, Loss: 0.362310594670683, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 512, Loss: 0.430692455478549, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 513, Loss: 0.28630763546618065, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 514, Loss: 0.44615700768765565, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 515, Loss: 0.2825304994158772, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 516, Loss: 0.26456717913821465, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 517, Loss: 0.3319286609721565, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 518, Loss: 0.30986030419880206, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 519, Loss: 0.37564518177622175, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 520, Loss: 0.46418537009034655, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 521, Loss: 0.568285325287393, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 522, Loss: 0.3967991995917661, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 523, Loss: 0.2715762533757907, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 524, Loss: 0.4127536181055122, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 525, Loss: 0.2529555049138072, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 526, Loss: 0.3238511984333394, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 527, Loss: 0.4202629818222411, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 528, Loss: 0.485401751636354, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 529, Loss: 0.33895083857872965, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 530, Loss: 0.37423858845390195, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 531, Loss: 0.4013553595801483, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 532, Loss: 0.2767706308317119, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 533, Loss: 0.445085802414254, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 534, Loss: 0.43372631947637097, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 535, Loss: 0.2737737932071765, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 536, Loss: 0.41522523416268886, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 537, Loss: 0.40018196843191844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 538, Loss: 0.5331050684053603, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 539, Loss: 0.28676486934651246, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 540, Loss: 0.28308021397429434, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 541, Loss: 0.5064382970993778, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 542, Loss: 0.2499291738836078, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 543, Loss: 0.3380639349020142, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 544, Loss: 0.2670091035114649, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 545, Loss: 0.347047402077557, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 546, Loss: 0.3667993289963276, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 547, Loss: 0.22482324683040225, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 548, Loss: 0.352734434583627, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 549, Loss: 0.36982570420689453, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 550, Loss: 0.46546747385720205, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 551, Loss: 0.38760281180184586, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 552, Loss: 0.5008210226618881, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 553, Loss: 0.26839762838509584, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 554, Loss: 0.3426516401909654, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 555, Loss: 0.3345952610513928, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 556, Loss: 0.4675523526805907, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 557, Loss: 0.3306773601173464, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 558, Loss: 0.42497787966276146, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 559, Loss: 0.3347561954820194, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 560, Loss: 0.27058960550092165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 561, Loss: 0.3581506838589138, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 562, Loss: 0.2703603658495378, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 563, Loss: 0.3937572192155993, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 564, Loss: 0.26629381930016044, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 565, Loss: 0.4230519014136332, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 566, Loss: 0.3914884440715966, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 567, Loss: 0.2866730995838021, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 568, Loss: 0.4167062011419169, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 569, Loss: 0.31265069025376924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 570, Loss: 0.3072276786588752, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 571, Loss: 0.26704241160037323, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 572, Loss: 0.24608280126278254, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 573, Loss: 0.24052455326002725, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 574, Loss: 0.4980546484965428, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 575, Loss: 0.4848553733714569, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 576, Loss: 0.3050538529072043, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 577, Loss: 0.30387889994640577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 578, Loss: 0.273689965685818, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 579, Loss: 0.26554000692835644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 580, Loss: 0.5239794862157716, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 581, Loss: 0.29660244232745825, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 582, Loss: 0.3009601837027655, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 583, Loss: 0.322685505004751, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 584, Loss: 0.4920681849864315, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 585, Loss: 0.29769596433873924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 586, Loss: 0.3958429718076787, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 587, Loss: 0.3852928797401932, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 588, Loss: 0.5514177067734983, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 589, Loss: 0.27974167121480575, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 590, Loss: 0.2956903986246352, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 591, Loss: 0.5038387168252936, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 592, Loss: 0.44075270397887367, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 593, Loss: 0.3294599198672991, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 594, Loss: 0.41867080103859333, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 595, Loss: 0.39292337547548306, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 596, Loss: 0.25697777869687544, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 597, Loss: 0.3480786794204743, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 598, Loss: 0.3145237816488081, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 599, Loss: 0.2573600258264634, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 600, Loss: 0.3084187335059779, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 601, Loss: 0.5703557084903337, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 602, Loss: 0.30512182039203223, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 603, Loss: 0.27844295515435014, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 604, Loss: 0.28439324013461376, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 605, Loss: 0.49286918497078663, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 606, Loss: 0.2477725677167748, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 607, Loss: 0.4132669682273524, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 608, Loss: 0.24335548920458105, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 609, Loss: 0.43442809279358907, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 610, Loss: 0.3204475655280228, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 611, Loss: 0.25225721588130356, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 612, Loss: 0.23901586692312776, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 613, Loss: 0.3353775730320616, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 614, Loss: 0.33016225480476563, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 615, Loss: 0.3640676652285621, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 616, Loss: 0.5533796597284844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 617, Loss: 0.34198190916739607, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 618, Loss: 0.26884984763218334, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 619, Loss: 0.29351549730136206, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 620, Loss: 0.3038704310939968, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 621, Loss: 0.26321897315681353, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 622, Loss: 0.30273368313602156, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 623, Loss: 0.29296727230477915, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 624, Loss: 0.3212304775736692, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 625, Loss: 0.26465356411511937, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 626, Loss: 0.321092957991484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 627, Loss: 0.2556625570571517, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 628, Loss: 0.24954385347765876, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 629, Loss: 0.2854171600702964, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 630, Loss: 0.2564402851659295, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 631, Loss: 0.3550598395947069, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 632, Loss: 0.3178754973197841, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 633, Loss: 0.36615551067210633, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 634, Loss: 0.28993890654994986, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 635, Loss: 0.5952129297678649, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 636, Loss: 0.3609816875834857, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 637, Loss: 0.2301048389651854, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 638, Loss: 0.28454347154282944, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 639, Loss: 0.48262740910645235, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 640, Loss: 0.30559994978184557, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 641, Loss: 0.22863922541879447, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 642, Loss: 0.31234702811315207, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 643, Loss: 0.31528970048507726, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 644, Loss: 0.4359909409765862, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 645, Loss: 0.4596438700956784, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 646, Loss: 0.2803381377130962, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 647, Loss: 0.24947276866634666, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 648, Loss: 0.29480199754511294, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 649, Loss: 0.3576421056287499, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 650, Loss: 0.3618933072560529, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 651, Loss: 0.3420104340905673, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 652, Loss: 0.2314038300166202, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 653, Loss: 0.30156196083593273, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 654, Loss: 0.24312729653690182, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 655, Loss: 0.2908831772035487, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 656, Loss: 0.6191057719319242, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 657, Loss: 0.2829822108783816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 658, Loss: 0.4707072581205579, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 659, Loss: 0.5427666073746962, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 660, Loss: 0.35795518243403035, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 661, Loss: 0.25413892324065945, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 662, Loss: 0.2688773016618131, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 663, Loss: 0.2521780250302329, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 664, Loss: 0.31069688443024074, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 665, Loss: 0.39061388365270855, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 666, Loss: 0.40164518574162633, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 667, Loss: 0.46389215031578923, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 668, Loss: 0.3436926361823776, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 669, Loss: 0.28438671060222115, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 670, Loss: 0.242843719138524, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 671, Loss: 0.3713723414335106, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 672, Loss: 0.3238909911249271, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 673, Loss: 0.32077609505668025, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 674, Loss: 0.3317443570325451, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 675, Loss: 0.4662290058446771, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 676, Loss: 0.3897428219259682, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 677, Loss: 0.39516144729476554, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 678, Loss: 0.37031975002023454, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 679, Loss: 0.2674066721818457, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 680, Loss: 0.2796717462735234, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 681, Loss: 0.3882591415491571, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 682, Loss: 0.4133350025204604, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 683, Loss: 0.39855357866300234, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 684, Loss: 0.3766956964710277, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 685, Loss: 0.325833456075504, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 686, Loss: 0.38989352771138597, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 687, Loss: 0.2839444845648577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 688, Loss: 0.38646542116389015, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 689, Loss: 0.2655503231834601, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 690, Loss: 0.39924627061263906, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 691, Loss: 0.25944107271931216, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 692, Loss: 0.4291194289741549, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 693, Loss: 0.45184013147063584, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 694, Loss: 0.48695941906599793, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 695, Loss: 0.47425742943097404, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 696, Loss: 0.38525914722916765, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 697, Loss: 0.31034597024027233, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 698, Loss: 0.4783734833451759, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 699, Loss: 0.26113482373810687, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 700, Loss: 0.2917669680569132, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 701, Loss: 0.4679190849009873, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 702, Loss: 0.3341720974252383, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 703, Loss: 0.30980235876473317, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 704, Loss: 0.30544255336475745, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 705, Loss: 0.23558532080921474, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 706, Loss: 0.4331988032842133, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 707, Loss: 0.2546462348200516, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 708, Loss: 0.34910977647012953, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 709, Loss: 0.3421037823631642, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 710, Loss: 0.293760510010419, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 711, Loss: 0.6521511541697609, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 712, Loss: 0.4500232424903057, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 713, Loss: 0.3299936629439387, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 714, Loss: 0.6211708779443866, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 715, Loss: 0.27336772527547043, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 716, Loss: 0.3917826114793257, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 717, Loss: 0.3472223145591422, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 718, Loss: 0.5381795662966955, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 719, Loss: 0.3627776033844735, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 720, Loss: 0.39808571662965775, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 721, Loss: 0.3412307651501951, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 722, Loss: 0.23302663055511463, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 723, Loss: 0.4184722423715952, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 724, Loss: 0.2607722101803653, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 725, Loss: 0.24900901125325975, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 726, Loss: 0.3082310047285385, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 727, Loss: 0.45681114614861373, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 728, Loss: 0.26699235658498494, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 729, Loss: 0.3886997756462659, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 730, Loss: 0.570992768414115, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 731, Loss: 0.27630228197546197, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 732, Loss: 0.23436870168123125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 733, Loss: 0.4071728366814622, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 734, Loss: 0.5053659401534378, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 735, Loss: 0.3050290532011079, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 736, Loss: 0.2598681696666133, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 737, Loss: 0.31725643476029086, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 738, Loss: 0.5874000462025659, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 739, Loss: 0.44297870806127604, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 740, Loss: 0.25566202503213314, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 741, Loss: 0.3206079419906892, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 742, Loss: 0.4800663107003921, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 743, Loss: 0.32243231110601267, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 744, Loss: 0.23217780879992828, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 745, Loss: 0.3838303513415535, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 746, Loss: 0.586069800164204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 747, Loss: 0.7578529488154794, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 748, Loss: 0.3682814469437611, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 749, Loss: 0.31819016069877604, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 750, Loss: 0.2953817952238803, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 751, Loss: 0.3333676547884037, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 752, Loss: 0.32936686752686745, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 753, Loss: 0.5938078546016452, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 754, Loss: 0.3118456005347545, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 755, Loss: 0.3343917985431512, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 756, Loss: 0.6084141638824276, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 757, Loss: 0.2986685069831504, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 758, Loss: 0.3691724305857584, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 759, Loss: 0.30403639656069625, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 760, Loss: 0.3361655811199067, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 761, Loss: 0.3931137157734463, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 762, Loss: 0.350646129918763, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 763, Loss: 0.27781745721796847, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 764, Loss: 0.23653442619779635, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 765, Loss: 0.2857875514132384, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 766, Loss: 0.23759708898312204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 767, Loss: 0.592213530006957, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 768, Loss: 0.34442962206868755, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 769, Loss: 0.5390882471737212, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 770, Loss: 0.31309771803303593, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 771, Loss: 0.46120487633510243, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 772, Loss: 0.287259866585434, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 773, Loss: 0.2750185020112805, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 774, Loss: 0.2762179415648585, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 775, Loss: 0.35814787833057177, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 776, Loss: 0.32956006557577155, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 777, Loss: 0.3426128704633995, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 778, Loss: 0.24165691791657, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 779, Loss: 0.332069665427647, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 780, Loss: 0.2554622677346968, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 781, Loss: 0.7671858537392622, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 782, Loss: 0.3437840698364659, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 783, Loss: 0.4128037981258331, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 784, Loss: 0.2853805369829637, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 785, Loss: 0.3329962074604996, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 786, Loss: 0.37934118396582567, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 787, Loss: 0.37620207289842034, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 788, Loss: 0.26975682919250477, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 789, Loss: 0.4001122141272658, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 790, Loss: 0.2658073783642717, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 791, Loss: 0.23337725780955862, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 792, Loss: 0.4139075050537075, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 793, Loss: 0.5973671008555943, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 794, Loss: 0.4227685124344427, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 795, Loss: 0.29362834177248875, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 796, Loss: 0.6395563405088798, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 797, Loss: 0.6204960108877067, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 798, Loss: 0.2411207091054729, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 799, Loss: 0.3687697946110516, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 800, Loss: 0.4319899616400391, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 801, Loss: 0.3199744275375618, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 802, Loss: 0.4040416873715278, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 803, Loss: 0.3176040522744966, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 804, Loss: 0.27994897203928665, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 805, Loss: 0.5069805750382446, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 806, Loss: 0.2763166134387646, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 807, Loss: 0.7003019531957952, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 808, Loss: 0.23501656479255653, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 809, Loss: 0.2754422931344918, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 810, Loss: 0.3010589903293446, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 811, Loss: 0.533771004415661, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 812, Loss: 0.40098042668827116, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 813, Loss: 0.450033286402898, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 814, Loss: 0.32713464574522394, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 815, Loss: 0.3128686888649219, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 816, Loss: 0.32836812663959203, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 817, Loss: 0.2808069283579846, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 818, Loss: 0.32092522814380503, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 819, Loss: 0.3373612405516023, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 820, Loss: 0.4486771364780315, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 821, Loss: 0.28303887559546453, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 822, Loss: 0.275702779414561, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 823, Loss: 0.45251623792237844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 824, Loss: 0.5504305423982887, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 825, Loss: 0.2743717176874761, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 826, Loss: 0.28526026941132837, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 827, Loss: 0.363060740147663, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 828, Loss: 0.2707079645635752, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 829, Loss: 0.31156104479276414, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 830, Loss: 0.3803209408064502, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 831, Loss: 0.2607888602353447, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 832, Loss: 0.26693570890632323, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 833, Loss: 0.36139970495155715, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 834, Loss: 0.2672004659856539, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 835, Loss: 0.4990392927746993, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 836, Loss: 0.44658602297822514, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 837, Loss: 0.4243090562502129, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 838, Loss: 0.27067215745185963, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 839, Loss: 0.2918254797133538, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 840, Loss: 0.47992052519653494, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 841, Loss: 0.36318661308259564, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 842, Loss: 0.3744634362708342, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 843, Loss: 0.4279480834490168, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 844, Loss: 0.2740366528226507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 845, Loss: 0.3824898340266971, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 846, Loss: 0.3630841503528532, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 847, Loss: 0.3033966067129882, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 848, Loss: 0.3343280218303629, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 849, Loss: 0.2745136039647452, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 850, Loss: 0.59692900140484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 851, Loss: 0.26847521527156787, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 852, Loss: 0.28522208883144273, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 853, Loss: 0.3174209352711543, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 854, Loss: 0.305983185810245, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 855, Loss: 0.2756146022204443, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 856, Loss: 0.46244172024324215, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 857, Loss: 0.79912658085875, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 858, Loss: 0.4004478923282647, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 859, Loss: 0.35653879258748417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 860, Loss: 0.2537493181297783, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 861, Loss: 0.508216555343896, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 862, Loss: 0.2418705816128852, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 863, Loss: 0.2651462848617175, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 864, Loss: 0.27552119958581556, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 865, Loss: 0.46925557065508317, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 866, Loss: 0.2741619794445208, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 867, Loss: 0.3031135651430612, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 868, Loss: 0.2854331042246886, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 869, Loss: 0.2669723532006613, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 870, Loss: 0.32342868122264073, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 871, Loss: 0.24216637952545744, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 872, Loss: 0.371869012229869, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 873, Loss: 0.46137665187526317, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 874, Loss: 0.23819993921817084, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 875, Loss: 0.41362958808621814, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 876, Loss: 0.3547135108138334, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 877, Loss: 0.2340785469294249, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 878, Loss: 0.2543332482474598, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 879, Loss: 0.30434598252364947, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 880, Loss: 0.2597500125137993, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 881, Loss: 0.3121982112404631, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 882, Loss: 0.2896708134366053, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 883, Loss: 0.27990200926726067, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 884, Loss: 0.2608041149779212, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 885, Loss: 0.4743328654411497, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 886, Loss: 0.3338772934501597, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 887, Loss: 0.3933430029209759, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 888, Loss: 0.4730999241459984, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 889, Loss: 0.30125161267775735, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 890, Loss: 0.27073407537736016, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 891, Loss: 0.46965407573826173, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 892, Loss: 0.4117632214820254, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 893, Loss: 0.23558373453644676, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 894, Loss: 0.2691424090397398, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 895, Loss: 0.48523627596995844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 896, Loss: 0.4292675165478398, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 897, Loss: 0.23137158394332052, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 898, Loss: 0.26448826234290757, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 899, Loss: 0.26982240744678787, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 900, Loss: 0.43203465103867617, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 901, Loss: 0.32545350592153766, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 902, Loss: 0.3975870089754684, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 903, Loss: 0.43280642262027924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 904, Loss: 0.26598054772183777, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 905, Loss: 0.38729487914663874, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 906, Loss: 0.3563670264995683, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 907, Loss: 0.3283537094791462, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 908, Loss: 0.469977414853298, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 909, Loss: 0.31107195568619067, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 910, Loss: 0.47208119340789484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 911, Loss: 0.3110798589842264, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 912, Loss: 0.32153081858872856, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 913, Loss: 0.2446096630638848, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 914, Loss: 0.292242779103012, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 915, Loss: 0.48386746260306657, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 916, Loss: 0.28043436766986213, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 917, Loss: 0.4583576362441181, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 918, Loss: 0.26192846973590633, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 919, Loss: 0.3048582713284899, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 920, Loss: 0.3080836448921033, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 921, Loss: 0.25750234622554974, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 922, Loss: 0.29896029675342783, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 923, Loss: 0.28540022239502083, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 924, Loss: 0.36350342223182813, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 925, Loss: 0.513680355441337, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 926, Loss: 0.37293986461467377, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 927, Loss: 0.4139857692828226, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 928, Loss: 0.43182177636386077, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 929, Loss: 0.2397623042348465, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 930, Loss: 0.29862175505312427, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 931, Loss: 0.3675116349178834, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 932, Loss: 0.40813597945111557, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 933, Loss: 0.2657710969158443, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 934, Loss: 0.4222947428856155, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 935, Loss: 0.641663788959384, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 936, Loss: 0.3862856586967133, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 937, Loss: 0.2510365985055597, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 938, Loss: 0.46219536462265387, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 939, Loss: 0.3365592632340016, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 940, Loss: 0.3680923894072583, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 941, Loss: 0.26428587454548275, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 942, Loss: 0.4187647360853514, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 943, Loss: 0.4064924202363958, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 944, Loss: 0.2909528714876815, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 945, Loss: 0.4444468539124865, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 946, Loss: 0.25961465207617834, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 947, Loss: 0.27401643206469756, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 948, Loss: 0.29088590423461125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 949, Loss: 0.3372971123005921, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 950, Loss: 0.2625487787871737, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 951, Loss: 0.2727275918045985, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 952, Loss: 0.2867561967130343, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 953, Loss: 0.3621680090157451, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 954, Loss: 0.30340502623646093, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 955, Loss: 0.28721353612924905, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 956, Loss: 0.27690430302035, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 957, Loss: 0.3506892522168459, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 958, Loss: 0.261224396755412, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 959, Loss: 0.28582410220996357, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 960, Loss: 0.2344998357560172, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 961, Loss: 0.24749933204868013, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 962, Loss: 0.2552674551495556, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 963, Loss: 0.41770755897119116, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 964, Loss: 0.315108894035418, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 965, Loss: 0.4139278201042869, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 966, Loss: 0.35688924556976814, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 967, Loss: 0.2918571157113528, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 968, Loss: 0.34960262759422067, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 969, Loss: 0.33381421828627605, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 970, Loss: 0.274499688266433, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 971, Loss: 0.3545997858057374, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 972, Loss: 0.3527880967382433, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 973, Loss: 0.41273719554512445, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 974, Loss: 0.5336021679154886, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 975, Loss: 0.24447684255558033, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 976, Loss: 0.31660229024020353, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 977, Loss: 0.2876118037406726, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 978, Loss: 0.3582735161406879, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 979, Loss: 0.2795759545895719, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 980, Loss: 0.309395161900583, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 981, Loss: 0.3439611171261999, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 982, Loss: 0.27094867659847865, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 983, Loss: 0.3181326576776764, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 984, Loss: 0.2658539014908643, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 985, Loss: 0.2990253855145099, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 986, Loss: 0.5534695140291142, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 987, Loss: 0.34751552487827675, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 988, Loss: 0.4072173166074374, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 989, Loss: 0.2871447192067631, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 990, Loss: 0.38611159089019154, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 991, Loss: 0.26772475904636295, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 992, Loss: 0.2665890380493618, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 993, Loss: 0.24041417735319434, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 994, Loss: 0.28577142290514546, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 995, Loss: 0.2783075437309088, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 996, Loss: 0.4129986121415071, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 997, Loss: 0.2500429230017623, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 998, Loss: 0.42136110092155565, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 999, Loss: 0.37414374450576016, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1000, Loss: 0.35744992711528184, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1001, Loss: 0.30276675365833133, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1002, Loss: 0.3448896972742593, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1003, Loss: 0.2501946596546597, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1004, Loss: 0.2597980589768526, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1005, Loss: 0.36341564394861203, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1006, Loss: 0.2690640888901203, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1007, Loss: 0.35705665568197453, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1008, Loss: 0.24386111805626484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1009, Loss: 0.34622231753174765, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1010, Loss: 0.3369765131575745, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1011, Loss: 0.3642543862088645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1012, Loss: 0.3696743305396687, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1013, Loss: 0.3032787315454581, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1014, Loss: 0.3968968972516771, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1015, Loss: 0.3316762846209512, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1016, Loss: 0.28533520131961004, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1017, Loss: 0.33332841530591306, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1018, Loss: 0.2937398452894909, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1019, Loss: 0.3992652335580458, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1020, Loss: 0.2446653894205052, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1021, Loss: 0.560609961468611, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1022, Loss: 0.3121348443303591, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1023, Loss: 0.2687126945802146, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1024, Loss: 0.35693147210777826, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1025, Loss: 0.35758851117632673, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1026, Loss: 0.3305327480472465, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1027, Loss: 0.2975454230984762, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1028, Loss: 0.5722477260289608, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1029, Loss: 0.2872695982445601, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1030, Loss: 0.29430068417516303, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1031, Loss: 0.3484168531808889, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1032, Loss: 0.336378028853794, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1033, Loss: 0.4280108943791331, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1034, Loss: 0.3528094203277052, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1035, Loss: 0.41636942602854854, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1036, Loss: 0.43228241564797704, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1037, Loss: 0.4292806250422655, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1038, Loss: 0.34047310093300726, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1039, Loss: 0.28774636320222935, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1040, Loss: 0.37746013096733755, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1041, Loss: 0.40559567372549254, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1042, Loss: 0.2290737959495024, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1043, Loss: 0.3780863331723534, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1044, Loss: 0.273295054157476, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1045, Loss: 0.3769551819400533, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1046, Loss: 0.2859123638493376, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1047, Loss: 0.3747921061804258, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1048, Loss: 0.2935319476848206, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1049, Loss: 0.5600105527705642, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1050, Loss: 0.4302706651713959, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1051, Loss: 0.25897598566963853, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1052, Loss: 0.45753006699568866, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1053, Loss: 0.4625938726443388, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1054, Loss: 0.2894095846874134, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1055, Loss: 0.26337316416985573, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1056, Loss: 0.2568351462618274, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1057, Loss: 0.3429183926036936, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1058, Loss: 0.2548728166842303, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1059, Loss: 0.2763121536434739, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1060, Loss: 0.3672862793465276, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1061, Loss: 0.2607519555183471, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1062, Loss: 0.4292036575125492, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1063, Loss: 0.2378867064423148, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1064, Loss: 0.4074639672725363, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1065, Loss: 0.37938444913386987, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1066, Loss: 0.3931302424202322, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1067, Loss: 0.2788897596716174, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1068, Loss: 0.3116356362212792, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1069, Loss: 0.3194160472662532, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1070, Loss: 0.24531804480345204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1071, Loss: 0.28945158799032283, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1072, Loss: 0.2539567241118816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1073, Loss: 0.40989557556378775, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1074, Loss: 0.33234513013564443, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1075, Loss: 0.3539744869813459, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1076, Loss: 0.28700127655690644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1077, Loss: 0.4007559663704904, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1078, Loss: 0.3713287165592972, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1079, Loss: 0.4471049347308844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1080, Loss: 0.26885332606992285, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1081, Loss: 0.4577501417874019, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1082, Loss: 0.2758008465485835, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1083, Loss: 0.2443824254099037, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1084, Loss: 0.3662939702908845, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1085, Loss: 0.5795834430941001, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1086, Loss: 0.46675890923594143, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1087, Loss: 0.555187252707855, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1088, Loss: 0.7567893118362881, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1089, Loss: 0.25104476182854113, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1090, Loss: 0.3519952698610468, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1091, Loss: 0.27438630928450836, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1092, Loss: 0.4468373701074129, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1093, Loss: 0.30587831659263115, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1094, Loss: 0.647010520977324, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1095, Loss: 0.27622648130426714, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1096, Loss: 0.47250760264739233, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1097, Loss: 0.25102326512265966, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1098, Loss: 0.40202243070612553, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1099, Loss: 0.2450191595282611, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1100, Loss: 0.3278411621565473, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1101, Loss: 0.3824733374003593, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1102, Loss: 0.44292197270076195, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1103, Loss: 0.44442151898961896, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1104, Loss: 0.43383609850837795, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1105, Loss: 0.26556968408089876, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1106, Loss: 0.37259349670039366, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1107, Loss: 0.33256287145833524, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1108, Loss: 0.31747512779912573, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1109, Loss: 0.2943019952096022, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1110, Loss: 0.2371877416298633, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1111, Loss: 0.6936940313130123, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1112, Loss: 0.366997985395878, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1113, Loss: 0.24958799340649476, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1114, Loss: 0.2515305971921787, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1115, Loss: 0.25458367408790167, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1116, Loss: 0.6429593692506528, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1117, Loss: 0.4362008952965775, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1118, Loss: 0.3898804793141365, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1119, Loss: 0.5662400651086906, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1120, Loss: 0.2729270226144023, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1121, Loss: 0.3171659110594109, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1122, Loss: 0.28602492827362014, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1123, Loss: 0.27651007530579985, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1124, Loss: 0.2577440178753981, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1125, Loss: 0.3271910856527983, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1126, Loss: 0.34565878689056906, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1127, Loss: 0.43521110047952694, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1128, Loss: 0.3026296994897433, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1129, Loss: 0.2680095689235529, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1130, Loss: 0.2594036190430482, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1131, Loss: 0.6552392855684754, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1132, Loss: 0.331000157624125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1133, Loss: 0.4148619750448975, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1134, Loss: 0.33745598087535006, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1135, Loss: 0.2756494794387706, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1136, Loss: 0.33482734859668317, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1137, Loss: 0.8657968691775286, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1138, Loss: 0.3996499924153821, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1139, Loss: 0.29176015530256827, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1140, Loss: 0.35212118530170533, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1141, Loss: 0.570064924896881, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1142, Loss: 0.37399554438700505, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1143, Loss: 0.47505819802956506, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1144, Loss: 0.37702461927628117, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1145, Loss: 0.2953696088236558, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1146, Loss: 0.31855321238952683, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1147, Loss: 0.2687794732161545, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1148, Loss: 0.27273198707233326, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1149, Loss: 0.4397378750131288, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1150, Loss: 0.4179090924160779, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1151, Loss: 0.4074203451848308, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1152, Loss: 0.36852247988334885, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1153, Loss: 0.2498328361270199, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1154, Loss: 0.2788196708339768, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1155, Loss: 0.3825236650459214, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1156, Loss: 0.2538026241055909, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1157, Loss: 0.32285249316421283, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1158, Loss: 0.2873229512401516, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1159, Loss: 0.3884209090393064, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1160, Loss: 0.46613902439543764, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1161, Loss: 0.33144297816299934, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1162, Loss: 0.3490684067731333, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1163, Loss: 0.27524201897842654, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1164, Loss: 0.39310071062176155, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1165, Loss: 0.3547645273101037, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1166, Loss: 0.261204044400058, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1167, Loss: 0.2749134416912973, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1168, Loss: 0.2498115756320235, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1169, Loss: 0.2936728551072697, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1170, Loss: 0.2596251476498577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1171, Loss: 0.4614116152111102, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1172, Loss: 0.25826458803566194, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1173, Loss: 0.3739186018345232, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1174, Loss: 0.3193490162751435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1175, Loss: 0.28643156812887527, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1176, Loss: 0.37857418682698407, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1177, Loss: 0.24872510438320228, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1178, Loss: 0.2743392626789365, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1179, Loss: 0.3063634217768051, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1180, Loss: 0.32738594938324445, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1181, Loss: 0.3914098678945267, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1182, Loss: 0.5558154316630282, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1183, Loss: 0.4387865553336644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1184, Loss: 0.5863918512487514, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1185, Loss: 0.24537757576722138, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1186, Loss: 0.2357885858512123, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1187, Loss: 0.3594833923945192, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1188, Loss: 0.637375745194915, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1189, Loss: 0.23530175268077844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1190, Loss: 0.3116704908567833, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1191, Loss: 0.4196476816029129, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1192, Loss: 0.32218255050683187, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1193, Loss: 0.46735120813935954, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1194, Loss: 0.3403057213023193, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1195, Loss: 0.2541012192688342, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1196, Loss: 0.34426907305181675, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1197, Loss: 0.27170082422086683, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1198, Loss: 0.5388569485413179, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1199, Loss: 0.39763607965171893, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1200, Loss: 0.31979626576192577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1201, Loss: 0.23384343782572115, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1202, Loss: 0.27519755241605476, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1203, Loss: 0.39583336983218276, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1204, Loss: 0.2713260850665853, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1205, Loss: 0.2285490616381791, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1206, Loss: 0.3079706756269292, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1207, Loss: 0.3798952916200144, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1208, Loss: 0.24587785627326086, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1209, Loss: 0.22393846262676564, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1210, Loss: 0.2945502710167418, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1211, Loss: 0.604325012982125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1212, Loss: 0.30591969266337893, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1213, Loss: 0.28321269629947865, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1214, Loss: 0.28559423620412316, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1215, Loss: 0.40280148267802296, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1216, Loss: 0.2629507762990588, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1217, Loss: 0.24111896437691466, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1218, Loss: 0.45257993440640354, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1219, Loss: 0.2598582266760625, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1220, Loss: 0.41070078553832634, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1221, Loss: 0.43152886234331633, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1222, Loss: 0.38893928563537516, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1223, Loss: 0.2699444304398715, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1224, Loss: 0.30312084952484236, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1225, Loss: 0.2501727663913455, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1226, Loss: 0.45717158904699323, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1227, Loss: 0.40351677661779956, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1228, Loss: 0.23542439990838965, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1229, Loss: 0.4164301816581179, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1230, Loss: 0.3133889994883236, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1231, Loss: 0.4437334703364003, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1232, Loss: 0.3143553740815992, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1233, Loss: 0.3332047532100394, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1234, Loss: 0.2864211527193564, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1235, Loss: 0.36013026264317005, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1236, Loss: 0.34940132380356237, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1237, Loss: 0.2841478987735061, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1238, Loss: 0.5788207586907101, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1239, Loss: 0.36633855147267713, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1240, Loss: 0.44456253375119326, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1241, Loss: 0.2533437146366169, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1242, Loss: 0.3098699255152132, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1243, Loss: 0.2949232898096574, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1244, Loss: 0.2514683913682291, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1245, Loss: 0.37463169338663305, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1246, Loss: 0.25349739601419147, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1247, Loss: 0.24750065121343492, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1248, Loss: 0.5072376697291072, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1249, Loss: 0.3355141576712432, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1250, Loss: 0.3925828405759624, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1251, Loss: 0.27006003883942126, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1252, Loss: 0.2828052245043076, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1253, Loss: 0.2643574426140161, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1254, Loss: 0.25373814874155876, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1255, Loss: 0.25074447107495623, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1256, Loss: 0.5251961925884598, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1257, Loss: 0.3716101241125811, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1258, Loss: 0.312272421807136, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1259, Loss: 0.4493341362473911, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1260, Loss: 0.28418250325190086, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1261, Loss: 0.3045045286770199, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1262, Loss: 0.42451698495281903, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1263, Loss: 0.37507693224971683, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1264, Loss: 0.28872681000569855, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1265, Loss: 0.34190819579025944, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1266, Loss: 0.31491977795386533, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1267, Loss: 0.39912647260388734, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1268, Loss: 0.3837698349707731, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1269, Loss: 0.2532582515669638, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1270, Loss: 0.30581384605418843, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1271, Loss: 0.38414001011154864, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1272, Loss: 0.287376789327511, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1273, Loss: 0.3589671068952921, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1274, Loss: 0.5088468126256933, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1275, Loss: 0.27475103386768335, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1276, Loss: 0.4089401535621369, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1277, Loss: 0.23876578601578458, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1278, Loss: 0.3316208740414252, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1279, Loss: 0.26115122065068763, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1280, Loss: 0.3856529655532061, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1281, Loss: 0.2847835788061859, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1282, Loss: 0.30823012007608963, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1283, Loss: 0.34465165954738136, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1284, Loss: 0.23763754238905332, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1285, Loss: 0.30731544981153464, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1286, Loss: 0.3424662174781221, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1287, Loss: 0.3443413077625469, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1288, Loss: 0.6786685697459074, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1289, Loss: 0.2969853927740901, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1290, Loss: 0.3159736106907453, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1291, Loss: 0.5436670235775694, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1292, Loss: 0.3687985970637786, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1293, Loss: 0.4842067919948145, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1294, Loss: 0.42107936993138245, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1295, Loss: 0.3009952450805416, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1296, Loss: 0.3460517297111613, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1297, Loss: 0.24471659472111254, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1298, Loss: 0.42323070646464034, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1299, Loss: 0.4364267598742248, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1300, Loss: 0.47060807936854254, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1301, Loss: 0.3420112092744011, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1302, Loss: 0.33126353884450066, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1303, Loss: 0.33279621861574327, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1304, Loss: 0.23665108884069336, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1305, Loss: 0.45676571074733086, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1306, Loss: 0.29160181944779606, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1307, Loss: 0.5655402549092746, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1308, Loss: 0.3139699648630315, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1309, Loss: 0.4053285431185415, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1310, Loss: 0.2687331567882375, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1311, Loss: 0.30483615738160436, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1312, Loss: 0.49513112409717397, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1313, Loss: 0.41611440015989026, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1314, Loss: 0.39863558037571645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1315, Loss: 0.4065675515443489, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1316, Loss: 0.36990496431380737, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1317, Loss: 0.264703559393269, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1318, Loss: 0.2609975456330406, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1319, Loss: 0.29417872160973746, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1320, Loss: 0.2377975201643957, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1321, Loss: 0.4774602421815311, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1322, Loss: 0.46535781626507966, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1323, Loss: 0.40863845941278887, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1324, Loss: 0.4060930927318121, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1325, Loss: 0.39377937476635094, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1326, Loss: 0.2411432812479956, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1327, Loss: 0.3071106758416255, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1328, Loss: 0.30604866441778067, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1329, Loss: 0.30372521768903527, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1330, Loss: 0.3035934584240674, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1331, Loss: 0.24342088277645513, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1332, Loss: 0.506005262468958, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1333, Loss: 0.4795162793614991, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1334, Loss: 0.4941602882819667, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1335, Loss: 0.30023305016353435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1336, Loss: 0.7795894611978373, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1337, Loss: 0.25723480295374695, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1338, Loss: 0.32318107069341107, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1339, Loss: 0.505590799203924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1340, Loss: 0.46628942704543475, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1341, Loss: 0.2505471686115791, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1342, Loss: 0.31291792550636843, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1343, Loss: 0.29478723398411466, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1344, Loss: 0.26132090732055874, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1345, Loss: 0.2955626039311489, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1346, Loss: 0.26628714343476645, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1347, Loss: 0.43438368715431463, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1348, Loss: 0.3660375597135535, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1349, Loss: 0.6126575919472745, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1350, Loss: 0.23948774083381788, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1351, Loss: 0.23548142573043257, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1352, Loss: 0.30349358709576363, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1353, Loss: 0.2916064002934416, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1354, Loss: 0.3540226496096125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1355, Loss: 0.2541530303213938, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1356, Loss: 0.3449534031067314, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1357, Loss: 0.30819049566518514, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1358, Loss: 0.48038383394588346, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1359, Loss: 0.3401083999331775, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1360, Loss: 0.38708777540104417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1361, Loss: 0.32556609629738803, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1362, Loss: 0.3673485823658983, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1363, Loss: 0.36324446436794644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1364, Loss: 0.308102841835686, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1365, Loss: 0.3459474287061835, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1366, Loss: 0.23988563063733, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1367, Loss: 0.3008940631472956, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1368, Loss: 0.5656798725431521, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1369, Loss: 0.31684384352371836, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1370, Loss: 0.48420075696238213, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1371, Loss: 0.6465070076064933, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1372, Loss: 0.3909331570139807, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1373, Loss: 0.3142445728606679, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1374, Loss: 0.4744555035005433, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1375, Loss: 0.2701642666509078, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1376, Loss: 0.2849114199574151, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1377, Loss: 0.502052664945095, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1378, Loss: 0.27293237096347417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1379, Loss: 0.26092174235333915, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1380, Loss: 0.4166614899621816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1381, Loss: 0.5663390316319816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1382, Loss: 0.26132368655596533, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1383, Loss: 0.506167109352152, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1384, Loss: 0.3052722576140868, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1385, Loss: 0.2755560332484347, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1386, Loss: 0.43723653646266053, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1387, Loss: 0.2846288819857713, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1388, Loss: 0.36238551390226137, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1389, Loss: 0.317010431691029, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1390, Loss: 0.5837273063908665, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1391, Loss: 0.5505514894360887, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1392, Loss: 0.33306528396403023, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1393, Loss: 0.3785296506367092, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1394, Loss: 0.46234580072388126, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1395, Loss: 0.35814923663081866, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1396, Loss: 0.24792070308918576, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1397, Loss: 0.24156274526564772, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1398, Loss: 0.3699965867765211, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1399, Loss: 0.4999106889410209, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1400, Loss: 0.4849077116704721, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1401, Loss: 0.3185766249525401, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1402, Loss: 0.305125569411431, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1403, Loss: 0.25097015800130856, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1404, Loss: 0.33019187681558526, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1405, Loss: 0.2426401156521022, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1406, Loss: 0.29455267942062924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1407, Loss: 0.3731188903203403, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1408, Loss: 0.3004711409486432, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1409, Loss: 0.24806273011879534, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1410, Loss: 0.3646167895829914, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1411, Loss: 0.2689923283243547, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1412, Loss: 0.5222178438797808, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1413, Loss: 0.41187396794963504, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1414, Loss: 0.41348271817348625, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1415, Loss: 0.42060935346060113, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1416, Loss: 0.3820830443929786, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1417, Loss: 0.37101839324099395, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1418, Loss: 0.2807448072236406, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1419, Loss: 0.4040435371040531, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1420, Loss: 0.25326719233076944, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1421, Loss: 0.2355131907159172, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1422, Loss: 0.3439791643090405, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1423, Loss: 0.3140970130625791, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1424, Loss: 0.42616543554208924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1425, Loss: 0.47344303399829735, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1426, Loss: 0.2352973961951772, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1427, Loss: 0.2993746225409176, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1428, Loss: 0.6456012062516292, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1429, Loss: 0.30748420903215357, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1430, Loss: 0.3383714045253405, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1431, Loss: 0.276901602219152, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1432, Loss: 0.37576626070930164, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1433, Loss: 0.3320961377933646, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1434, Loss: 0.3890771624118047, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1435, Loss: 0.2925046589853501, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1436, Loss: 0.3394866878194261, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1437, Loss: 0.2921749048694094, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1438, Loss: 0.26679236717519605, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1439, Loss: 0.2570806295331112, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1440, Loss: 0.3860286416323152, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1441, Loss: 0.3265464668098357, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1442, Loss: 0.3759745068323779, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1443, Loss: 0.30400751886670735, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1444, Loss: 0.23040339578241517, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1445, Loss: 0.2675051069770642, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1446, Loss: 0.22880871577890868, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1447, Loss: 0.2560180754585043, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1448, Loss: 0.2699700632122284, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1449, Loss: 0.3393501274700811, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1450, Loss: 0.25751016002225896, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1451, Loss: 0.2678604616186149, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1452, Loss: 0.2725213843534744, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1453, Loss: 0.24908934336501384, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1454, Loss: 0.30334303287629094, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1455, Loss: 0.42521945425082297, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1456, Loss: 0.2894949782303044, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1457, Loss: 0.26601518499203464, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1458, Loss: 0.2525707659346721, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1459, Loss: 0.3372410455626699, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1460, Loss: 0.31718030869816616, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1461, Loss: 0.48969148546915536, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1462, Loss: 0.536662499848016, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1463, Loss: 0.2855678563883597, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1464, Loss: 0.3692015722630105, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1465, Loss: 0.25174713304340274, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1466, Loss: 0.2726005371750638, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1467, Loss: 0.4404440621249316, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1468, Loss: 0.3970962500227002, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1469, Loss: 0.45112090770060664, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1470, Loss: 0.3856478696672859, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1471, Loss: 0.24494960307471209, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1472, Loss: 0.4571986870428056, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1473, Loss: 0.2876294327518403, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1474, Loss: 0.30368771399955374, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1475, Loss: 0.31773211834752146, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1476, Loss: 0.3089461747025093, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1477, Loss: 0.3488977030779526, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1478, Loss: 0.3789280577894083, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1479, Loss: 0.39331616643074874, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1480, Loss: 0.6530633636207032, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1481, Loss: 0.37730623273917074, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1482, Loss: 0.3327618345738697, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1483, Loss: 0.3045773676513324, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1484, Loss: 0.36541533142934846, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1485, Loss: 0.44417338789159144, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1486, Loss: 0.25656506903239845, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1487, Loss: 0.2931436116458385, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1488, Loss: 0.2682368225079464, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1489, Loss: 0.2312766501412636, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1490, Loss: 0.3042958014087613, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1491, Loss: 0.3434399337332494, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1492, Loss: 0.3766082067453138, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1493, Loss: 0.3707261360215835, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1494, Loss: 0.2763568632811139, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1495, Loss: 0.325572832014957, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1496, Loss: 0.23767564663036744, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1497, Loss: 0.33969099449585627, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1498, Loss: 0.32682745867876417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1499, Loss: 0.38371912708246386, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1500, Loss: 0.44260255559272343, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1501, Loss: 0.49839117212095263, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1502, Loss: 0.2895900548681748, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1503, Loss: 0.2840965717335535, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1504, Loss: 0.3332999678783575, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1505, Loss: 0.43032649048211, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1506, Loss: 0.26638321725246833, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1507, Loss: 0.2452941843612535, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1508, Loss: 0.38197874231492907, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1509, Loss: 0.34573415943996844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1510, Loss: 0.28452291630596277, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1511, Loss: 0.3329487621362699, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1512, Loss: 0.6967369413211361, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1513, Loss: 0.39862439600188504, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1514, Loss: 0.31496494664315994, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1515, Loss: 0.4342873306443687, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1516, Loss: 0.33746045893022253, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1517, Loss: 0.5124910666502998, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1518, Loss: 0.3301722145415501, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1519, Loss: 0.29720851569044415, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1520, Loss: 0.38745498135320844, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1521, Loss: 0.28439016088953933, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1522, Loss: 0.3791756145063828, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1523, Loss: 0.34566583881527096, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1524, Loss: 0.25497473399585757, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1525, Loss: 0.3770307749784073, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1526, Loss: 0.2803829489539675, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1527, Loss: 0.38089245598344634, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1528, Loss: 0.43749134778850474, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1529, Loss: 0.42021242142887816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1530, Loss: 0.3291061161203268, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1531, Loss: 0.2573143381262103, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1532, Loss: 0.25513546155308586, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1533, Loss: 0.30347472500568806, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1534, Loss: 0.2584467425531718, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1535, Loss: 0.2871513053061181, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1536, Loss: 0.41656638470308527, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1537, Loss: 0.3898404132820931, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1538, Loss: 0.32721503103952, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1539, Loss: 0.2458636289851243, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1540, Loss: 0.2668540711997888, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1541, Loss: 0.28370955707769446, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1542, Loss: 0.30221476714621176, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1543, Loss: 0.268425421163362, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1544, Loss: 0.3332319620129487, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1545, Loss: 0.4463788906075311, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1546, Loss: 0.23336260950198462, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1547, Loss: 0.34310113542171927, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1548, Loss: 0.28092841434176447, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1549, Loss: 0.34872240596347526, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1550, Loss: 0.3490747622896361, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1551, Loss: 0.4227566893828889, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1552, Loss: 0.23214272508122896, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1553, Loss: 0.3653624133120405, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1554, Loss: 0.36039986177785815, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1555, Loss: 0.4730582621247106, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1556, Loss: 0.2980854621703514, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1557, Loss: 0.2938828728041437, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1558, Loss: 0.28432763877849876, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1559, Loss: 0.2664680288607202, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1560, Loss: 0.7261964495947801, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1561, Loss: 0.3970178647953026, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1562, Loss: 0.7533537989346948, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1563, Loss: 0.2840062088336157, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1564, Loss: 0.2437978973070855, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1565, Loss: 0.33038684389931866, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1566, Loss: 0.28429100866668794, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1567, Loss: 0.40010456827481544, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1568, Loss: 0.24906739097103922, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1569, Loss: 0.3359439205768997, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1570, Loss: 0.28743778645218243, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1571, Loss: 0.24046053052389924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1572, Loss: 0.28955852183050024, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1573, Loss: 0.409652682667232, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1574, Loss: 0.2971669643567643, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1575, Loss: 0.5484504212698449, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1576, Loss: 0.36472062644240455, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1577, Loss: 0.223501199122449, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1578, Loss: 0.40600043010882736, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1579, Loss: 0.7177885995860644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1580, Loss: 0.5361771165690825, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1581, Loss: 0.32389952753423024, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1582, Loss: 0.43648822674059484, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1583, Loss: 0.3893036095492427, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1584, Loss: 0.37607541908914277, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1585, Loss: 0.2482549686995679, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1586, Loss: 0.3051431697590577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1587, Loss: 0.2827874969363001, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1588, Loss: 0.31456502742305165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1589, Loss: 0.30631183081933216, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1590, Loss: 0.26876097026103785, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1591, Loss: 0.23590009231356207, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1592, Loss: 0.35546917806500145, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1593, Loss: 0.26062954134993294, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1594, Loss: 0.31328113689646586, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1595, Loss: 0.32545692167212525, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1596, Loss: 0.275065219904376, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1597, Loss: 0.28773388414880374, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1598, Loss: 0.26352372622395065, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1599, Loss: 0.26153540429890154, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1600, Loss: 0.32838724117626567, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1601, Loss: 0.40436504606153884, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1602, Loss: 0.25957818502713964, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1603, Loss: 0.5170725871084946, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1604, Loss: 0.43954913135168194, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1605, Loss: 0.42776639627908364, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1606, Loss: 0.4156870848057925, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1607, Loss: 0.31267685422855895, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1608, Loss: 0.397096258636936, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1609, Loss: 0.27647296705299634, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1610, Loss: 0.4276511719466507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1611, Loss: 0.41494345812731304, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1612, Loss: 0.3280259386156421, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1613, Loss: 0.37200290368981803, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1614, Loss: 0.3717328812410766, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1615, Loss: 0.35086467160733753, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1616, Loss: 0.5857876859050942, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1617, Loss: 0.2811027429066506, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1618, Loss: 0.3200334203250095, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1619, Loss: 0.3587491621765029, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1620, Loss: 0.3631918846688583, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1621, Loss: 0.34758926105969545, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1622, Loss: 0.3076250827115041, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1623, Loss: 0.26962843716084717, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1624, Loss: 0.3733068852370038, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1625, Loss: 0.3554976728430507, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1626, Loss: 0.2280149600710885, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1627, Loss: 0.37249573321895746, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1628, Loss: 0.35396427349694737, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1629, Loss: 0.24419191436744264, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1630, Loss: 0.24329685582647165, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1631, Loss: 0.2676747462483366, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1632, Loss: 0.39224309400817425, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1633, Loss: 0.2512315824345748, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1634, Loss: 0.3814916004330636, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1635, Loss: 0.34915480927898224, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1636, Loss: 0.26993128499889796, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1637, Loss: 0.3104966449107319, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1638, Loss: 0.253752179111437, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1639, Loss: 0.3128940137822022, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1640, Loss: 0.29036266095885277, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1641, Loss: 0.4178697332930461, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1642, Loss: 0.2469409773090759, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1643, Loss: 0.32679580920312734, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1644, Loss: 0.48109997494922907, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1645, Loss: 0.3394134441718376, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1646, Loss: 0.30920594171432947, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1647, Loss: 0.2711911730589577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1648, Loss: 0.2772845879356176, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1649, Loss: 0.2379339412680946, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1650, Loss: 0.5423486935828823, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1651, Loss: 0.5909688412669755, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1652, Loss: 0.2526462226258112, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1653, Loss: 0.2940701299235937, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1654, Loss: 0.5858631780800312, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1655, Loss: 0.3416664153823514, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1656, Loss: 0.4719786447206673, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1657, Loss: 0.3026154971113174, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1658, Loss: 0.4138906987756608, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1659, Loss: 0.4067612382547854, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1660, Loss: 0.27671635202684297, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1661, Loss: 0.5199384726806819, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1662, Loss: 0.29905528544619603, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1663, Loss: 0.28221471791221453, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1664, Loss: 0.24374482059512373, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1665, Loss: 0.2505181452940385, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1666, Loss: 0.4491491952044795, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1667, Loss: 0.37484056604020577, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1668, Loss: 0.4092599013274167, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1669, Loss: 0.4125463092446205, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1670, Loss: 0.2631892677268816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1671, Loss: 0.3163653296310518, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1672, Loss: 0.4599870721039029, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1673, Loss: 0.3094500059369457, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1674, Loss: 0.40697286236153163, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1675, Loss: 0.3884202214527487, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1676, Loss: 0.37461253564114816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1677, Loss: 0.3575746208836788, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1678, Loss: 0.4297982161766943, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1679, Loss: 0.23624873821431513, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1680, Loss: 0.32495576056692127, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1681, Loss: 0.3479104876321063, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1682, Loss: 0.2830094357925924, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1683, Loss: 0.3526597143918001, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1684, Loss: 0.29718621461265504, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1685, Loss: 0.3801238975384641, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1686, Loss: 0.5179442215111661, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1687, Loss: 0.24937539535005426, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1688, Loss: 0.24323095148176344, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1689, Loss: 0.7418625273909896, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1690, Loss: 0.25302247563674607, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1691, Loss: 0.37557459675119204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1692, Loss: 0.3575767630940631, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1693, Loss: 0.28477203438515863, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1694, Loss: 0.42172983741517406, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1695, Loss: 0.2569818640190849, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1696, Loss: 0.28539908438176664, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1697, Loss: 0.3438464535589921, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1698, Loss: 0.316191271360203, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1699, Loss: 0.30015903430053736, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1700, Loss: 0.25661179970606307, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1701, Loss: 0.3009020698223205, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1702, Loss: 0.31049078635645816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1703, Loss: 0.3869764035072623, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1704, Loss: 0.26539073998175505, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1705, Loss: 0.35964902347861094, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1706, Loss: 0.38562298705316106, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1707, Loss: 0.35088264176784373, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1708, Loss: 0.2725341505003901, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1709, Loss: 0.23341648498833417, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1710, Loss: 0.3519106218098073, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1711, Loss: 0.32451059871783783, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1712, Loss: 0.3233341028379171, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1713, Loss: 0.312648457431672, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1714, Loss: 0.3856697646876592, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1715, Loss: 0.38028645515634985, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1716, Loss: 0.41026295055309914, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1717, Loss: 0.3833413575279435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1718, Loss: 0.35001650447554, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1719, Loss: 0.2949967243511961, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1720, Loss: 0.367990178790925, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1721, Loss: 0.3438600623313955, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1722, Loss: 0.48885712327478864, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1723, Loss: 0.2416705814003777, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1724, Loss: 0.2377750255622158, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1725, Loss: 0.29001279954520925, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1726, Loss: 0.3387778192087838, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1727, Loss: 0.40413237894630094, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1728, Loss: 0.3233668153929428, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1729, Loss: 0.4933625981981751, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1730, Loss: 0.3349953052397173, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1731, Loss: 0.2404092454014865, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1732, Loss: 0.26847423619208854, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1733, Loss: 0.47793335784308055, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1734, Loss: 0.29388716271748383, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1735, Loss: 0.34759309035442243, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1736, Loss: 0.4389141078007948, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1737, Loss: 0.32147649497161424, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1738, Loss: 0.2564773060286994, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1739, Loss: 0.44123986607163235, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1740, Loss: 0.26265837316206064, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1741, Loss: 0.38367080198565773, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1742, Loss: 0.2862489174654741, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1743, Loss: 0.300004369487779, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1744, Loss: 0.3844253723818576, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1745, Loss: 0.46883640832812296, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1746, Loss: 0.35589855647773655, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1747, Loss: 0.3024726845505552, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1748, Loss: 0.36932677648000745, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1749, Loss: 0.39294698827611596, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1750, Loss: 0.5192480325974755, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1751, Loss: 0.24512867651316933, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1752, Loss: 0.25901460295765855, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1753, Loss: 0.4155012339652209, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1754, Loss: 0.34546453063531635, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1755, Loss: 0.26577415935441256, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1756, Loss: 0.27090347680387794, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1757, Loss: 0.6692127590422227, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1758, Loss: 0.47776885548725584, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1759, Loss: 0.4105702494818343, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1760, Loss: 0.28879397548823355, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1761, Loss: 0.2841363700740478, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1762, Loss: 0.627276100506809, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1763, Loss: 0.3688382033181935, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1764, Loss: 0.33760494543176967, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1765, Loss: 0.35287504872668213, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1766, Loss: 0.3773322622101115, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1767, Loss: 0.27648412813574147, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1768, Loss: 0.31125049068631144, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1769, Loss: 0.5437835859130044, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1770, Loss: 0.40572247189343347, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1771, Loss: 0.3843131000616452, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1772, Loss: 0.29278707010005006, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1773, Loss: 0.3296921872731135, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1774, Loss: 0.288748247745926, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1775, Loss: 0.27615194281313565, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1776, Loss: 0.28194106607310854, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1777, Loss: 0.34299608911606816, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1778, Loss: 0.27672498671399226, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1779, Loss: 0.25813777514797814, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1780, Loss: 0.2803536609230425, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1781, Loss: 0.33440606589761895, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1782, Loss: 0.6029449989101227, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1783, Loss: 0.362701140542781, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1784, Loss: 0.23992709589699845, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1785, Loss: 0.37697557767407763, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1786, Loss: 0.2778306098807065, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1787, Loss: 0.3114999952357039, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1788, Loss: 0.30247452335551345, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1789, Loss: 0.28785456223276173, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1790, Loss: 0.4767942587441967, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1791, Loss: 0.33249398202518887, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1792, Loss: 0.31197522980213327, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1793, Loss: 0.36143218508807884, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1794, Loss: 0.24869834096779356, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1795, Loss: 0.29382183537274326, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1796, Loss: 0.4337905329352526, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1797, Loss: 0.2986504658797382, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1798, Loss: 0.26002065331299423, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1799, Loss: 0.25217134766292765, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1800, Loss: 0.259735756292916, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1801, Loss: 0.28211141495768355, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1802, Loss: 0.38531181228160183, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1803, Loss: 0.29499281631299223, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1804, Loss: 0.3161906305363344, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1805, Loss: 0.320823814778092, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1806, Loss: 0.45659444117249665, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1807, Loss: 0.5237262596888502, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1808, Loss: 0.46657495653224823, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1809, Loss: 0.28944553454507316, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1810, Loss: 0.29162050320054805, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1811, Loss: 0.2711898232824008, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1812, Loss: 0.33584051845579216, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1813, Loss: 0.28481959617303887, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1814, Loss: 0.3208267646236644, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1815, Loss: 0.24117052991738128, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1816, Loss: 0.2621180376501582, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1817, Loss: 0.3697687838176777, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1818, Loss: 0.295263849483575, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1819, Loss: 0.31013842661052343, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1820, Loss: 0.3396787102802489, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1821, Loss: 0.32423044892936914, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1822, Loss: 0.29475100091318657, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1823, Loss: 0.3076924934407622, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1824, Loss: 0.23050667796116361, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1825, Loss: 0.2822846514753091, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1826, Loss: 0.5774972743108128, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1827, Loss: 0.2594204398040084, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1828, Loss: 0.30304439099083913, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1829, Loss: 0.5936540712085077, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1830, Loss: 0.3020251984434753, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1831, Loss: 0.2840027306706614, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1832, Loss: 0.3118053378362028, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1833, Loss: 0.30672335707883125, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1834, Loss: 0.31355992928134213, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1835, Loss: 0.33318434647194073, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1836, Loss: 0.2783928354195908, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1837, Loss: 0.24891100345136702, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1838, Loss: 0.30398773049976435, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1839, Loss: 0.31372808855296663, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1840, Loss: 0.5092598437970115, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1841, Loss: 0.36292958696483396, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1842, Loss: 0.3380474343640889, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1843, Loss: 0.26086599367212465, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1844, Loss: 0.29134468581536105, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1845, Loss: 0.3608548817289353, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1846, Loss: 0.31008389622236343, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1847, Loss: 0.29477442958529204, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1848, Loss: 0.24280620320511553, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1849, Loss: 0.520470515761108, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1850, Loss: 0.3013296015602023, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1851, Loss: 0.3096544912417941, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1852, Loss: 0.4769030176012942, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1853, Loss: 0.24845986315682972, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1854, Loss: 0.31516416314420004, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1855, Loss: 0.25665634536016996, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1856, Loss: 0.30614268058137784, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1857, Loss: 0.3768542432246308, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1858, Loss: 0.2606400693351941, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1859, Loss: 0.5463619655733272, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1860, Loss: 0.604168226153219, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1861, Loss: 0.31780180943961744, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1862, Loss: 0.2522318890443993, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1863, Loss: 0.3817063161483335, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1864, Loss: 0.3093820293558127, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1865, Loss: 0.4431454364874652, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1866, Loss: 0.3333530145877362, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1867, Loss: 0.33385012517961665, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1868, Loss: 0.2489535382759546, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1869, Loss: 0.6778116167998318, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1870, Loss: 0.35780694811815894, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1871, Loss: 0.27081629608322777, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1872, Loss: 0.2959282528274972, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1873, Loss: 0.2900374859655586, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1874, Loss: 0.28673589464835014, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Batch 1875, Loss: 0.6133000141416699, Batch Size: 32, Learning Rate: 6.948508388496091e-05\n",
      "Epoch 11, Updated Learning Rate: 5.906232130221677e-05\n",
      "Epoch 11, Average Loss: 0.351441177567177, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1, Loss: 0.6798115788328047, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 2, Loss: 0.3933185501253804, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 3, Loss: 0.2496987836303462, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 4, Loss: 0.35746710826526895, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 5, Loss: 0.289263492796606, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 6, Loss: 0.3000992318811185, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 7, Loss: 0.2710723190792944, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 8, Loss: 0.3869998900461049, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 9, Loss: 0.2585950550480426, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 10, Loss: 0.48490811153202973, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 11, Loss: 0.2517542870838407, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 12, Loss: 0.7334133385756477, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 13, Loss: 0.2865984126319412, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 14, Loss: 0.30353138982702854, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 15, Loss: 0.2748847061295723, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 16, Loss: 0.5229055252220081, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 17, Loss: 0.4484297859638347, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 18, Loss: 0.3135878765856543, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 19, Loss: 0.2713912199499915, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 20, Loss: 0.38628795807156435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 21, Loss: 0.6406929771477808, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 22, Loss: 0.28466944691479706, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 23, Loss: 0.3428364314206666, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 24, Loss: 0.4581951901637178, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 25, Loss: 0.2683777187177847, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 26, Loss: 0.28292593747792544, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 27, Loss: 0.33460111414037796, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 28, Loss: 0.25635479578483494, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 29, Loss: 0.48300354517329713, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 30, Loss: 0.3652887157834085, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 31, Loss: 0.33815338905810327, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 32, Loss: 0.30495400370951703, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 33, Loss: 0.29040692108266275, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 34, Loss: 0.3113181406408162, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 35, Loss: 0.2652474447242868, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 36, Loss: 0.3163005653148971, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 37, Loss: 0.3134730520397282, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 38, Loss: 0.28358647764682554, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 39, Loss: 0.27382011495190045, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 40, Loss: 0.3454015269724624, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 41, Loss: 0.29526546483929084, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 42, Loss: 0.2546511129822334, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 43, Loss: 0.5339443378668272, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 44, Loss: 0.30082992675768033, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 45, Loss: 0.2841871999569699, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 46, Loss: 0.37522102675057845, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 47, Loss: 0.33251962167364485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 48, Loss: 0.3003791384085588, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 49, Loss: 0.8074256396794935, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 50, Loss: 0.3037561907814173, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 51, Loss: 0.350765413420039, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 52, Loss: 0.2566390789687464, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 53, Loss: 0.3089067191678488, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 54, Loss: 0.5132837113616074, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 55, Loss: 0.29544473063235516, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 56, Loss: 0.40971365527026027, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 57, Loss: 0.29567668675811565, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 58, Loss: 0.36290928640795717, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 59, Loss: 0.23732202855631374, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 60, Loss: 0.42035723036784034, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 61, Loss: 0.24228355297715537, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 62, Loss: 0.45166117871405204, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 63, Loss: 0.29397870192393555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 64, Loss: 0.30514331700019065, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 65, Loss: 0.34694871979274455, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 66, Loss: 0.356986159659593, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 67, Loss: 0.29641325182355094, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 68, Loss: 0.24171788893926366, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 69, Loss: 0.3789702189089955, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 70, Loss: 0.4752946943280983, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 71, Loss: 0.738944326251222, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 72, Loss: 0.2327859679216283, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 73, Loss: 0.3626573115416527, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 74, Loss: 0.38271302699665555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 75, Loss: 0.4223739446931363, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 76, Loss: 0.2624313727072823, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 77, Loss: 0.24156804196149728, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 78, Loss: 0.3568754475405819, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 79, Loss: 0.3301459154108211, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 80, Loss: 0.3643947317898445, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 81, Loss: 0.30247921270141787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 82, Loss: 0.6368059885583246, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 83, Loss: 0.2712620845060958, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 84, Loss: 0.2833580498598126, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 85, Loss: 0.46326939593014815, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 86, Loss: 0.23976556013280267, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 87, Loss: 0.2706892055533908, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 88, Loss: 0.23990822024348826, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 89, Loss: 0.32060676238878333, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 90, Loss: 0.25097388522701064, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 91, Loss: 0.26921359745263207, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 92, Loss: 0.2636869736623198, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 93, Loss: 0.314409148319935, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 94, Loss: 0.2832212098665121, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 95, Loss: 0.49419382491314034, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 96, Loss: 0.32407825008981134, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 97, Loss: 0.2601437062809707, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 98, Loss: 0.26510479221365235, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 99, Loss: 0.26162673425847416, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 100, Loss: 0.3121130204516477, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 101, Loss: 0.31223380358821756, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 102, Loss: 0.26675472834612163, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 103, Loss: 0.25135717230477106, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 104, Loss: 0.3003026499339986, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 105, Loss: 0.39508026349607595, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 106, Loss: 0.42708583326709193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 107, Loss: 0.2731581323591833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 108, Loss: 0.24580307129110687, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 109, Loss: 0.2559112392210673, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 110, Loss: 0.29432748618569626, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 111, Loss: 0.3443850127438786, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 112, Loss: 0.3370903228330774, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 113, Loss: 0.2833225581683, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 114, Loss: 0.37119109854062965, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 115, Loss: 0.2848878351130043, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 116, Loss: 0.3383521158958616, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 117, Loss: 0.5543355828898968, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 118, Loss: 0.31120157764791934, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 119, Loss: 0.2483363122219047, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 120, Loss: 0.4584547017277458, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 121, Loss: 0.25650940068636724, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 122, Loss: 0.28833869468671325, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 123, Loss: 0.29692727614391595, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 124, Loss: 0.25783307646736564, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 125, Loss: 0.38362186186108566, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 126, Loss: 0.2775720373252585, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 127, Loss: 0.3591216184876004, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 128, Loss: 0.3205738743273282, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 129, Loss: 0.6538335139893352, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 130, Loss: 0.7141208319874, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 131, Loss: 0.2531724682719317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 132, Loss: 0.4321942206384345, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 133, Loss: 0.346154064766552, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 134, Loss: 0.33014907300691243, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 135, Loss: 0.29259163895040996, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 136, Loss: 0.4954941581258283, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 137, Loss: 0.2950955608157403, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 138, Loss: 0.4249245990477829, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 139, Loss: 0.2590636961510295, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 140, Loss: 0.4147692499830241, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 141, Loss: 0.3024900844358478, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 142, Loss: 0.24285183552988215, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 143, Loss: 0.5184391269965032, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 144, Loss: 0.5346413800745273, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 145, Loss: 0.35040549183474284, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 146, Loss: 0.5146372876724749, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 147, Loss: 0.5250144557780325, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 148, Loss: 0.2921152198423748, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 149, Loss: 0.24810194381376874, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 150, Loss: 0.36988666511556667, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 151, Loss: 0.3214501003280948, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 152, Loss: 0.3512190022641923, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 153, Loss: 0.2591653473345977, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 154, Loss: 0.39355615987626935, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 155, Loss: 0.30559061651303987, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 156, Loss: 0.3121903959783543, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 157, Loss: 0.25222224909158786, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 158, Loss: 0.30806291858973145, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 159, Loss: 0.2282874136796292, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 160, Loss: 0.33499137851831823, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 161, Loss: 0.23995201713578665, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 162, Loss: 0.3901708126693099, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 163, Loss: 0.3348808765664448, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 164, Loss: 0.4310313459573132, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 165, Loss: 0.26654957398252, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 166, Loss: 0.31826368074790456, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 167, Loss: 0.2950531614023956, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 168, Loss: 0.3377737716992708, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 169, Loss: 0.4543098540014555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 170, Loss: 0.2416209167924154, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 171, Loss: 0.42740370096991953, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 172, Loss: 0.2659791339463175, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 173, Loss: 0.3217445540444632, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 174, Loss: 0.31276757346441353, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 175, Loss: 0.24006575379976608, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 176, Loss: 0.27670194148883415, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 177, Loss: 0.3896718984596751, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 178, Loss: 0.2393806922339873, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 179, Loss: 0.30674003619581053, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 180, Loss: 0.4564574282157673, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 181, Loss: 0.3543114822449128, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 182, Loss: 0.312176581855681, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 183, Loss: 0.31326016071874263, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 184, Loss: 0.4785475294032991, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 185, Loss: 0.3141375723683969, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 186, Loss: 0.3077527199733896, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 187, Loss: 0.5157430083452355, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 188, Loss: 0.3895118473111919, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 189, Loss: 0.36153261096334866, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 190, Loss: 0.2938383555277902, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 191, Loss: 0.40219803140688454, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 192, Loss: 0.34148918169219505, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 193, Loss: 0.26415939257896937, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 194, Loss: 0.2873669757835494, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 195, Loss: 0.34812395020148523, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 196, Loss: 0.29939611276661293, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 197, Loss: 0.526102604143802, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 198, Loss: 0.3085374264922506, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 199, Loss: 0.47642942901790586, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 200, Loss: 0.3239695734574643, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 201, Loss: 0.3325579023504161, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 202, Loss: 0.4556929419598206, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 203, Loss: 0.4012258827567955, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 204, Loss: 0.5824509919405434, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 205, Loss: 0.2446099394340448, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 206, Loss: 0.2522255369878754, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 207, Loss: 0.5366068714116307, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 208, Loss: 0.46881956911828204, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 209, Loss: 0.6445944508648145, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 210, Loss: 0.26520282593127836, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 211, Loss: 0.2744571162645531, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 212, Loss: 0.457383070531152, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 213, Loss: 0.2798736129389381, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 214, Loss: 0.3094946798215098, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 215, Loss: 0.29789043749523914, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 216, Loss: 0.23860530933685886, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 217, Loss: 0.35446007307510274, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 218, Loss: 0.3447671514826101, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 219, Loss: 0.291860367444131, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 220, Loss: 0.32899884999756196, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 221, Loss: 0.24177536553136303, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 222, Loss: 0.378621129477119, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 223, Loss: 0.5414290721619381, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 224, Loss: 0.2457023634370683, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 225, Loss: 0.33128921993592264, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 226, Loss: 0.4507347975612283, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 227, Loss: 0.3975503914525317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 228, Loss: 0.42056661220485625, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 229, Loss: 0.2803345012573924, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 230, Loss: 0.3491441619796182, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 231, Loss: 0.4020068063876736, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 232, Loss: 0.6127191086566328, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 233, Loss: 0.3287993061929894, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 234, Loss: 0.4060190464839798, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 235, Loss: 0.35594937925113523, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 236, Loss: 0.2765866940442999, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 237, Loss: 0.31958793621350023, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 238, Loss: 0.3251004666988716, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 239, Loss: 0.29113017464544755, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 240, Loss: 0.2777159540340719, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 241, Loss: 0.2497048738872332, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 242, Loss: 0.24405217818143748, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 243, Loss: 0.2728717507370969, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 244, Loss: 0.2839914574271255, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 245, Loss: 0.318541468809979, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 246, Loss: 0.30408603175989435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 247, Loss: 0.28640739488763356, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 248, Loss: 0.2798028387557878, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 249, Loss: 0.2808425407585677, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 250, Loss: 0.33932324858284646, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 251, Loss: 0.3492856509728729, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 252, Loss: 0.5347712419555335, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 253, Loss: 0.25359813140302695, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 254, Loss: 0.2545907328414728, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 255, Loss: 0.2944361311501119, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 256, Loss: 0.27740250104530456, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 257, Loss: 0.33263089845120886, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 258, Loss: 0.3982499842035233, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 259, Loss: 0.23033714880939127, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 260, Loss: 0.3389514470746346, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 261, Loss: 0.3492358522801773, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 262, Loss: 0.3608983515642911, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 263, Loss: 0.35651345323562056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 264, Loss: 0.4774932516536028, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 265, Loss: 0.247114975470858, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 266, Loss: 0.2951254764739456, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 267, Loss: 0.28958053366215086, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 268, Loss: 0.3105354755943231, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 269, Loss: 0.3380573594049029, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 270, Loss: 0.2985797300850056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 271, Loss: 0.2961184507073431, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 272, Loss: 0.4290398526798397, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 273, Loss: 0.30449247222651415, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 274, Loss: 0.29132463918189727, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 275, Loss: 0.39672995804993766, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 276, Loss: 0.32044497565554203, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 277, Loss: 0.2553507448219706, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 278, Loss: 0.4654737561020484, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 279, Loss: 0.25557760880636265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 280, Loss: 0.2711420031534688, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 281, Loss: 0.24774688367956402, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 282, Loss: 0.29379574411587417, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 283, Loss: 0.4277419630299317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 284, Loss: 0.32566432571808435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 285, Loss: 0.3033917303619187, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 286, Loss: 0.286067072566833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 287, Loss: 0.4729406994667291, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 288, Loss: 0.4318219817459453, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 289, Loss: 0.3088109961474401, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 290, Loss: 0.32612321641111675, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 291, Loss: 0.28830887010577905, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 292, Loss: 0.2453670597791337, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 293, Loss: 0.30919093425829625, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 294, Loss: 0.24841108818468025, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 295, Loss: 0.26802789546978634, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 296, Loss: 0.3187801151140853, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 297, Loss: 0.34884741489155985, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 298, Loss: 0.3110949191433294, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 299, Loss: 0.27735149858504043, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 300, Loss: 0.43745283642829247, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 301, Loss: 0.29349015206326046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 302, Loss: 0.28507767043656207, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 303, Loss: 0.4058841861699024, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 304, Loss: 0.3532127511918839, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 305, Loss: 0.26258838695916553, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 306, Loss: 0.31910898648263, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 307, Loss: 0.3250410548834213, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 308, Loss: 0.29079868331055353, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 309, Loss: 0.3321494432309765, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 310, Loss: 0.27477993486371716, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 311, Loss: 0.2741945977863164, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 312, Loss: 0.2874940289880088, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 313, Loss: 0.5375767309324581, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 314, Loss: 0.2854357694818345, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 315, Loss: 0.2910593229533787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 316, Loss: 0.2622221173973873, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 317, Loss: 0.40665836743414685, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 318, Loss: 0.2604754420327976, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 319, Loss: 0.3118659141830195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 320, Loss: 0.27169125278546025, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 321, Loss: 0.40955107902856447, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 322, Loss: 0.2925234836929145, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 323, Loss: 0.28550954421149616, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 324, Loss: 0.3243579881572872, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 325, Loss: 0.322246624260773, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 326, Loss: 0.25418776144143385, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 327, Loss: 0.23549640339025046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 328, Loss: 0.25523775403756754, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 329, Loss: 0.2862631808616469, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 330, Loss: 0.2708760831389035, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 331, Loss: 0.5614391975290561, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 332, Loss: 0.5462645920051037, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 333, Loss: 0.40497814248097996, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 334, Loss: 0.2598902137292242, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 335, Loss: 0.3106052359340774, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 336, Loss: 0.3031060245927049, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 337, Loss: 0.2639130104665348, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 338, Loss: 0.2713713073201184, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 339, Loss: 0.2996909351898995, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 340, Loss: 0.23208128651547377, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 341, Loss: 0.6528595488355848, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 342, Loss: 0.3128626279321803, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 343, Loss: 0.30614245725089856, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 344, Loss: 0.2673801734896707, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 345, Loss: 0.3029592559512001, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 346, Loss: 0.42779047374039914, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 347, Loss: 0.2624793715241658, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 348, Loss: 0.3739014036344726, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 349, Loss: 0.45106518504401827, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 350, Loss: 0.4500642710701075, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 351, Loss: 0.2772146588708889, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 352, Loss: 0.23372179421301142, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 353, Loss: 0.3180001873670733, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 354, Loss: 0.2617035316935937, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 355, Loss: 0.2802583734422903, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 356, Loss: 0.3880767541268637, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 357, Loss: 0.3160991515089311, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 358, Loss: 0.3861924609258036, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 359, Loss: 0.3783038007886392, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 360, Loss: 0.27987349156335595, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 361, Loss: 0.28615840138173065, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 362, Loss: 0.32781973662777325, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 363, Loss: 0.371074630622291, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 364, Loss: 0.3129745470293816, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 365, Loss: 0.2798064964664557, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 366, Loss: 0.34963273282685925, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 367, Loss: 0.3076343315264891, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 368, Loss: 0.2751752124725049, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 369, Loss: 0.4851220457428568, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 370, Loss: 0.26947297482396193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 371, Loss: 0.40559569673393103, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 372, Loss: 0.45802601025962314, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 373, Loss: 0.3876348195606036, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 374, Loss: 0.27172533658427, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 375, Loss: 0.24447262799221509, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 376, Loss: 0.29269586328492647, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 377, Loss: 0.6031062445508906, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 378, Loss: 0.2899878705315001, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 379, Loss: 0.3376163630232723, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 380, Loss: 0.5621203539870616, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 381, Loss: 0.35198565103213375, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 382, Loss: 0.514133946010161, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 383, Loss: 0.4448102657871843, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 384, Loss: 0.43310143042255245, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 385, Loss: 0.3101837382116624, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 386, Loss: 0.29462643979252295, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 387, Loss: 0.42941645055953204, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 388, Loss: 0.5146369305231319, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 389, Loss: 0.43178699690333955, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 390, Loss: 0.3192218772451353, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 391, Loss: 0.2808237747686965, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 392, Loss: 0.23658163184091602, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 393, Loss: 0.3335940097966978, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 394, Loss: 0.3688185717587487, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 395, Loss: 0.25067698663201843, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 396, Loss: 0.372639356411216, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 397, Loss: 0.33943226632154544, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 398, Loss: 0.28525809134569724, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 399, Loss: 0.3134776599489892, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 400, Loss: 0.32265558814596074, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 401, Loss: 0.3077206165297484, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 402, Loss: 0.2556832284255252, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 403, Loss: 0.407024259816194, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 404, Loss: 0.24007507259761215, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 405, Loss: 0.518852791261485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 406, Loss: 0.4286086608807134, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 407, Loss: 0.32851418978482133, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 408, Loss: 0.3994206358667801, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 409, Loss: 0.24939415093493475, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 410, Loss: 0.31326784747204506, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 411, Loss: 0.43714749548506227, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 412, Loss: 0.2664831370326911, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 413, Loss: 0.4366723411539746, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 414, Loss: 0.687400972147183, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 415, Loss: 0.36810904796112254, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 416, Loss: 0.316095916181773, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 417, Loss: 0.29731140985094173, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 418, Loss: 0.24870819002486935, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 419, Loss: 0.27734984718121813, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 420, Loss: 0.3364284797733202, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 421, Loss: 0.37144514189022126, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 422, Loss: 0.3045830148147035, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 423, Loss: 0.3516185248026167, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 424, Loss: 0.30277723173510485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 425, Loss: 0.30984762626911305, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 426, Loss: 0.4172646202554211, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 427, Loss: 0.24415372147045497, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 428, Loss: 0.3655588040719906, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 429, Loss: 0.2778247511641856, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 430, Loss: 0.32562255968871623, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 431, Loss: 0.32593067691078603, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 432, Loss: 0.5207776044092336, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 433, Loss: 0.2819540227811116, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 434, Loss: 0.3640517714827163, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 435, Loss: 0.3244680430533062, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 436, Loss: 0.4226242108500398, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 437, Loss: 0.39509124474341717, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 438, Loss: 0.3625253826909083, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 439, Loss: 0.24060385732365014, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 440, Loss: 0.23745427234170224, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 441, Loss: 0.3203530811827778, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 442, Loss: 0.2821474862550609, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 443, Loss: 0.32516065161065244, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 444, Loss: 0.2831295745823801, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 445, Loss: 0.26693725936356255, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 446, Loss: 0.5534853715692989, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 447, Loss: 0.3077117637905369, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 448, Loss: 0.40164230571660087, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 449, Loss: 0.2932220738087159, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 450, Loss: 0.4317443214205342, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 451, Loss: 0.2322426774097953, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 452, Loss: 0.34647683067400703, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 453, Loss: 0.29703148307866606, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 454, Loss: 0.39518964566976056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 455, Loss: 0.28267582840730215, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 456, Loss: 0.3519163554000681, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 457, Loss: 0.258657212952249, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 458, Loss: 0.3247469299327491, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 459, Loss: 0.28847499835373885, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 460, Loss: 0.3251584142427068, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 461, Loss: 0.2498762595972463, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 462, Loss: 0.2591652189959756, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 463, Loss: 0.27384045476933705, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 464, Loss: 0.2540848449691554, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 465, Loss: 0.6028404117578984, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 466, Loss: 0.3512618896589754, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 467, Loss: 0.34329463764982915, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 468, Loss: 0.3354748750893552, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 469, Loss: 0.2713425659125374, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 470, Loss: 0.5434520762049283, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 471, Loss: 0.30912203487034956, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 472, Loss: 0.3790660606525441, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 473, Loss: 0.2806120108629159, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 474, Loss: 0.45656820687218724, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 475, Loss: 0.23460755988620763, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 476, Loss: 0.34432281975893786, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 477, Loss: 0.43969660054054877, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 478, Loss: 0.3296852047780828, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 479, Loss: 0.4441858238982791, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 480, Loss: 0.4633106912458514, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 481, Loss: 0.2634828533128629, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 482, Loss: 0.24483505107625186, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 483, Loss: 0.36072730067891, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 484, Loss: 0.2361379165063291, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 485, Loss: 0.5702086830913995, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 486, Loss: 0.35897045622088525, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 487, Loss: 0.3656126102481282, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 488, Loss: 0.5522411686057551, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 489, Loss: 0.2920739077202833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 490, Loss: 0.28442079883962956, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 491, Loss: 0.29994949529980286, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 492, Loss: 0.3016189013802962, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 493, Loss: 0.4059571153674214, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 494, Loss: 0.32315308915543367, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 495, Loss: 0.2743488346353803, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 496, Loss: 0.33361863159475247, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 497, Loss: 0.33957481877130385, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 498, Loss: 0.29219529274629863, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 499, Loss: 0.29845916923220517, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 500, Loss: 0.2919138339980105, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 501, Loss: 0.26529044194725165, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 502, Loss: 0.3562352971850099, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 503, Loss: 0.5437750829831776, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 504, Loss: 0.31222016252659757, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 505, Loss: 0.3850667542683825, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 506, Loss: 0.26434807177740655, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 507, Loss: 0.35016886157541227, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 508, Loss: 0.4198875391672697, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 509, Loss: 0.3484091889722595, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 510, Loss: 0.25333461984967676, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 511, Loss: 0.30894290075741376, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 512, Loss: 0.28392516102441046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 513, Loss: 0.3016417539301667, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 514, Loss: 0.5391388594608443, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 515, Loss: 0.28682473900059496, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 516, Loss: 0.2543374797509588, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 517, Loss: 0.2808677429365869, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 518, Loss: 0.3697208465424564, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 519, Loss: 0.44396117915838623, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 520, Loss: 0.32169626107266236, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 521, Loss: 0.3342016220397089, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 522, Loss: 0.35828056056778695, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 523, Loss: 0.28486849248945056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 524, Loss: 0.3738379127771725, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 525, Loss: 0.28718704019960567, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 526, Loss: 0.36669546569776945, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 527, Loss: 0.2573362479560826, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 528, Loss: 0.29327969768566836, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 529, Loss: 0.3538700949428979, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 530, Loss: 0.36872952674631504, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 531, Loss: 0.44692034015384124, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 532, Loss: 0.3193832472462512, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 533, Loss: 0.345756407426899, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 534, Loss: 0.4206049798918139, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 535, Loss: 0.3454961251758826, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 536, Loss: 0.44473813081199176, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 537, Loss: 0.34866918100772193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 538, Loss: 0.642413570883694, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 539, Loss: 0.31692509813260206, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 540, Loss: 0.31260398312310705, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 541, Loss: 0.329657646570071, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 542, Loss: 0.28227134246821284, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 543, Loss: 0.4831052286369858, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 544, Loss: 0.24234691437962916, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 545, Loss: 0.27392118973787566, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 546, Loss: 0.28455960829409005, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 547, Loss: 0.3079932337871865, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 548, Loss: 0.3719728992841089, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 549, Loss: 0.37095007602723673, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 550, Loss: 0.3393922076248029, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 551, Loss: 0.36079098253156083, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 552, Loss: 0.36082535959223994, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 553, Loss: 0.2717025398642289, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 554, Loss: 0.43304340070615865, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 555, Loss: 0.3627482393637125, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 556, Loss: 0.48708158904917, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 557, Loss: 0.25487213363385613, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 558, Loss: 0.34445090879683055, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 559, Loss: 0.34001924051668153, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 560, Loss: 0.26468920634370807, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 561, Loss: 0.38395985568531094, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 562, Loss: 0.25787651557169206, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 563, Loss: 0.41316774327216077, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 564, Loss: 0.23466686231512868, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 565, Loss: 0.39964511253900675, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 566, Loss: 0.3579899199179737, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 567, Loss: 0.23214070540469153, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 568, Loss: 0.3439134287975506, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 569, Loss: 0.28575574909780266, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 570, Loss: 0.25836291225568336, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 571, Loss: 0.286661200492343, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 572, Loss: 0.25123152950992417, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 573, Loss: 0.22977433827812496, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 574, Loss: 0.46500369676987496, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 575, Loss: 0.4558415526528872, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 576, Loss: 0.24389601178148668, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 577, Loss: 0.3072221789297581, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 578, Loss: 0.2660583173311137, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 579, Loss: 0.4177458328458442, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 580, Loss: 0.5026459713258085, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 581, Loss: 0.34330697777477825, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 582, Loss: 0.42170647576961834, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 583, Loss: 0.2967454855179906, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 584, Loss: 0.47980367334279833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 585, Loss: 0.25154459053384015, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 586, Loss: 0.38602876684523435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 587, Loss: 0.34029007159886737, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 588, Loss: 0.5748816483662635, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 589, Loss: 0.2762345077352102, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 590, Loss: 0.2520046606656263, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 591, Loss: 0.43888775769199373, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 592, Loss: 0.34156102187245, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 593, Loss: 0.24089766489563508, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 594, Loss: 0.4444312319368596, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 595, Loss: 0.2713408397477127, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 596, Loss: 0.2539470344648771, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 597, Loss: 0.3774970933220317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 598, Loss: 0.5443209486962447, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 599, Loss: 0.25348363638658444, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 600, Loss: 0.3120735673162804, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 601, Loss: 0.5894994006953764, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 602, Loss: 0.413590973431574, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 603, Loss: 0.2843437676924898, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 604, Loss: 0.24748152890210118, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 605, Loss: 0.36466358330636783, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 606, Loss: 0.2707948382229905, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 607, Loss: 0.30913708652401894, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 608, Loss: 0.23723634378677058, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 609, Loss: 0.260698774328801, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 610, Loss: 0.28381081840944744, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 611, Loss: 0.26157690614969015, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 612, Loss: 0.32226709011207866, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 613, Loss: 0.3077539469677903, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 614, Loss: 0.3763048794809297, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 615, Loss: 0.38316207900607635, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 616, Loss: 0.5462811667941467, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 617, Loss: 0.39260713145195947, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 618, Loss: 0.3703460927791705, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 619, Loss: 0.23645866894310502, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 620, Loss: 0.3326715895539357, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 621, Loss: 0.3267988738404165, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 622, Loss: 0.28594220483137445, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 623, Loss: 0.30109174633125113, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 624, Loss: 0.4366028592911241, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 625, Loss: 0.2791008730890414, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 626, Loss: 0.2730056786909389, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 627, Loss: 0.2799055026161903, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 628, Loss: 0.2719287642159296, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 629, Loss: 0.3316504741894349, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 630, Loss: 0.2772292618358181, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 631, Loss: 0.31861981056757094, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 632, Loss: 0.27692398583413846, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 633, Loss: 0.44484921462027294, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 634, Loss: 0.38044526499541176, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 635, Loss: 0.5455188969312672, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 636, Loss: 0.38351307597147205, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 637, Loss: 0.26790907828483657, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 638, Loss: 0.3349571378051529, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 639, Loss: 0.27525215833908107, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 640, Loss: 0.25864535275831735, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 641, Loss: 0.35576822877474407, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 642, Loss: 0.3168849569487563, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 643, Loss: 0.2744664536154003, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 644, Loss: 0.3842852624559533, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 645, Loss: 0.3856326497833272, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 646, Loss: 0.4101278845573, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 647, Loss: 0.2594578400310812, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 648, Loss: 0.3310190864986774, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 649, Loss: 0.34269839427049004, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 650, Loss: 0.2827200260767126, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 651, Loss: 0.41698292556838856, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 652, Loss: 0.2374332245895608, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 653, Loss: 0.3381168917621302, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 654, Loss: 0.25984843432707094, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 655, Loss: 0.30173985793847913, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 656, Loss: 0.3906342359915025, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 657, Loss: 0.27395824891244847, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 658, Loss: 0.4744339156931009, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 659, Loss: 0.3183868797750972, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 660, Loss: 0.34769403630199713, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 661, Loss: 0.24479699997779228, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 662, Loss: 0.27757424283759574, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 663, Loss: 0.3112125789504234, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 664, Loss: 0.3219466162950453, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 665, Loss: 0.25465139361913486, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 666, Loss: 0.3296238967879194, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 667, Loss: 0.31917269876965193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 668, Loss: 0.3544447520782942, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 669, Loss: 0.3494771257878667, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 670, Loss: 0.2661431756732751, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 671, Loss: 0.3053347979297697, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 672, Loss: 0.24634976597486524, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 673, Loss: 0.3580662718318955, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 674, Loss: 0.4072145600980833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 675, Loss: 0.262391935893978, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 676, Loss: 0.30250926434471115, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 677, Loss: 0.34445191401923786, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 678, Loss: 0.3906959490459957, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 679, Loss: 0.2520293237483792, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 680, Loss: 0.3035528847221672, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 681, Loss: 0.40218978819885093, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 682, Loss: 0.5944456945317123, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 683, Loss: 0.3678782380392965, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 684, Loss: 0.26330977884353085, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 685, Loss: 0.5100937245224597, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 686, Loss: 0.38337279709880084, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 687, Loss: 0.23834286036079635, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 688, Loss: 0.4542511995254063, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 689, Loss: 0.2687714758869176, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 690, Loss: 0.35053655842232456, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 691, Loss: 0.3333675627853975, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 692, Loss: 0.5205202913918865, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 693, Loss: 0.3343444194195193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 694, Loss: 0.45056807114205555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 695, Loss: 0.48081235769998265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 696, Loss: 0.439249456110749, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 697, Loss: 0.34086428842888733, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 698, Loss: 0.26393723454346135, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 699, Loss: 0.25897216911333343, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 700, Loss: 0.28887212910218896, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 701, Loss: 0.31518339545738483, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 702, Loss: 0.3259258994569375, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 703, Loss: 0.3994540108630564, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 704, Loss: 0.5591048770885729, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 705, Loss: 0.30304490424354125, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 706, Loss: 0.3913649469039615, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 707, Loss: 0.23215740039517682, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 708, Loss: 0.41719285040809295, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 709, Loss: 0.3731907900408097, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 710, Loss: 0.26115392505325224, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 711, Loss: 0.6057945044291742, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 712, Loss: 0.4451944316079234, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 713, Loss: 0.2950399815412485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 714, Loss: 0.6908629528262829, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 715, Loss: 0.2636667572272923, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 716, Loss: 0.4323476957900543, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 717, Loss: 0.309155709500727, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 718, Loss: 0.6110513701447962, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 719, Loss: 0.27175572250746066, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 720, Loss: 0.38619420034432705, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 721, Loss: 0.36373602851762465, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 722, Loss: 0.30553151207518425, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 723, Loss: 0.2894609835615649, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 724, Loss: 0.2591253143818694, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 725, Loss: 0.3587868635660457, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 726, Loss: 0.23404594248811256, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 727, Loss: 0.37229271898631194, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 728, Loss: 0.235493873106064, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 729, Loss: 0.4809464599956428, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 730, Loss: 0.4769311015280172, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 731, Loss: 0.300020908495327, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 732, Loss: 0.29140400619599, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 733, Loss: 0.41013556251891936, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 734, Loss: 0.4109000866303065, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 735, Loss: 0.2729385995447586, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 736, Loss: 0.2871070820618217, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 737, Loss: 0.271385754461109, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 738, Loss: 0.535912995648298, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 739, Loss: 0.5530414549933008, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 740, Loss: 0.23400138865110853, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 741, Loss: 0.47299016783421866, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 742, Loss: 0.4746791086349812, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 743, Loss: 0.457268354659264, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 744, Loss: 0.23022991793830655, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 745, Loss: 0.3532716795486217, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 746, Loss: 0.5841582169211138, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 747, Loss: 0.46230888301916373, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 748, Loss: 0.36755016357602904, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 749, Loss: 0.4439841560395286, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 750, Loss: 0.36096701081539195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 751, Loss: 0.3227743862761962, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 752, Loss: 0.36662462732054535, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 753, Loss: 0.3323189048051312, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 754, Loss: 0.4020365533193785, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 755, Loss: 0.27789257717942867, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 756, Loss: 0.4166005351384935, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 757, Loss: 0.35226908074806507, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 758, Loss: 0.4384580788914242, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 759, Loss: 0.3016982273339426, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 760, Loss: 0.32857129962803905, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 761, Loss: 0.23352511808503268, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 762, Loss: 0.37196763688580353, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 763, Loss: 0.3157645325666099, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 764, Loss: 0.27997130594027736, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 765, Loss: 0.29155442690427685, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 766, Loss: 0.2449724472545231, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 767, Loss: 0.380306304137377, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 768, Loss: 0.4780975153316373, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 769, Loss: 0.46343805289337925, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 770, Loss: 0.3024582201806658, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 771, Loss: 0.34918540779611407, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 772, Loss: 0.3809280849330462, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 773, Loss: 0.25855563321968716, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 774, Loss: 0.3089625243486347, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 775, Loss: 0.33449165864524394, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 776, Loss: 0.4835413371556691, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 777, Loss: 0.428795227511932, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 778, Loss: 0.29080407108287254, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 779, Loss: 0.4452399270817198, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 780, Loss: 0.24935619015565555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 781, Loss: 0.6917373963557615, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 782, Loss: 0.29711685156805356, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 783, Loss: 0.25278989118735357, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 784, Loss: 0.37966145836995924, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 785, Loss: 0.2502295418305321, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 786, Loss: 0.537407563684914, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 787, Loss: 0.34960382886969044, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 788, Loss: 0.22848753526441448, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 789, Loss: 0.5606262279331997, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 790, Loss: 0.27704294139137636, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 791, Loss: 0.22897744470921205, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 792, Loss: 0.3295797663286739, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 793, Loss: 0.6885837342392587, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 794, Loss: 0.38763861942921357, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 795, Loss: 0.2667475910802804, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 796, Loss: 0.5232766183216327, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 797, Loss: 0.36662445990405645, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 798, Loss: 0.287312845196003, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 799, Loss: 0.3231092937016192, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 800, Loss: 0.2502019039672192, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 801, Loss: 0.26561494965615073, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 802, Loss: 0.3369776633592897, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 803, Loss: 0.4665365492016798, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 804, Loss: 0.31887915285512863, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 805, Loss: 0.3765597866152489, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 806, Loss: 0.29381322232118423, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 807, Loss: 0.41675292702979627, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 808, Loss: 0.26609126139340367, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 809, Loss: 0.24255110687862896, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 810, Loss: 0.3311104635872435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 811, Loss: 0.5456083804161277, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 812, Loss: 0.3779537080357659, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 813, Loss: 0.363600280651239, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 814, Loss: 0.28844072359213524, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 815, Loss: 0.2543711795029714, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 816, Loss: 0.37580545800386783, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 817, Loss: 0.2500168522858191, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 818, Loss: 0.294086238463526, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 819, Loss: 0.36105371239281236, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 820, Loss: 0.2751074227993261, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 821, Loss: 0.307640811502336, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 822, Loss: 0.2432151873248392, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 823, Loss: 0.3809784053871705, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 824, Loss: 0.5445404145878247, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 825, Loss: 0.26704753026952416, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 826, Loss: 0.28089328367734395, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 827, Loss: 0.3505210209568601, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 828, Loss: 0.3485312456173817, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 829, Loss: 0.33816906480102, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 830, Loss: 0.2948350675099529, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 831, Loss: 0.29631631399051545, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 832, Loss: 0.2787080424206745, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 833, Loss: 0.3007333141291869, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 834, Loss: 0.3819047567450997, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 835, Loss: 0.3397806862890532, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 836, Loss: 0.31968169073358543, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 837, Loss: 0.3009656759701949, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 838, Loss: 0.3171095165277447, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 839, Loss: 0.24196855936403833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 840, Loss: 0.3091076586461837, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 841, Loss: 0.4665157513992847, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 842, Loss: 0.31871652021083785, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 843, Loss: 0.466993636317183, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 844, Loss: 0.3223170069486, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 845, Loss: 0.35190611934711336, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 846, Loss: 0.3554405226570246, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 847, Loss: 0.27475791382379805, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 848, Loss: 0.3543597587119115, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 849, Loss: 0.3222410676880609, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 850, Loss: 0.5877844405188233, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 851, Loss: 0.3509005573052411, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 852, Loss: 0.2581921175879743, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 853, Loss: 0.2898941208863106, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 854, Loss: 0.27011932061010524, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 855, Loss: 0.2870639292595847, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 856, Loss: 0.3722404866045482, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 857, Loss: 0.5488199289560296, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 858, Loss: 0.3681565984314217, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 859, Loss: 0.4007941566462295, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 860, Loss: 0.2851394113475718, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 861, Loss: 0.4698464320491938, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 862, Loss: 0.2860687237550962, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 863, Loss: 0.31073001766123065, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 864, Loss: 0.3488155899806752, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 865, Loss: 0.3782496287708458, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 866, Loss: 0.25452361354694686, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 867, Loss: 0.30144844481775657, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 868, Loss: 0.28581421979703364, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 869, Loss: 0.29543448310843407, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 870, Loss: 0.3161728077352203, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 871, Loss: 0.2649566263738196, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 872, Loss: 0.2827623186456688, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 873, Loss: 0.30926822269189064, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 874, Loss: 0.23240205910943543, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 875, Loss: 0.412945889375403, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 876, Loss: 0.3320341677396219, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 877, Loss: 0.28567670172246096, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 878, Loss: 0.2832042059283111, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 879, Loss: 0.2499741968073032, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 880, Loss: 0.34347313866528384, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 881, Loss: 0.31250400620916285, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 882, Loss: 0.3035230162448498, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 883, Loss: 0.29857739666669847, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 884, Loss: 0.27548748787955574, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 885, Loss: 0.3716766878303232, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 886, Loss: 0.2887810545004029, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 887, Loss: 0.41098700979530195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 888, Loss: 0.5029925273686162, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 889, Loss: 0.29936763739715544, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 890, Loss: 0.23809251007416093, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 891, Loss: 0.31402987504546787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 892, Loss: 0.2519223986510292, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 893, Loss: 0.2521430422537469, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 894, Loss: 0.3119252381306493, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 895, Loss: 0.4160556358499426, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 896, Loss: 0.33224253431961437, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 897, Loss: 0.26375380154562805, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 898, Loss: 0.2355725699053952, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 899, Loss: 0.24690232617694802, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 900, Loss: 0.6233713285797684, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 901, Loss: 0.4019744598875333, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 902, Loss: 0.37931092920863574, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 903, Loss: 0.4462930774701328, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 904, Loss: 0.3145967940143654, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 905, Loss: 0.3924141011879758, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 906, Loss: 0.30078564871678837, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 907, Loss: 0.2360159501629193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 908, Loss: 0.23562798276832053, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 909, Loss: 0.3285352588829562, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 910, Loss: 0.6705244006616625, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 911, Loss: 0.25077508645321606, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 912, Loss: 0.3456091830856286, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 913, Loss: 0.2825420131453966, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 914, Loss: 0.25933972134652933, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 915, Loss: 0.593380413799297, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 916, Loss: 0.28819823196333083, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 917, Loss: 0.35923405121019514, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 918, Loss: 0.24754641607984826, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 919, Loss: 0.4073523877887715, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 920, Loss: 0.2714875378364724, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 921, Loss: 0.2654900790135475, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 922, Loss: 0.24250369184048112, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 923, Loss: 0.3153567356358555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 924, Loss: 0.3734751658481038, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 925, Loss: 0.6572523354454246, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 926, Loss: 0.6113534893122622, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 927, Loss: 0.4197722029081799, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 928, Loss: 0.38028385366564144, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 929, Loss: 0.24221742511917652, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 930, Loss: 0.3168170271114914, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 931, Loss: 0.33553733699807053, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 932, Loss: 0.3017178207499417, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 933, Loss: 0.25311317653697396, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 934, Loss: 0.30767996195279074, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 935, Loss: 0.3157098102771496, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 936, Loss: 0.27592166553372793, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 937, Loss: 0.2596710641488197, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 938, Loss: 0.4709531270245913, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 939, Loss: 0.3084205468660598, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 940, Loss: 0.2860542045841695, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 941, Loss: 0.2659966530222876, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 942, Loss: 0.2357300258583082, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 943, Loss: 0.3161623911174146, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 944, Loss: 0.2941752452148316, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 945, Loss: 0.38282867386074293, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 946, Loss: 0.2617589412764554, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 947, Loss: 0.24855083319750898, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 948, Loss: 0.25896859677259754, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 949, Loss: 0.4232648573579837, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 950, Loss: 0.26117798800025727, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 951, Loss: 0.25107311654347164, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 952, Loss: 0.4318091211787515, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 953, Loss: 0.47219953671775516, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 954, Loss: 0.3243887824362322, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 955, Loss: 0.29474751666499815, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 956, Loss: 0.2640125896642348, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 957, Loss: 0.25538394447131024, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 958, Loss: 0.30241373871262217, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 959, Loss: 0.33468923641786325, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 960, Loss: 0.301105576152715, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 961, Loss: 0.3814544359035251, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 962, Loss: 0.2743638571853049, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 963, Loss: 0.3352915980019985, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 964, Loss: 0.30377985458376167, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 965, Loss: 0.345763001933591, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 966, Loss: 0.3419601031627103, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 967, Loss: 0.28679374767743615, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 968, Loss: 0.2771948602110059, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 969, Loss: 0.34068062999246473, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 970, Loss: 0.27532142593012554, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 971, Loss: 0.29853432597276797, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 972, Loss: 0.48102760467102035, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 973, Loss: 0.3115503129387357, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 974, Loss: 0.34939717853471397, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 975, Loss: 0.40156446229983156, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 976, Loss: 0.4256693484823475, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 977, Loss: 0.251198228222837, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 978, Loss: 0.4153854219919638, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 979, Loss: 0.27442794727713266, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 980, Loss: 0.3062588166025665, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 981, Loss: 0.3486699544534436, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 982, Loss: 0.23364473581987014, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 983, Loss: 0.33562748128418485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 984, Loss: 0.35392540069217876, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 985, Loss: 0.27391650289853675, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 986, Loss: 0.4577473481294908, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 987, Loss: 0.3638124243399008, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 988, Loss: 0.3178523130112772, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 989, Loss: 0.24949416686810033, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 990, Loss: 0.4366898317441531, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 991, Loss: 0.4250263679143737, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 992, Loss: 0.305917653965052, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 993, Loss: 0.2519546031178033, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 994, Loss: 0.2524339816652709, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 995, Loss: 0.40324398962208097, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 996, Loss: 0.47569260662188584, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 997, Loss: 0.25490250549142596, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 998, Loss: 0.4371528999690631, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 999, Loss: 0.3411227477953662, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1000, Loss: 0.43834733240243995, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1001, Loss: 0.3695572110550541, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1002, Loss: 0.2894672674694701, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1003, Loss: 0.2638457148064053, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1004, Loss: 0.3682244025758281, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1005, Loss: 0.35544829312827203, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1006, Loss: 0.2703552578815417, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1007, Loss: 0.3788267488253235, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1008, Loss: 0.22954852203122114, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1009, Loss: 0.45365058536614555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1010, Loss: 0.2804065083738305, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1011, Loss: 0.25941202627103666, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1012, Loss: 0.4171345943555883, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1013, Loss: 0.28497635997457393, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1014, Loss: 0.34944212794374035, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1015, Loss: 0.4164960355610797, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1016, Loss: 0.24901852779539835, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1017, Loss: 0.39668193188837997, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1018, Loss: 0.3188242050547993, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1019, Loss: 0.3850332355674346, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1020, Loss: 0.32299333836588484, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1021, Loss: 0.6130785947164239, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1022, Loss: 0.3037095375194432, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1023, Loss: 0.29704294839294904, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1024, Loss: 0.30700491997898327, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1025, Loss: 0.23843000803015058, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1026, Loss: 0.496745079279873, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1027, Loss: 0.28277291069510724, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1028, Loss: 0.3236292352418366, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1029, Loss: 0.30303320941460654, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1030, Loss: 0.30390838664454234, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1031, Loss: 0.45538043615633694, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1032, Loss: 0.34247873747430196, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1033, Loss: 0.3931377075695867, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1034, Loss: 0.3712130835906139, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1035, Loss: 0.358673724845655, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1036, Loss: 0.33347270774135246, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1037, Loss: 0.3010006498442907, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1038, Loss: 0.31630589821360094, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1039, Loss: 0.252428085166175, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1040, Loss: 0.279011545481751, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1041, Loss: 0.2489438565724224, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1042, Loss: 0.24001180922560314, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1043, Loss: 0.38539245832592817, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1044, Loss: 0.2577425258641872, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1045, Loss: 0.6347363168633477, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1046, Loss: 0.250282686272871, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1047, Loss: 0.525566790471643, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1048, Loss: 0.2537916971249188, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1049, Loss: 0.606665466749688, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1050, Loss: 0.3779980742672227, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1051, Loss: 0.3810734399502872, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1052, Loss: 0.32420674110796605, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1053, Loss: 0.3037670306166818, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1054, Loss: 0.3315704041010205, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1055, Loss: 0.24566267189122734, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1056, Loss: 0.29800230193135524, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1057, Loss: 0.2861516387579408, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1058, Loss: 0.3151774924205598, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1059, Loss: 0.33846038285792995, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1060, Loss: 0.29392073993000195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1061, Loss: 0.35647172650396375, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1062, Loss: 0.5643188424118347, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1063, Loss: 0.24943503451558022, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1064, Loss: 0.4275736836723961, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1065, Loss: 0.5037800169721363, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1066, Loss: 0.2926328981440856, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1067, Loss: 0.26594290165879536, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1068, Loss: 0.2692473490013808, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1069, Loss: 0.3403597753536195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1070, Loss: 0.3047949090410904, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1071, Loss: 0.29336940656262206, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1072, Loss: 0.24672718318608855, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1073, Loss: 0.44245947522549056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1074, Loss: 0.26133176740956743, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1075, Loss: 0.3825486674451346, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1076, Loss: 0.26771355075445424, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1077, Loss: 0.27108248013724945, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1078, Loss: 0.32162988443860474, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1079, Loss: 0.3453535176594431, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1080, Loss: 0.24580854887281609, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1081, Loss: 0.29912738400159555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1082, Loss: 0.2953021214906939, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1083, Loss: 0.26553115910126596, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1084, Loss: 0.2550782196037128, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1085, Loss: 0.49197956096257683, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1086, Loss: 0.2615792913993675, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1087, Loss: 0.5640649726357619, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1088, Loss: 0.4947033362320047, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1089, Loss: 0.2616285762621432, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1090, Loss: 0.268771811655238, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1091, Loss: 0.34202191613250205, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1092, Loss: 0.4689154375932675, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1093, Loss: 0.39761980063535796, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1094, Loss: 0.4981728436165862, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1095, Loss: 0.31435728704957766, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1096, Loss: 0.4508407389416711, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1097, Loss: 0.3352448079464112, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1098, Loss: 0.35681478092989416, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1099, Loss: 0.24230477727180796, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1100, Loss: 0.25643403564533784, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1101, Loss: 0.6249921609103208, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1102, Loss: 0.40459046344842453, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1103, Loss: 0.4941218973507804, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1104, Loss: 0.4784553686614249, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1105, Loss: 0.2600222153334979, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1106, Loss: 0.3108118722950086, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1107, Loss: 0.26524813937741964, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1108, Loss: 0.24595138911272652, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1109, Loss: 0.24397624548792116, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1110, Loss: 0.2615919672557637, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1111, Loss: 0.6403522902632435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1112, Loss: 0.37717029006747105, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1113, Loss: 0.2543220168866189, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1114, Loss: 0.31697170897712157, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1115, Loss: 0.3086935183448902, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1116, Loss: 0.5383465771565555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1117, Loss: 0.4500426854202599, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1118, Loss: 0.25763264499364424, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1119, Loss: 0.5603711595626122, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1120, Loss: 0.3140669990985869, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1121, Loss: 0.3026513980651593, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1122, Loss: 0.31171762895124766, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1123, Loss: 0.2630017470118121, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1124, Loss: 0.24012348579594966, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1125, Loss: 0.31141499885100976, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1126, Loss: 0.30342701191594285, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1127, Loss: 0.5887162728714173, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1128, Loss: 0.2426364684899602, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1129, Loss: 0.2770305337487477, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1130, Loss: 0.2680740875576432, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1131, Loss: 0.5617848843771773, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1132, Loss: 0.2392130180767924, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1133, Loss: 0.38298903741215123, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1134, Loss: 0.3859912926394473, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1135, Loss: 0.29594137623080685, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1136, Loss: 0.25372370454400667, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1137, Loss: 0.7180902571450266, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1138, Loss: 0.33329034677555835, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1139, Loss: 0.27486010197030725, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1140, Loss: 0.317447979017652, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1141, Loss: 0.3852235655842636, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1142, Loss: 0.33538495394702506, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1143, Loss: 0.5691661499691054, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1144, Loss: 0.4099159455911138, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1145, Loss: 0.41920340473728424, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1146, Loss: 0.4441234317492717, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1147, Loss: 0.40067900500585196, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1148, Loss: 0.2682725543845105, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1149, Loss: 0.4528657079004117, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1150, Loss: 0.5457736699909257, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1151, Loss: 0.3950767637217827, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1152, Loss: 0.3376484326062927, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1153, Loss: 0.2621491002223812, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1154, Loss: 0.298590489674821, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1155, Loss: 0.2739800379183723, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1156, Loss: 0.24792364697618327, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1157, Loss: 0.303405017505866, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1158, Loss: 0.42430134511300555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1159, Loss: 0.3370036169973748, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1160, Loss: 0.3688743553818098, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1161, Loss: 0.44184684917511075, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1162, Loss: 0.33845810504733165, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1163, Loss: 0.3647060443292977, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1164, Loss: 0.33550303183708896, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1165, Loss: 0.29172031189944914, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1166, Loss: 0.2595877091121858, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1167, Loss: 0.27935530995730395, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1168, Loss: 0.26286349511481166, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1169, Loss: 0.300811652644317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1170, Loss: 0.2869114292877688, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1171, Loss: 0.58119371024329, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1172, Loss: 0.2568715252087899, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1173, Loss: 0.43719388652302393, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1174, Loss: 0.3401925635630787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1175, Loss: 0.3672515164840599, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1176, Loss: 0.3778723452418906, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1177, Loss: 0.257277391771667, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1178, Loss: 0.2740566544530222, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1179, Loss: 0.34495377089259915, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1180, Loss: 0.4588863898051002, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1181, Loss: 0.2733819673932737, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1182, Loss: 0.6113295644519666, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1183, Loss: 0.28886380837760617, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1184, Loss: 0.4504970153843616, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1185, Loss: 0.2682309738074178, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1186, Loss: 0.2391988416630732, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1187, Loss: 0.27834600469566984, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1188, Loss: 0.5438173083380122, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1189, Loss: 0.23954257312663857, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1190, Loss: 0.2757043214316204, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1191, Loss: 0.2940644758391412, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1192, Loss: 0.2747216949382402, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1193, Loss: 0.4757149999225725, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1194, Loss: 0.33886014021444816, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1195, Loss: 0.4342845258368927, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1196, Loss: 0.28781435304902, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1197, Loss: 0.25428617540864185, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1198, Loss: 0.3618374313823193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1199, Loss: 0.3421188555994874, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1200, Loss: 0.2498660186276868, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1201, Loss: 0.28590827887621384, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1202, Loss: 0.3971490663280791, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1203, Loss: 0.4613351397447125, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1204, Loss: 0.3222002234342193, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1205, Loss: 0.257030257511334, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1206, Loss: 0.2722832614462926, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1207, Loss: 0.3310596280890913, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1208, Loss: 0.27590837072964425, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1209, Loss: 0.23969860153562267, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1210, Loss: 0.2552071080376878, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1211, Loss: 0.4020069499046305, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1212, Loss: 0.2835555843222265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1213, Loss: 0.41030504799702017, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1214, Loss: 0.24029182937790233, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1215, Loss: 0.4465922454219391, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1216, Loss: 0.261862540945485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1217, Loss: 0.27807085672484455, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1218, Loss: 0.6683859751900686, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1219, Loss: 0.25416828972247346, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1220, Loss: 0.3126215319277152, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1221, Loss: 0.48281128120048045, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1222, Loss: 0.6304097598448865, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1223, Loss: 0.30695664438900194, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1224, Loss: 0.2896069453008152, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1225, Loss: 0.24896696752343317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1226, Loss: 0.3018037795848265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1227, Loss: 0.3322592828897627, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1228, Loss: 0.2602602756560362, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1229, Loss: 0.49835631429034366, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1230, Loss: 0.25166201277628697, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1231, Loss: 0.4416369759493355, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1232, Loss: 0.2747665892652833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1233, Loss: 0.26495458976243086, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1234, Loss: 0.2959732919189679, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1235, Loss: 0.33355314958018456, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1236, Loss: 0.41803836227065616, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1237, Loss: 0.23715652952045238, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1238, Loss: 0.3183972340559162, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1239, Loss: 0.39451448941767664, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1240, Loss: 0.32194896395506645, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1241, Loss: 0.25884049023956, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1242, Loss: 0.4218287380172663, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1243, Loss: 0.4733850773969859, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1244, Loss: 0.22709156501523667, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1245, Loss: 0.33018080032481056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1246, Loss: 0.2530790766076833, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1247, Loss: 0.28093595701200363, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1248, Loss: 0.3886209039239261, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1249, Loss: 0.2463046160998576, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1250, Loss: 0.4281163216848661, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1251, Loss: 0.4384690581402311, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1252, Loss: 0.31382091777170734, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1253, Loss: 0.3619748938285292, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1254, Loss: 0.2650866277519049, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1255, Loss: 0.2389648789491286, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1256, Loss: 0.6172737879172904, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1257, Loss: 0.3292644132659689, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1258, Loss: 0.2711266458918637, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1259, Loss: 0.43995463197147544, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1260, Loss: 0.25030150302987725, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1261, Loss: 0.2735968719629364, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1262, Loss: 0.35404359502229177, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1263, Loss: 0.3579526679582904, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1264, Loss: 0.23802507355539046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1265, Loss: 0.2921618333541574, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1266, Loss: 0.2789581152043295, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1267, Loss: 0.29183684292309, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1268, Loss: 0.37709149174954437, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1269, Loss: 0.3102900569665261, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1270, Loss: 0.31494672648089617, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1271, Loss: 0.3710559412569774, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1272, Loss: 0.3435381078140483, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1273, Loss: 0.28756828512913196, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1274, Loss: 0.38410928048555315, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1275, Loss: 0.2545293315588152, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1276, Loss: 0.3232338686598462, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1277, Loss: 0.29045575966598536, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1278, Loss: 0.4438533423365157, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1279, Loss: 0.2661685941063765, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1280, Loss: 0.37748533128620854, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1281, Loss: 0.2835522914698622, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1282, Loss: 0.3414312683924914, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1283, Loss: 0.23410655728623625, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1284, Loss: 0.4475248376090122, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1285, Loss: 0.23964120212571816, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1286, Loss: 0.3077387826539675, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1287, Loss: 0.361808290837399, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1288, Loss: 0.4279793218667439, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1289, Loss: 0.23090893944381968, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1290, Loss: 0.4645161458197189, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1291, Loss: 0.45262157922185053, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1292, Loss: 0.4130904131354528, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1293, Loss: 0.47195597806962686, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1294, Loss: 0.3028472140048157, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1295, Loss: 0.3127838138946928, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1296, Loss: 0.3228710269104629, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1297, Loss: 0.26777721293622025, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1298, Loss: 0.4308827943601782, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1299, Loss: 0.4213982184721638, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1300, Loss: 0.32326426579483525, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1301, Loss: 0.3507133585180259, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1302, Loss: 0.33572348476047403, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1303, Loss: 0.3033382627289434, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1304, Loss: 0.23769508403391615, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1305, Loss: 0.3829940892422371, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1306, Loss: 0.3454206781708611, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1307, Loss: 0.44099614598516346, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1308, Loss: 0.3654919495267649, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1309, Loss: 0.41823795061179947, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1310, Loss: 0.2611433612740153, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1311, Loss: 0.279487297624953, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1312, Loss: 0.4381137057832168, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1313, Loss: 0.28303423906550246, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1314, Loss: 0.4341298185234964, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1315, Loss: 0.27253507106668684, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1316, Loss: 0.3983497936596162, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1317, Loss: 0.25452801377965134, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1318, Loss: 0.24016231703344199, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1319, Loss: 0.2818547405858015, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1320, Loss: 0.2786712607099822, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1321, Loss: 0.26491229315492715, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1322, Loss: 0.5441173960430312, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1323, Loss: 0.3989718015641089, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1324, Loss: 0.539973421133654, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1325, Loss: 0.325837109205994, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1326, Loss: 0.33411597975419155, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1327, Loss: 0.4634912955204101, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1328, Loss: 0.38115716643272324, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1329, Loss: 0.3972147201310251, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1330, Loss: 0.38765389748683804, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1331, Loss: 0.27330408492233715, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1332, Loss: 0.29058775455565267, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1333, Loss: 0.32168588721092406, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1334, Loss: 0.3308929970600427, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1335, Loss: 0.2889024969515943, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1336, Loss: 0.6427454506324656, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1337, Loss: 0.23306166879573317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1338, Loss: 0.2848646591145225, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1339, Loss: 0.565501653759522, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1340, Loss: 0.4518729986449297, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1341, Loss: 0.3355640002270487, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1342, Loss: 0.34823853729174725, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1343, Loss: 0.23305225416444994, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1344, Loss: 0.2623272439580523, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1345, Loss: 0.2557662130856957, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1346, Loss: 0.27055395586710185, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1347, Loss: 0.6424226057156719, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1348, Loss: 0.412507527614743, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1349, Loss: 0.46884743745173973, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1350, Loss: 0.24721035364937652, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1351, Loss: 0.2575889419953642, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1352, Loss: 0.5499979226635197, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1353, Loss: 0.36351839608702585, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1354, Loss: 0.373841449045493, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1355, Loss: 0.37261378595378236, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1356, Loss: 0.3438770992954092, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1357, Loss: 0.26861613361664355, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1358, Loss: 0.3200005645800907, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1359, Loss: 0.24511295715419662, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1360, Loss: 0.31827854642598874, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1361, Loss: 0.3104791827460226, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1362, Loss: 0.32217491305779566, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1363, Loss: 0.5558423522719764, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1364, Loss: 0.27341938340990046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1365, Loss: 0.2879233181841945, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1366, Loss: 0.2566706761647982, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1367, Loss: 0.2951400592662046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1368, Loss: 0.3794782151463247, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1369, Loss: 0.32612591493327836, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1370, Loss: 0.31029391028694986, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1371, Loss: 0.4842718857563616, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1372, Loss: 0.44521142782997347, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1373, Loss: 0.5076611797046284, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1374, Loss: 0.6199283508787268, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1375, Loss: 0.24804372490022475, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1376, Loss: 0.30673936194935636, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1377, Loss: 0.6414908607973595, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1378, Loss: 0.3487740888125381, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1379, Loss: 0.23197982485774443, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1380, Loss: 0.6727716164008215, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1381, Loss: 0.4729117268742787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1382, Loss: 0.3086366127708071, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1383, Loss: 0.4218976837349787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1384, Loss: 0.27350453551599857, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1385, Loss: 0.26435076472364016, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1386, Loss: 0.3529502054677664, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1387, Loss: 0.24855188098287462, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1388, Loss: 0.34431834486468693, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1389, Loss: 0.4602228425720194, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1390, Loss: 0.6168217183173155, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1391, Loss: 0.413142054685505, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1392, Loss: 0.33176451636598103, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1393, Loss: 0.26708216402210544, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1394, Loss: 0.4044493505601549, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1395, Loss: 0.31452163357201274, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1396, Loss: 0.24905908433675383, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1397, Loss: 0.3323610236559992, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1398, Loss: 0.3558396457530433, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1399, Loss: 0.46218503198676764, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1400, Loss: 0.33841573475170905, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1401, Loss: 0.27079395406570433, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1402, Loss: 0.23894793949825674, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1403, Loss: 0.24354677835832997, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1404, Loss: 0.2717570147375579, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1405, Loss: 0.28126799225444066, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1406, Loss: 0.3145427549375502, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1407, Loss: 0.30116821847122965, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1408, Loss: 0.31800925003465097, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1409, Loss: 0.22745628179169458, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1410, Loss: 0.3104424429269702, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1411, Loss: 0.3421372256271191, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1412, Loss: 0.4703505017656783, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1413, Loss: 0.45848737540230006, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1414, Loss: 0.25334466325832156, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1415, Loss: 0.27919917263386335, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1416, Loss: 0.33707726949271294, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1417, Loss: 0.3039306662099496, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1418, Loss: 0.2960595277711088, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1419, Loss: 0.3062597286671819, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1420, Loss: 0.33471831040709527, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1421, Loss: 0.23351175550374362, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1422, Loss: 0.2659610809464244, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1423, Loss: 0.3588500703590399, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1424, Loss: 0.4307187627288802, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1425, Loss: 0.4285304821145263, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1426, Loss: 0.24032292968095464, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1427, Loss: 0.333365372521808, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1428, Loss: 0.6962824917641388, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1429, Loss: 0.3082100478019968, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1430, Loss: 0.5496176197285573, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1431, Loss: 0.3073630564514, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1432, Loss: 0.38599011053588494, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1433, Loss: 0.3102744128448819, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1434, Loss: 0.2647088166589315, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1435, Loss: 0.27244751316916593, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1436, Loss: 0.3468857341116912, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1437, Loss: 0.3924476857120553, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1438, Loss: 0.27592722739915787, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1439, Loss: 0.2385771042574625, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1440, Loss: 0.2856167839374608, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1441, Loss: 0.47251168726625825, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1442, Loss: 0.2962381910733226, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1443, Loss: 0.28273772026679495, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1444, Loss: 0.34445411800707826, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1445, Loss: 0.3536152811221752, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1446, Loss: 0.2443012605064483, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1447, Loss: 0.226716570538021, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1448, Loss: 0.2541749188305496, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1449, Loss: 0.24317511977831766, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1450, Loss: 0.32142460308359994, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1451, Loss: 0.24443203840891034, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1452, Loss: 0.2853483421178509, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1453, Loss: 0.23706867450761265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1454, Loss: 0.2575079822377612, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1455, Loss: 0.5057680814912618, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1456, Loss: 0.24220818503318625, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1457, Loss: 0.3081723507456983, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1458, Loss: 0.31195709532554006, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1459, Loss: 0.43033411125083987, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1460, Loss: 0.2431950943991735, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1461, Loss: 0.4433882466458974, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1462, Loss: 0.48519769543803926, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1463, Loss: 0.25795609968375144, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1464, Loss: 0.4588274413479081, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1465, Loss: 0.2412628168547428, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1466, Loss: 0.2899812696342813, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1467, Loss: 0.3885713004327832, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1468, Loss: 0.35858302430009503, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1469, Loss: 0.3523884963545248, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1470, Loss: 0.4478543865208584, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1471, Loss: 0.30686692162253354, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1472, Loss: 0.30163578079900766, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1473, Loss: 0.2505946649407825, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1474, Loss: 0.3109895165175482, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1475, Loss: 0.3807745281853555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1476, Loss: 0.39730211230375, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1477, Loss: 0.44145083700727356, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1478, Loss: 0.36886790851114304, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1479, Loss: 0.2814769980257133, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1480, Loss: 0.4964690910069905, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1481, Loss: 0.4135931130957035, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1482, Loss: 0.25019106894650345, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1483, Loss: 0.3944342642892866, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1484, Loss: 0.43021602517103746, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1485, Loss: 0.3786823509699244, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1486, Loss: 0.2614484386442585, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1487, Loss: 0.38757985356572944, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1488, Loss: 0.25113653750714243, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1489, Loss: 0.2554423719763794, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1490, Loss: 0.23367078523846072, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1491, Loss: 0.27401086107276584, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1492, Loss: 0.3092192223965497, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1493, Loss: 0.35270296896705, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1494, Loss: 0.2643665569943725, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1495, Loss: 0.2904134753227793, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1496, Loss: 0.2758970652729479, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1497, Loss: 0.3438278756230945, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1498, Loss: 0.2672696926583465, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1499, Loss: 0.3927629426377762, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1500, Loss: 0.46705362496833297, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1501, Loss: 0.4213562207684335, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1502, Loss: 0.4280239742500441, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1503, Loss: 0.25111944324664615, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1504, Loss: 0.2934394438194369, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1505, Loss: 0.28700165057062965, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1506, Loss: 0.27094577873461534, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1507, Loss: 0.23053139948120155, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1508, Loss: 0.3905106222008659, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1509, Loss: 0.3867305649756778, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1510, Loss: 0.2990698201632034, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1511, Loss: 0.30968106842316423, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1512, Loss: 0.8403715521697248, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1513, Loss: 0.6143619334864466, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1514, Loss: 0.31889700100189194, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1515, Loss: 0.37056096112720543, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1516, Loss: 0.34030712279457265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1517, Loss: 0.3591893427380155, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1518, Loss: 0.3212684858186046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1519, Loss: 0.278990811871222, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1520, Loss: 0.3458771965869093, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1521, Loss: 0.4066838405224641, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1522, Loss: 0.3347808604586044, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1523, Loss: 0.30633051204767275, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1524, Loss: 0.27706477133667556, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1525, Loss: 0.3976740737311798, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1526, Loss: 0.2927885957313656, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1527, Loss: 0.3890954114875054, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1528, Loss: 0.34079399675834243, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1529, Loss: 0.3207729768720575, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1530, Loss: 0.28778877395879066, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1531, Loss: 0.26589049855802527, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1532, Loss: 0.3647511305331234, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1533, Loss: 0.4382292361458703, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1534, Loss: 0.30850160666295023, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1535, Loss: 0.2548847092161697, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1536, Loss: 0.3199612823491663, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1537, Loss: 0.43613417296034185, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1538, Loss: 0.2775226090732802, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1539, Loss: 0.23386398026716268, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1540, Loss: 0.3089394288104933, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1541, Loss: 0.2821528822687107, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1542, Loss: 0.26277950823885315, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1543, Loss: 0.3108662064538129, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1544, Loss: 0.3209995581542594, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1545, Loss: 0.48084965663158524, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1546, Loss: 0.24566660886478314, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1547, Loss: 0.39295853193210456, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1548, Loss: 0.28651116868653537, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1549, Loss: 0.3334038218543765, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1550, Loss: 0.2614175635904599, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1551, Loss: 0.6272379147472219, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1552, Loss: 0.27894498135978174, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1553, Loss: 0.3526773177200779, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1554, Loss: 0.4110886552060257, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1555, Loss: 0.3544754322789636, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1556, Loss: 0.28943277579142734, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1557, Loss: 0.31839223492149366, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1558, Loss: 0.324564886586686, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1559, Loss: 0.29495810029249686, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1560, Loss: 0.4248942795932006, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1561, Loss: 0.2662794062490336, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1562, Loss: 0.6034351835200314, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1563, Loss: 0.27437273785112637, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1564, Loss: 0.32552447736738954, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1565, Loss: 0.2634885599445411, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1566, Loss: 0.2743749479067449, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1567, Loss: 0.354601173375257, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1568, Loss: 0.24431686971081326, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1569, Loss: 0.5556503394217798, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1570, Loss: 0.4057604517016171, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1571, Loss: 0.28653181877016143, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1572, Loss: 0.3023771939555484, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1573, Loss: 0.30270786182608056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1574, Loss: 0.2627231547702836, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1575, Loss: 0.47683085893167443, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1576, Loss: 0.4626678028747613, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1577, Loss: 0.22283816993941472, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1578, Loss: 0.32881601347642686, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1579, Loss: 0.7588027054581006, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1580, Loss: 0.4623324413958287, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1581, Loss: 0.37090435542861266, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1582, Loss: 0.489423094852405, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1583, Loss: 0.43758950243285855, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1584, Loss: 0.42290852250556654, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1585, Loss: 0.2525401509846877, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1586, Loss: 0.31659457277504766, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1587, Loss: 0.26528926814287246, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1588, Loss: 0.30357536184936784, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1589, Loss: 0.37631024727199225, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1590, Loss: 0.27728585876571216, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1591, Loss: 0.24658150476521457, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1592, Loss: 0.26526465515204706, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1593, Loss: 0.23879144361607485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1594, Loss: 0.27109435947507804, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1595, Loss: 0.2676501497064627, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1596, Loss: 0.23706941273784718, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1597, Loss: 0.4143269574409401, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1598, Loss: 0.35925646202290995, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1599, Loss: 0.32308779899375445, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1600, Loss: 0.3461838139948988, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1601, Loss: 0.643601335962666, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1602, Loss: 0.38640041396934216, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1603, Loss: 0.4094308053671717, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1604, Loss: 0.45471523129831326, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1605, Loss: 0.32998978910886756, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1606, Loss: 0.34488692292164785, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1607, Loss: 0.23290265366563764, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1608, Loss: 0.4009389786123265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1609, Loss: 0.25723643107306304, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1610, Loss: 0.26930404507457995, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1611, Loss: 0.3825253987451212, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1612, Loss: 0.37600883793712747, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1613, Loss: 0.29665867776827026, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1614, Loss: 0.3811496514040605, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1615, Loss: 0.3208880343654091, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1616, Loss: 0.5779795457328774, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1617, Loss: 0.28316684677565657, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1618, Loss: 0.3482443206555156, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1619, Loss: 0.24090776230318067, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1620, Loss: 0.49056632675279793, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1621, Loss: 0.2853871454855437, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1622, Loss: 0.24665319151648982, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1623, Loss: 0.2426288209120557, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1624, Loss: 0.42627878217702786, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1625, Loss: 0.3744204397989843, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1626, Loss: 0.2376736656834576, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1627, Loss: 0.3236945967100072, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1628, Loss: 0.3069911057780531, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1629, Loss: 0.24091988834998457, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1630, Loss: 0.2849284443186743, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1631, Loss: 0.42911690659161195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1632, Loss: 0.33701370507041395, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1633, Loss: 0.26234484663581814, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1634, Loss: 0.41462766990139144, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1635, Loss: 0.4355158054553372, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1636, Loss: 0.27033149160519365, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1637, Loss: 0.3104277953908025, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1638, Loss: 0.2580238549912877, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1639, Loss: 0.33423999234581137, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1640, Loss: 0.2824613733010779, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1641, Loss: 0.3442499646392734, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1642, Loss: 0.2747542195200465, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1643, Loss: 0.3014205981920487, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1644, Loss: 0.42049485140281395, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1645, Loss: 0.37286058627000873, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1646, Loss: 0.37433632281850315, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1647, Loss: 0.2693579343474938, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1648, Loss: 0.2715304519956001, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1649, Loss: 0.23510294812233426, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1650, Loss: 0.5564402869194534, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1651, Loss: 0.47386324769141497, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1652, Loss: 0.29042635887181717, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1653, Loss: 0.28274215902369154, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1654, Loss: 0.3542576899983112, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1655, Loss: 0.44181727342648797, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1656, Loss: 0.3890629667784574, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1657, Loss: 0.3367446134348643, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1658, Loss: 0.36554256874674573, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1659, Loss: 0.27730693588944605, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1660, Loss: 0.3234977334107383, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1661, Loss: 0.2791709424961396, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1662, Loss: 0.3045237628389018, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1663, Loss: 0.3868387833988619, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1664, Loss: 0.2700813783861454, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1665, Loss: 0.2611122470591133, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1666, Loss: 0.4475312703483191, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1667, Loss: 0.2885907261868195, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1668, Loss: 0.5073488942072364, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1669, Loss: 0.3277482428475119, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1670, Loss: 0.2587229291842621, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1671, Loss: 0.39931241361287617, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1672, Loss: 0.48714459377424585, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1673, Loss: 0.31884670432681844, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1674, Loss: 0.3965476454187977, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1675, Loss: 0.3381413413494874, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1676, Loss: 0.4546962657893717, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1677, Loss: 0.49730400922022777, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1678, Loss: 0.5020947743772141, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1679, Loss: 0.24335398884300555, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1680, Loss: 0.3137971423818476, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1681, Loss: 0.3701490778019042, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1682, Loss: 0.23891343380239585, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1683, Loss: 0.28223829437369835, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1684, Loss: 0.29868093088556336, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1685, Loss: 0.336016691426274, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1686, Loss: 0.9333976206105143, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1687, Loss: 0.3050732122432341, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1688, Loss: 0.25645225310553277, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1689, Loss: 0.4957006344542576, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1690, Loss: 0.2961884006858362, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1691, Loss: 0.3443182586954428, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1692, Loss: 0.3326458871621956, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1693, Loss: 0.38861801681116753, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1694, Loss: 0.36946542120244397, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1695, Loss: 0.2580951764855608, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1696, Loss: 0.34430883940329865, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1697, Loss: 0.47752793599450827, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1698, Loss: 0.27688415429297303, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1699, Loss: 0.2855192651828924, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1700, Loss: 0.29254710173711623, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1701, Loss: 0.29143506068553726, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1702, Loss: 0.31527648175985856, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1703, Loss: 0.3087369187286104, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1704, Loss: 0.43977007562064946, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1705, Loss: 0.30920580296017264, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1706, Loss: 0.3418427729396991, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1707, Loss: 0.35542798166603773, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1708, Loss: 0.3790477641662824, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1709, Loss: 0.22779823681411954, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1710, Loss: 0.25930660575061215, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1711, Loss: 0.30044736877207046, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1712, Loss: 0.4340090501720518, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1713, Loss: 0.2969085326202928, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1714, Loss: 0.2785354060628972, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1715, Loss: 0.37817833667218803, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1716, Loss: 0.39047318217939553, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1717, Loss: 0.3144877798882561, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1718, Loss: 0.4359615665002884, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1719, Loss: 0.24608348568127852, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1720, Loss: 0.36217515632613273, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1721, Loss: 0.3347223342080993, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1722, Loss: 0.335168487965309, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1723, Loss: 0.3389693711498998, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1724, Loss: 0.23385636204154442, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1725, Loss: 0.3126724194557975, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1726, Loss: 0.2788410792626749, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1727, Loss: 0.37042181831037485, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1728, Loss: 0.3184881093384322, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1729, Loss: 0.3381417572546773, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1730, Loss: 0.35224095398396116, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1731, Loss: 0.28375438673122394, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1732, Loss: 0.32090486271491475, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1733, Loss: 0.6111640998472041, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1734, Loss: 0.28736275775996517, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1735, Loss: 0.38271665814036127, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1736, Loss: 0.3843384482350397, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1737, Loss: 0.30272587546891566, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1738, Loss: 0.2425216299055284, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1739, Loss: 0.3023796926056581, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1740, Loss: 0.33620628038829503, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1741, Loss: 0.25994828684070254, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1742, Loss: 0.25131329517320633, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1743, Loss: 0.31863216057695004, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1744, Loss: 0.48823511757963023, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1745, Loss: 0.354454466007062, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1746, Loss: 0.2600523334886322, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1747, Loss: 0.2869079932067138, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1748, Loss: 0.4777694229916356, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1749, Loss: 0.26045897024135756, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1750, Loss: 0.5595393926029237, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1751, Loss: 0.2564822276269541, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1752, Loss: 0.32593948001671236, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1753, Loss: 0.27887356270281033, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1754, Loss: 0.34642740395936655, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1755, Loss: 0.2875722776238774, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1756, Loss: 0.28054423254016064, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1757, Loss: 0.39477300914033864, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1758, Loss: 0.28131443734136075, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1759, Loss: 0.35660785480334056, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1760, Loss: 0.3445243836584932, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1761, Loss: 0.24126339153378956, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1762, Loss: 0.6095105140752004, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1763, Loss: 0.4222288654021139, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1764, Loss: 0.2984998591572305, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1765, Loss: 0.3448163792370095, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1766, Loss: 0.25788592740227495, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1767, Loss: 0.23497269774923824, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1768, Loss: 0.34933593879376945, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1769, Loss: 0.38975089897226234, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1770, Loss: 0.2944762585872223, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1771, Loss: 0.3985347003895314, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1772, Loss: 0.29200388589248516, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1773, Loss: 0.29393163275833734, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1774, Loss: 0.43481484700589257, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1775, Loss: 0.34585854826235984, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1776, Loss: 0.2904188330144435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1777, Loss: 0.28683262561047296, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1778, Loss: 0.33132860517423757, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1779, Loss: 0.3083390191466274, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1780, Loss: 0.23275166132092678, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1781, Loss: 0.3478174385219498, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1782, Loss: 0.4019023696808177, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1783, Loss: 0.42619311808308435, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1784, Loss: 0.329880446385166, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1785, Loss: 0.24866880576989409, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1786, Loss: 0.3471317536558297, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1787, Loss: 0.34085167987731146, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1788, Loss: 0.33130013893140575, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1789, Loss: 0.3478722286510493, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1790, Loss: 0.4912276578891779, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1791, Loss: 0.2628717788887383, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1792, Loss: 0.27628751073128105, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1793, Loss: 0.4384862699898081, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1794, Loss: 0.22799639369765826, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1795, Loss: 0.24448665491054003, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1796, Loss: 0.5935456001489254, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1797, Loss: 0.27831519295933554, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1798, Loss: 0.2742640016342323, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1799, Loss: 0.31228135731414003, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1800, Loss: 0.2966825626154513, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1801, Loss: 0.3389298582443793, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1802, Loss: 0.45456420084627236, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1803, Loss: 0.24766425063574676, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1804, Loss: 0.5323990050290092, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1805, Loss: 0.269130585012853, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1806, Loss: 0.4185898115988682, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1807, Loss: 0.3167829233784638, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1808, Loss: 0.39971917499535325, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1809, Loss: 0.23975946474530255, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1810, Loss: 0.3251292973486156, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1811, Loss: 0.317774554800329, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1812, Loss: 0.33509493827777265, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1813, Loss: 0.26591052310120294, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1814, Loss: 0.28775361437058355, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1815, Loss: 0.27438788040048906, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1816, Loss: 0.28345122626666325, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1817, Loss: 0.2825844098544425, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1818, Loss: 0.4170177471094644, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1819, Loss: 0.2531583254258626, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1820, Loss: 0.2956760335804572, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1821, Loss: 0.2960295462837661, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1822, Loss: 0.27843818618047284, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1823, Loss: 0.2892649992826691, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1824, Loss: 0.2403403894361801, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1825, Loss: 0.2775453224968863, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1826, Loss: 0.6832409654446204, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1827, Loss: 0.26707413290249565, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1828, Loss: 0.34916485294574984, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1829, Loss: 0.5545384740202977, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1830, Loss: 0.2915543050054654, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1831, Loss: 0.3022540906735061, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1832, Loss: 0.34999903572390945, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1833, Loss: 0.24009434096088317, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1834, Loss: 0.2720068664140666, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1835, Loss: 0.4424919629340996, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1836, Loss: 0.23913067090899748, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1837, Loss: 0.28675623010371964, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1838, Loss: 0.30808645161300674, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1839, Loss: 0.25824374032713576, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1840, Loss: 0.4373143643171857, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1841, Loss: 0.333738929693315, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1842, Loss: 0.2866586633916864, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1843, Loss: 0.2690886467044292, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1844, Loss: 0.24780466173008053, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1845, Loss: 0.3179992188439559, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1846, Loss: 0.2878684001195796, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1847, Loss: 0.2997593323133959, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1848, Loss: 0.26784855254655837, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1849, Loss: 0.3292534503039341, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1850, Loss: 0.2567778789200604, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1851, Loss: 0.28790068397529767, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1852, Loss: 0.7534573343988182, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1853, Loss: 0.23668428168427919, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1854, Loss: 0.2561484514566874, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1855, Loss: 0.3476515833726763, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1856, Loss: 0.33824882784415083, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1857, Loss: 0.3519405490230489, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1858, Loss: 0.4016378058610092, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1859, Loss: 0.5551198950440434, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1860, Loss: 0.5084860861763545, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1861, Loss: 0.38876470449640954, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1862, Loss: 0.23725034820213647, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1863, Loss: 0.4593552759831633, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1864, Loss: 0.31651487635895614, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1865, Loss: 0.3590732452601516, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1866, Loss: 0.2744349884984571, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1867, Loss: 0.33377195293670736, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1868, Loss: 0.24944523821199502, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1869, Loss: 0.45354836464022685, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1870, Loss: 0.26469356151212575, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1871, Loss: 0.30539839506186445, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1872, Loss: 0.4018802564020158, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1873, Loss: 0.27320557315758864, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1874, Loss: 0.28401234775332035, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Batch 1875, Loss: 0.518886924069798, Batch Size: 32, Learning Rate: 5.906232130221677e-05\n",
      "Epoch 12, Updated Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 12, Average Loss: 0.3441497283342801, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1, Loss: 0.5719960061558697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 2, Loss: 0.3167976431451549, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 3, Loss: 0.2681923755874419, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 4, Loss: 0.37155621474958156, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 5, Loss: 0.2809084604011351, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 6, Loss: 0.3259957478083403, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 7, Loss: 0.2536858997762473, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 8, Loss: 0.5015634895151357, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 9, Loss: 0.22870795212779657, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 10, Loss: 0.5030279223454475, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 11, Loss: 0.23634038687086714, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 12, Loss: 0.6683203482723261, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 13, Loss: 0.5124905081381148, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 14, Loss: 0.32873763935698835, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 15, Loss: 0.2600396741203427, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 16, Loss: 0.5162418074570432, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 17, Loss: 0.2957742327949815, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 18, Loss: 0.3955458828017149, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 19, Loss: 0.274572658878339, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 20, Loss: 0.26856387633291107, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 21, Loss: 0.410691427787305, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 22, Loss: 0.27329049177820486, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 23, Loss: 0.3303180774162964, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 24, Loss: 0.6031254077991466, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 25, Loss: 0.3547652041875663, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 26, Loss: 0.24607557501955216, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 27, Loss: 0.28876195318837483, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 28, Loss: 0.26257606963340185, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 29, Loss: 0.41465514506725626, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 30, Loss: 0.2712607043202194, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 31, Loss: 0.26665536184774363, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 32, Loss: 0.23860937008869038, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 33, Loss: 0.3274713439890411, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 34, Loss: 0.2397650740193787, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 35, Loss: 0.37704343770457915, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 36, Loss: 0.3499259755582584, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 37, Loss: 0.30913679071607636, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 38, Loss: 0.31205087858326985, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 39, Loss: 0.3179397613738882, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 40, Loss: 0.42715562039258226, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 41, Loss: 0.23058685577011223, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 42, Loss: 0.23596570192564337, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 43, Loss: 0.43924303929108127, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 44, Loss: 0.3602412185834886, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 45, Loss: 0.3219063712466366, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 46, Loss: 0.3526975857780259, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 47, Loss: 0.25747349386260115, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 48, Loss: 0.25429025678876177, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 49, Loss: 0.8894814518176665, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 50, Loss: 0.29786340264256184, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 51, Loss: 0.36386979556545107, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 52, Loss: 0.2970904045617496, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 53, Loss: 0.284893405017356, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 54, Loss: 0.35863515387776174, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 55, Loss: 0.34219699520491575, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 56, Loss: 0.3427245517865928, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 57, Loss: 0.310197568107518, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 58, Loss: 0.33296739931376873, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 59, Loss: 0.3083384506368996, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 60, Loss: 0.38069856019237003, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 61, Loss: 0.3163809153509248, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 62, Loss: 0.24967902234464812, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 63, Loss: 0.36125246422219043, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 64, Loss: 0.3946605427911696, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 65, Loss: 0.2832138058516852, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 66, Loss: 0.4045333475323976, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 67, Loss: 0.4111245255817484, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 68, Loss: 0.34299602135382123, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 69, Loss: 0.43232742982243577, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 70, Loss: 0.538198619763746, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 71, Loss: 0.5891289752507354, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 72, Loss: 0.23906049950977654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 73, Loss: 0.2883847714266128, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 74, Loss: 0.43487998065184463, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 75, Loss: 0.5722292636078314, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 76, Loss: 0.2577603323098439, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 77, Loss: 0.24717332273210166, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 78, Loss: 0.34589767913451014, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 79, Loss: 0.3751632827459873, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 80, Loss: 0.40432239865407893, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 81, Loss: 0.3050417844121302, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 82, Loss: 0.552367131625732, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 83, Loss: 0.23673791627957025, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 84, Loss: 0.32630028773916, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 85, Loss: 0.3049569100007217, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 86, Loss: 0.3122701417432513, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 87, Loss: 0.3341405484254313, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 88, Loss: 0.2639050033790043, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 89, Loss: 0.3247412705531064, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 90, Loss: 0.23936884909727354, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 91, Loss: 0.2767468373424616, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 92, Loss: 0.25367143919371915, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 93, Loss: 0.2876780031407172, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 94, Loss: 0.2657119470415015, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 95, Loss: 0.5260590693537421, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 96, Loss: 0.31333286051116127, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 97, Loss: 0.24239724151224057, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 98, Loss: 0.3090700952296914, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 99, Loss: 0.2609001065079549, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 100, Loss: 0.29922484465890903, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 101, Loss: 0.32095988691717725, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 102, Loss: 0.2672999728581062, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 103, Loss: 0.3253417115997449, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 104, Loss: 0.25842547361534524, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 105, Loss: 0.3197816484299149, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 106, Loss: 0.2878598046235391, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 107, Loss: 0.27824917956006745, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 108, Loss: 0.23769035819408993, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 109, Loss: 0.3495230038703385, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 110, Loss: 0.2709084876224857, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 111, Loss: 0.3460634913219357, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 112, Loss: 0.3939643719449206, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 113, Loss: 0.31514553401433076, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 114, Loss: 0.35106213837963607, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 115, Loss: 0.27006622875130776, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 116, Loss: 0.29814249569788176, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 117, Loss: 0.5369517779021379, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 118, Loss: 0.28486156480845465, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 119, Loss: 0.4793586857039208, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 120, Loss: 0.3933965362361619, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 121, Loss: 0.3524960482137209, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 122, Loss: 0.27693552118311043, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 123, Loss: 0.4749517774797346, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 124, Loss: 0.3300570570067438, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 125, Loss: 0.3408203560502493, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 126, Loss: 0.36461468231905814, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 127, Loss: 0.2928081902368156, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 128, Loss: 0.29409941214062973, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 129, Loss: 0.4429956959211466, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 130, Loss: 0.4089488041822422, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 131, Loss: 0.4479733312962443, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 132, Loss: 0.3408914318543904, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 133, Loss: 0.3371683486999236, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 134, Loss: 0.3014009255619097, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 135, Loss: 0.34514987352436133, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 136, Loss: 0.30622755489039416, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 137, Loss: 0.24547099140894726, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 138, Loss: 0.3335083129521971, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 139, Loss: 0.2626903832677734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 140, Loss: 0.3594483470145716, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 141, Loss: 0.2519795437725598, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 142, Loss: 0.26042326375453656, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 143, Loss: 0.49556655739078426, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 144, Loss: 0.33057912635281195, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 145, Loss: 0.29189641942449684, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 146, Loss: 0.26716421031846405, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 147, Loss: 0.46467413764835686, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 148, Loss: 0.24057594962739448, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 149, Loss: 0.25455907386146304, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 150, Loss: 0.24224311648921068, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 151, Loss: 0.3888488344830237, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 152, Loss: 0.29385516312640547, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 153, Loss: 0.25538612367452856, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 154, Loss: 0.4162676256741704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 155, Loss: 0.24120025766521153, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 156, Loss: 0.34232900456863286, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 157, Loss: 0.2430887270483169, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 158, Loss: 0.23693666377446945, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 159, Loss: 0.23872448224439322, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 160, Loss: 0.41779970696336244, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 161, Loss: 0.245790651591129, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 162, Loss: 0.37022202224679057, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 163, Loss: 0.2600302974773405, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 164, Loss: 0.407090585955155, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 165, Loss: 0.2746793589532265, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 166, Loss: 0.33264868120196933, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 167, Loss: 0.32744468578768793, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 168, Loss: 0.28846524560904935, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 169, Loss: 0.265236853708608, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 170, Loss: 0.23233302626127422, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 171, Loss: 0.3020339915079115, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 172, Loss: 0.3083897455766671, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 173, Loss: 0.27886545268422763, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 174, Loss: 0.3755748733255394, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 175, Loss: 0.2820279042184533, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 176, Loss: 0.2523611589750806, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 177, Loss: 0.30360334882898227, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 178, Loss: 0.2814795983583181, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 179, Loss: 0.27019311243077826, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 180, Loss: 0.5040242157945376, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 181, Loss: 0.30546499204694944, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 182, Loss: 0.2722554421797485, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 183, Loss: 0.259329849389019, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 184, Loss: 0.2627536732710853, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 185, Loss: 0.324671399596391, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 186, Loss: 0.28815745118454356, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 187, Loss: 0.45269942992326834, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 188, Loss: 0.39998928966783637, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 189, Loss: 0.34807897778998886, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 190, Loss: 0.25065081505866654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 191, Loss: 0.34215641291989785, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 192, Loss: 0.41195203199769503, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 193, Loss: 0.23846661785346193, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 194, Loss: 0.26940636499296733, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 195, Loss: 0.2561975386591095, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 196, Loss: 0.4169627977791872, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 197, Loss: 0.4019114406367148, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 198, Loss: 0.2426094825030754, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 199, Loss: 0.4444976809859456, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 200, Loss: 0.3152359478054324, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 201, Loss: 0.44167951416799967, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 202, Loss: 0.26423059217807604, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 203, Loss: 0.2730081396612324, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 204, Loss: 0.4695687048676822, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 205, Loss: 0.2569709375709591, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 206, Loss: 0.4329525229006617, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 207, Loss: 0.48682600138512205, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 208, Loss: 0.33920327942661144, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 209, Loss: 0.3388803171440369, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 210, Loss: 0.38283057849812224, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 211, Loss: 0.27195554261615545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 212, Loss: 0.34444485731912383, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 213, Loss: 0.4450191308938028, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 214, Loss: 0.41006446330336266, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 215, Loss: 0.2930487308346643, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 216, Loss: 0.3589358710642304, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 217, Loss: 0.2999389361643269, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 218, Loss: 0.3187121892254738, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 219, Loss: 0.26821390936935, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 220, Loss: 0.39387727579219983, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 221, Loss: 0.24700059818030712, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 222, Loss: 0.29009863619569615, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 223, Loss: 0.3127214018882789, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 224, Loss: 0.26464767517323545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 225, Loss: 0.35742624483326996, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 226, Loss: 0.5049639928056882, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 227, Loss: 0.4990164192517992, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 228, Loss: 0.34248203309007985, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 229, Loss: 0.24677759334767496, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 230, Loss: 0.24544142062036906, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 231, Loss: 0.31333947470626844, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 232, Loss: 0.6111656908909713, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 233, Loss: 0.3516661161856692, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 234, Loss: 0.42596086154747526, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 235, Loss: 0.37781531104142757, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 236, Loss: 0.2801648984693206, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 237, Loss: 0.2812072424541623, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 238, Loss: 0.24566793253521793, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 239, Loss: 0.3188149139563668, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 240, Loss: 0.23988231089787548, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 241, Loss: 0.3845100900722388, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 242, Loss: 0.2271121344522735, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 243, Loss: 0.23041288829603276, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 244, Loss: 0.3431689917976463, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 245, Loss: 0.3745203191153095, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 246, Loss: 0.3299797041381166, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 247, Loss: 0.29115873444945206, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 248, Loss: 0.335687089660741, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 249, Loss: 0.4125872304608636, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 250, Loss: 0.33148947759026265, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 251, Loss: 0.3159069848071468, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 252, Loss: 0.45645014166786213, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 253, Loss: 0.22895599867008898, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 254, Loss: 0.22942273060381507, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 255, Loss: 0.2610841647984022, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 256, Loss: 0.28691903378561684, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 257, Loss: 0.4083751310107393, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 258, Loss: 0.34284588031074187, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 259, Loss: 0.27991142174110895, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 260, Loss: 0.3840905819507599, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 261, Loss: 0.27231468672898457, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 262, Loss: 0.33131923130639496, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 263, Loss: 0.2692261914895845, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 264, Loss: 0.32729262604046366, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 265, Loss: 0.24939536556668349, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 266, Loss: 0.2710081638982565, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 267, Loss: 0.3639589160582154, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 268, Loss: 0.3032739602361124, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 269, Loss: 0.38264773719366285, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 270, Loss: 0.36785307418484897, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 271, Loss: 0.3352434137127217, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 272, Loss: 0.29921201824771076, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 273, Loss: 0.26019330629050647, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 274, Loss: 0.396029902141876, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 275, Loss: 0.37293452439773855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 276, Loss: 0.30450186263261586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 277, Loss: 0.23874717737680168, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 278, Loss: 0.3799306548730975, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 279, Loss: 0.30891643761869525, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 280, Loss: 0.24049237403137697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 281, Loss: 0.3813426366819158, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 282, Loss: 0.28032157588123874, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 283, Loss: 0.34852999519311484, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 284, Loss: 0.3991362920427506, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 285, Loss: 0.33988764831515705, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 286, Loss: 0.23601432050395468, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 287, Loss: 0.4361499424432856, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 288, Loss: 0.503203110717232, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 289, Loss: 0.27826906362540377, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 290, Loss: 0.3567424661283346, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 291, Loss: 0.4522777811993903, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 292, Loss: 0.2667034538960814, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 293, Loss: 0.26324275887675025, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 294, Loss: 0.23382087280548697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 295, Loss: 0.2626180162108703, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 296, Loss: 0.34033101635078866, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 297, Loss: 0.2797108502709441, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 298, Loss: 0.27859687164158126, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 299, Loss: 0.34190737315972736, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 300, Loss: 0.4338340684018491, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 301, Loss: 0.38007222075609537, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 302, Loss: 0.28013566763739867, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 303, Loss: 0.39808494972395686, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 304, Loss: 0.2459012170231655, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 305, Loss: 0.2344747558356388, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 306, Loss: 0.30569405592282384, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 307, Loss: 0.29422482731037, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 308, Loss: 0.32253413188056734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 309, Loss: 0.30537034868012003, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 310, Loss: 0.35599475461322405, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 311, Loss: 0.2466759733675497, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 312, Loss: 0.22794443623172006, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 313, Loss: 0.3421290609562708, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 314, Loss: 0.2772397726398424, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 315, Loss: 0.23301487645227023, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 316, Loss: 0.2610042779181541, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 317, Loss: 0.47268853041101877, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 318, Loss: 0.26604549982572034, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 319, Loss: 0.29913952718661646, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 320, Loss: 0.3793598369740936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 321, Loss: 0.48767650294599973, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 322, Loss: 0.30898294478488825, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 323, Loss: 0.36014053630556025, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 324, Loss: 0.29267601059018306, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 325, Loss: 0.27214767279021473, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 326, Loss: 0.2900806938406466, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 327, Loss: 0.2621417447708952, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 328, Loss: 0.2789333704162207, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 329, Loss: 0.35200930291881816, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 330, Loss: 0.35680061564343735, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 331, Loss: 0.40249721593628396, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 332, Loss: 0.3662721620350348, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 333, Loss: 0.2873246171783589, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 334, Loss: 0.259209462116897, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 335, Loss: 0.41716608045314274, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 336, Loss: 0.261449195406369, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 337, Loss: 0.29650492203994355, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 338, Loss: 0.34537367803774793, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 339, Loss: 0.4610301383770875, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 340, Loss: 0.23368220340892876, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 341, Loss: 0.49096719319863313, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 342, Loss: 0.2755281425064985, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 343, Loss: 0.37359280570746667, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 344, Loss: 0.2583415540464936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 345, Loss: 0.2425056594451983, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 346, Loss: 0.32782597425194987, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 347, Loss: 0.39436979207518796, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 348, Loss: 0.3250625299954823, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 349, Loss: 0.26192494200795086, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 350, Loss: 0.33839566065652465, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 351, Loss: 0.3432906722435464, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 352, Loss: 0.29940867954494954, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 353, Loss: 0.5053548782098576, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 354, Loss: 0.2703188520328813, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 355, Loss: 0.25298186432409414, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 356, Loss: 0.429480347884331, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 357, Loss: 0.37984089439193625, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 358, Loss: 0.2737942052682144, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 359, Loss: 0.3867001470501011, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 360, Loss: 0.2668860541088998, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 361, Loss: 0.31979339284498665, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 362, Loss: 0.2609252978062578, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 363, Loss: 0.3437568229628269, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 364, Loss: 0.23420795691209942, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 365, Loss: 0.2775106927575105, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 366, Loss: 0.28675669015455213, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 367, Loss: 0.34768284008585254, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 368, Loss: 0.28252867911159474, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 369, Loss: 0.6496057607718339, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 370, Loss: 0.3392441848558912, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 371, Loss: 0.39829946135815786, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 372, Loss: 0.3846069349941423, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 373, Loss: 0.3929918564533762, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 374, Loss: 0.3561675458604848, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 375, Loss: 0.26691081135623307, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 376, Loss: 0.25745511904601565, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 377, Loss: 0.5930271765603213, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 378, Loss: 0.2535187794873299, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 379, Loss: 0.26023503125057945, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 380, Loss: 0.5093002893934195, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 381, Loss: 0.4091837713615733, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 382, Loss: 0.4329635736176586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 383, Loss: 0.4075170277209772, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 384, Loss: 0.4614478903151243, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 385, Loss: 0.3560270078606704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 386, Loss: 0.2860482475295151, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 387, Loss: 0.3916256041489055, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 388, Loss: 0.32617061505373657, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 389, Loss: 0.3946035634445353, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 390, Loss: 0.2997672508510332, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 391, Loss: 0.34267123882601497, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 392, Loss: 0.4320419445936615, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 393, Loss: 0.33118044970913924, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 394, Loss: 0.3142136423992299, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 395, Loss: 0.2926043211915837, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 396, Loss: 0.4066159807310697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 397, Loss: 0.2931008218117749, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 398, Loss: 0.30983795042826795, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 399, Loss: 0.25739057751439837, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 400, Loss: 0.2765105398766339, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 401, Loss: 0.2800002927347265, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 402, Loss: 0.28384551783273815, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 403, Loss: 0.32625660540994383, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 404, Loss: 0.23529649731054122, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 405, Loss: 0.428134943357336, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 406, Loss: 0.3373411471609836, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 407, Loss: 0.3113747942753418, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 408, Loss: 0.3474024636825087, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 409, Loss: 0.34924861174624755, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 410, Loss: 0.2392950311777284, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 411, Loss: 0.4090782442705163, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 412, Loss: 0.23817268241297906, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 413, Loss: 0.39526532985245255, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 414, Loss: 0.8099448780568381, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 415, Loss: 0.3912470460940338, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 416, Loss: 0.3994468505943287, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 417, Loss: 0.2952879121989522, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 418, Loss: 0.26833951718981874, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 419, Loss: 0.277643083009099, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 420, Loss: 0.38977469212430543, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 421, Loss: 0.3178509369043024, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 422, Loss: 0.4065363712636383, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 423, Loss: 0.2851169870650931, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 424, Loss: 0.3244944218329318, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 425, Loss: 0.28363178194588895, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 426, Loss: 0.3392062013804947, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 427, Loss: 0.27162883878429006, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 428, Loss: 0.3373680872332683, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 429, Loss: 0.2923534119355387, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 430, Loss: 0.3379808993188867, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 431, Loss: 0.2542609352494783, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 432, Loss: 0.46223901285041236, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 433, Loss: 0.27211314468915004, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 434, Loss: 0.2654097111271142, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 435, Loss: 0.2518534554740579, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 436, Loss: 0.3210455355559394, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 437, Loss: 0.27996553564406446, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 438, Loss: 0.34317142416481733, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 439, Loss: 0.32549930257109627, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 440, Loss: 0.29956995869358766, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 441, Loss: 0.25494157980249943, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 442, Loss: 0.24847290244636275, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 443, Loss: 0.33192466182443536, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 444, Loss: 0.31723691626316425, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 445, Loss: 0.2748198049560494, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 446, Loss: 0.3850542656581881, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 447, Loss: 0.4077814864906263, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 448, Loss: 0.2527016195687924, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 449, Loss: 0.2722148590327026, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 450, Loss: 0.5358688995182992, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 451, Loss: 0.23098885016755175, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 452, Loss: 0.32240710141518825, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 453, Loss: 0.2636841946518026, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 454, Loss: 0.35783608561996555, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 455, Loss: 0.23297440887131604, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 456, Loss: 0.30058288914247255, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 457, Loss: 0.3765420907532704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 458, Loss: 0.4116916935746534, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 459, Loss: 0.27655030557956445, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 460, Loss: 0.4122012277774668, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 461, Loss: 0.23464554388483252, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 462, Loss: 0.24682863711463734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 463, Loss: 0.23667340065274448, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 464, Loss: 0.25788722772805583, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 465, Loss: 0.3459754691350452, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 466, Loss: 0.2959831064051816, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 467, Loss: 0.35132704180606134, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 468, Loss: 0.2602409239490883, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 469, Loss: 0.2647207637238112, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 470, Loss: 0.3571342257989838, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 471, Loss: 0.3105409267520536, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 472, Loss: 0.5206061764662517, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 473, Loss: 0.3197616892247068, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 474, Loss: 0.5153748999539062, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 475, Loss: 0.27960759283461695, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 476, Loss: 0.3180590712103969, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 477, Loss: 0.299167807909489, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 478, Loss: 0.3397538498672838, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 479, Loss: 0.38836082856945964, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 480, Loss: 0.38069847051899297, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 481, Loss: 0.39005645469894923, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 482, Loss: 0.24112684175751956, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 483, Loss: 0.344785361235012, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 484, Loss: 0.2373433676750718, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 485, Loss: 0.7102981808345692, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 486, Loss: 0.269506497912262, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 487, Loss: 0.36577785762103565, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 488, Loss: 0.709181399642588, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 489, Loss: 0.3801211672841509, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 490, Loss: 0.2812811578101759, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 491, Loss: 0.34755302919342734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 492, Loss: 0.26282594910317325, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 493, Loss: 0.42399246541635444, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 494, Loss: 0.33068507851474155, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 495, Loss: 0.4795559672328967, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 496, Loss: 0.26361133137147835, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 497, Loss: 0.30586637101815894, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 498, Loss: 0.271820326886256, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 499, Loss: 0.3448119932631669, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 500, Loss: 0.3372464576232403, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 501, Loss: 0.24895596831856948, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 502, Loss: 0.3047984783937319, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 503, Loss: 0.6063484002593162, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 504, Loss: 0.29579175100265115, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 505, Loss: 0.30530242382246187, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 506, Loss: 0.2732257559266019, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 507, Loss: 0.37092748054832936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 508, Loss: 0.384506318052526, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 509, Loss: 0.5331557764536325, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 510, Loss: 0.24521363124653472, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 511, Loss: 0.34163760237916, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 512, Loss: 0.2988443517971083, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 513, Loss: 0.28476752189918997, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 514, Loss: 0.4536093355000772, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 515, Loss: 0.2299475888135927, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 516, Loss: 0.30377475298466694, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 517, Loss: 0.33268551966453336, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 518, Loss: 0.3967841980577357, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 519, Loss: 0.2620913740520276, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 520, Loss: 0.564334820247377, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 521, Loss: 0.43241805012825574, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 522, Loss: 0.33109201559288604, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 523, Loss: 0.2875333264902111, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 524, Loss: 0.3143159135288174, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 525, Loss: 0.33106593465214273, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 526, Loss: 0.29543430984539454, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 527, Loss: 0.5006875659013024, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 528, Loss: 0.2865501119176793, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 529, Loss: 0.3205236329379528, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 530, Loss: 0.29594724471026196, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 531, Loss: 0.47235515885453633, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 532, Loss: 0.24686939748395803, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 533, Loss: 0.3845594897221657, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 534, Loss: 0.46588761395350875, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 535, Loss: 0.30165186878861144, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 536, Loss: 0.4358500787705697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 537, Loss: 0.3213081047575953, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 538, Loss: 0.4490674388831053, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 539, Loss: 0.33736598206413126, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 540, Loss: 0.30012783803045584, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 541, Loss: 0.44738386194051394, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 542, Loss: 0.2639817180372011, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 543, Loss: 0.42109919866611595, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 544, Loss: 0.23473914803615348, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 545, Loss: 0.32536234355353955, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 546, Loss: 0.3363368769648565, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 547, Loss: 0.23580315305794652, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 548, Loss: 0.3109416125040782, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 549, Loss: 0.4476999579496328, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 550, Loss: 0.33391609865075605, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 551, Loss: 0.27891201247879677, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 552, Loss: 0.38143548238278474, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 553, Loss: 0.253128207272485, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 554, Loss: 0.4077380642338333, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 555, Loss: 0.27034464966552757, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 556, Loss: 0.47265508561674385, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 557, Loss: 0.24882939281701855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 558, Loss: 0.37443469818596564, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 559, Loss: 0.2992476929421152, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 560, Loss: 0.3720241515939756, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 561, Loss: 0.27690623511171797, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 562, Loss: 0.28435305186291626, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 563, Loss: 0.4095283112815295, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 564, Loss: 0.24055246624770538, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 565, Loss: 0.3407385716526369, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 566, Loss: 0.40535268335457547, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 567, Loss: 0.32388670222250643, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 568, Loss: 0.3527726443063663, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 569, Loss: 0.33350446668962375, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 570, Loss: 0.29029548036064934, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 571, Loss: 0.31969326510892726, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 572, Loss: 0.27706983160389786, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 573, Loss: 0.2420116502952832, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 574, Loss: 0.5484545743547755, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 575, Loss: 0.334662591248509, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 576, Loss: 0.23366021698675388, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 577, Loss: 0.23446691321881422, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 578, Loss: 0.38979269293996543, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 579, Loss: 0.2750641424884403, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 580, Loss: 0.3763163405963518, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 581, Loss: 0.35342281227958217, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 582, Loss: 0.39968965697557235, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 583, Loss: 0.273780150442253, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 584, Loss: 0.5499202841256111, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 585, Loss: 0.2560804256590706, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 586, Loss: 0.35121124486015, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 587, Loss: 0.39709426850693974, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 588, Loss: 0.5878802408526514, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 589, Loss: 0.26916323709897033, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 590, Loss: 0.3952725594960909, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 591, Loss: 0.43654169853542923, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 592, Loss: 0.3453649614813692, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 593, Loss: 0.2726391541555393, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 594, Loss: 0.37317815908441526, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 595, Loss: 0.2995614904329023, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 596, Loss: 0.2367385767906799, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 597, Loss: 0.4474883660252569, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 598, Loss: 0.3664973676588797, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 599, Loss: 0.25105633238083347, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 600, Loss: 0.2892733985489075, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 601, Loss: 0.3518675852269336, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 602, Loss: 0.34680890668653097, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 603, Loss: 0.29547087415213985, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 604, Loss: 0.23994980708226818, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 605, Loss: 0.3799713559226562, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 606, Loss: 0.27330606685000214, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 607, Loss: 0.27914657874543797, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 608, Loss: 0.2359495944189201, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 609, Loss: 0.41948382778213733, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 610, Loss: 0.42717927519882154, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 611, Loss: 0.25853198748680983, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 612, Loss: 0.28947643718224114, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 613, Loss: 0.273449305913326, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 614, Loss: 0.2900102685325046, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 615, Loss: 0.33766092025311345, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 616, Loss: 0.38376262099486264, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 617, Loss: 0.297389377192148, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 618, Loss: 0.26916246140958977, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 619, Loss: 0.24006981006122852, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 620, Loss: 0.24157535118948753, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 621, Loss: 0.30984017293808447, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 622, Loss: 0.31241338345894076, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 623, Loss: 0.32375219351231144, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 624, Loss: 0.29791038273045517, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 625, Loss: 0.2764984984038664, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 626, Loss: 0.2629038501295918, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 627, Loss: 0.36654778843669256, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 628, Loss: 0.3177164116444116, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 629, Loss: 0.30480945227975864, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 630, Loss: 0.3306418349391177, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 631, Loss: 0.41638569272712544, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 632, Loss: 0.26379291990978665, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 633, Loss: 0.27204612755101976, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 634, Loss: 0.25741483566580603, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 635, Loss: 0.41730265583716697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 636, Loss: 0.40336484256561295, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 637, Loss: 0.2725658061832084, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 638, Loss: 0.41065721974445235, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 639, Loss: 0.41543641406815646, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 640, Loss: 0.28516917096685956, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 641, Loss: 0.2403648276777825, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 642, Loss: 0.41974919146455264, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 643, Loss: 0.43696006948391336, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 644, Loss: 0.44387316410950667, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 645, Loss: 0.3278654226780032, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 646, Loss: 0.35133534552096923, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 647, Loss: 0.24603662946697277, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 648, Loss: 0.26960551234612196, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 649, Loss: 0.2307902432317122, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 650, Loss: 0.2555321404265608, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 651, Loss: 0.38900620955977727, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 652, Loss: 0.23708073881867076, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 653, Loss: 0.2491438481728204, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 654, Loss: 0.2521107271552572, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 655, Loss: 0.24980300537879074, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 656, Loss: 0.39457729075956083, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 657, Loss: 0.26814431973662645, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 658, Loss: 0.4288388780553033, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 659, Loss: 0.2553183877861911, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 660, Loss: 0.2384862957983523, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 661, Loss: 0.26498864489450796, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 662, Loss: 0.28929128598748194, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 663, Loss: 0.3223325330157788, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 664, Loss: 0.343328639153369, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 665, Loss: 0.2921816605513806, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 666, Loss: 0.30393675835718936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 667, Loss: 0.3474632515474675, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 668, Loss: 0.3741727397338967, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 669, Loss: 0.30042651347338706, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 670, Loss: 0.32212651623476474, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 671, Loss: 0.2738393151305722, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 672, Loss: 0.2863349146508769, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 673, Loss: 0.3180724066968753, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 674, Loss: 0.3009752987814823, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 675, Loss: 0.38019431005092147, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 676, Loss: 0.24999663983878195, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 677, Loss: 0.2635086062849195, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 678, Loss: 0.31820166661784244, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 679, Loss: 0.26180649787740173, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 680, Loss: 0.23940457900060855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 681, Loss: 0.36771982266077197, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 682, Loss: 0.5485587544200237, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 683, Loss: 0.3498533843086989, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 684, Loss: 0.3589945018745087, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 685, Loss: 0.5190221376291151, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 686, Loss: 0.3790528648308604, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 687, Loss: 0.2922789726224302, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 688, Loss: 0.3607033774989188, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 689, Loss: 0.3391727814429403, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 690, Loss: 0.4029498730088732, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 691, Loss: 0.3358627011959425, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 692, Loss: 0.4421844424721546, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 693, Loss: 0.3365998249745943, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 694, Loss: 0.36647375132736265, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 695, Loss: 0.5068031317522554, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 696, Loss: 0.3184154077382997, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 697, Loss: 0.2774051289764654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 698, Loss: 0.28925631748267683, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 699, Loss: 0.3040597313352221, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 700, Loss: 0.2839717904230991, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 701, Loss: 0.3600809079154824, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 702, Loss: 0.33445691134236705, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 703, Loss: 0.357105823914609, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 704, Loss: 0.45991900618081105, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 705, Loss: 0.24985000461722967, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 706, Loss: 0.5910355982635404, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 707, Loss: 0.2713492656908278, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 708, Loss: 0.32516005459638647, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 709, Loss: 0.3055089225772254, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 710, Loss: 0.25600976294117517, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 711, Loss: 0.7234905252649545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 712, Loss: 0.3513084063667758, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 713, Loss: 0.31156954493028755, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 714, Loss: 0.6832246597125146, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 715, Loss: 0.2622968429616537, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 716, Loss: 0.4380238567620717, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 717, Loss: 0.321806376308847, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 718, Loss: 0.7572876792168853, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 719, Loss: 0.324718478168906, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 720, Loss: 0.38637904157023084, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 721, Loss: 0.5331444003354071, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 722, Loss: 0.3130470571725171, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 723, Loss: 0.3334885435732831, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 724, Loss: 0.2525940943050072, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 725, Loss: 0.3109331291584272, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 726, Loss: 0.28101624658360363, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 727, Loss: 0.32582708769304114, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 728, Loss: 0.2885382954794773, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 729, Loss: 0.24978296468033234, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 730, Loss: 0.3663115974333393, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 731, Loss: 0.3079429733537902, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 732, Loss: 0.2951028138490712, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 733, Loss: 0.35831438132831844, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 734, Loss: 0.3264551835110744, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 735, Loss: 0.3196015500451788, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 736, Loss: 0.2545054584069072, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 737, Loss: 0.3143074284715307, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 738, Loss: 0.6098258236514376, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 739, Loss: 0.536439173821562, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 740, Loss: 0.24164074231153654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 741, Loss: 0.39491932539887564, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 742, Loss: 0.4019012509877851, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 743, Loss: 0.41731532297763074, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 744, Loss: 0.2395598183902321, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 745, Loss: 0.3199750834992145, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 746, Loss: 0.5024092325596814, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 747, Loss: 0.5548436206808613, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 748, Loss: 0.36818579086308334, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 749, Loss: 0.40707129252161783, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 750, Loss: 0.26801769885567917, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 751, Loss: 0.37319549692978204, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 752, Loss: 0.3766642793871271, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 753, Loss: 0.35447632341397445, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 754, Loss: 0.3829067967456997, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 755, Loss: 0.2664977997214518, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 756, Loss: 0.3156387573900603, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 757, Loss: 0.27069220682072004, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 758, Loss: 0.2656116437051947, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 759, Loss: 0.3778739694066901, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 760, Loss: 0.23647538443387564, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 761, Loss: 0.2685544370524181, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 762, Loss: 0.28145436978525423, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 763, Loss: 0.2655758965895991, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 764, Loss: 0.24362163503627415, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 765, Loss: 0.3451386397211983, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 766, Loss: 0.37081704928735015, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 767, Loss: 0.32320222598196935, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 768, Loss: 0.3929111932053234, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 769, Loss: 0.5347070730373511, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 770, Loss: 0.3140113210051692, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 771, Loss: 0.3132255702341023, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 772, Loss: 0.42789840289523917, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 773, Loss: 0.24441895226026705, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 774, Loss: 0.29496721340891513, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 775, Loss: 0.4030608913649856, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 776, Loss: 0.3646716795089463, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 777, Loss: 0.3060546454537175, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 778, Loss: 0.23222526993854417, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 779, Loss: 0.33577652072071595, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 780, Loss: 0.25078022409925294, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 781, Loss: 0.6999642381227993, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 782, Loss: 0.29217002267272957, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 783, Loss: 0.3165581071097134, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 784, Loss: 0.2736372985563178, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 785, Loss: 0.2727017803345149, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 786, Loss: 0.5469743503209494, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 787, Loss: 0.3408806337898017, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 788, Loss: 0.2817409486635587, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 789, Loss: 0.27083311544614946, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 790, Loss: 0.2374924974304846, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 791, Loss: 0.24060302165400263, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 792, Loss: 0.38059609573307057, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 793, Loss: 0.7754292974733817, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 794, Loss: 0.2535158473284955, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 795, Loss: 0.24614235853099747, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 796, Loss: 0.4248922023859292, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 797, Loss: 0.3611677419541852, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 798, Loss: 0.2381528182619338, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 799, Loss: 0.30608983112726473, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 800, Loss: 0.32354579972220887, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 801, Loss: 0.34077219425971456, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 802, Loss: 0.35689416525802636, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 803, Loss: 0.30700656255085534, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 804, Loss: 0.34495889304576544, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 805, Loss: 0.4873832397879865, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 806, Loss: 0.2637846123772543, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 807, Loss: 0.5859282819519354, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 808, Loss: 0.24706614223912396, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 809, Loss: 0.24280514440470385, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 810, Loss: 0.2556647932008732, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 811, Loss: 0.48683297198271724, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 812, Loss: 0.36895802848840803, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 813, Loss: 0.312643289475525, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 814, Loss: 0.30841493218455013, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 815, Loss: 0.26332656455375564, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 816, Loss: 0.3926243555786939, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 817, Loss: 0.2455147864954919, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 818, Loss: 0.3582336016905273, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 819, Loss: 0.36524708035046394, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 820, Loss: 0.30320652332751946, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 821, Loss: 0.3223569555627518, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 822, Loss: 0.25041429816519295, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 823, Loss: 0.3362693045973868, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 824, Loss: 0.28093920865084404, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 825, Loss: 0.2871796945788545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 826, Loss: 0.4242249920030872, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 827, Loss: 0.3804279766067613, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 828, Loss: 0.2440990424996333, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 829, Loss: 0.3063828618638863, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 830, Loss: 0.25462126161699083, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 831, Loss: 0.34036488273997295, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 832, Loss: 0.2887545907429286, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 833, Loss: 0.2944667554267773, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 834, Loss: 0.2774779630913613, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 835, Loss: 0.4540441682889724, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 836, Loss: 0.2574341324420166, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 837, Loss: 0.34671544673928917, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 838, Loss: 0.3281416773049681, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 839, Loss: 0.34197505956102975, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 840, Loss: 0.3373750222195519, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 841, Loss: 0.2847503866814829, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 842, Loss: 0.4077047989728444, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 843, Loss: 0.6326738272620235, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 844, Loss: 0.34804619818032423, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 845, Loss: 0.4031321763725976, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 846, Loss: 0.2966435598665842, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 847, Loss: 0.2886131136337306, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 848, Loss: 0.3687167054565892, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 849, Loss: 0.2378334329286441, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 850, Loss: 0.5692110069000716, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 851, Loss: 0.2576195002768103, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 852, Loss: 0.2641195746181719, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 853, Loss: 0.32087674379270936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 854, Loss: 0.3350859511823481, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 855, Loss: 0.26545077791828736, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 856, Loss: 0.33379432635311546, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 857, Loss: 0.6747463548602695, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 858, Loss: 0.3359571026995007, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 859, Loss: 0.3280807334072537, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 860, Loss: 0.25487381289676825, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 861, Loss: 0.3407582214193059, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 862, Loss: 0.24402166967166136, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 863, Loss: 0.3524943230460794, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 864, Loss: 0.4454315664155671, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 865, Loss: 0.4008640453604892, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 866, Loss: 0.25195872963429006, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 867, Loss: 0.25350776037701495, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 868, Loss: 0.26003023972535816, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 869, Loss: 0.3027517601235685, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 870, Loss: 0.2615735896139459, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 871, Loss: 0.2762001873116779, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 872, Loss: 0.4111835484609482, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 873, Loss: 0.30508549309135863, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 874, Loss: 0.22933640340110206, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 875, Loss: 0.3649694672670859, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 876, Loss: 0.2876343086303695, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 877, Loss: 0.24278488223017236, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 878, Loss: 0.28119618925640905, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 879, Loss: 0.280718962501431, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 880, Loss: 0.3556337444650117, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 881, Loss: 0.332468680375792, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 882, Loss: 0.3334771180052237, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 883, Loss: 0.2454521902007199, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 884, Loss: 0.26627413629824537, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 885, Loss: 0.24220657236170723, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 886, Loss: 0.2665512620613696, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 887, Loss: 0.2733705208633401, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 888, Loss: 0.3946672865543544, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 889, Loss: 0.4220013481566141, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 890, Loss: 0.23909369579919962, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 891, Loss: 0.31812580240639465, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 892, Loss: 0.3305615743717758, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 893, Loss: 0.28073970483139327, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 894, Loss: 0.2717943898053129, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 895, Loss: 0.48806174615445297, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 896, Loss: 0.40116214747625245, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 897, Loss: 0.2313826488945022, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 898, Loss: 0.24053964896643334, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 899, Loss: 0.3689038977554674, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 900, Loss: 0.44261025927363185, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 901, Loss: 0.2995104138091147, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 902, Loss: 0.28374037408203845, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 903, Loss: 0.4135842141390659, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 904, Loss: 0.4581288605099998, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 905, Loss: 0.3634246629939454, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 906, Loss: 0.40736666917851216, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 907, Loss: 0.288516387944008, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 908, Loss: 0.24839888304979055, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 909, Loss: 0.34857889365542466, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 910, Loss: 0.45034606211810724, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 911, Loss: 0.2466509688215007, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 912, Loss: 0.2862643088579893, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 913, Loss: 0.273780537624848, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 914, Loss: 0.22447347884839536, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 915, Loss: 0.45961601239799843, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 916, Loss: 0.25544145824363823, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 917, Loss: 0.3246904953506189, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 918, Loss: 0.3032962397117107, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 919, Loss: 0.3260455668503649, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 920, Loss: 0.2631274212922751, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 921, Loss: 0.2869218834964068, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 922, Loss: 0.24868701464802695, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 923, Loss: 0.30863473759947335, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 924, Loss: 0.38582967295762827, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 925, Loss: 0.5669708100013503, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 926, Loss: 0.5971387847567469, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 927, Loss: 0.4529328320121197, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 928, Loss: 0.3445751106420713, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 929, Loss: 0.2756138270185712, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 930, Loss: 0.26048803435481194, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 931, Loss: 0.32955861735265773, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 932, Loss: 0.4273930709274648, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 933, Loss: 0.2691300987551805, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 934, Loss: 0.40988137248390455, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 935, Loss: 0.3388136219528084, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 936, Loss: 0.305872371533937, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 937, Loss: 0.28413441825545827, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 938, Loss: 0.6291393642601578, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 939, Loss: 0.28646779465605154, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 940, Loss: 0.2997430422354548, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 941, Loss: 0.2615777570052744, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 942, Loss: 0.2793009060681473, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 943, Loss: 0.3100658874736053, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 944, Loss: 0.3274941653799559, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 945, Loss: 0.35709859969591473, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 946, Loss: 0.2708324070219183, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 947, Loss: 0.24843868052648793, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 948, Loss: 0.30900322050560664, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 949, Loss: 0.562146489506761, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 950, Loss: 0.2547722249637445, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 951, Loss: 0.28423911189067724, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 952, Loss: 0.3758413401986419, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 953, Loss: 0.3061766339780763, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 954, Loss: 0.302015696339461, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 955, Loss: 0.30603587004697436, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 956, Loss: 0.3275797203355526, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 957, Loss: 0.31097516442233125, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 958, Loss: 0.2995858223493171, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 959, Loss: 0.3612433676348338, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 960, Loss: 0.23212133543166283, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 961, Loss: 0.3590477601730955, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 962, Loss: 0.2924621053248343, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 963, Loss: 0.5072758073447883, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 964, Loss: 0.2599674760632761, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 965, Loss: 0.33353300284069654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 966, Loss: 0.3535415253342833, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 967, Loss: 0.33839426182444754, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 968, Loss: 0.2687422568432516, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 969, Loss: 0.2826111694877573, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 970, Loss: 0.3101559921262189, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 971, Loss: 0.28463697631379586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 972, Loss: 0.39720499081386174, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 973, Loss: 0.273488519029918, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 974, Loss: 0.5246015314232666, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 975, Loss: 0.2869675078496128, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 976, Loss: 0.31998548442271313, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 977, Loss: 0.29128852371637254, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 978, Loss: 0.3299708323792764, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 979, Loss: 0.2346257210199988, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 980, Loss: 0.28790750992635566, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 981, Loss: 0.32169480751107915, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 982, Loss: 0.2593362730974845, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 983, Loss: 0.2941013310728008, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 984, Loss: 0.32344773750133704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 985, Loss: 0.24709771066879024, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 986, Loss: 0.4649442074798955, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 987, Loss: 0.3193509811506876, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 988, Loss: 0.26895868447081916, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 989, Loss: 0.2755830124259984, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 990, Loss: 0.6993449457284757, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 991, Loss: 0.26653988870643097, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 992, Loss: 0.3237822749541409, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 993, Loss: 0.29303309202254346, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 994, Loss: 0.28089777972321517, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 995, Loss: 0.25010060916097687, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 996, Loss: 0.5720985877214589, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 997, Loss: 0.27572479392658394, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 998, Loss: 0.22984391162209325, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 999, Loss: 0.40328251026845713, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1000, Loss: 0.2904437748069895, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1001, Loss: 0.3934246993432232, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1002, Loss: 0.3692626554068874, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1003, Loss: 0.2324942837606616, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1004, Loss: 0.23157066450184846, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1005, Loss: 0.4903755324597785, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1006, Loss: 0.30206029866043116, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1007, Loss: 0.3280383370254015, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1008, Loss: 0.23830728738904622, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1009, Loss: 0.3224333085439523, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1010, Loss: 0.4865376719124892, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1011, Loss: 0.23603765519782152, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1012, Loss: 0.3892758837665669, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1013, Loss: 0.3379900975877136, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1014, Loss: 0.32360541832421125, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1015, Loss: 0.23897128210228716, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1016, Loss: 0.24564643749402992, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1017, Loss: 0.4057466706369601, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1018, Loss: 0.36979657255608733, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1019, Loss: 0.37001630768748, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1020, Loss: 0.2469129478795995, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1021, Loss: 0.4937353407009495, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1022, Loss: 0.5042323931867772, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1023, Loss: 0.3318712010539211, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1024, Loss: 0.35562620755633834, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1025, Loss: 0.23059975653563156, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1026, Loss: 0.3755651000372435, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1027, Loss: 0.373111138465899, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1028, Loss: 0.3608447162474451, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1029, Loss: 0.3366261679376729, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1030, Loss: 0.3175255341922348, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1031, Loss: 0.3353423418295072, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1032, Loss: 0.2548067089065425, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1033, Loss: 0.325887046847882, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1034, Loss: 0.26883028730377156, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1035, Loss: 0.4050270352266586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1036, Loss: 0.2641004538347132, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1037, Loss: 0.2607105487509236, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1038, Loss: 0.49331121470873734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1039, Loss: 0.2473000376167852, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1040, Loss: 0.3501833970160427, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1041, Loss: 0.27705997033673907, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1042, Loss: 0.2585666682315367, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1043, Loss: 0.35950475764102274, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1044, Loss: 0.25996099562010233, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1045, Loss: 0.46199102316219687, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1046, Loss: 0.24289983752808264, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1047, Loss: 0.41511913165789704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1048, Loss: 0.34492357698049003, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1049, Loss: 0.7825710256481136, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1050, Loss: 0.4413346936404505, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1051, Loss: 0.3686412107491228, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1052, Loss: 0.5779744446591643, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1053, Loss: 0.39584076249282585, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1054, Loss: 0.3512626183503618, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1055, Loss: 0.27153970506468567, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1056, Loss: 0.3135530803799577, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1057, Loss: 0.34824705604783346, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1058, Loss: 0.3546123558651376, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1059, Loss: 0.32215857977132223, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1060, Loss: 0.3821254815676743, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1061, Loss: 0.3091442751995269, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1062, Loss: 0.40104677207289974, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1063, Loss: 0.39330928522007397, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1064, Loss: 0.4413128662053899, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1065, Loss: 0.5165473765297706, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1066, Loss: 0.3104555559448539, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1067, Loss: 0.26401224453264555, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1068, Loss: 0.35135442068586786, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1069, Loss: 0.44527512895637783, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1070, Loss: 0.2944822936624421, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1071, Loss: 0.27032725218300024, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1072, Loss: 0.25059798884067586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1073, Loss: 0.3195580379585691, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1074, Loss: 0.2430436195810503, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1075, Loss: 0.3921692685278274, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1076, Loss: 0.33965321901535994, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1077, Loss: 0.32552999638230007, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1078, Loss: 0.2565075477952714, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1079, Loss: 0.2755628720600693, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1080, Loss: 0.26441047061842826, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1081, Loss: 0.3384046578056056, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1082, Loss: 0.2581397703456423, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1083, Loss: 0.32254957344323043, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1084, Loss: 0.2520253978409532, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1085, Loss: 0.4757937129056561, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1086, Loss: 0.3963716125122503, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1087, Loss: 0.6158920253354545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1088, Loss: 0.5066850435758138, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1089, Loss: 0.28905070291808355, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1090, Loss: 0.32911000324375345, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1091, Loss: 0.38386132887499064, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1092, Loss: 0.526720090410516, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1093, Loss: 0.30212274022204055, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1094, Loss: 0.6364746530436554, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1095, Loss: 0.23460912063726885, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1096, Loss: 0.4303249292075815, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1097, Loss: 0.2772747260844132, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1098, Loss: 0.37183802364752083, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1099, Loss: 0.24393883301662075, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1100, Loss: 0.2656016910327117, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1101, Loss: 0.4642224208771149, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1102, Loss: 0.32572449362043243, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1103, Loss: 0.4286431257571264, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1104, Loss: 0.560875538899764, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1105, Loss: 0.24395242912949736, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1106, Loss: 0.24976369889199826, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1107, Loss: 0.32078225943501293, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1108, Loss: 0.23217538532445975, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1109, Loss: 0.31698447012518643, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1110, Loss: 0.23920309138014223, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1111, Loss: 0.6289625638498269, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1112, Loss: 0.3275820453139371, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1113, Loss: 0.26987477091604933, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1114, Loss: 0.29895434161125545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1115, Loss: 0.2565813468900544, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1116, Loss: 0.5205993257978637, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1117, Loss: 0.7540073235411276, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1118, Loss: 0.2498475842383005, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1119, Loss: 0.5042645061036065, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1120, Loss: 0.30786830726293707, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1121, Loss: 0.3685288439315342, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1122, Loss: 0.2843581287314624, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1123, Loss: 0.36900765946763825, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1124, Loss: 0.2701285634288023, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1125, Loss: 0.2473932084607462, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1126, Loss: 0.4166343845933444, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1127, Loss: 0.37529785452831105, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1128, Loss: 0.2649292320015524, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1129, Loss: 0.25658987069625866, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1130, Loss: 0.23842107861271544, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1131, Loss: 0.4129539654669574, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1132, Loss: 0.26231600888222956, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1133, Loss: 0.2653229475551337, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1134, Loss: 0.3767956543862866, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1135, Loss: 0.35257290070047287, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1136, Loss: 0.3382611254707433, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1137, Loss: 0.6883313080192994, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1138, Loss: 0.2909358829312657, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1139, Loss: 0.24676015696996909, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1140, Loss: 0.31787000865206805, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1141, Loss: 0.48780671897975747, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1142, Loss: 0.39615824437210867, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1143, Loss: 0.3084076020134573, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1144, Loss: 0.27815165030175765, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1145, Loss: 0.5424037600520617, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1146, Loss: 0.33612724462662524, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1147, Loss: 0.3076332751287312, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1148, Loss: 0.28549773969387754, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1149, Loss: 0.46309054832841234, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1150, Loss: 0.39305994957789425, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1151, Loss: 0.478928781424597, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1152, Loss: 0.28074934381724903, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1153, Loss: 0.31793143738727975, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1154, Loss: 0.28538090610246364, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1155, Loss: 0.3053490081102638, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1156, Loss: 0.34766663995662156, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1157, Loss: 0.269419051568762, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1158, Loss: 0.3205455750988041, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1159, Loss: 0.25455360742539274, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1160, Loss: 0.48104604327305545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1161, Loss: 0.41329704455750127, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1162, Loss: 0.4108667924778888, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1163, Loss: 0.25030223269116136, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1164, Loss: 0.4826808600467474, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1165, Loss: 0.33754008238505945, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1166, Loss: 0.3295633253306638, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1167, Loss: 0.2676811090668632, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1168, Loss: 0.2889335146210354, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1169, Loss: 0.3010094743524299, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1170, Loss: 0.2516479694955267, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1171, Loss: 0.5886689123076615, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1172, Loss: 0.31488140708858503, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1173, Loss: 0.3442638902838115, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1174, Loss: 0.2817651175926791, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1175, Loss: 0.3419116876902888, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1176, Loss: 0.252718561878226, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1177, Loss: 0.23481733787867382, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1178, Loss: 0.2816850879555275, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1179, Loss: 0.26398471841185606, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1180, Loss: 0.34999462201392884, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1181, Loss: 0.3396864158528533, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1182, Loss: 0.5119031151265877, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1183, Loss: 0.36347145382100376, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1184, Loss: 0.40320314723289225, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1185, Loss: 0.25831096700418654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1186, Loss: 0.28354740604537215, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1187, Loss: 0.33852455804375403, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1188, Loss: 0.3340302375734666, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1189, Loss: 0.26678666094712256, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1190, Loss: 0.317509617529419, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1191, Loss: 0.3148808691364499, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1192, Loss: 0.303639678826108, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1193, Loss: 0.5041857611376717, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1194, Loss: 0.3808675756428846, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1195, Loss: 0.3326693222996325, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1196, Loss: 0.4416304595664101, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1197, Loss: 0.27212970508027545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1198, Loss: 0.312103096150643, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1199, Loss: 0.3616218378876521, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1200, Loss: 0.25562085721945305, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1201, Loss: 0.2809429740793244, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1202, Loss: 0.2520512840287325, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1203, Loss: 0.3890691442174663, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1204, Loss: 0.2769936160540838, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1205, Loss: 0.23936037035732233, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1206, Loss: 0.3460070779177473, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1207, Loss: 0.30515905415385836, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1208, Loss: 0.23726845843002717, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1209, Loss: 0.2477949161162523, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1210, Loss: 0.3180520242093766, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1211, Loss: 0.42251812390561644, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1212, Loss: 0.337009589172367, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1213, Loss: 0.36428269238260147, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1214, Loss: 0.24063457410933264, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1215, Loss: 0.3345764452604678, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1216, Loss: 0.31541667738592033, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1217, Loss: 0.25700470398096675, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1218, Loss: 0.43594197079565566, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1219, Loss: 0.2345656189755792, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1220, Loss: 0.3163593747399379, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1221, Loss: 0.4912192047037808, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1222, Loss: 0.44893733371692357, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1223, Loss: 0.2783395748003427, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1224, Loss: 0.30117065770815493, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1225, Loss: 0.33516902150959377, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1226, Loss: 0.3090329562005662, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1227, Loss: 0.4647947935457467, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1228, Loss: 0.2812260303267744, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1229, Loss: 0.545928358297314, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1230, Loss: 0.28635429339578433, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1231, Loss: 0.4067946917133548, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1232, Loss: 0.30033423977288676, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1233, Loss: 0.27844499022413627, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1234, Loss: 0.2942120231426632, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1235, Loss: 0.3496388132702638, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1236, Loss: 0.4433322818140154, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1237, Loss: 0.2917152156960691, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1238, Loss: 0.4101817608875107, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1239, Loss: 0.3214644273026324, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1240, Loss: 0.430513593015479, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1241, Loss: 0.4125638055325415, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1242, Loss: 0.2551611618172552, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1243, Loss: 0.5487381886166942, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1244, Loss: 0.2273863069964233, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1245, Loss: 0.44508145089677065, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1246, Loss: 0.28956652274801936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1247, Loss: 0.3601921411591025, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1248, Loss: 0.335720016189515, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1249, Loss: 0.2648059036033078, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1250, Loss: 0.30089388303064774, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1251, Loss: 0.2876264929650638, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1252, Loss: 0.4152571699004339, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1253, Loss: 0.3096745053112938, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1254, Loss: 0.38019425225239617, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1255, Loss: 0.25783262272147867, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1256, Loss: 0.6379099667832441, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1257, Loss: 0.29216691082021634, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1258, Loss: 0.26020417543528446, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1259, Loss: 0.29391298606498495, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1260, Loss: 0.3261749603290514, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1261, Loss: 0.26097758445528685, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1262, Loss: 0.5650075240771713, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1263, Loss: 0.33039841081826865, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1264, Loss: 0.2834793480896037, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1265, Loss: 0.3488093008036599, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1266, Loss: 0.25132642873429295, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1267, Loss: 0.2521775497690504, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1268, Loss: 0.3948579558449615, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1269, Loss: 0.275327813209176, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1270, Loss: 0.25764932048448486, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1271, Loss: 0.28598970957201286, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1272, Loss: 0.2669578797349125, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1273, Loss: 0.2786444923627959, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1274, Loss: 0.43471748266109567, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1275, Loss: 0.6158602080437294, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1276, Loss: 0.28120547667682655, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1277, Loss: 0.35328282653749926, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1278, Loss: 0.47786371569704456, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1279, Loss: 0.2472162408198464, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1280, Loss: 0.30024499129275917, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1281, Loss: 0.2412051563428213, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1282, Loss: 0.2706714716758758, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1283, Loss: 0.23979849127976, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1284, Loss: 0.375465083005207, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1285, Loss: 0.24895851728581972, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1286, Loss: 0.31004445950330595, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1287, Loss: 0.34809855803214645, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1288, Loss: 0.5814484313215798, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1289, Loss: 0.26760000364712994, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1290, Loss: 0.39369872672915013, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1291, Loss: 0.31261380342549167, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1292, Loss: 0.3250199320417599, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1293, Loss: 0.48552020914391875, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1294, Loss: 0.28807069434382826, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1295, Loss: 0.2937184203248734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1296, Loss: 0.37522320513912444, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1297, Loss: 0.3739979092742661, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1298, Loss: 0.5654014365498757, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1299, Loss: 0.2942926746642822, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1300, Loss: 0.2639479797119831, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1301, Loss: 0.3206458699285509, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1302, Loss: 0.28787352205572, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1303, Loss: 0.259343701368469, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1304, Loss: 0.26275178201072236, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1305, Loss: 0.4192902340550658, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1306, Loss: 0.27092515286766244, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1307, Loss: 0.35803772868942807, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1308, Loss: 0.25607445915095645, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1309, Loss: 0.4394778350742433, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1310, Loss: 0.25883672801522956, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1311, Loss: 0.2627890325685651, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1312, Loss: 0.31140678073936806, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1313, Loss: 0.30336846450340005, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1314, Loss: 0.3496041558516684, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1315, Loss: 0.32328490962159984, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1316, Loss: 0.27250257655477317, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1317, Loss: 0.2571689850170888, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1318, Loss: 0.3228253540732182, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1319, Loss: 0.289299130571085, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1320, Loss: 0.24564127060752194, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1321, Loss: 0.34474283328878175, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1322, Loss: 0.5212075979762671, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1323, Loss: 0.29602503072573944, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1324, Loss: 0.5364990725474412, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1325, Loss: 0.3939279179632101, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1326, Loss: 0.29991731200714766, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1327, Loss: 0.2515052956655275, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1328, Loss: 0.3027590040902357, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1329, Loss: 0.2828168729412025, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1330, Loss: 0.27807806051733297, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1331, Loss: 0.231381084210178, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1332, Loss: 0.26987747030478043, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1333, Loss: 0.38712720803105527, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1334, Loss: 0.39025922932025825, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1335, Loss: 0.26709104042140297, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1336, Loss: 0.5985862308943168, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1337, Loss: 0.2535119020474084, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1338, Loss: 0.24710070912333942, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1339, Loss: 0.4328575362734827, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1340, Loss: 0.42150312399642365, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1341, Loss: 0.2407516765341139, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1342, Loss: 0.3071255671614836, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1343, Loss: 0.26425056059486374, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1344, Loss: 0.4088296957685405, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1345, Loss: 0.3160744314019227, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1346, Loss: 0.2510763663775828, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1347, Loss: 0.47962955858911427, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1348, Loss: 0.3378488058865497, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1349, Loss: 0.5348603443258338, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1350, Loss: 0.23074779859884653, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1351, Loss: 0.3041060951088487, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1352, Loss: 0.2630186333324906, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1353, Loss: 0.27228882460208464, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1354, Loss: 0.24889939681976142, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1355, Loss: 0.2862539805983406, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1356, Loss: 0.2834167296280785, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1357, Loss: 0.41889546853516735, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1358, Loss: 0.30668210943755153, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1359, Loss: 0.3388716737898698, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1360, Loss: 0.3560838288093277, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1361, Loss: 0.2628829486762291, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1362, Loss: 0.2840273813096885, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1363, Loss: 0.42566981287198646, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1364, Loss: 0.3162299594451186, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1365, Loss: 0.28815724228530004, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1366, Loss: 0.4025883958841139, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1367, Loss: 0.3225912869211437, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1368, Loss: 0.3479243504280427, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1369, Loss: 0.3312285986478195, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1370, Loss: 0.38002478778759674, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1371, Loss: 0.3779899261762917, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1372, Loss: 0.33069122001587925, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1373, Loss: 0.30014625930699323, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1374, Loss: 0.4452617523981607, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1375, Loss: 0.2325589294692503, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1376, Loss: 0.3536909589970633, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1377, Loss: 0.5086567439727386, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1378, Loss: 0.2750929181077815, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1379, Loss: 0.22892409890961316, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1380, Loss: 0.4063619897685459, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1381, Loss: 0.3546163138444377, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1382, Loss: 0.2624514096176428, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1383, Loss: 0.3420379342243068, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1384, Loss: 0.24735088979289868, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1385, Loss: 0.26132596956888815, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1386, Loss: 0.2651681753718543, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1387, Loss: 0.2666582269784515, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1388, Loss: 0.3260503376072002, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1389, Loss: 0.36982954259403156, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1390, Loss: 0.3711495584397916, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1391, Loss: 0.4239267347986364, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1392, Loss: 0.26804680232115874, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1393, Loss: 0.3410637102279821, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1394, Loss: 0.389386075989146, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1395, Loss: 0.3351742142499623, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1396, Loss: 0.2691504618780367, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1397, Loss: 0.22800508911534165, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1398, Loss: 0.37887988103602854, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1399, Loss: 0.4149420264555834, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1400, Loss: 0.44120775476219076, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1401, Loss: 0.30946206177498803, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1402, Loss: 0.33896751060645464, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1403, Loss: 0.2773727099257799, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1404, Loss: 0.4121658966896764, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1405, Loss: 0.24925027477488693, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1406, Loss: 0.4657700634594986, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1407, Loss: 0.257673824297915, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1408, Loss: 0.2825543050844818, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1409, Loss: 0.23800110381006367, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1410, Loss: 0.3402264266099574, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1411, Loss: 0.3811009612420897, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1412, Loss: 0.6202217979482599, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1413, Loss: 0.36514090164303786, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1414, Loss: 0.3451313760056049, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1415, Loss: 0.33903902914748674, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1416, Loss: 0.4281219445464892, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1417, Loss: 0.28448412702870907, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1418, Loss: 0.31245511394819614, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1419, Loss: 0.31021788787294846, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1420, Loss: 0.3027652981512715, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1421, Loss: 0.290256183133306, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1422, Loss: 0.43175270380077807, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1423, Loss: 0.40609194836279117, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1424, Loss: 0.48573629566337406, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1425, Loss: 0.6213430366958291, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1426, Loss: 0.37891574180498305, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1427, Loss: 0.296227812584742, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1428, Loss: 0.47141589304929055, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1429, Loss: 0.3339248046126799, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1430, Loss: 0.3982948467405705, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1431, Loss: 0.33823674286170347, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1432, Loss: 0.3908926462115345, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1433, Loss: 0.29754349734748275, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1434, Loss: 0.23085404032759285, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1435, Loss: 0.28550093613161925, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1436, Loss: 0.28659955037522533, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1437, Loss: 0.30063177664611723, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1438, Loss: 0.281864690176684, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1439, Loss: 0.3164005654635459, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1440, Loss: 0.2395256683758407, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1441, Loss: 0.41358371241317954, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1442, Loss: 0.4290041622066362, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1443, Loss: 0.2551725821303732, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1444, Loss: 0.30714889312115795, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1445, Loss: 0.3133066079814531, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1446, Loss: 0.23823465746824632, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1447, Loss: 0.2332813242901799, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1448, Loss: 0.2637100473828448, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1449, Loss: 0.3507636706998236, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1450, Loss: 0.3270673452195797, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1451, Loss: 0.40842971021950747, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1452, Loss: 0.3728806301858147, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1453, Loss: 0.2489984537835004, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1454, Loss: 0.24500594381442814, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1455, Loss: 0.36194855846075386, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1456, Loss: 0.24475030901060174, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1457, Loss: 0.3062260381809524, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1458, Loss: 0.32356801110638234, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1459, Loss: 0.3574889612629658, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1460, Loss: 0.36142669288459645, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1461, Loss: 0.43435892190989045, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1462, Loss: 0.46505323916681285, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1463, Loss: 0.3132394193026734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1464, Loss: 0.32642885332409666, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1465, Loss: 0.2614878313607823, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1466, Loss: 0.3551416295707591, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1467, Loss: 0.38247812964985933, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1468, Loss: 0.4552289135141502, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1469, Loss: 0.3480845857427626, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1470, Loss: 0.6014033462131886, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1471, Loss: 0.24699881062719184, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1472, Loss: 0.4091777851103586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1473, Loss: 0.2291513486351278, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1474, Loss: 0.2966219003127988, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1475, Loss: 0.39881536142030205, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1476, Loss: 0.43014405783161014, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1477, Loss: 0.3259974906366044, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1478, Loss: 0.3414661530547347, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1479, Loss: 0.38050351744469363, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1480, Loss: 0.666642956929269, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1481, Loss: 0.2886455873811452, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1482, Loss: 0.24927265346427924, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1483, Loss: 0.258310728742699, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1484, Loss: 0.296835698700112, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1485, Loss: 0.34915369783327715, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1486, Loss: 0.2502104819803122, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1487, Loss: 0.3003710460023041, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1488, Loss: 0.28875236093202844, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1489, Loss: 0.2854353450128615, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1490, Loss: 0.28241962958586936, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1491, Loss: 0.3020420636623847, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1492, Loss: 0.38987268242402673, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1493, Loss: 0.2607579672375302, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1494, Loss: 0.29940890825713873, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1495, Loss: 0.28301892287947944, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1496, Loss: 0.252532051917074, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1497, Loss: 0.31421562553648114, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1498, Loss: 0.3214749409493366, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1499, Loss: 0.34496938546069944, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1500, Loss: 0.3963024953826335, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1501, Loss: 0.39500452640479844, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1502, Loss: 0.3031302774532581, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1503, Loss: 0.34644155701067886, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1504, Loss: 0.30745620475133273, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1505, Loss: 0.4715996955051472, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1506, Loss: 0.24490568135690344, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1507, Loss: 0.25572952685589456, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1508, Loss: 0.506586598239352, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1509, Loss: 0.2728166130931714, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1510, Loss: 0.320701048597163, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1511, Loss: 0.3044183249567779, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1512, Loss: 0.8164487065708826, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1513, Loss: 0.39124136617287203, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1514, Loss: 0.25350471729406865, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1515, Loss: 0.2890269569474415, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1516, Loss: 0.41513646471424925, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1517, Loss: 0.27557847485566656, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1518, Loss: 0.277618507238578, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1519, Loss: 0.4606135935591261, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1520, Loss: 0.4388920731318342, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1521, Loss: 0.42645194197896374, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1522, Loss: 0.3340320451511751, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1523, Loss: 0.3975609311587272, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1524, Loss: 0.2814242083428931, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1525, Loss: 0.3253636534663702, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1526, Loss: 0.2442908695272631, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1527, Loss: 0.32714958416428874, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1528, Loss: 0.36458525620769366, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1529, Loss: 0.2873308527130104, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1530, Loss: 0.2958352789904346, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1531, Loss: 0.3790159416547897, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1532, Loss: 0.3006057888612029, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1533, Loss: 0.3606932530698196, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1534, Loss: 0.2635868907002013, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1535, Loss: 0.2713383759589505, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1536, Loss: 0.32628606090440654, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1537, Loss: 0.3537676988084727, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1538, Loss: 0.23650470822539224, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1539, Loss: 0.2486911679055775, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1540, Loss: 0.39758409491771673, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1541, Loss: 0.3551250266435745, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1542, Loss: 0.3581318776637268, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1543, Loss: 0.2799233503539799, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1544, Loss: 0.25638956007806885, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1545, Loss: 0.28411003974735716, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1546, Loss: 0.24545736166220164, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1547, Loss: 0.34033753128619465, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1548, Loss: 0.34526760646418586, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1549, Loss: 0.3778060013992268, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1550, Loss: 0.37364875302689604, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1551, Loss: 0.3963944867878483, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1552, Loss: 0.2410056484400166, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1553, Loss: 0.4192505245147836, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1554, Loss: 0.44408261555447537, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1555, Loss: 0.4703020840283643, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1556, Loss: 0.2590405298473941, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1557, Loss: 0.4008257718078916, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1558, Loss: 0.25964047561856424, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1559, Loss: 0.233391362301475, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1560, Loss: 0.3925278609522368, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1561, Loss: 0.2857117416007982, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1562, Loss: 0.8224197139704545, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1563, Loss: 0.25010747520848226, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1564, Loss: 0.26761401816528446, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1565, Loss: 0.28732628219436174, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1566, Loss: 0.3028020792853346, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1567, Loss: 0.35992408792991426, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1568, Loss: 0.27645929083332393, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1569, Loss: 0.5463551051169004, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1570, Loss: 0.3586343302926886, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1571, Loss: 0.25369132389721144, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1572, Loss: 0.3161238176263969, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1573, Loss: 0.3734097111944028, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1574, Loss: 0.2575204367868507, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1575, Loss: 0.3645377078313101, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1576, Loss: 0.376966961169364, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1577, Loss: 0.22616121859954377, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1578, Loss: 0.38429994978218124, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1579, Loss: 0.4813927321123186, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1580, Loss: 0.3588981691192661, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1581, Loss: 0.2738200095836444, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1582, Loss: 0.45030298758421494, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1583, Loss: 0.2922402099826457, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1584, Loss: 0.2927916045686531, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1585, Loss: 0.23273061065223655, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1586, Loss: 0.3349045240386387, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1587, Loss: 0.2314966587401207, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1588, Loss: 0.2942791536430675, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1589, Loss: 0.3688199373909131, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1590, Loss: 0.3067109051552543, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1591, Loss: 0.2547232512229607, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1592, Loss: 0.5778719425917864, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1593, Loss: 0.2567293601526507, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1594, Loss: 0.2558897043440212, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1595, Loss: 0.3296031686103813, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1596, Loss: 0.25030537282735404, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1597, Loss: 0.46715802413091667, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1598, Loss: 0.3234816056313989, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1599, Loss: 0.30125301639470725, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1600, Loss: 0.2578167840191377, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1601, Loss: 0.3246883123816874, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1602, Loss: 0.3077749435312309, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1603, Loss: 0.3810503952041262, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1604, Loss: 0.2892358335444699, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1605, Loss: 0.3632766985510506, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1606, Loss: 0.3238819811069238, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1607, Loss: 0.36036380856242045, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1608, Loss: 0.30802763977945674, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1609, Loss: 0.27634880384729005, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1610, Loss: 0.2619701616399757, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1611, Loss: 0.3851735011891646, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1612, Loss: 0.4344730211310468, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1613, Loss: 0.26836911443893857, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1614, Loss: 0.3810594638035322, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1615, Loss: 0.2609028133069832, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1616, Loss: 0.4153299465761914, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1617, Loss: 0.29978555786409233, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1618, Loss: 0.3221575715007141, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1619, Loss: 0.3725099043185932, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1620, Loss: 0.3514793785070982, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1621, Loss: 0.31564527839847845, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1622, Loss: 0.2733762453224328, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1623, Loss: 0.27893384240075914, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1624, Loss: 0.40164265517915865, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1625, Loss: 0.3700855279201607, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1626, Loss: 0.23174222758832996, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1627, Loss: 0.3571740717330333, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1628, Loss: 0.24222453508406822, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1629, Loss: 0.32274061764450584, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1630, Loss: 0.27260239001807296, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1631, Loss: 0.259269910950904, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1632, Loss: 0.2752806711530289, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1633, Loss: 0.4202511763540214, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1634, Loss: 0.5162983878607356, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1635, Loss: 0.3427800068205855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1636, Loss: 0.26847599314344744, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1637, Loss: 0.3023841200292344, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1638, Loss: 0.28702599466627, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1639, Loss: 0.2999800853826208, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1640, Loss: 0.27088512787530683, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1641, Loss: 0.42586125357872984, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1642, Loss: 0.2720617718183259, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1643, Loss: 0.2617450269210827, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1644, Loss: 0.6697983510088703, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1645, Loss: 0.3471572621429856, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1646, Loss: 0.2962238543095782, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1647, Loss: 0.28114100934458214, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1648, Loss: 0.3006998790939527, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1649, Loss: 0.25977667575847424, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1650, Loss: 0.4883682693152203, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1651, Loss: 0.40764208164925225, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1652, Loss: 0.3190663764102152, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1653, Loss: 0.266278427763252, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1654, Loss: 0.37877020518421595, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1655, Loss: 0.3434347759679142, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1656, Loss: 0.3767586988379745, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1657, Loss: 0.25993355002408697, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1658, Loss: 0.39232484912015636, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1659, Loss: 0.29031973765123525, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1660, Loss: 0.2939598258480536, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1661, Loss: 0.29753682137357856, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1662, Loss: 0.38380707123657265, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1663, Loss: 0.2768376362911706, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1664, Loss: 0.3158848909699401, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1665, Loss: 0.25270597028552416, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1666, Loss: 0.43003789842172313, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1667, Loss: 0.4215315150883054, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1668, Loss: 0.39767264169864125, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1669, Loss: 0.3528223767649098, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1670, Loss: 0.2502712746796458, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1671, Loss: 0.510473540262653, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1672, Loss: 0.38041300863046534, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1673, Loss: 0.38250910510870784, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1674, Loss: 0.35442477001790285, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1675, Loss: 0.488523230012452, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1676, Loss: 0.31568615571921427, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1677, Loss: 0.3536005296141814, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1678, Loss: 0.38514004261291424, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1679, Loss: 0.3028965189057071, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1680, Loss: 0.4106666757635648, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1681, Loss: 0.36203308129579015, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1682, Loss: 0.24053593635686304, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1683, Loss: 0.24466216239143848, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1684, Loss: 0.3336799505492791, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1685, Loss: 0.3737295230794012, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1686, Loss: 0.6967443646322663, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1687, Loss: 0.24850359936173175, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1688, Loss: 0.2798441833600531, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1689, Loss: 0.529235894390661, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1690, Loss: 0.3086411379967343, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1691, Loss: 0.4221870769484603, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1692, Loss: 0.332543218845563, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1693, Loss: 0.276634655878124, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1694, Loss: 0.39211262159090976, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1695, Loss: 0.25726728633435475, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1696, Loss: 0.3223834744713434, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1697, Loss: 0.5100669964649218, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1698, Loss: 0.34117040953406086, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1699, Loss: 0.34738714892441663, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1700, Loss: 0.24681029949016917, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1701, Loss: 0.30295552880766785, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1702, Loss: 0.5104345785593368, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1703, Loss: 0.4409756269829045, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1704, Loss: 0.26360241749906194, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1705, Loss: 0.24390650560886026, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1706, Loss: 0.4564736020876556, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1707, Loss: 0.35702049235811595, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1708, Loss: 0.350130187270702, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1709, Loss: 0.23486630437567893, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1710, Loss: 0.2615411935212006, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1711, Loss: 0.25987817634023536, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1712, Loss: 0.5390917713065676, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1713, Loss: 0.3308488246018197, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1714, Loss: 0.2991558561050315, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1715, Loss: 0.4116828759490657, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1716, Loss: 0.3486768700868448, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1717, Loss: 0.24648781055154068, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1718, Loss: 0.41350871088404584, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1719, Loss: 0.42029482582381417, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1720, Loss: 0.3984771714265929, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1721, Loss: 0.41185999115487265, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1722, Loss: 0.3266389778814381, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1723, Loss: 0.31648170401058573, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1724, Loss: 0.2562040380493873, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1725, Loss: 0.30540148030083797, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1726, Loss: 0.26525753725393303, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1727, Loss: 0.37604903436657233, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1728, Loss: 0.2709532908871271, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1729, Loss: 0.32614815035207856, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1730, Loss: 0.3818649816800729, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1731, Loss: 0.28928788923714976, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1732, Loss: 0.33098733386982887, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1733, Loss: 0.6044212473157298, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1734, Loss: 0.32268325692116884, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1735, Loss: 0.35721434105463246, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1736, Loss: 0.44410315665066247, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1737, Loss: 0.3029960303543, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1738, Loss: 0.25830139632149496, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1739, Loss: 0.38628095303734, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1740, Loss: 0.26913330053644063, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1741, Loss: 0.30152968801859037, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1742, Loss: 0.2470511170088694, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1743, Loss: 0.38030832383201535, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1744, Loss: 0.4910665307210599, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1745, Loss: 0.5146301016974922, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1746, Loss: 0.2441331507576662, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1747, Loss: 0.2840525687737016, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1748, Loss: 0.2876389683097341, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1749, Loss: 0.27668157084297773, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1750, Loss: 0.3969914101249945, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1751, Loss: 0.25533299419729766, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1752, Loss: 0.3318284776276467, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1753, Loss: 0.2546136722720292, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1754, Loss: 0.31367010191621814, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1755, Loss: 0.31593858200278585, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1756, Loss: 0.2501497979968855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1757, Loss: 0.40680598481286867, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1758, Loss: 0.36309229095188167, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1759, Loss: 0.28867431344615085, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1760, Loss: 0.3537996992608424, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1761, Loss: 0.25635424256432476, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1762, Loss: 0.43440350639952896, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1763, Loss: 0.5834173805808233, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1764, Loss: 0.2576724534680574, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1765, Loss: 0.33550368883585574, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1766, Loss: 0.2658772058323932, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1767, Loss: 0.2908719942604479, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1768, Loss: 0.45654179573304043, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1769, Loss: 0.41822039496623253, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1770, Loss: 0.3327122395437288, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1771, Loss: 0.445282045561454, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1772, Loss: 0.23580015040213465, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1773, Loss: 0.347684366819504, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1774, Loss: 0.41832133256576165, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1775, Loss: 0.29382908693208987, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1776, Loss: 0.3712186330956826, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1777, Loss: 0.27786100082715864, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1778, Loss: 0.2704243039885224, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1779, Loss: 0.25800398981542855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1780, Loss: 0.23217073738632305, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1781, Loss: 0.32304975995031476, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1782, Loss: 0.37756794680552863, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1783, Loss: 0.3028381044880973, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1784, Loss: 0.2718808104813704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1785, Loss: 0.4194125312654883, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1786, Loss: 0.345218228003182, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1787, Loss: 0.2729468426580157, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1788, Loss: 0.2833751605820164, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1789, Loss: 0.3226467940620173, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1790, Loss: 0.3497490603043077, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1791, Loss: 0.27548994161489854, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1792, Loss: 0.3440022931217445, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1793, Loss: 0.43300452727197336, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1794, Loss: 0.23649101702523867, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1795, Loss: 0.3210392141022979, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1796, Loss: 0.5798177399538812, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1797, Loss: 0.2639273378892288, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1798, Loss: 0.2604304727524308, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1799, Loss: 0.2673917450990275, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1800, Loss: 0.26691963300992066, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1801, Loss: 0.2556473008141804, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1802, Loss: 0.3592080816051094, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1803, Loss: 0.31716812370238007, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1804, Loss: 0.41459506410556307, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1805, Loss: 0.32552897695276606, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1806, Loss: 0.4480381247879063, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1807, Loss: 0.39305652428237225, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1808, Loss: 0.3634745769444453, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1809, Loss: 0.23605739862005737, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1810, Loss: 0.2657622818798699, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1811, Loss: 0.28454279822615197, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1812, Loss: 0.2917892039593026, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1813, Loss: 0.36907728145401875, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1814, Loss: 0.319184166698125, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1815, Loss: 0.27487668926566206, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1816, Loss: 0.31102813966376947, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1817, Loss: 0.3330480598916897, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1818, Loss: 0.4251344106786858, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1819, Loss: 0.2573710924386875, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1820, Loss: 0.29790645139687855, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1821, Loss: 0.3191860013697533, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1822, Loss: 0.2631771733751979, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1823, Loss: 0.2544915403813361, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1824, Loss: 0.25017404396558074, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1825, Loss: 0.38992495806651256, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1826, Loss: 0.3398283940315974, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1827, Loss: 0.2656509552255423, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1828, Loss: 0.3764827047780961, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1829, Loss: 0.3820125551702847, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1830, Loss: 0.28462433307374546, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1831, Loss: 0.27110835133604205, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1832, Loss: 0.29127496307178147, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1833, Loss: 0.2549461793168261, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1834, Loss: 0.25500439149874743, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1835, Loss: 0.3106241184736758, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1836, Loss: 0.2960278468519592, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1837, Loss: 0.26065563138038944, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1838, Loss: 0.31480092852798447, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1839, Loss: 0.30956772612891925, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1840, Loss: 0.388898788197274, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1841, Loss: 0.4500315948521324, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1842, Loss: 0.36715147870004095, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1843, Loss: 0.24180016655304704, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1844, Loss: 0.2716279596252593, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1845, Loss: 0.30111605860546065, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1846, Loss: 0.38218600850459583, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1847, Loss: 0.272418435116655, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1848, Loss: 0.26461197856717805, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1849, Loss: 0.5201141907132785, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1850, Loss: 0.3093671465684597, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1851, Loss: 0.24466831460191682, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1852, Loss: 0.6128171952231566, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1853, Loss: 0.2483503599173062, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1854, Loss: 0.3210285735094973, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1855, Loss: 0.37378699184442143, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1856, Loss: 0.35749831971094603, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1857, Loss: 0.2802782418171996, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1858, Loss: 0.34017932715300014, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1859, Loss: 0.4902116801390307, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1860, Loss: 0.5842451105388077, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1861, Loss: 0.39844151263145605, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1862, Loss: 0.24370736066713738, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1863, Loss: 0.39081923890867765, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1864, Loss: 0.35496792832084934, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1865, Loss: 0.4070818956929695, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1866, Loss: 0.2719983195130289, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1867, Loss: 0.32696457316956384, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1868, Loss: 0.23882205258523903, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1869, Loss: 0.5640617103309619, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1870, Loss: 0.2898433236219352, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1871, Loss: 0.24925210597840342, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1872, Loss: 0.2786732635266399, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1873, Loss: 0.25913572559080894, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1874, Loss: 0.28693206360394047, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Batch 1875, Loss: 0.679673515751334, Batch Size: 32, Learning Rate: 5.0202973106884255e-05\n",
      "Epoch 13, Updated Learning Rate: 4.267252714085162e-05\n",
      "Epoch 13, Average Loss: 0.33796237085155056, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1, Loss: 0.5010992559355958, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 2, Loss: 0.29408976252338764, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 3, Loss: 0.23816248057125897, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 4, Loss: 0.5360038946153054, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 5, Loss: 0.316934445342765, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 6, Loss: 0.2408152352067742, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 7, Loss: 0.28990922984966266, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 8, Loss: 0.38164408441098996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 9, Loss: 0.23058740043490833, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 10, Loss: 0.49442763473020857, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 11, Loss: 0.2650784404777936, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 12, Loss: 0.5018165197295842, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 13, Loss: 0.3265849055786683, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 14, Loss: 0.4021258930333209, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 15, Loss: 0.4614751721945177, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 16, Loss: 0.3750290983823298, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 17, Loss: 0.35904706926058344, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 18, Loss: 0.322427480409407, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 19, Loss: 0.23868984460995685, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 20, Loss: 0.3245051990059237, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 21, Loss: 0.5437442278155951, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 22, Loss: 0.39375386596492606, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 23, Loss: 0.3011939862777923, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 24, Loss: 0.5310985384409707, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 25, Loss: 0.3908199378265581, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 26, Loss: 0.318456207080865, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 27, Loss: 0.3583341743407985, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 28, Loss: 0.3048419062950932, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 29, Loss: 0.39668863139380106, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 30, Loss: 0.3253341087216506, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 31, Loss: 0.3055128161526672, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 32, Loss: 0.255440139563235, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 33, Loss: 0.2498083250053481, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 34, Loss: 0.30499036137606145, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 35, Loss: 0.2684565210604085, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 36, Loss: 0.30968609667321123, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 37, Loss: 0.3200013712613582, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 38, Loss: 0.3953443937924032, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 39, Loss: 0.31007069839230317, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 40, Loss: 0.39476678039211566, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 41, Loss: 0.2759669965923078, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 42, Loss: 0.2373150841524951, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 43, Loss: 0.5114523840470288, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 44, Loss: 0.3907939565585826, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 45, Loss: 0.2763539818417041, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 46, Loss: 0.4033409222100839, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 47, Loss: 0.27828769970067396, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 48, Loss: 0.3224344471874194, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 49, Loss: 0.548001958258153, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 50, Loss: 0.3649037477938135, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 51, Loss: 0.4030696615111973, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 52, Loss: 0.27061064713690114, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 53, Loss: 0.2649206305998886, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 54, Loss: 0.374458991134456, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 55, Loss: 0.25208132500634944, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 56, Loss: 0.3639694028453015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 57, Loss: 0.4458830447990061, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 58, Loss: 0.2759532438245163, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 59, Loss: 0.26240357965086875, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 60, Loss: 0.5433738301589055, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 61, Loss: 0.3229682905033436, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 62, Loss: 0.31358925626440154, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 63, Loss: 0.3679724897442322, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 64, Loss: 0.30275547124353575, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 65, Loss: 0.30922920667056, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 66, Loss: 0.3851992053352755, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 67, Loss: 0.3711034088896704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 68, Loss: 0.2422933086318856, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 69, Loss: 0.3207556762183768, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 70, Loss: 0.49132664185985364, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 71, Loss: 0.5782965310345921, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 72, Loss: 0.25646736792088776, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 73, Loss: 0.2645328601036197, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 74, Loss: 0.467785449135403, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 75, Loss: 0.30748362383250005, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 76, Loss: 0.3042661665923099, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 77, Loss: 0.260726148612697, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 78, Loss: 0.36379263702711245, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 79, Loss: 0.25732968329605105, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 80, Loss: 0.3636626967118144, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 81, Loss: 0.37870699426214816, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 82, Loss: 0.4686681522807602, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 83, Loss: 0.2409079375316025, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 84, Loss: 0.24785931954689333, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 85, Loss: 0.5004400876744246, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 86, Loss: 0.3903554540559167, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 87, Loss: 0.3283578902276558, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 88, Loss: 0.2904803951788547, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 89, Loss: 0.3179628746136951, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 90, Loss: 0.28120177411128966, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 91, Loss: 0.28195885618044764, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 92, Loss: 0.24345778365769813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 93, Loss: 0.305438259137007, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 94, Loss: 0.23212232392442508, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 95, Loss: 0.5091529283555858, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 96, Loss: 0.38658769747713534, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 97, Loss: 0.24165570817512255, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 98, Loss: 0.26543495108419307, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 99, Loss: 0.29176617552856354, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 100, Loss: 0.3343339761433908, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 101, Loss: 0.31013305057894314, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 102, Loss: 0.28351432324285847, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 103, Loss: 0.38707537372258716, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 104, Loss: 0.27695370332016855, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 105, Loss: 0.30905588824695596, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 106, Loss: 0.41218450831955955, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 107, Loss: 0.29437721858455973, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 108, Loss: 0.2844839772600989, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 109, Loss: 0.35071746355826905, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 110, Loss: 0.28005699691370656, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 111, Loss: 0.2971234480694074, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 112, Loss: 0.3117368004017833, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 113, Loss: 0.30214543248200576, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 114, Loss: 0.41497922592779, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 115, Loss: 0.27063249211423573, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 116, Loss: 0.29093750762774334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 117, Loss: 0.4902401857859704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 118, Loss: 0.26171579254966804, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 119, Loss: 0.424909488537894, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 120, Loss: 0.41004888284823104, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 121, Loss: 0.2930355685662172, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 122, Loss: 0.34517576303266595, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 123, Loss: 0.4596929185700378, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 124, Loss: 0.29683584683334063, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 125, Loss: 0.4280927496838508, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 126, Loss: 0.43093254988805285, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 127, Loss: 0.343901172134881, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 128, Loss: 0.392452588805124, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 129, Loss: 0.5140350726374743, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 130, Loss: 0.507470080271577, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 131, Loss: 0.28221402981344834, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 132, Loss: 0.2803936861745876, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 133, Loss: 0.459619229366236, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 134, Loss: 0.3111987373041971, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 135, Loss: 0.32543279428605765, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 136, Loss: 0.44108606385114996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 137, Loss: 0.28211355204178, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 138, Loss: 0.26955037663670983, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 139, Loss: 0.29224069376709516, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 140, Loss: 0.31457155049718566, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 141, Loss: 0.3027340760687977, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 142, Loss: 0.3465161006993161, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 143, Loss: 0.5128543695370946, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 144, Loss: 0.41187416628753526, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 145, Loss: 0.26116742655766517, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 146, Loss: 0.32041318017891995, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 147, Loss: 0.6211189145592731, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 148, Loss: 0.321195585488465, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 149, Loss: 0.28863262291310227, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 150, Loss: 0.35187233809553053, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 151, Loss: 0.3539011971061502, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 152, Loss: 0.32010616427322364, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 153, Loss: 0.2838902006319774, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 154, Loss: 0.28462498631573574, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 155, Loss: 0.2411096963548056, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 156, Loss: 0.4172007589600714, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 157, Loss: 0.25790217116858627, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 158, Loss: 0.27219788959592595, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 159, Loss: 0.22773986170435703, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 160, Loss: 0.40315671793070984, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 161, Loss: 0.3504993704271063, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 162, Loss: 0.24204041123744952, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 163, Loss: 0.32646869686004165, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 164, Loss: 0.46738398865000946, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 165, Loss: 0.27018508951903364, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 166, Loss: 0.32052142567661634, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 167, Loss: 0.24703792238385502, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 168, Loss: 0.2924377389833463, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 169, Loss: 0.27926958801608637, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 170, Loss: 0.2518918367806168, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 171, Loss: 0.3477295163695492, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 172, Loss: 0.3147163242199664, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 173, Loss: 0.31705902590195717, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 174, Loss: 0.27941865061392446, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 175, Loss: 0.23872061066715033, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 176, Loss: 0.2690987561730101, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 177, Loss: 0.43370816161471104, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 178, Loss: 0.2526621921003948, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 179, Loss: 0.2612091937438701, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 180, Loss: 0.33456775719852666, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 181, Loss: 0.3846972120665183, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 182, Loss: 0.27776210441211424, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 183, Loss: 0.4538691870078553, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 184, Loss: 0.3898146648735491, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 185, Loss: 0.3082033375784471, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 186, Loss: 0.31300056502034335, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 187, Loss: 0.524955445534804, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 188, Loss: 0.3903815315357372, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 189, Loss: 0.6600171388416263, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 190, Loss: 0.27737779363570164, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 191, Loss: 0.3464369581674132, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 192, Loss: 0.2984762022255695, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 193, Loss: 0.27759035349148226, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 194, Loss: 0.26035049123232196, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 195, Loss: 0.35061689021735865, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 196, Loss: 0.557967530510763, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 197, Loss: 0.5163013299819239, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 198, Loss: 0.2583683572711316, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 199, Loss: 0.5434562281700641, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 200, Loss: 0.30789037375336437, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 201, Loss: 0.3316609150092015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 202, Loss: 0.2681637812082058, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 203, Loss: 0.39512310423267455, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 204, Loss: 0.3992230569711661, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 205, Loss: 0.2925069732520294, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 206, Loss: 0.3493826015434125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 207, Loss: 0.4642501995507591, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 208, Loss: 0.3148614174066538, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 209, Loss: 0.6028021910664574, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 210, Loss: 0.273478665347886, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 211, Loss: 0.39219914639353615, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 212, Loss: 0.5253359918623319, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 213, Loss: 0.3054750274241577, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 214, Loss: 0.47617639389795297, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 215, Loss: 0.27265080977979256, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 216, Loss: 0.36643937750918165, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 217, Loss: 0.3435096870863359, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 218, Loss: 0.3297892651184493, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 219, Loss: 0.2495110959173012, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 220, Loss: 0.3090544248862156, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 221, Loss: 0.24684845511686293, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 222, Loss: 0.33253073724440463, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 223, Loss: 0.46863273118508464, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 224, Loss: 0.2677544445922837, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 225, Loss: 0.4221254862096342, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 226, Loss: 0.5375954505377355, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 227, Loss: 0.4866571081452466, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 228, Loss: 0.5960878349920343, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 229, Loss: 0.26705739531587563, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 230, Loss: 0.24970652945810765, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 231, Loss: 0.3283167497618335, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 232, Loss: 0.6626372915226579, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 233, Loss: 0.3123248073198843, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 234, Loss: 0.48327874866418075, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 235, Loss: 0.3282676625701587, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 236, Loss: 0.2597899953285131, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 237, Loss: 0.36896579091236603, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 238, Loss: 0.25709998590581995, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 239, Loss: 0.2906817719378021, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 240, Loss: 0.3581319652687628, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 241, Loss: 0.24911939711433193, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 242, Loss: 0.2585832858166551, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 243, Loss: 0.27646301456491135, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 244, Loss: 0.329605608163205, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 245, Loss: 0.4304967015749501, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 246, Loss: 0.37152053991625805, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 247, Loss: 0.31622858731129866, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 248, Loss: 0.2541732779675847, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 249, Loss: 0.4145064016735041, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 250, Loss: 0.3868439219431168, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 251, Loss: 0.34080508366126966, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 252, Loss: 0.40820372120616155, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 253, Loss: 0.23701544999909588, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 254, Loss: 0.2321360183633392, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 255, Loss: 0.26140803545862334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 256, Loss: 0.24538270749606672, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 257, Loss: 0.43363477129515937, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 258, Loss: 0.2961830790855693, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 259, Loss: 0.29779084040647613, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 260, Loss: 0.27473015842612164, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 261, Loss: 0.25086790477404397, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 262, Loss: 0.2889597148269577, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 263, Loss: 0.41584054331417464, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 264, Loss: 0.3015210547975522, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 265, Loss: 0.2804566975567641, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 266, Loss: 0.27750422987822104, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 267, Loss: 0.3271432195518025, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 268, Loss: 0.3505187761135763, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 269, Loss: 0.3821376574548866, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 270, Loss: 0.4234646783611614, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 271, Loss: 0.4156478145047079, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 272, Loss: 0.2684134991756457, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 273, Loss: 0.3229931140022656, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 274, Loss: 0.411029875437366, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 275, Loss: 0.5515292876761502, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 276, Loss: 0.3470649578563954, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 277, Loss: 0.28951846483188465, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 278, Loss: 0.32077243818890755, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 279, Loss: 0.2571659249006214, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 280, Loss: 0.2978272695492298, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 281, Loss: 0.23689908986786704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 282, Loss: 0.2769081063788429, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 283, Loss: 0.34120387567004756, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 284, Loss: 0.29386591095886344, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 285, Loss: 0.27503371496995643, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 286, Loss: 0.25684679448413267, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 287, Loss: 0.39567815094724235, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 288, Loss: 0.4815044877496314, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 289, Loss: 0.2509468980948417, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 290, Loss: 0.33097337666861143, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 291, Loss: 0.4361348730985124, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 292, Loss: 0.25290542695470936, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 293, Loss: 0.2654775927377353, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 294, Loss: 0.3187631911525946, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 295, Loss: 0.23765553019672392, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 296, Loss: 0.24861319895177608, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 297, Loss: 0.25695767914320267, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 298, Loss: 0.259266328020347, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 299, Loss: 0.2388928447065526, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 300, Loss: 0.4367974982789511, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 301, Loss: 0.24269500746879766, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 302, Loss: 0.25503606184046085, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 303, Loss: 0.49166602247104363, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 304, Loss: 0.30119488912836834, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 305, Loss: 0.2351625666827749, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 306, Loss: 0.33771309111119785, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 307, Loss: 0.45580293156471596, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 308, Loss: 0.2978943986406831, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 309, Loss: 0.335137268079749, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 310, Loss: 0.2651134631058245, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 311, Loss: 0.3024542706065822, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 312, Loss: 0.27773962772298716, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 313, Loss: 0.38935085825011695, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 314, Loss: 0.2733658620614656, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 315, Loss: 0.2411386583108459, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 316, Loss: 0.2880223285066851, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 317, Loss: 0.3804620106378639, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 318, Loss: 0.3510601628956652, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 319, Loss: 0.34974506696837815, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 320, Loss: 0.2724236818303776, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 321, Loss: 0.40167647371338333, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 322, Loss: 0.3976665771049438, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 323, Loss: 0.3405737719540932, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 324, Loss: 0.2471914083668274, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 325, Loss: 0.31049925413714485, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 326, Loss: 0.34678341377648725, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 327, Loss: 0.25568320991974247, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 328, Loss: 0.2703431592104786, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 329, Loss: 0.31408272488910677, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 330, Loss: 0.25340241277720704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 331, Loss: 0.3611507876408795, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 332, Loss: 0.4766020827136318, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 333, Loss: 0.28841057578882484, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 334, Loss: 0.3419834660846638, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 335, Loss: 0.34759945907410217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 336, Loss: 0.3138606621574258, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 337, Loss: 0.2778275753616278, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 338, Loss: 0.4831940522260515, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 339, Loss: 0.3606076478970812, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 340, Loss: 0.23713962308371433, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 341, Loss: 0.5504221236587417, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 342, Loss: 0.40320170758278584, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 343, Loss: 0.29723189595830374, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 344, Loss: 0.2689914685885453, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 345, Loss: 0.2581011853230347, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 346, Loss: 0.34904894732069924, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 347, Loss: 0.3877583915368178, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 348, Loss: 0.2563451936053074, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 349, Loss: 0.3976068261905275, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 350, Loss: 0.4227794893273191, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 351, Loss: 0.2710597892812113, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 352, Loss: 0.23769967548738585, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 353, Loss: 0.3216297657808761, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 354, Loss: 0.33424338532806047, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 355, Loss: 0.2592590985166672, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 356, Loss: 0.3445303890496429, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 357, Loss: 0.25621821179510273, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 358, Loss: 0.31306445827269935, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 359, Loss: 0.3136513943543324, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 360, Loss: 0.2801237929341088, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 361, Loss: 0.33392825156913863, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 362, Loss: 0.37287321276619423, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 363, Loss: 0.29577553692903763, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 364, Loss: 0.2494306909466071, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 365, Loss: 0.2916776935246133, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 366, Loss: 0.24945241614123453, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 367, Loss: 0.30136036661416177, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 368, Loss: 0.2483952342902469, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 369, Loss: 0.3587195243104798, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 370, Loss: 0.2895442355180754, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 371, Loss: 0.4178022046790726, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 372, Loss: 0.3498164675184613, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 373, Loss: 0.46152416510504213, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 374, Loss: 0.24529685683696326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 375, Loss: 0.2961276043551566, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 376, Loss: 0.24274406730276965, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 377, Loss: 0.7247660154654352, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 378, Loss: 0.26276191571432084, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 379, Loss: 0.25283475209817086, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 380, Loss: 0.30121205456437405, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 381, Loss: 0.3396629365142825, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 382, Loss: 0.4586279497971658, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 383, Loss: 0.7333920900988757, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 384, Loss: 0.5491036800072383, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 385, Loss: 0.3315775384019377, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 386, Loss: 0.26437940315516456, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 387, Loss: 0.3450081991769111, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 388, Loss: 0.465089275198605, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 389, Loss: 0.3733685749763389, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 390, Loss: 0.2444872125167222, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 391, Loss: 0.28722333046537724, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 392, Loss: 0.29721777562271845, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 393, Loss: 0.29194812376836854, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 394, Loss: 0.32599747884632807, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 395, Loss: 0.2742081412413391, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 396, Loss: 0.38661762114613196, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 397, Loss: 0.32816214736302596, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 398, Loss: 0.2577415970228789, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 399, Loss: 0.2458690991915874, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 400, Loss: 0.27025610519730847, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 401, Loss: 0.34339520481434893, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 402, Loss: 0.26867987716001185, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 403, Loss: 0.31016223671630616, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 404, Loss: 0.3045166058733759, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 405, Loss: 0.3781739383547501, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 406, Loss: 0.3388767465227307, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 407, Loss: 0.31431818134809814, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 408, Loss: 0.3278718584214905, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 409, Loss: 0.2526651160926414, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 410, Loss: 0.3523776002717587, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 411, Loss: 0.40546781655425146, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 412, Loss: 0.2567019627880117, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 413, Loss: 0.3779056182181362, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 414, Loss: 0.5584252901056562, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 415, Loss: 0.47238792061043033, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 416, Loss: 0.37156565715708156, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 417, Loss: 0.26027151202226584, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 418, Loss: 0.257874984699482, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 419, Loss: 0.3081660944527217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 420, Loss: 0.4349087358903918, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 421, Loss: 0.29661337156879275, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 422, Loss: 0.35020077716745307, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 423, Loss: 0.34007609986589854, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 424, Loss: 0.2403644989958746, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 425, Loss: 0.3585590746970875, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 426, Loss: 0.37119382039930326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 427, Loss: 0.24214570846391298, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 428, Loss: 0.2643209830621907, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 429, Loss: 0.29310061736956217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 430, Loss: 0.4886724764058247, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 431, Loss: 0.27357995607037405, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 432, Loss: 0.4437896616667097, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 433, Loss: 0.2349517306752534, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 434, Loss: 0.3965176155323072, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 435, Loss: 0.33095569154622934, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 436, Loss: 0.40968938098254093, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 437, Loss: 0.25643912143703, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 438, Loss: 0.2837863217491008, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 439, Loss: 0.26863441951545963, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 440, Loss: 0.238882251310868, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 441, Loss: 0.3422532973808121, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 442, Loss: 0.29099506973448297, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 443, Loss: 0.3875919126853309, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 444, Loss: 0.2870098877307878, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 445, Loss: 0.3066910512044231, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 446, Loss: 0.5139960447963652, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 447, Loss: 0.2829493409225008, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 448, Loss: 0.36244292969429687, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 449, Loss: 0.26253263814463307, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 450, Loss: 0.4290574265515243, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 451, Loss: 0.27272423981998634, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 452, Loss: 0.31255226069024283, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 453, Loss: 0.33960692150145955, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 454, Loss: 0.31091763384764404, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 455, Loss: 0.23599282192152637, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 456, Loss: 0.25212124539168945, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 457, Loss: 0.2633105672719326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 458, Loss: 0.2533934903690784, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 459, Loss: 0.3378274861940282, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 460, Loss: 0.32314881293205533, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 461, Loss: 0.2373673443511317, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 462, Loss: 0.28286832084112884, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 463, Loss: 0.2783330424064638, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 464, Loss: 0.32507670559514024, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 465, Loss: 0.4337487861081477, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 466, Loss: 0.23975125775441086, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 467, Loss: 0.2814778884485207, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 468, Loss: 0.2776044487227474, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 469, Loss: 0.24439173075935283, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 470, Loss: 0.5802562044637629, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 471, Loss: 0.281732486822285, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 472, Loss: 0.501212449640287, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 473, Loss: 0.37301714783955253, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 474, Loss: 0.6194981068165004, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 475, Loss: 0.336515072951342, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 476, Loss: 0.2503112056716791, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 477, Loss: 0.43899281596746037, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 478, Loss: 0.5200860526184616, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 479, Loss: 0.5177184759963622, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 480, Loss: 0.4008289412894105, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 481, Loss: 0.3037413004328286, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 482, Loss: 0.2774997857650757, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 483, Loss: 0.5858244997075536, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 484, Loss: 0.25434420990403334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 485, Loss: 0.6523379932956996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 486, Loss: 0.2557618814595402, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 487, Loss: 0.37553967803233723, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 488, Loss: 0.6701901525969696, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 489, Loss: 0.3644101680123989, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 490, Loss: 0.2724119176303334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 491, Loss: 0.3289919974603493, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 492, Loss: 0.2932995864536957, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 493, Loss: 0.4529970916765673, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 494, Loss: 0.27556682189340115, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 495, Loss: 0.4681292546846574, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 496, Loss: 0.2531378883616995, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 497, Loss: 0.3042601215111683, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 498, Loss: 0.24074760640737924, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 499, Loss: 0.27874335138018724, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 500, Loss: 0.32356635849760085, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 501, Loss: 0.2740034326445375, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 502, Loss: 0.28521849325071025, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 503, Loss: 0.5497363367991074, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 504, Loss: 0.2895533604937255, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 505, Loss: 0.36629968851674055, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 506, Loss: 0.32897489690811654, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 507, Loss: 0.3105118780855484, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 508, Loss: 0.3835580080158193, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 509, Loss: 0.3499377702971507, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 510, Loss: 0.37259246649935573, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 511, Loss: 0.3422287460376433, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 512, Loss: 0.2815267642552886, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 513, Loss: 0.29393503622876427, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 514, Loss: 0.336365508994695, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 515, Loss: 0.2686656216342719, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 516, Loss: 0.28211842019135813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 517, Loss: 0.32056921991208587, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 518, Loss: 0.3277443042034952, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 519, Loss: 0.2769602367900653, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 520, Loss: 0.264640729020946, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 521, Loss: 0.2981848344289105, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 522, Loss: 0.3399987148039023, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 523, Loss: 0.2545610743103576, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 524, Loss: 0.3889158186542879, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 525, Loss: 0.2957972091229663, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 526, Loss: 0.40505902128808624, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 527, Loss: 0.29618738542461515, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 528, Loss: 0.2571463510047092, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 529, Loss: 0.35309295968424137, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 530, Loss: 0.29825085460955536, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 531, Loss: 0.4470046061009636, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 532, Loss: 0.32935740373718425, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 533, Loss: 0.40624420761997154, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 534, Loss: 0.2847732853403788, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 535, Loss: 0.29582190055138435, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 536, Loss: 0.6223900481882059, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 537, Loss: 0.41713155017936, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 538, Loss: 0.33526943073108817, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 539, Loss: 0.3231554363410462, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 540, Loss: 0.2920464542895144, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 541, Loss: 0.2809604074195342, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 542, Loss: 0.27682596880957044, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 543, Loss: 0.4410177978369234, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 544, Loss: 0.2619499684242908, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 545, Loss: 0.3470976615629061, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 546, Loss: 0.4011007316373828, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 547, Loss: 0.23331393009588755, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 548, Loss: 0.2966029679916239, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 549, Loss: 0.26922138837833876, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 550, Loss: 0.38207523699664375, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 551, Loss: 0.3101791882104189, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 552, Loss: 0.6026889974532406, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 553, Loss: 0.24448914941606986, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 554, Loss: 0.31066135488910024, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 555, Loss: 0.3265779260446246, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 556, Loss: 0.4548414944643876, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 557, Loss: 0.2927357070624358, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 558, Loss: 0.3835191452501776, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 559, Loss: 0.30963094643948463, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 560, Loss: 0.3328714644166756, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 561, Loss: 0.31349097400575493, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 562, Loss: 0.2610226214811762, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 563, Loss: 0.433770917282537, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 564, Loss: 0.23711245925850177, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 565, Loss: 0.4089564728939471, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 566, Loss: 0.4151649753058567, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 567, Loss: 0.27537487673000616, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 568, Loss: 0.36842206943517847, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 569, Loss: 0.47514246251053194, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 570, Loss: 0.32633946228186933, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 571, Loss: 0.2617270207216632, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 572, Loss: 0.2384360736308397, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 573, Loss: 0.3071447438228044, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 574, Loss: 0.5106608092469935, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 575, Loss: 0.30089355787550864, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 576, Loss: 0.24339891314609388, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 577, Loss: 0.2689439368112454, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 578, Loss: 0.23582393258256285, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 579, Loss: 0.2821885308105061, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 580, Loss: 0.39788435437791914, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 581, Loss: 0.4188131009468934, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 582, Loss: 0.4726187371437569, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 583, Loss: 0.33592922171667167, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 584, Loss: 0.6205607924309358, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 585, Loss: 0.2392538027886867, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 586, Loss: 0.34676159706333565, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 587, Loss: 0.41314734985956897, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 588, Loss: 0.6281630877946484, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 589, Loss: 0.2798770132132107, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 590, Loss: 0.28062720739085134, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 591, Loss: 0.32317359972527915, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 592, Loss: 0.36405192737154424, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 593, Loss: 0.5046830568853156, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 594, Loss: 0.3669011364962406, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 595, Loss: 0.2877827949469722, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 596, Loss: 0.3365415252988104, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 597, Loss: 0.2743743407443329, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 598, Loss: 0.38225247792781114, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 599, Loss: 0.2614407612752609, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 600, Loss: 0.3925970703584791, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 601, Loss: 0.44007596080082445, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 602, Loss: 0.24510362782079478, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 603, Loss: 0.36707914860881485, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 604, Loss: 0.32611400988794526, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 605, Loss: 0.42190776697087695, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 606, Loss: 0.286044641065528, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 607, Loss: 0.2564540600349018, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 608, Loss: 0.25550374678823073, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 609, Loss: 0.3691541102600725, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 610, Loss: 0.37002180881751534, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 611, Loss: 0.2463799454942996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 612, Loss: 0.295084292833656, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 613, Loss: 0.32841264938514325, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 614, Loss: 0.28847879454956227, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 615, Loss: 0.4197498264027506, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 616, Loss: 0.40844781889865545, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 617, Loss: 0.2827490592848279, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 618, Loss: 0.27682814018716984, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 619, Loss: 0.23632368305824972, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 620, Loss: 0.3113178183546145, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 621, Loss: 0.4003825982301292, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 622, Loss: 0.323128291120241, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 623, Loss: 0.2652601630288047, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 624, Loss: 0.45443310030711004, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 625, Loss: 0.25092717618455873, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 626, Loss: 0.2532058314830899, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 627, Loss: 0.2602135076094729, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 628, Loss: 0.26697745006477125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 629, Loss: 0.27395126507887024, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 630, Loss: 0.2520298799899894, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 631, Loss: 0.2742882814755719, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 632, Loss: 0.30927058715966, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 633, Loss: 0.29208535309900036, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 634, Loss: 0.27029083439316426, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 635, Loss: 0.5046620648902249, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 636, Loss: 0.3429687265387328, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 637, Loss: 0.2777346025796525, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 638, Loss: 0.31358692048488945, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 639, Loss: 0.36817208758430503, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 640, Loss: 0.2814217248296057, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 641, Loss: 0.25131186046693477, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 642, Loss: 0.36118003976667135, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 643, Loss: 0.27632164197506026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 644, Loss: 0.4059642055957507, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 645, Loss: 0.38532723234690636, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 646, Loss: 0.32093190598599314, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 647, Loss: 0.22758187963170867, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 648, Loss: 0.3495749991886986, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 649, Loss: 0.3346940389419189, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 650, Loss: 0.2329995088344577, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 651, Loss: 0.38027706668043587, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 652, Loss: 0.23587544712989802, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 653, Loss: 0.26477515213735964, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 654, Loss: 0.25438195718380724, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 655, Loss: 0.27276938350653496, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 656, Loss: 0.45200663617483283, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 657, Loss: 0.32037586433851695, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 658, Loss: 0.5472175038251607, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 659, Loss: 0.3028399391171118, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 660, Loss: 0.23705197521950647, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 661, Loss: 0.2544769905032824, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 662, Loss: 0.35280739147992446, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 663, Loss: 0.34833488891056347, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 664, Loss: 0.34075021114230125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 665, Loss: 0.25932853011531365, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 666, Loss: 0.44567485251477745, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 667, Loss: 0.45970206280384784, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 668, Loss: 0.2615384215979515, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 669, Loss: 0.3259391003369944, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 670, Loss: 0.2573711793849187, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 671, Loss: 0.2652126611992397, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 672, Loss: 0.3321502689436757, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 673, Loss: 0.28627924834180934, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 674, Loss: 0.3392373082122461, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 675, Loss: 0.359683201845579, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 676, Loss: 0.3231153884775996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 677, Loss: 0.32584398913816326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 678, Loss: 0.27755838637417846, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 679, Loss: 0.2480607416411771, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 680, Loss: 0.2735534766672386, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 681, Loss: 0.44588572901929757, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 682, Loss: 0.41043178641411027, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 683, Loss: 0.3222539064954189, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 684, Loss: 0.29636661384242524, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 685, Loss: 0.2866928125055592, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 686, Loss: 0.3534274429320067, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 687, Loss: 0.25675660126619726, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 688, Loss: 0.38405199026059333, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 689, Loss: 0.3028586112428155, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 690, Loss: 0.3294587179313306, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 691, Loss: 0.320470726954153, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 692, Loss: 0.3339319798406959, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 693, Loss: 0.33044906447626554, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 694, Loss: 0.40930683102825793, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 695, Loss: 0.5176488709604813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 696, Loss: 0.3046271461451743, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 697, Loss: 0.5059885596120559, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 698, Loss: 0.31734845524352057, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 699, Loss: 0.2780373193307234, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 700, Loss: 0.31321748140103045, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 701, Loss: 0.42757913136736503, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 702, Loss: 0.31862332715879366, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 703, Loss: 0.3806809438041687, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 704, Loss: 0.44499123723986345, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 705, Loss: 0.41648126703400945, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 706, Loss: 0.35955537912152313, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 707, Loss: 0.2320966029872044, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 708, Loss: 0.32681263449122616, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 709, Loss: 0.3980981429827758, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 710, Loss: 0.24603117350176512, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 711, Loss: 0.5282867446209979, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 712, Loss: 0.4772190250227124, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 713, Loss: 0.42448823008282605, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 714, Loss: 0.41905502936391636, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 715, Loss: 0.2549759590755071, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 716, Loss: 0.41975463514571326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 717, Loss: 0.29434230708393316, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 718, Loss: 0.7236413077873433, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 719, Loss: 0.29859916316431884, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 720, Loss: 0.27509687121444154, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 721, Loss: 0.4200559683617068, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 722, Loss: 0.32619609214454803, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 723, Loss: 0.43749822828623386, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 724, Loss: 0.23225399504419575, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 725, Loss: 0.3486459023157971, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 726, Loss: 0.2729463598096964, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 727, Loss: 0.46067545569963686, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 728, Loss: 0.2457062675664474, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 729, Loss: 0.3084907139925679, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 730, Loss: 0.37689799166369875, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 731, Loss: 0.2920682021753749, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 732, Loss: 0.28107797312100274, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 733, Loss: 0.33871327962097997, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 734, Loss: 0.2688879005803926, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 735, Loss: 0.27876790239952026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 736, Loss: 0.25622245083111556, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 737, Loss: 0.27995429349376116, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 738, Loss: 0.5021651650060679, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 739, Loss: 0.5843432826241033, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 740, Loss: 0.23658710089609095, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 741, Loss: 0.39633268671020205, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 742, Loss: 0.42034948077166023, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 743, Loss: 0.44755364042794865, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 744, Loss: 0.31954136716964854, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 745, Loss: 0.2999587837734113, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 746, Loss: 0.525570717542778, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 747, Loss: 0.37372077838183015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 748, Loss: 0.33894597069126803, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 749, Loss: 0.3466509253085003, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 750, Loss: 0.2594335430225741, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 751, Loss: 0.29286967163508004, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 752, Loss: 0.35549674454086677, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 753, Loss: 0.4123886586322938, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 754, Loss: 0.400991738745454, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 755, Loss: 0.28830287922652914, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 756, Loss: 0.35249641596953296, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 757, Loss: 0.30917610854203814, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 758, Loss: 0.43685202945765145, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 759, Loss: 0.2629373205199658, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 760, Loss: 0.3195607352358947, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 761, Loss: 0.28174246970564293, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 762, Loss: 0.27772282100006973, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 763, Loss: 0.28205115040694384, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 764, Loss: 0.23479402650724682, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 765, Loss: 0.4176372102761806, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 766, Loss: 0.24565510454969838, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 767, Loss: 0.5415187366354749, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 768, Loss: 0.429402716571722, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 769, Loss: 0.5084291179509066, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 770, Loss: 0.33438775974768287, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 771, Loss: 0.34267531558381364, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 772, Loss: 0.2942984362014279, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 773, Loss: 0.24902909102159465, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 774, Loss: 0.29519592546614415, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 775, Loss: 0.31250126357010327, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 776, Loss: 0.3014233340662379, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 777, Loss: 0.25113961772883653, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 778, Loss: 0.25670857474957304, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 779, Loss: 0.348652732962759, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 780, Loss: 0.243452979171614, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 781, Loss: 0.6824831324876585, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 782, Loss: 0.5157602590669149, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 783, Loss: 0.3228994498490626, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 784, Loss: 0.31695972725917215, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 785, Loss: 0.293758504887519, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 786, Loss: 0.4355889002828412, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 787, Loss: 0.343332332578026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 788, Loss: 0.23411145665476182, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 789, Loss: 0.46063482921752713, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 790, Loss: 0.40870180049456295, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 791, Loss: 0.23184680468320287, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 792, Loss: 0.3701659073025764, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 793, Loss: 0.4492166556970837, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 794, Loss: 0.2873787187968842, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 795, Loss: 0.27749801233300464, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 796, Loss: 0.40152267883494636, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 797, Loss: 0.37874280728394233, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 798, Loss: 0.2922943741486899, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 799, Loss: 0.266766424781982, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 800, Loss: 0.3530457536506082, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 801, Loss: 0.3229070525200821, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 802, Loss: 0.2571260249777936, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 803, Loss: 0.2978173941923217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 804, Loss: 0.35178307569657064, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 805, Loss: 0.2509963124923279, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 806, Loss: 0.2837743238198866, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 807, Loss: 0.4924489768601114, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 808, Loss: 0.29321818698214747, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 809, Loss: 0.2515822951482104, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 810, Loss: 0.3310990625408214, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 811, Loss: 0.5422900867400683, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 812, Loss: 0.274543298142809, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 813, Loss: 0.26090805105493553, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 814, Loss: 0.2354769290551485, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 815, Loss: 0.2506032884120532, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 816, Loss: 0.40459709370639163, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 817, Loss: 0.3269631788039926, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 818, Loss: 0.2873647990634818, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 819, Loss: 0.2827275585365118, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 820, Loss: 0.2730212105627532, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 821, Loss: 0.27872665357820053, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 822, Loss: 0.278440793257128, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 823, Loss: 0.3323804881311774, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 824, Loss: 0.41367612083509026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 825, Loss: 0.2789407619038977, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 826, Loss: 0.4436823037023192, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 827, Loss: 0.35745920436316503, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 828, Loss: 0.28161437893265756, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 829, Loss: 0.3232861126389049, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 830, Loss: 0.3638543762209756, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 831, Loss: 0.45110430793819845, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 832, Loss: 0.2959123870714162, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 833, Loss: 0.3117962052935837, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 834, Loss: 0.37107467757125034, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 835, Loss: 0.49380152840545977, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 836, Loss: 0.31672546504978255, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 837, Loss: 0.3848842381836184, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 838, Loss: 0.27477591662808215, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 839, Loss: 0.28116303608157517, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 840, Loss: 0.2637294478650534, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 841, Loss: 0.30770451656541065, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 842, Loss: 0.35300926981626324, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 843, Loss: 0.40399135433794264, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 844, Loss: 0.3023638308853937, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 845, Loss: 0.4180179862632277, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 846, Loss: 0.3868764410352298, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 847, Loss: 0.3280235737237868, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 848, Loss: 0.2936529823968378, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 849, Loss: 0.29197588456878537, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 850, Loss: 0.6234276916788801, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 851, Loss: 0.24412533701485123, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 852, Loss: 0.3267252495012261, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 853, Loss: 0.3735417300955808, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 854, Loss: 0.3233033544490461, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 855, Loss: 0.24359148187493404, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 856, Loss: 0.3656652921938861, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 857, Loss: 0.8347725609987277, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 858, Loss: 0.3104449119064118, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 859, Loss: 0.2755355101417401, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 860, Loss: 0.25130632841742473, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 861, Loss: 0.6509940322720548, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 862, Loss: 0.2678797007946021, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 863, Loss: 0.4234123765785362, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 864, Loss: 0.27393065432600194, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 865, Loss: 0.3433396023580315, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 866, Loss: 0.32435454845734546, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 867, Loss: 0.2745867994272263, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 868, Loss: 0.2855915875089496, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 869, Loss: 0.2359268794200564, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 870, Loss: 0.3348047995408513, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 871, Loss: 0.25145153568304207, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 872, Loss: 0.2675084191109154, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 873, Loss: 0.32796687991314705, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 874, Loss: 0.28593566023400857, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 875, Loss: 0.3032589182567182, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 876, Loss: 0.30037528221368925, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 877, Loss: 0.27043796531694286, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 878, Loss: 0.25205567204079604, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 879, Loss: 0.2395651780930073, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 880, Loss: 0.5269356274357385, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 881, Loss: 0.3533115114232339, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 882, Loss: 0.2526500478676927, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 883, Loss: 0.3012857496929445, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 884, Loss: 0.24578423649876607, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 885, Loss: 0.3384467813492365, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 886, Loss: 0.2489940064642457, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 887, Loss: 0.2820563418766867, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 888, Loss: 0.4781994844464903, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 889, Loss: 0.3063900208667334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 890, Loss: 0.2771599594370548, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 891, Loss: 0.27092624260194964, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 892, Loss: 0.4598407784806109, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 893, Loss: 0.2319865513131426, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 894, Loss: 0.3680466521671299, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 895, Loss: 0.3760095647864563, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 896, Loss: 0.35643110191983385, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 897, Loss: 0.2323617942567708, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 898, Loss: 0.24844054496599677, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 899, Loss: 0.25715490430898225, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 900, Loss: 0.42254350946754654, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 901, Loss: 0.24838340133708225, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 902, Loss: 0.42970699770205734, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 903, Loss: 0.30240454895689706, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 904, Loss: 0.3228165211999815, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 905, Loss: 0.35029543611303776, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 906, Loss: 0.4138744790594945, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 907, Loss: 0.4127573893591241, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 908, Loss: 0.26738282994543144, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 909, Loss: 0.48908644596076767, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 910, Loss: 0.38613082832317813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 911, Loss: 0.3513685520469841, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 912, Loss: 0.3033860389589064, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 913, Loss: 0.2686995242063457, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 914, Loss: 0.233451820434812, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 915, Loss: 0.45696549992359825, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 916, Loss: 0.28632941327946326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 917, Loss: 0.3349061734746548, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 918, Loss: 0.258946996628821, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 919, Loss: 0.2541524945730477, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 920, Loss: 0.3657401881363114, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 921, Loss: 0.26940397729230814, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 922, Loss: 0.260825828101844, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 923, Loss: 0.28621459008635064, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 924, Loss: 0.3155480166977708, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 925, Loss: 0.725959260926438, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 926, Loss: 0.35628509918809986, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 927, Loss: 0.46441811357914475, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 928, Loss: 0.3869993801384076, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 929, Loss: 0.2767818538134361, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 930, Loss: 0.341429452353579, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 931, Loss: 0.28436867306069863, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 932, Loss: 0.3451108149994012, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 933, Loss: 0.2681034359475029, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 934, Loss: 0.29973507371507063, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 935, Loss: 0.3808628947045811, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 936, Loss: 0.26310692336771213, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 937, Loss: 0.26915557738703855, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 938, Loss: 0.44077292784902766, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 939, Loss: 0.24456231827625252, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 940, Loss: 0.30020593297841786, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 941, Loss: 0.2755066970038318, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 942, Loss: 0.2889487582414034, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 943, Loss: 0.3600399898482953, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 944, Loss: 0.35179560356720563, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 945, Loss: 0.3821438335271002, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 946, Loss: 0.2722381663837947, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 947, Loss: 0.24958963098888423, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 948, Loss: 0.30376895157450634, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 949, Loss: 0.42837892814737843, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 950, Loss: 0.2591795150892957, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 951, Loss: 0.3297515361787584, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 952, Loss: 0.4532137416485622, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 953, Loss: 0.41195295065160586, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 954, Loss: 0.33887385683555704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 955, Loss: 0.2615947325193743, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 956, Loss: 0.2875913033231682, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 957, Loss: 0.2603577373372714, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 958, Loss: 0.26211252732615875, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 959, Loss: 0.2750552078589061, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 960, Loss: 0.3013947882440796, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 961, Loss: 0.2900630347605222, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 962, Loss: 0.378461630294669, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 963, Loss: 0.3972734736205204, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 964, Loss: 0.33417785141882406, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 965, Loss: 0.28769885767236125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 966, Loss: 0.452590189977224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 967, Loss: 0.3796942447977109, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 968, Loss: 0.2861454048825017, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 969, Loss: 0.2974529763646029, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 970, Loss: 0.2763705470301836, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 971, Loss: 0.30642798667129667, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 972, Loss: 0.3284485117847361, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 973, Loss: 0.28052439442412663, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 974, Loss: 0.29529120353449245, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 975, Loss: 0.2564733945745979, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 976, Loss: 0.28242130205345994, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 977, Loss: 0.29759851872818355, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 978, Loss: 0.2550823267972845, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 979, Loss: 0.31205117564966495, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 980, Loss: 0.2932732139855332, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 981, Loss: 0.37635968931782393, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 982, Loss: 0.2306097201182815, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 983, Loss: 0.3030012456084284, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 984, Loss: 0.29874566415593884, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 985, Loss: 0.25002910586845, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 986, Loss: 0.41814917592952583, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 987, Loss: 0.27886445808988275, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 988, Loss: 0.35315874896644184, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 989, Loss: 0.30056596372111943, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 990, Loss: 0.32893361516785713, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 991, Loss: 0.3849409073280626, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 992, Loss: 0.23497314502391584, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 993, Loss: 0.2639221532744679, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 994, Loss: 0.26302345154846474, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 995, Loss: 0.3127571162687608, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 996, Loss: 0.4254769508779369, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 997, Loss: 0.2630302671181722, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 998, Loss: 0.26167761211989377, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 999, Loss: 0.3074599447859977, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1000, Loss: 0.3387300030637581, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1001, Loss: 0.2790089336801769, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1002, Loss: 0.29194529234949074, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1003, Loss: 0.2755567633445235, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1004, Loss: 0.2469546644058326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1005, Loss: 0.4168953996343658, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1006, Loss: 0.3011237782258778, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1007, Loss: 0.30749963207457476, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1008, Loss: 0.2519882591768261, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1009, Loss: 0.35949208831815904, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1010, Loss: 0.2907142872096033, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1011, Loss: 0.2683622287287752, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1012, Loss: 0.439140961864784, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1013, Loss: 0.46648351494740936, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1014, Loss: 0.2534630929941583, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1015, Loss: 0.46355212806320834, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1016, Loss: 0.23708521138123217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1017, Loss: 0.4110163053472501, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1018, Loss: 0.45416893824831894, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1019, Loss: 0.385761535036839, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1020, Loss: 0.23556298016318628, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1021, Loss: 0.5058823589034367, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1022, Loss: 0.44748115834720664, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1023, Loss: 0.24440748516877592, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1024, Loss: 0.36862023703399793, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1025, Loss: 0.3607926092608461, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1026, Loss: 0.33253901610615144, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1027, Loss: 0.3003951977793019, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1028, Loss: 0.27260794049626824, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1029, Loss: 0.3439215924260297, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1030, Loss: 0.36186940063868867, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1031, Loss: 0.5428835470107729, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1032, Loss: 0.33952293735559175, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1033, Loss: 0.4669620576492175, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1034, Loss: 0.24554818209649487, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1035, Loss: 0.5361488088006193, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1036, Loss: 0.3556204051089591, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1037, Loss: 0.3556939851669151, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1038, Loss: 0.4288053992555644, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1039, Loss: 0.30335901490034123, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1040, Loss: 0.2553416899922562, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1041, Loss: 0.2847928884642309, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1042, Loss: 0.2664148967913966, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1043, Loss: 0.35297527237852033, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1044, Loss: 0.32830670586077004, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1045, Loss: 0.37644692121086687, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1046, Loss: 0.3338809373262078, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1047, Loss: 0.3973781617732126, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1048, Loss: 0.3130426256431466, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1049, Loss: 0.3829067356211385, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1050, Loss: 0.41026542531080645, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1051, Loss: 0.3076823805773283, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1052, Loss: 0.4798436688010784, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1053, Loss: 0.3437645367021506, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1054, Loss: 0.3783722248428607, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1055, Loss: 0.2892747711977943, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1056, Loss: 0.2794958947757303, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1057, Loss: 0.25529210077896697, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1058, Loss: 0.3133001026373668, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1059, Loss: 0.3503747152623472, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1060, Loss: 0.2818055762245438, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1061, Loss: 0.28075247992213553, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1062, Loss: 0.45000316353235753, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1063, Loss: 0.2830329084519512, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1064, Loss: 0.3263547618539433, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1065, Loss: 0.31325484065487197, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1066, Loss: 0.38526593053822233, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1067, Loss: 0.28018521549932396, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1068, Loss: 0.282046788132083, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1069, Loss: 0.35796798081835324, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1070, Loss: 0.25991360134165375, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1071, Loss: 0.2517307975934766, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1072, Loss: 0.2355982524543503, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1073, Loss: 0.43359114760327383, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1074, Loss: 0.2743387354826086, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1075, Loss: 0.32083494462651685, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1076, Loss: 0.30303596529378735, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1077, Loss: 0.3057471616845475, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1078, Loss: 0.2580558664364276, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1079, Loss: 0.3637228235461921, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1080, Loss: 0.30733657961075356, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1081, Loss: 0.4111491672359546, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1082, Loss: 0.32429330664047434, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1083, Loss: 0.2705904306252842, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1084, Loss: 0.3079839700526473, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1085, Loss: 0.5474535286589285, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1086, Loss: 0.3142728555600315, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1087, Loss: 0.5628459632932503, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1088, Loss: 0.6253615911503105, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1089, Loss: 0.2629004264714958, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1090, Loss: 0.30384083948304375, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1091, Loss: 0.3203763022948224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1092, Loss: 0.5910738499148989, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1093, Loss: 0.3161702905705234, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1094, Loss: 0.557600244681143, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1095, Loss: 0.2584321152103615, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1096, Loss: 0.4166711725470816, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1097, Loss: 0.3015639314354497, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1098, Loss: 0.29725017896682127, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1099, Loss: 0.28323598444840603, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1100, Loss: 0.22983800654840061, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1101, Loss: 0.4395159384726326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1102, Loss: 0.34046269341902935, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1103, Loss: 0.38911750505455484, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1104, Loss: 0.42289414445205675, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1105, Loss: 0.24496858031627214, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1106, Loss: 0.24909841086559653, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1107, Loss: 0.3781947281851686, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1108, Loss: 0.2315751699253498, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1109, Loss: 0.2707076993137888, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1110, Loss: 0.24332217788691465, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1111, Loss: 0.886792625938954, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1112, Loss: 0.2609937270633334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1113, Loss: 0.25761693660450413, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1114, Loss: 0.3041064320690334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1115, Loss: 0.2530519255872192, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1116, Loss: 0.4334000084741857, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1117, Loss: 0.4858689187283546, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1118, Loss: 0.2866455518762021, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1119, Loss: 0.6230165686813094, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1120, Loss: 0.2559703393615163, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1121, Loss: 0.3045720626234648, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1122, Loss: 0.29375103672950786, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1123, Loss: 0.27440228322518395, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1124, Loss: 0.33532964068913834, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1125, Loss: 0.3388664720884378, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1126, Loss: 0.387015252155649, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1127, Loss: 0.38323691388304315, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1128, Loss: 0.28071694403514813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1129, Loss: 0.26723178897349426, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1130, Loss: 0.2724862757429753, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1131, Loss: 0.32786790951013234, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1132, Loss: 0.33599987152804917, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1133, Loss: 0.31586898605268904, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1134, Loss: 0.35920893037400603, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1135, Loss: 0.3404081331363559, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1136, Loss: 0.297537294735076, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1137, Loss: 0.7111524084136509, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1138, Loss: 0.38230358729129565, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1139, Loss: 0.2917284553155186, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1140, Loss: 0.3666837297788643, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1141, Loss: 0.5123989097268727, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1142, Loss: 0.32123799018738336, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1143, Loss: 0.4157341046906008, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1144, Loss: 0.31704744198671275, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1145, Loss: 0.5437588639212529, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1146, Loss: 0.3479626607720445, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1147, Loss: 0.2514954079283125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1148, Loss: 0.26568687601467617, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1149, Loss: 0.38824213570386257, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1150, Loss: 0.3181749624621525, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1151, Loss: 0.49932878085655563, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1152, Loss: 0.2922105123553851, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1153, Loss: 0.24861608702382798, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1154, Loss: 0.29893449774188113, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1155, Loss: 0.2871795526665976, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1156, Loss: 0.2529423080692034, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1157, Loss: 0.3100132451578447, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1158, Loss: 0.4140469601425061, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1159, Loss: 0.3318650384950262, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1160, Loss: 0.47837839337989563, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1161, Loss: 0.335159795805466, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1162, Loss: 0.5224626295346201, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1163, Loss: 0.3650024157772026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1164, Loss: 0.4367773568597497, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1165, Loss: 0.29284780294387586, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1166, Loss: 0.2543764758994791, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1167, Loss: 0.23260359206958142, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1168, Loss: 0.2804320405015811, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1169, Loss: 0.2831755501057287, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1170, Loss: 0.2671742752056586, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1171, Loss: 0.376898544225578, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1172, Loss: 0.2601312162627903, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1173, Loss: 0.3359715063950125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1174, Loss: 0.40607945378953864, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1175, Loss: 0.3470660315842436, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1176, Loss: 0.3076513440661435, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1177, Loss: 0.24620057822298835, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1178, Loss: 0.29037948850935125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1179, Loss: 0.4344578426445157, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1180, Loss: 0.3585180258841297, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1181, Loss: 0.2602451771864906, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1182, Loss: 0.4733788599686676, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1183, Loss: 0.3571183651864012, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1184, Loss: 0.44836174137874, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1185, Loss: 0.2885640652850436, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1186, Loss: 0.23908333567418208, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1187, Loss: 0.3200818216990694, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1188, Loss: 0.525490489440813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1189, Loss: 0.24643634716329116, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1190, Loss: 0.286142279072974, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1191, Loss: 0.3697212797573609, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1192, Loss: 0.2974499478501368, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1193, Loss: 0.382713903706225, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1194, Loss: 0.31030658378787535, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1195, Loss: 0.2530533372834625, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1196, Loss: 0.3029240347564648, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1197, Loss: 0.23492067386234927, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1198, Loss: 0.3047014180729343, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1199, Loss: 0.3102729990011074, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1200, Loss: 0.3231146765239855, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1201, Loss: 0.2501310642352804, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1202, Loss: 0.27651111039385523, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1203, Loss: 0.39686400451660275, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1204, Loss: 0.24341303925452373, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1205, Loss: 0.31543698727792147, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1206, Loss: 0.27953560369805763, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1207, Loss: 0.29118644082763473, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1208, Loss: 0.312321009370106, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1209, Loss: 0.2403011817244748, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1210, Loss: 0.2688691433717403, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1211, Loss: 0.3955028340279937, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1212, Loss: 0.40171985573660224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1213, Loss: 0.3379872181307951, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1214, Loss: 0.33622253501930455, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1215, Loss: 0.3388296722804246, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1216, Loss: 0.32304460377919486, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1217, Loss: 0.24394974102431607, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1218, Loss: 0.4161287615098015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1219, Loss: 0.24556772118374132, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1220, Loss: 0.2768287649873397, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1221, Loss: 0.5998254531620258, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1222, Loss: 0.5331302778121657, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1223, Loss: 0.2928438420448659, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1224, Loss: 0.33360875631632636, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1225, Loss: 0.25060570161424817, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1226, Loss: 0.39935388570768093, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1227, Loss: 0.3328828669024891, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1228, Loss: 0.24212779982969024, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1229, Loss: 0.48211575018426184, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1230, Loss: 0.259075731737983, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1231, Loss: 0.3980369538643228, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1232, Loss: 0.2894417539647399, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1233, Loss: 0.2762216117086194, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1234, Loss: 0.34626939743308627, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1235, Loss: 0.2813949392983996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1236, Loss: 0.4446737727272321, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1237, Loss: 0.24203624260477527, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1238, Loss: 0.31732552555703003, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1239, Loss: 0.36590357720447697, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1240, Loss: 0.3670396254183431, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1241, Loss: 0.28813615642881646, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1242, Loss: 0.3291894601961348, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1243, Loss: 0.2632644093621936, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1244, Loss: 0.26772914048992735, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1245, Loss: 0.33508122285522746, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1246, Loss: 0.2514112319542219, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1247, Loss: 0.2809030924478867, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1248, Loss: 0.406586276272418, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1249, Loss: 0.23904975296451686, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1250, Loss: 0.3319754053336671, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1251, Loss: 0.29535241646303745, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1252, Loss: 0.288679067364411, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1253, Loss: 0.28235284624097234, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1254, Loss: 0.24414448439442216, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1255, Loss: 0.26198856156989137, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1256, Loss: 0.5167666815369502, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1257, Loss: 0.3165170844160493, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1258, Loss: 0.23681391450591568, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1259, Loss: 0.3327014867061254, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1260, Loss: 0.34654084134362284, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1261, Loss: 0.3377451153013312, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1262, Loss: 0.3393277834899699, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1263, Loss: 0.4461985638316952, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1264, Loss: 0.2973410989258318, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1265, Loss: 0.38677805940475385, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1266, Loss: 0.3202189601525889, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1267, Loss: 0.2505468355748863, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1268, Loss: 0.420865363895455, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1269, Loss: 0.3185211258925003, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1270, Loss: 0.3313702411433617, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1271, Loss: 0.30074184474113713, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1272, Loss: 0.27292411187240373, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1273, Loss: 0.39178461561091626, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1274, Loss: 0.5076018513464224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1275, Loss: 0.24567771475016026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1276, Loss: 0.5053844387092861, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1277, Loss: 0.23752154542646522, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1278, Loss: 0.37826706304184055, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1279, Loss: 0.2881911973398215, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1280, Loss: 0.3992739751544608, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1281, Loss: 0.2775578143677749, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1282, Loss: 0.25835080706575897, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1283, Loss: 0.332997635139084, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1284, Loss: 0.3923827775352978, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1285, Loss: 0.30549742818502434, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1286, Loss: 0.31427168997217814, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1287, Loss: 0.33094648687913486, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1288, Loss: 0.7486380277169411, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1289, Loss: 0.24876387649224507, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1290, Loss: 0.3197852228505693, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1291, Loss: 0.4679764278008354, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1292, Loss: 0.2735323781762747, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1293, Loss: 0.4638576218035081, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1294, Loss: 0.2642296728122453, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1295, Loss: 0.26092657684552717, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1296, Loss: 0.26598770448428605, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1297, Loss: 0.28808685917888055, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1298, Loss: 0.41202140039344015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1299, Loss: 0.2783232378796077, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1300, Loss: 0.3859786568855727, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1301, Loss: 0.33148965202410274, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1302, Loss: 0.31016566463683737, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1303, Loss: 0.3646611557173224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1304, Loss: 0.250278218262722, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1305, Loss: 0.3634107334769534, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1306, Loss: 0.2611802105402409, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1307, Loss: 0.3286913603029098, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1308, Loss: 0.30642558201180636, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1309, Loss: 0.37030228715770797, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1310, Loss: 0.405680663523961, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1311, Loss: 0.2678414680416366, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1312, Loss: 0.3389992533083927, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1313, Loss: 0.2820211410516667, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1314, Loss: 0.4827290652531717, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1315, Loss: 0.293484411257996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1316, Loss: 0.2848854759236723, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1317, Loss: 0.235086209383385, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1318, Loss: 0.23588239828157628, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1319, Loss: 0.26699467205900523, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1320, Loss: 0.29941948693399484, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1321, Loss: 0.5096562436010302, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1322, Loss: 0.5449018481230281, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1323, Loss: 0.3240095689499446, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1324, Loss: 0.4649842704710665, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1325, Loss: 0.2607244293277133, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1326, Loss: 0.30618432692231884, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1327, Loss: 0.306632483889909, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1328, Loss: 0.3891160753787203, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1329, Loss: 0.27541678283263715, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1330, Loss: 0.2787230980023904, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1331, Loss: 0.22690511055017853, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1332, Loss: 0.3282502143491649, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1333, Loss: 0.3581352355506917, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1334, Loss: 0.4218339569470877, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1335, Loss: 0.28464801110996496, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1336, Loss: 0.6906393297477897, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1337, Loss: 0.23304691417367035, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1338, Loss: 0.2988401033323941, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1339, Loss: 0.4341444724876112, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1340, Loss: 0.38856880787506454, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1341, Loss: 0.27000540129704914, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1342, Loss: 0.3103702234111262, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1343, Loss: 0.23794005825960635, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1344, Loss: 0.26744449753668065, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1345, Loss: 0.2489252633314139, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1346, Loss: 0.24678640860746903, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1347, Loss: 0.5872687712353873, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1348, Loss: 0.3382575011316037, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1349, Loss: 0.5310065294560699, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1350, Loss: 0.2280197259103645, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1351, Loss: 0.2574287254825578, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1352, Loss: 0.3756049950339492, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1353, Loss: 0.2833214706221138, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1354, Loss: 0.37412362087760287, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1355, Loss: 0.2717067089789943, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1356, Loss: 0.26509538755339523, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1357, Loss: 0.3028664992079221, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1358, Loss: 0.3181478455237553, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1359, Loss: 0.24225053202287197, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1360, Loss: 0.2720790113416487, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1361, Loss: 0.3182828719394962, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1362, Loss: 0.31183206818592446, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1363, Loss: 0.49232610931224996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1364, Loss: 0.36246389751991714, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1365, Loss: 0.3019539975408305, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1366, Loss: 0.2578609027810912, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1367, Loss: 0.30098994584969435, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1368, Loss: 0.5013375295699753, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1369, Loss: 0.357871365516724, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1370, Loss: 0.4828680920262658, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1371, Loss: 0.4155102842716317, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1372, Loss: 0.2600147341620147, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1373, Loss: 0.3263035227280593, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1374, Loss: 0.5468583241231797, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1375, Loss: 0.22970935772488613, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1376, Loss: 0.3079612516496105, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1377, Loss: 0.5041252661005309, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1378, Loss: 0.3980642526820797, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1379, Loss: 0.2989594531037413, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1380, Loss: 0.4733540245025559, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1381, Loss: 0.48952214081059464, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1382, Loss: 0.24866405355039645, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1383, Loss: 0.36805252109429987, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1384, Loss: 0.26038338165562863, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1385, Loss: 0.2591941866031591, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1386, Loss: 0.4403371991905307, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1387, Loss: 0.24967592578244988, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1388, Loss: 0.23361335764600288, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1389, Loss: 0.2959554085267307, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1390, Loss: 0.5089479343105532, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1391, Loss: 0.39547808320078187, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1392, Loss: 0.31487989518725484, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1393, Loss: 0.388589207702767, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1394, Loss: 0.3526990752964424, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1395, Loss: 0.36559035095874576, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1396, Loss: 0.25922312041881157, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1397, Loss: 0.25609575273553215, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1398, Loss: 0.3682367625760209, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1399, Loss: 0.279621973546231, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1400, Loss: 0.44271190254773396, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1401, Loss: 0.24140660300649436, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1402, Loss: 0.3742322458418223, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1403, Loss: 0.23502901192500505, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1404, Loss: 0.3355467852468055, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1405, Loss: 0.3141861841758675, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1406, Loss: 0.4473938872903025, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1407, Loss: 0.32439060192132424, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1408, Loss: 0.3252273654854459, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1409, Loss: 0.23571757653505074, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1410, Loss: 0.33538522329964826, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1411, Loss: 0.31133632039136006, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1412, Loss: 0.4907358747987622, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1413, Loss: 0.4092278951413183, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1414, Loss: 0.3446945750852879, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1415, Loss: 0.2625400815936866, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1416, Loss: 0.4162428993726761, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1417, Loss: 0.2511645074035833, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1418, Loss: 0.28134924418296653, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1419, Loss: 0.31603453652120195, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1420, Loss: 0.23878653237410064, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1421, Loss: 0.24228212656120673, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1422, Loss: 0.3084970510883287, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1423, Loss: 0.34179159954934535, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1424, Loss: 0.46557611644187613, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1425, Loss: 0.4600826275828568, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1426, Loss: 0.23999289547501962, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1427, Loss: 0.28922895262954296, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1428, Loss: 0.41505715665516496, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1429, Loss: 0.25097470036761016, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1430, Loss: 0.24754023585849516, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1431, Loss: 0.23536737728863905, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1432, Loss: 0.3476678262881504, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1433, Loss: 0.2954477794185629, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1434, Loss: 0.3546086214403801, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1435, Loss: 0.26830512103202603, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1436, Loss: 0.33011305433617133, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1437, Loss: 0.36707708505053677, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1438, Loss: 0.33077192555761514, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1439, Loss: 0.2769551655364462, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1440, Loss: 0.43005061467830896, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1441, Loss: 0.4127882722368553, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1442, Loss: 0.2955157633184984, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1443, Loss: 0.2754570576604014, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1444, Loss: 0.326415991294513, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1445, Loss: 0.3464523412934331, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1446, Loss: 0.23796027548316176, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1447, Loss: 0.2515168631632996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1448, Loss: 0.27844855404450686, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1449, Loss: 0.305467854000844, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1450, Loss: 0.2893854021725746, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1451, Loss: 0.25382760233121887, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1452, Loss: 0.3387121933845759, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1453, Loss: 0.23300910500129443, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1454, Loss: 0.28990969843252057, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1455, Loss: 0.3333213031996181, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1456, Loss: 0.24217422161317217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1457, Loss: 0.2360910568242542, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1458, Loss: 0.3190020814426276, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1459, Loss: 0.29730849198011594, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1460, Loss: 0.29859828684115564, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1461, Loss: 0.4263083340381343, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1462, Loss: 0.393974434680324, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1463, Loss: 0.2583882944927094, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1464, Loss: 0.3689317521460248, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1465, Loss: 0.2543071769467926, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1466, Loss: 0.33609304128420087, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1467, Loss: 0.37043694070033556, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1468, Loss: 0.2933765884289235, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1469, Loss: 0.2714485292040681, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1470, Loss: 0.6087055869746731, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1471, Loss: 0.3174405283566994, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1472, Loss: 0.3874229153955163, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1473, Loss: 0.24406891569334505, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1474, Loss: 0.3154719065360997, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1475, Loss: 0.38128545529860647, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1476, Loss: 0.40980436511522805, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1477, Loss: 0.33373857387194394, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1478, Loss: 0.29163155030685217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1479, Loss: 0.3422637325728395, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1480, Loss: 0.3363539521648639, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1481, Loss: 0.4445385742056257, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1482, Loss: 0.36291293134750335, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1483, Loss: 0.3307380830527051, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1484, Loss: 0.35969199816078967, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1485, Loss: 0.37265197822209983, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1486, Loss: 0.24007322769327985, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1487, Loss: 0.3006827600307673, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1488, Loss: 0.3417531785982174, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1489, Loss: 0.23522024928378746, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1490, Loss: 0.24306649563922966, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1491, Loss: 0.2635592458725334, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1492, Loss: 0.32204817442771894, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1493, Loss: 0.25811723544442716, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1494, Loss: 0.2732668878768677, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1495, Loss: 0.34576556638518113, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1496, Loss: 0.2348701182589747, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1497, Loss: 0.4042610488227684, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1498, Loss: 0.26607199276131266, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1499, Loss: 0.46009372488472844, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1500, Loss: 0.3820959098920178, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1501, Loss: 0.3110952272205193, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1502, Loss: 0.3017768146677772, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1503, Loss: 0.29119223182922643, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1504, Loss: 0.24069473589128976, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1505, Loss: 0.30651188971745874, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1506, Loss: 0.3180734453418613, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1507, Loss: 0.23800784402563066, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1508, Loss: 0.4773972761895092, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1509, Loss: 0.27442020233319, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1510, Loss: 0.26162510533430744, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1511, Loss: 0.3410956117105987, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1512, Loss: 0.931111553408567, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1513, Loss: 0.3087988608280644, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1514, Loss: 0.3121396059840312, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1515, Loss: 0.47256151014856107, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1516, Loss: 0.32780735693130697, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1517, Loss: 0.3388787123632332, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1518, Loss: 0.3644762740598535, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1519, Loss: 0.28678924089351276, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1520, Loss: 0.35226198222982513, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1521, Loss: 0.45485734366224584, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1522, Loss: 0.43030129274085416, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1523, Loss: 0.3168302743878425, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1524, Loss: 0.24822312327998292, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1525, Loss: 0.3028625513006207, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1526, Loss: 0.27184723837524266, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1527, Loss: 0.33294475509868293, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1528, Loss: 0.4036379561894663, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1529, Loss: 0.36006304224905644, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1530, Loss: 0.32863509999253715, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1531, Loss: 0.23882518221254748, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1532, Loss: 0.2863080311422977, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1533, Loss: 0.3290576364412696, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1534, Loss: 0.29042504531731045, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1535, Loss: 0.26239833951817854, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1536, Loss: 0.34059480211997095, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1537, Loss: 0.3392512449329196, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1538, Loss: 0.25163962101236026, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1539, Loss: 0.23532786684293103, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1540, Loss: 0.27808448722798695, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1541, Loss: 0.32322020682697555, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1542, Loss: 0.3592939772527254, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1543, Loss: 0.25735665324780893, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1544, Loss: 0.2924657220215984, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1545, Loss: 0.3769212864411584, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1546, Loss: 0.3114483516193215, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1547, Loss: 0.34068570456904834, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1548, Loss: 0.4250566309537728, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1549, Loss: 0.3186043359221098, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1550, Loss: 0.23595980697590152, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1551, Loss: 0.593339891030586, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1552, Loss: 0.2587144843836747, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1553, Loss: 0.4392071618394203, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1554, Loss: 0.4156197135331672, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1555, Loss: 0.34859687734496553, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1556, Loss: 0.2989292525235364, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1557, Loss: 0.3093040512426989, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1558, Loss: 0.25146664300293126, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1559, Loss: 0.28742411029929293, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1560, Loss: 0.4499989328058648, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1561, Loss: 0.31388478965640737, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1562, Loss: 0.7117623696053365, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1563, Loss: 0.2677990267237142, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1564, Loss: 0.35567443603818655, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1565, Loss: 0.29212945705805154, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1566, Loss: 0.3085682939243517, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1567, Loss: 0.3621830167969986, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1568, Loss: 0.3086061356047712, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1569, Loss: 0.3931913236629653, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1570, Loss: 0.3743081201561679, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1571, Loss: 0.25318644106611, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1572, Loss: 0.24106734777695907, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1573, Loss: 0.35851086918178243, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1574, Loss: 0.27090584433702525, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1575, Loss: 0.46879739557620115, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1576, Loss: 0.40620440584961204, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1577, Loss: 0.2371878812904606, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1578, Loss: 0.31445520817418493, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1579, Loss: 0.48597502586591224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1580, Loss: 0.4074994737800517, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1581, Loss: 0.33048150375478336, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1582, Loss: 0.34285034880539395, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1583, Loss: 0.4481839595102147, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1584, Loss: 0.30527804553598703, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1585, Loss: 0.2325922729821867, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1586, Loss: 0.28588272557591704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1587, Loss: 0.25858518161185845, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1588, Loss: 0.30487515849547814, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1589, Loss: 0.28046519803352765, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1590, Loss: 0.3052478790402539, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1591, Loss: 0.235297999548527, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1592, Loss: 0.28674595900934086, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1593, Loss: 0.26910556502466265, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1594, Loss: 0.455311790028351, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1595, Loss: 0.3610841699073968, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1596, Loss: 0.27182103927870666, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1597, Loss: 0.5274672457544233, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1598, Loss: 0.2623329830663964, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1599, Loss: 0.36811829908112514, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1600, Loss: 0.3048660136396902, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1601, Loss: 0.3240248953164205, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1602, Loss: 0.30068368270875273, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1603, Loss: 0.3968652569820052, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1604, Loss: 0.41841906981927923, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1605, Loss: 0.39901216253123517, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1606, Loss: 0.3851510002197842, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1607, Loss: 0.24852324877662973, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1608, Loss: 0.36298616333394573, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1609, Loss: 0.32325069936076767, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1610, Loss: 0.38501893851861946, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1611, Loss: 0.25703959706073726, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1612, Loss: 0.30448396034459335, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1613, Loss: 0.3393829152851524, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1614, Loss: 0.3497517700561383, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1615, Loss: 0.2503701792646516, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1616, Loss: 0.54057702331748, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1617, Loss: 0.29053824404546136, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1618, Loss: 0.2556195015999875, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1619, Loss: 0.27516890613082734, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1620, Loss: 0.34580994777790364, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1621, Loss: 0.3345350519372931, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1622, Loss: 0.3658433482781791, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1623, Loss: 0.2609723929586175, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1624, Loss: 0.2904113603178125, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1625, Loss: 0.3474550474954152, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1626, Loss: 0.30158372700146324, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1627, Loss: 0.37737694272021527, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1628, Loss: 0.23135506182885066, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1629, Loss: 0.37547076318168593, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1630, Loss: 0.23710115027247278, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1631, Loss: 0.25054668520053, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1632, Loss: 0.3440020351960535, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1633, Loss: 0.26099681498096644, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1634, Loss: 0.3505739191965971, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1635, Loss: 0.29104230324860797, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1636, Loss: 0.30230298131473665, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1637, Loss: 0.28116217044871666, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1638, Loss: 0.25895879938592115, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1639, Loss: 0.291184249475236, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1640, Loss: 0.25155352857547797, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1641, Loss: 0.4117402890000601, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1642, Loss: 0.28817531617038383, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1643, Loss: 0.2647464224334938, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1644, Loss: 0.5598586228264212, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1645, Loss: 0.33358423664872555, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1646, Loss: 0.3266287110126462, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1647, Loss: 0.2529843028416062, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1648, Loss: 0.2774711188580106, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1649, Loss: 0.2542060599824343, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1650, Loss: 0.5864738119452857, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1651, Loss: 0.3852959939930968, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1652, Loss: 0.2933563642012083, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1653, Loss: 0.2934208868930369, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1654, Loss: 0.2569796206734743, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1655, Loss: 0.3737858467760673, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1656, Loss: 0.3801134811087578, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1657, Loss: 0.293155759040729, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1658, Loss: 0.30281806632742597, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1659, Loss: 0.26482334158497595, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1660, Loss: 0.30801632276094393, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1661, Loss: 0.40862270182064653, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1662, Loss: 0.2799477131409291, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1663, Loss: 0.35980959633501913, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1664, Loss: 0.29748443975964767, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1665, Loss: 0.2693202714032361, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1666, Loss: 0.3819792589626352, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1667, Loss: 0.3636906604185651, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1668, Loss: 0.34468369635031015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1669, Loss: 0.3060740613012428, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1670, Loss: 0.2381955911016983, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1671, Loss: 0.3119955529576305, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1672, Loss: 0.2997386309004042, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1673, Loss: 0.2535247630886417, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1674, Loss: 0.4883999535545972, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1675, Loss: 0.36685797060231096, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1676, Loss: 0.343266361727456, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1677, Loss: 0.5513457348400199, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1678, Loss: 0.3762350588000134, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1679, Loss: 0.26084328641798205, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1680, Loss: 0.41814329293453845, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1681, Loss: 0.28017868472839313, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1682, Loss: 0.28522111245377774, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1683, Loss: 0.31935483360667094, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1684, Loss: 0.2555800814399872, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1685, Loss: 0.35428054749269133, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1686, Loss: 0.5962560768170326, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1687, Loss: 0.23735534072425943, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1688, Loss: 0.26192953542258657, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1689, Loss: 0.3982901410225189, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1690, Loss: 0.2996939536549704, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1691, Loss: 0.504471224019073, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1692, Loss: 0.36172122277824303, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1693, Loss: 0.25708849235958714, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1694, Loss: 0.28655405076845464, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1695, Loss: 0.24846951328693015, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1696, Loss: 0.4044643906981862, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1697, Loss: 0.518284147039483, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1698, Loss: 0.28557873605756706, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1699, Loss: 0.36638143903231934, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1700, Loss: 0.3493377120133474, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1701, Loss: 0.3082000271873283, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1702, Loss: 0.24702648647881897, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1703, Loss: 0.3541034920881656, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1704, Loss: 0.3445451682133895, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1705, Loss: 0.27636079061214436, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1706, Loss: 0.42605404367956035, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1707, Loss: 0.4137626988978865, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1708, Loss: 0.25267595031595497, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1709, Loss: 0.2548318974578199, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1710, Loss: 0.29145840899938247, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1711, Loss: 0.35747854691746217, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1712, Loss: 0.48394796769277787, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1713, Loss: 0.2931808829356386, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1714, Loss: 0.46912145225333923, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1715, Loss: 0.37224880326782367, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1716, Loss: 0.39309995451262714, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1717, Loss: 0.2498539196841813, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1718, Loss: 0.6339906830082236, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1719, Loss: 0.3051310834283371, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1720, Loss: 0.44364720304989447, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1721, Loss: 0.45670634284079725, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1722, Loss: 0.35584044911723606, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1723, Loss: 0.2475647203469914, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1724, Loss: 0.24642843597110076, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1725, Loss: 0.3567264119552606, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1726, Loss: 0.2303704392457444, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1727, Loss: 0.38463694467922604, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1728, Loss: 0.38693398405618984, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1729, Loss: 0.41752560998721866, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1730, Loss: 0.3056758708182387, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1731, Loss: 0.2587537219797413, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1732, Loss: 0.2743001154178758, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1733, Loss: 0.4778680580168675, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1734, Loss: 0.2692942101922034, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1735, Loss: 0.2621537185089905, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1736, Loss: 0.29168818127974633, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1737, Loss: 0.3361020444928311, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1738, Loss: 0.2326492007582516, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1739, Loss: 0.241670637869109, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1740, Loss: 0.281009645110672, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1741, Loss: 0.2736987784202167, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1742, Loss: 0.24305761445183455, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1743, Loss: 0.3791175179544444, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1744, Loss: 0.3027176408597221, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1745, Loss: 0.3143347139705195, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1746, Loss: 0.29403150515870435, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1747, Loss: 0.28838595150270135, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1748, Loss: 0.5321647561090417, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1749, Loss: 0.31624751897815184, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1750, Loss: 0.387984968456812, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1751, Loss: 0.2648033695517772, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1752, Loss: 0.29481286304141496, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1753, Loss: 0.2665847245566827, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1754, Loss: 0.325449011782681, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1755, Loss: 0.281248743533087, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1756, Loss: 0.2661274940147281, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1757, Loss: 0.423838109841942, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1758, Loss: 0.3140527864881183, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1759, Loss: 0.2847625570784821, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1760, Loss: 0.3366605881387134, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1761, Loss: 0.23645410462707303, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1762, Loss: 0.4028020412424931, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1763, Loss: 0.38449713400082963, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1764, Loss: 0.24905597262482926, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1765, Loss: 0.37870458189037404, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1766, Loss: 0.2851067974727009, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1767, Loss: 0.27788858792208565, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1768, Loss: 0.3791833259812306, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1769, Loss: 0.432245585190277, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1770, Loss: 0.3268919458738375, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1771, Loss: 0.4360682479155018, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1772, Loss: 0.2802526351645003, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1773, Loss: 0.31176537330827797, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1774, Loss: 0.3485622593174091, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1775, Loss: 0.23823363037743547, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1776, Loss: 0.36838166865325284, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1777, Loss: 0.35056144176099624, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1778, Loss: 0.2633707705120648, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1779, Loss: 0.3307307848591271, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1780, Loss: 0.3143522542763731, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1781, Loss: 0.246166906313066, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1782, Loss: 0.5093375722512248, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1783, Loss: 0.26144405583843533, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1784, Loss: 0.31405274690872254, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1785, Loss: 0.3439899080082934, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1786, Loss: 0.3120494332345425, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1787, Loss: 0.25712584683851847, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1788, Loss: 0.30183961820547456, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1789, Loss: 0.2913299667260951, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1790, Loss: 0.36847491079631683, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1791, Loss: 0.28462761408989506, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1792, Loss: 0.25305992255459925, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1793, Loss: 0.4698159849040697, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1794, Loss: 0.2507836413978145, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1795, Loss: 0.3398085128254721, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1796, Loss: 0.3921477706223301, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1797, Loss: 0.2429027039008773, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1798, Loss: 0.24611896933846034, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1799, Loss: 0.24414647190698563, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1800, Loss: 0.23977596457294312, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1801, Loss: 0.31401920621427815, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1802, Loss: 0.39791031758679996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1803, Loss: 0.2726062591585266, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1804, Loss: 0.4025854339147602, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1805, Loss: 0.3036508269501783, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1806, Loss: 0.5191234357233517, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1807, Loss: 0.4918708266375917, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1808, Loss: 0.5415173808881, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1809, Loss: 0.27973537251343833, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1810, Loss: 0.2779313450940519, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1811, Loss: 0.2896050894362522, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1812, Loss: 0.40518279050348904, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1813, Loss: 0.30574247461351256, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1814, Loss: 0.26701922081986085, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1815, Loss: 0.2611378516686731, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1816, Loss: 0.25503037572290765, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1817, Loss: 0.35756444546497224, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1818, Loss: 0.3123351867884203, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1819, Loss: 0.24666758487546908, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1820, Loss: 0.4244028088180801, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1821, Loss: 0.24636989914339003, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1822, Loss: 0.2897402007915688, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1823, Loss: 0.2719909660298722, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1824, Loss: 0.2547549561102091, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1825, Loss: 0.29414248148052025, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1826, Loss: 0.2807621141616612, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1827, Loss: 0.23456555043671667, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1828, Loss: 0.28794074377461776, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1829, Loss: 0.41363808796031376, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1830, Loss: 0.3581533590949795, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1831, Loss: 0.31377226481474807, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1832, Loss: 0.2737713698292384, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1833, Loss: 0.28152641043412946, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1834, Loss: 0.350307717197801, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1835, Loss: 0.34082830411530524, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1836, Loss: 0.25830436309317123, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1837, Loss: 0.282002946043618, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1838, Loss: 0.25398283110716047, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1839, Loss: 0.25629385153692863, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1840, Loss: 0.4056079321920662, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1841, Loss: 0.2694191380648868, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1842, Loss: 0.30906550105202996, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1843, Loss: 0.2795550690434142, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1844, Loss: 0.28251366845046494, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1845, Loss: 0.33439741221961006, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1846, Loss: 0.27786352029250116, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1847, Loss: 0.38843882778711114, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1848, Loss: 0.2482640345330701, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1849, Loss: 0.28052653044873277, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1850, Loss: 0.28899536166839157, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1851, Loss: 0.32261546497652455, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1852, Loss: 0.4798089270016209, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1853, Loss: 0.2623181557605317, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1854, Loss: 0.2836185820430367, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1855, Loss: 0.26969794599757163, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1856, Loss: 0.3437660979255177, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1857, Loss: 0.2887657474856087, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1858, Loss: 0.30207423742325523, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1859, Loss: 0.49734791644335985, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1860, Loss: 0.4856007457464684, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1861, Loss: 0.4414564050933678, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1862, Loss: 0.2491750358590894, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1863, Loss: 0.4950581417837108, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1864, Loss: 0.26639939269107304, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1865, Loss: 0.491511701350116, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1866, Loss: 0.3508139753647518, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1867, Loss: 0.3012955782326111, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1868, Loss: 0.3594932432699218, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1869, Loss: 0.31337834448053525, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1870, Loss: 0.24709338376962034, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1871, Loss: 0.30996745625397615, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1872, Loss: 0.26763641824844514, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1873, Loss: 0.24931467967677312, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1874, Loss: 0.26633319149816564, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Batch 1875, Loss: 0.3953346244698901, Batch Size: 32, Learning Rate: 4.267252714085162e-05\n",
      "Epoch 14, Updated Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 14, Average Loss: 0.33623365638031383, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1, Loss: 0.65927582263206, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 2, Loss: 0.3293967518926612, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 3, Loss: 0.2992251265166511, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 4, Loss: 0.5111941446869027, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 5, Loss: 0.32398423800136866, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 6, Loss: 0.37759557378014696, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 7, Loss: 0.2952652105741977, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 8, Loss: 0.33294302202999015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 9, Loss: 0.35363356103466487, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 10, Loss: 0.4694541905548324, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 11, Loss: 0.2704278724454475, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 12, Loss: 0.476699881026145, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 13, Loss: 0.39240026590346466, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 14, Loss: 0.30735384464296056, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 15, Loss: 0.3298534242346082, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 16, Loss: 0.3902500285839738, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 17, Loss: 0.3299201746531053, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 18, Loss: 0.29532054197179847, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 19, Loss: 0.2490800407674862, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 20, Loss: 0.2543967421931196, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 21, Loss: 0.38834032884051994, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 22, Loss: 0.3262166654356634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 23, Loss: 0.38856139381794524, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 24, Loss: 0.4857572694040404, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 25, Loss: 0.33748232571990316, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 26, Loss: 0.2414991055198331, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 27, Loss: 0.2646052399420858, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 28, Loss: 0.24358371961258707, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 29, Loss: 0.5699899824923848, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 30, Loss: 0.375487452850657, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 31, Loss: 0.27286237811464603, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 32, Loss: 0.3062900876587405, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 33, Loss: 0.2822787861480893, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 34, Loss: 0.2493502642082772, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 35, Loss: 0.2514599853967968, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 36, Loss: 0.26504982317592296, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 37, Loss: 0.2542949529444499, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 38, Loss: 0.30279135151287956, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 39, Loss: 0.3573018682880596, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 40, Loss: 0.3338692650471554, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 41, Loss: 0.32505090267187986, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 42, Loss: 0.24085714134509004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 43, Loss: 0.40875634522344373, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 44, Loss: 0.3184060938094555, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 45, Loss: 0.23601837750145746, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 46, Loss: 0.43322489692006816, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 47, Loss: 0.5043777389621958, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 48, Loss: 0.2837527682354414, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 49, Loss: 0.4829010774784564, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 50, Loss: 0.25577864221156915, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 51, Loss: 0.434753807539521, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 52, Loss: 0.2866081078969552, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 53, Loss: 0.3270944157531751, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 54, Loss: 0.35020605485199713, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 55, Loss: 0.41455180935613417, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 56, Loss: 0.3143251316333786, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 57, Loss: 0.2498466236963619, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 58, Loss: 0.28183012019043274, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 59, Loss: 0.30806417747489934, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 60, Loss: 0.43240662396036667, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 61, Loss: 0.2857692560010552, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 62, Loss: 0.2762404639748777, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 63, Loss: 0.2721931196421368, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 64, Loss: 0.27274294173041536, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 65, Loss: 0.3400581567096451, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 66, Loss: 0.2704343356963939, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 67, Loss: 0.4241523517761132, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 68, Loss: 0.28879868550182286, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 69, Loss: 0.27974430937110173, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 70, Loss: 0.5618535053056506, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 71, Loss: 0.6483666142045865, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 72, Loss: 0.23338370431489788, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 73, Loss: 0.27654758642315885, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 74, Loss: 0.42710617119434235, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 75, Loss: 0.34247316088389124, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 76, Loss: 0.274401911391432, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 77, Loss: 0.25803459255270467, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 78, Loss: 0.32643750152075124, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 79, Loss: 0.3509228595447201, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 80, Loss: 0.3920735472342447, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 81, Loss: 0.3628999865349266, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 82, Loss: 0.3404146702631151, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 83, Loss: 0.33083023208593687, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 84, Loss: 0.30178486331454407, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 85, Loss: 0.3068235523185094, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 86, Loss: 0.34262770744143467, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 87, Loss: 0.36332855429455063, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 88, Loss: 0.25426779649618036, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 89, Loss: 0.2763536621987276, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 90, Loss: 0.29945955254643963, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 91, Loss: 0.22998333947861047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 92, Loss: 0.24086215496608465, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 93, Loss: 0.35064568026112575, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 94, Loss: 0.2534790498825251, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 95, Loss: 0.3015432413459146, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 96, Loss: 0.3749588926107224, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 97, Loss: 0.23899228198747055, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 98, Loss: 0.24554803025916597, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 99, Loss: 0.2604255133890022, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 100, Loss: 0.37435786097976287, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 101, Loss: 0.36511595068388836, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 102, Loss: 0.27491062169648756, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 103, Loss: 0.2504435365643228, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 104, Loss: 0.3512700104105123, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 105, Loss: 0.28464357573275245, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 106, Loss: 0.2923963567718077, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 107, Loss: 0.2704953913523871, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 108, Loss: 0.2940921083762479, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 109, Loss: 0.30695426600775655, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 110, Loss: 0.30429327103614634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 111, Loss: 0.3796167759274308, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 112, Loss: 0.28818046494412897, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 113, Loss: 0.2745305911374981, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 114, Loss: 0.30381427823812646, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 115, Loss: 0.2645720275129482, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 116, Loss: 0.24866911953347473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 117, Loss: 0.5436365299629105, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 118, Loss: 0.24215494674410593, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 119, Loss: 0.3832597081064687, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 120, Loss: 0.378472007422893, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 121, Loss: 0.30996694419666804, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 122, Loss: 0.28992751229246205, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 123, Loss: 0.4682138495276703, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 124, Loss: 0.2953323322003265, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 125, Loss: 0.33204825646757563, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 126, Loss: 0.33863842328325133, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 127, Loss: 0.3679199431310701, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 128, Loss: 0.3620206505711207, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 129, Loss: 0.3862801619434163, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 130, Loss: 0.45022512669710646, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 131, Loss: 0.3709626132991473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 132, Loss: 0.32282183404061304, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 133, Loss: 0.28202293338005946, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 134, Loss: 0.2778137373479847, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 135, Loss: 0.4249372100275754, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 136, Loss: 0.3814897245075974, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 137, Loss: 0.23473183452121013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 138, Loss: 0.25798260692025293, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 139, Loss: 0.2609388383932723, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 140, Loss: 0.3635276561873252, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 141, Loss: 0.24757276341771633, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 142, Loss: 0.26742929943094523, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 143, Loss: 0.4762938433506182, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 144, Loss: 0.33915674836507226, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 145, Loss: 0.32977105452607003, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 146, Loss: 0.28100112486467943, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 147, Loss: 0.4541593983749893, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 148, Loss: 0.2931407821147681, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 149, Loss: 0.24414142222259094, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 150, Loss: 0.2808906110883129, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 151, Loss: 0.32918479581518767, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 152, Loss: 0.31234235997957777, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 153, Loss: 0.3101329520084268, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 154, Loss: 0.3452867637856954, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 155, Loss: 0.2550042053315731, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 156, Loss: 0.3021910368846385, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 157, Loss: 0.23150365648298946, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 158, Loss: 0.28003288306473917, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 159, Loss: 0.24333597676829385, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 160, Loss: 0.34279381718175533, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 161, Loss: 0.24853815400949045, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 162, Loss: 0.31777681331775637, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 163, Loss: 0.2850195979163025, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 164, Loss: 0.4369913709315786, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 165, Loss: 0.31991481710118075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 166, Loss: 0.3377320681059039, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 167, Loss: 0.27352241402589456, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 168, Loss: 0.2853429309375827, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 169, Loss: 0.3717708758294984, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 170, Loss: 0.2785040619653717, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 171, Loss: 0.28656894094213314, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 172, Loss: 0.36698291491550533, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 173, Loss: 0.24338552478757167, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 174, Loss: 0.3122204746001888, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 175, Loss: 0.2786542889925634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 176, Loss: 0.2504755594155015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 177, Loss: 0.372389091582797, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 178, Loss: 0.2787144893771108, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 179, Loss: 0.26798959549474266, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 180, Loss: 0.31560779300682834, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 181, Loss: 0.3074944910986177, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 182, Loss: 0.31469289965340325, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 183, Loss: 0.41469596290735794, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 184, Loss: 0.32561707199765066, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 185, Loss: 0.24976813487421773, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 186, Loss: 0.26809035481905963, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 187, Loss: 0.5227017560161135, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 188, Loss: 0.3734890441145345, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 189, Loss: 0.43420255878954506, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 190, Loss: 0.24341439635887963, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 191, Loss: 0.5146571694275603, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 192, Loss: 0.3779889157381141, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 193, Loss: 0.26791452583167585, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 194, Loss: 0.29579496835473634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 195, Loss: 0.3601767713040128, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 196, Loss: 0.3274402292615188, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 197, Loss: 0.39340202662814144, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 198, Loss: 0.31187208831636415, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 199, Loss: 0.3767492579021269, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 200, Loss: 0.30743132955631114, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 201, Loss: 0.40286029277370206, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 202, Loss: 0.2877141000717722, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 203, Loss: 0.3665637106919952, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 204, Loss: 0.3362963247733126, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 205, Loss: 0.25791207620973117, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 206, Loss: 0.2876732694685855, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 207, Loss: 0.6038244462226826, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 208, Loss: 0.4519468785584767, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 209, Loss: 0.4770929649739685, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 210, Loss: 0.24817262637085305, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 211, Loss: 0.2425949736850769, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 212, Loss: 0.2883130751323433, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 213, Loss: 0.3688326889355442, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 214, Loss: 0.41884822029154906, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 215, Loss: 0.41125862242416966, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 216, Loss: 0.33218328935179053, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 217, Loss: 0.27154744197453473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 218, Loss: 0.3412129077478182, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 219, Loss: 0.23733047059418405, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 220, Loss: 0.27569789049252613, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 221, Loss: 0.25786776460069655, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 222, Loss: 0.27065075737352623, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 223, Loss: 0.3312206665653149, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 224, Loss: 0.3090135555338899, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 225, Loss: 0.36301237775431855, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 226, Loss: 0.5186056718177684, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 227, Loss: 0.5646720249421309, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 228, Loss: 0.5244027087069248, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 229, Loss: 0.2989051483511243, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 230, Loss: 0.312845644303748, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 231, Loss: 0.42694101410723057, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 232, Loss: 0.7178305991981897, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 233, Loss: 0.28007836348036824, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 234, Loss: 0.4454529581758152, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 235, Loss: 0.546224138371023, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 236, Loss: 0.2661293331667952, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 237, Loss: 0.32837152341465825, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 238, Loss: 0.2925654117148396, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 239, Loss: 0.24448097485082235, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 240, Loss: 0.3526907991382153, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 241, Loss: 0.2493509284763289, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 242, Loss: 0.27665907985236377, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 243, Loss: 0.24842975364712228, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 244, Loss: 0.3001123556573116, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 245, Loss: 0.38240684203530056, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 246, Loss: 0.2841793978264586, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 247, Loss: 0.3098729846908982, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 248, Loss: 0.3082188047869692, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 249, Loss: 0.26541132788460625, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 250, Loss: 0.667557703262449, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 251, Loss: 0.33600770071291586, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 252, Loss: 0.3274779802157227, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 253, Loss: 0.24808770756671084, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 254, Loss: 0.2866190515416338, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 255, Loss: 0.40991899576952073, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 256, Loss: 0.28092411995116484, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 257, Loss: 0.33837105931993555, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 258, Loss: 0.39333838896629286, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 259, Loss: 0.36658102074325843, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 260, Loss: 0.2584563345939543, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 261, Loss: 0.2714581315748819, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 262, Loss: 0.28720409567122396, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 263, Loss: 0.3131356380258956, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 264, Loss: 0.34224790350870793, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 265, Loss: 0.25408957955665995, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 266, Loss: 0.33767631804522064, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 267, Loss: 0.2959610658001071, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 268, Loss: 0.2390557601150523, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 269, Loss: 0.3948388675746538, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 270, Loss: 0.2990736448566787, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 271, Loss: 0.38867214724083377, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 272, Loss: 0.2618956011490279, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 273, Loss: 0.2808609168553703, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 274, Loss: 0.2832796276322758, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 275, Loss: 0.2765442076069414, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 276, Loss: 0.29727382991350404, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 277, Loss: 0.257710037361817, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 278, Loss: 0.4089416721686643, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 279, Loss: 0.3017639669983265, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 280, Loss: 0.30966270680038, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 281, Loss: 0.23586709113178436, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 282, Loss: 0.2640550227424608, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 283, Loss: 0.31776688947229403, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 284, Loss: 0.2943739376175602, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 285, Loss: 0.2893524445451944, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 286, Loss: 0.23171930295396903, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 287, Loss: 0.3490019262909914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 288, Loss: 0.5511444570854138, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 289, Loss: 0.3361105674750442, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 290, Loss: 0.3382104909968132, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 291, Loss: 0.3191421114204111, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 292, Loss: 0.3564891123722328, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 293, Loss: 0.29646752990069936, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 294, Loss: 0.2415307623024377, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 295, Loss: 0.22924472411555097, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 296, Loss: 0.3572911435541197, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 297, Loss: 0.3608553702376679, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 298, Loss: 0.4619136897809072, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 299, Loss: 0.2723459303856198, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 300, Loss: 0.28827885050019203, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 301, Loss: 0.30421282409074285, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 302, Loss: 0.3397049330660723, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 303, Loss: 0.40335910936591923, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 304, Loss: 0.35657529408971467, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 305, Loss: 0.26748596036405714, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 306, Loss: 0.3235131912604157, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 307, Loss: 0.28038603634037096, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 308, Loss: 0.2548684859467824, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 309, Loss: 0.24634841240699806, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 310, Loss: 0.2839268213105781, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 311, Loss: 0.25134384061882636, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 312, Loss: 0.22904987902642385, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 313, Loss: 0.4807630891804108, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 314, Loss: 0.2557302844674108, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 315, Loss: 0.322520648120836, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 316, Loss: 0.2904355656197249, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 317, Loss: 0.539656070089637, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 318, Loss: 0.3835915100345908, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 319, Loss: 0.2611099750139632, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 320, Loss: 0.2810832756384923, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 321, Loss: 0.47485618541068164, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 322, Loss: 0.28963587203101065, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 323, Loss: 0.26433670981565593, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 324, Loss: 0.2425573565401354, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 325, Loss: 0.2931836309120269, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 326, Loss: 0.2515768433662558, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 327, Loss: 0.24117115364786484, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 328, Loss: 0.2538250833724925, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 329, Loss: 0.28428449547447276, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 330, Loss: 0.26925465130047416, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 331, Loss: 0.3771151529964286, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 332, Loss: 0.29126522671525235, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 333, Loss: 0.384552243249332, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 334, Loss: 0.2644117520230142, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 335, Loss: 0.3362574601715156, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 336, Loss: 0.33203250518433874, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 337, Loss: 0.28698159715042637, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 338, Loss: 0.26927968176747846, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 339, Loss: 0.3063703299280548, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 340, Loss: 0.22992934634138198, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 341, Loss: 0.5950553766061443, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 342, Loss: 0.27497278055002483, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 343, Loss: 0.29673579205458306, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 344, Loss: 0.29818976501173694, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 345, Loss: 0.26502846215743403, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 346, Loss: 0.2734494663213851, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 347, Loss: 0.283161717268959, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 348, Loss: 0.33002648903715776, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 349, Loss: 0.38493342314100304, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 350, Loss: 0.5728806387876463, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 351, Loss: 0.35362222411886546, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 352, Loss: 0.2530379390842228, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 353, Loss: 0.30307445752292533, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 354, Loss: 0.24217736756921018, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 355, Loss: 0.24877880360149965, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 356, Loss: 0.4620857057418468, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 357, Loss: 0.3429884585860643, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 358, Loss: 0.3724789121413721, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 359, Loss: 0.28792935889514115, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 360, Loss: 0.23218234843532468, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 361, Loss: 0.24930011995282697, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 362, Loss: 0.36368323521406387, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 363, Loss: 0.35370430559792476, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 364, Loss: 0.25591585964757274, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 365, Loss: 0.2953202097168409, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 366, Loss: 0.29293538902979455, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 367, Loss: 0.2824238158294131, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 368, Loss: 0.31392726749491473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 369, Loss: 0.574830775230607, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 370, Loss: 0.2586491719185971, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 371, Loss: 0.3743139031957241, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 372, Loss: 0.25847991829404404, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 373, Loss: 0.34359823650327737, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 374, Loss: 0.3008526897837929, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 375, Loss: 0.2451491985774233, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 376, Loss: 0.2729154937027796, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 377, Loss: 0.46703788464338497, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 378, Loss: 0.25610108236090257, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 379, Loss: 0.2533495944214597, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 380, Loss: 0.32951728839761374, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 381, Loss: 0.32833833840737914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 382, Loss: 0.5375282619889443, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 383, Loss: 0.7487532203699474, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 384, Loss: 0.4004130588715018, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 385, Loss: 0.42633460572771653, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 386, Loss: 0.2880945614983378, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 387, Loss: 0.29264943847920055, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 388, Loss: 0.42832394119392825, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 389, Loss: 0.4027430316633439, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 390, Loss: 0.24248910115759284, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 391, Loss: 0.2812884626119296, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 392, Loss: 0.2377632544469583, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 393, Loss: 0.31258524807499455, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 394, Loss: 0.29779627367657413, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 395, Loss: 0.2381986577438228, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 396, Loss: 0.41377326911422396, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 397, Loss: 0.2851094891771669, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 398, Loss: 0.24594777871039844, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 399, Loss: 0.32714729190079317, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 400, Loss: 0.31844905607696505, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 401, Loss: 0.3434987939601285, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 402, Loss: 0.2709528662721876, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 403, Loss: 0.2969190544107359, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 404, Loss: 0.26566445054940085, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 405, Loss: 0.4212251142678482, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 406, Loss: 0.49728565450975404, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 407, Loss: 0.33404195364199957, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 408, Loss: 0.2623756235469449, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 409, Loss: 0.26589703524604263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 410, Loss: 0.2738940023408709, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 411, Loss: 0.40488854414027764, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 412, Loss: 0.2581678694216353, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 413, Loss: 0.36555983226198885, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 414, Loss: 0.6859344563936411, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 415, Loss: 0.4369156859187254, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 416, Loss: 0.4111971718066171, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 417, Loss: 0.31381201102400913, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 418, Loss: 0.32648752419657734, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 419, Loss: 0.2708488946531741, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 420, Loss: 0.5827485739593736, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 421, Loss: 0.38986852074700235, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 422, Loss: 0.32839104835149935, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 423, Loss: 0.3145497212696512, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 424, Loss: 0.33139663300871025, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 425, Loss: 0.41614174512684965, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 426, Loss: 0.29295626612686876, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 427, Loss: 0.30242514419205296, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 428, Loss: 0.28046745972342335, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 429, Loss: 0.314424847559545, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 430, Loss: 0.33665431133563045, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 431, Loss: 0.27159719890736245, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 432, Loss: 0.49111514412728313, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 433, Loss: 0.24291602094276366, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 434, Loss: 0.26377105464474065, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 435, Loss: 0.3393738203962006, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 436, Loss: 0.34372166778439, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 437, Loss: 0.28328372952130304, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 438, Loss: 0.3734290533002698, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 439, Loss: 0.25460936021964986, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 440, Loss: 0.23789755239807803, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 441, Loss: 0.36921763026748844, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 442, Loss: 0.26676165700571997, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 443, Loss: 0.3316327976972843, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 444, Loss: 0.24336411085772047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 445, Loss: 0.3054981836546592, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 446, Loss: 0.38595592671636847, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 447, Loss: 0.265324742359914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 448, Loss: 0.35563114462329337, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 449, Loss: 0.23264666678212365, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 450, Loss: 0.32128557997314555, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 451, Loss: 0.24469663322973195, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 452, Loss: 0.33531406502904937, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 453, Loss: 0.2593469073750348, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 454, Loss: 0.35760038160469465, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 455, Loss: 0.3221361428554315, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 456, Loss: 0.311152233810521, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 457, Loss: 0.277693754459437, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 458, Loss: 0.29243539788622486, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 459, Loss: 0.2687627990534689, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 460, Loss: 0.42718105537514933, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 461, Loss: 0.3076102884349058, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 462, Loss: 0.2507078338672152, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 463, Loss: 0.24353149359637188, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 464, Loss: 0.29225499889579715, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 465, Loss: 0.5383233311101592, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 466, Loss: 0.2672575095303166, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 467, Loss: 0.3243370690173595, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 468, Loss: 0.2575183134097006, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 469, Loss: 0.24348689374436355, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 470, Loss: 0.47320962723852844, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 471, Loss: 0.2761563268867704, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 472, Loss: 0.4630625978687798, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 473, Loss: 0.25186841404546045, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 474, Loss: 0.48177261469066435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 475, Loss: 0.3461877244572221, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 476, Loss: 0.2685771178855264, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 477, Loss: 0.36364216650738657, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 478, Loss: 0.4016210686809843, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 479, Loss: 0.576139732912692, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 480, Loss: 0.3073456697928974, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 481, Loss: 0.2799250316384402, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 482, Loss: 0.2723875149879031, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 483, Loss: 0.3634458425549171, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 484, Loss: 0.2895453806271373, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 485, Loss: 0.44186146579067487, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 486, Loss: 0.23106534996044648, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 487, Loss: 0.2602758072586393, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 488, Loss: 0.6408208568851035, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 489, Loss: 0.39707302979272063, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 490, Loss: 0.26505519484271317, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 491, Loss: 0.29226753074694783, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 492, Loss: 0.3493220093886432, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 493, Loss: 0.5338673670434851, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 494, Loss: 0.3203968672876708, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 495, Loss: 0.26688760032955017, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 496, Loss: 0.304065752827975, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 497, Loss: 0.2925973798666939, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 498, Loss: 0.2473620103844873, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 499, Loss: 0.29583774259980716, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 500, Loss: 0.339063722460943, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 501, Loss: 0.24467572687360317, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 502, Loss: 0.3139104549222327, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 503, Loss: 0.5639058503908946, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 504, Loss: 0.3516344408652623, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 505, Loss: 0.26181729561795797, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 506, Loss: 0.262821476659852, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 507, Loss: 0.3809024923472666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 508, Loss: 0.48687158939239716, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 509, Loss: 0.3906376059273504, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 510, Loss: 0.3968885258476692, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 511, Loss: 0.31974792167564015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 512, Loss: 0.3400151353959077, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 513, Loss: 0.281386549026944, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 514, Loss: 0.40775623233615244, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 515, Loss: 0.27470709141748395, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 516, Loss: 0.2746374023317435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 517, Loss: 0.2983368627856811, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 518, Loss: 0.370627413823154, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 519, Loss: 0.28575130685182115, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 520, Loss: 0.33633820515984625, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 521, Loss: 0.37728331535177173, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 522, Loss: 0.2959538961584929, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 523, Loss: 0.25663809113718783, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 524, Loss: 0.4752666650471975, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 525, Loss: 0.263801700584326, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 526, Loss: 0.35477146030009704, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 527, Loss: 0.4972093015908711, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 528, Loss: 0.36418267208825755, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 529, Loss: 0.25731258482940317, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 530, Loss: 0.39800063247506556, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 531, Loss: 0.4167166450557652, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 532, Loss: 0.236415536465541, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 533, Loss: 0.3279555671418818, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 534, Loss: 0.3857048717280631, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 535, Loss: 0.28525490896452643, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 536, Loss: 0.37128344405880476, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 537, Loss: 0.47345893599940236, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 538, Loss: 0.3487416777770614, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 539, Loss: 0.40904453631095067, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 540, Loss: 0.23642610004074202, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 541, Loss: 0.32030972545679914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 542, Loss: 0.3221662705339307, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 543, Loss: 0.27422721363179414, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 544, Loss: 0.24935323693219633, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 545, Loss: 0.25564141427484205, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 546, Loss: 0.395642832739602, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 547, Loss: 0.2538691906513799, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 548, Loss: 0.28418748241034797, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 549, Loss: 0.39217628876747934, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 550, Loss: 0.31284306685035385, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 551, Loss: 0.3592946993254448, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 552, Loss: 0.34117225891864644, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 553, Loss: 0.26974647746021047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 554, Loss: 0.3181810272531421, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 555, Loss: 0.2525322659317131, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 556, Loss: 0.6078185245053541, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 557, Loss: 0.23665958483302946, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 558, Loss: 0.37271907281649475, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 559, Loss: 0.3043512992394269, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 560, Loss: 0.34386879391851155, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 561, Loss: 0.313470449252085, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 562, Loss: 0.25441202567073157, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 563, Loss: 0.4265763444165973, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 564, Loss: 0.2383099518379808, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 565, Loss: 0.4373969678544277, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 566, Loss: 0.3348090941459155, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 567, Loss: 0.3482569792189308, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 568, Loss: 0.39944437262961424, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 569, Loss: 0.3092419040061696, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 570, Loss: 0.27817489606330914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 571, Loss: 0.3170357188210904, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 572, Loss: 0.2853659859604032, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 573, Loss: 0.32082722289863, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 574, Loss: 0.5788687890967594, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 575, Loss: 0.4345984379660113, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 576, Loss: 0.24170001424435386, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 577, Loss: 0.3244654764522275, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 578, Loss: 0.3392640086528481, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 579, Loss: 0.3801602155371059, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 580, Loss: 0.5520544316063303, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 581, Loss: 0.37378370365773095, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 582, Loss: 0.4413830763182848, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 583, Loss: 0.2835314189839707, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 584, Loss: 0.4746458467429878, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 585, Loss: 0.26731368253588583, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 586, Loss: 0.40629993391861674, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 587, Loss: 0.36456265766312435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 588, Loss: 0.469028440183445, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 589, Loss: 0.2707685315858257, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 590, Loss: 0.24417163780229215, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 591, Loss: 0.4315876880122853, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 592, Loss: 0.2786434120659462, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 593, Loss: 0.2770601979008869, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 594, Loss: 0.4158302201181393, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 595, Loss: 0.3582955766640733, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 596, Loss: 0.2539124513504673, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 597, Loss: 0.38531905210112083, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 598, Loss: 0.3225964882419758, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 599, Loss: 0.2535472238212089, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 600, Loss: 0.301325467927776, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 601, Loss: 0.343332783194667, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 602, Loss: 0.37806764823966954, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 603, Loss: 0.28445979686632034, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 604, Loss: 0.2675084150776134, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 605, Loss: 0.42429647214318356, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 606, Loss: 0.26207736986919566, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 607, Loss: 0.31409059820804197, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 608, Loss: 0.23747277668732278, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 609, Loss: 0.32189551760160384, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 610, Loss: 0.24483114858652658, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 611, Loss: 0.3175133025282519, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 612, Loss: 0.2720964067294884, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 613, Loss: 0.2489353888536254, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 614, Loss: 0.3346255088185304, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 615, Loss: 0.2810287736093823, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 616, Loss: 0.4066213584872108, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 617, Loss: 0.3624050268532806, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 618, Loss: 0.3118122593708965, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 619, Loss: 0.24767441396902576, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 620, Loss: 0.3753840370548541, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 621, Loss: 0.31807298287642416, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 622, Loss: 0.30953409852252, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 623, Loss: 0.2775882521197519, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 624, Loss: 0.4271264365820496, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 625, Loss: 0.2910176310425471, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 626, Loss: 0.29095550772674905, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 627, Loss: 0.28961419800339955, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 628, Loss: 0.42871666046372564, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 629, Loss: 0.2724838693000873, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 630, Loss: 0.34665604109819914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 631, Loss: 0.35628085914579544, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 632, Loss: 0.24803654832421412, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 633, Loss: 0.2742421235606414, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 634, Loss: 0.23431200677254588, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 635, Loss: 0.454523035473686, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 636, Loss: 0.2911597273853804, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 637, Loss: 0.2632531336825003, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 638, Loss: 0.3711645231698598, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 639, Loss: 0.5178505030290524, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 640, Loss: 0.2303273314250379, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 641, Loss: 0.3888867832380392, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 642, Loss: 0.3511774095687097, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 643, Loss: 0.3228617568594351, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 644, Loss: 0.4135541963792199, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 645, Loss: 0.3692442272005952, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 646, Loss: 0.31403154158416946, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 647, Loss: 0.24120092691321968, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 648, Loss: 0.3511653800778407, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 649, Loss: 0.2702606660416875, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 650, Loss: 0.26149988825529, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 651, Loss: 0.5252585785792799, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 652, Loss: 0.23587776350233705, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 653, Loss: 0.28208065831776075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 654, Loss: 0.3172632219985697, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 655, Loss: 0.3072923404615536, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 656, Loss: 0.47672571942274533, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 657, Loss: 0.26595469604724764, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 658, Loss: 0.44924582956129, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 659, Loss: 0.44353852109840286, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 660, Loss: 0.35369652171798194, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 661, Loss: 0.3464753727615062, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 662, Loss: 0.23791986950157562, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 663, Loss: 0.3177628897578211, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 664, Loss: 0.3370281823536725, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 665, Loss: 0.2725150468749338, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 666, Loss: 0.27953901680846394, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 667, Loss: 0.28934536404405825, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 668, Loss: 0.4477100263359692, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 669, Loss: 0.327610236290752, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 670, Loss: 0.25292136260182563, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 671, Loss: 0.26225560934897496, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 672, Loss: 0.2662009940659462, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 673, Loss: 0.3177040418138233, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 674, Loss: 0.3004503434790826, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 675, Loss: 0.40758887669266025, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 676, Loss: 0.3633093900681026, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 677, Loss: 0.2852119065835787, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 678, Loss: 0.4850825436945855, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 679, Loss: 0.23847648063365343, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 680, Loss: 0.4274145662479876, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 681, Loss: 0.4064346369597637, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 682, Loss: 0.41918306420051266, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 683, Loss: 0.3005924468522555, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 684, Loss: 0.28054003022084895, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 685, Loss: 0.26152588675992927, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 686, Loss: 0.5005542621639977, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 687, Loss: 0.26008618804580713, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 688, Loss: 0.33787772914925307, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 689, Loss: 0.30290842219068936, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 690, Loss: 0.41281681686593397, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 691, Loss: 0.3477311148280782, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 692, Loss: 0.49205041376777525, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 693, Loss: 0.3273617128460143, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 694, Loss: 0.401646997215589, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 695, Loss: 0.4630633496627162, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 696, Loss: 0.27159615762512906, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 697, Loss: 0.37812975328569676, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 698, Loss: 0.35552970814842666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 699, Loss: 0.31503030794718545, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 700, Loss: 0.2607771881939164, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 701, Loss: 0.38703510494919713, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 702, Loss: 0.3120185999846089, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 703, Loss: 0.3650296584230235, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 704, Loss: 0.37795670411449783, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 705, Loss: 0.2523231015293556, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 706, Loss: 0.40859243583595917, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 707, Loss: 0.25795039439298, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 708, Loss: 0.2752198649535013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 709, Loss: 0.43786853467718, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 710, Loss: 0.24691049672895773, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 711, Loss: 0.5058736803871149, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 712, Loss: 0.38395064995885786, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 713, Loss: 0.293065368851152, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 714, Loss: 0.6047301076618129, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 715, Loss: 0.2667695590470736, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 716, Loss: 0.3677489361137929, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 717, Loss: 0.34200895752223004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 718, Loss: 0.7874245028996987, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 719, Loss: 0.23756470931934362, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 720, Loss: 0.26089503034874467, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 721, Loss: 0.30933151986074947, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 722, Loss: 0.32249348160886265, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 723, Loss: 0.2641239073071175, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 724, Loss: 0.26484893863377973, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 725, Loss: 0.23969472645331713, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 726, Loss: 0.2656486512375443, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 727, Loss: 0.2951570436873894, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 728, Loss: 0.23621157040462668, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 729, Loss: 0.4949766779322525, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 730, Loss: 0.4628379951134154, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 731, Loss: 0.25334613154289204, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 732, Loss: 0.23765299824897895, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 733, Loss: 0.41766741077974995, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 734, Loss: 0.3851983768793875, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 735, Loss: 0.31112278621242595, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 736, Loss: 0.30225856336303736, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 737, Loss: 0.2488945582449089, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 738, Loss: 0.47772285968234185, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 739, Loss: 0.5157871268927603, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 740, Loss: 0.25861731822280587, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 741, Loss: 0.24616796318643788, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 742, Loss: 0.41065637497006086, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 743, Loss: 0.44120435668595365, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 744, Loss: 0.27712004124618916, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 745, Loss: 0.35656734694774306, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 746, Loss: 0.5250310392729154, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 747, Loss: 0.683786731284226, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 748, Loss: 0.37282109551367837, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 749, Loss: 0.3934979928602028, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 750, Loss: 0.25946208430404677, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 751, Loss: 0.3011113768024439, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 752, Loss: 0.337035301855719, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 753, Loss: 0.3520002268834648, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 754, Loss: 0.45433802394986444, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 755, Loss: 0.3209813231567883, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 756, Loss: 0.3417076288252207, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 757, Loss: 0.34982711652239507, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 758, Loss: 0.3491629840865187, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 759, Loss: 0.3461234799051633, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 760, Loss: 0.23554917178075174, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 761, Loss: 0.28976392840241894, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 762, Loss: 0.2816245716355858, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 763, Loss: 0.29515822714499784, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 764, Loss: 0.24224211799363715, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 765, Loss: 0.35166233528098845, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 766, Loss: 0.3131079035581696, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 767, Loss: 0.42473492815532626, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 768, Loss: 0.4253873806617896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 769, Loss: 0.4656440538212112, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 770, Loss: 0.2639573088980431, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 771, Loss: 0.31154009236811614, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 772, Loss: 0.24451334814301773, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 773, Loss: 0.2661788950124782, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 774, Loss: 0.2726821196072703, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 775, Loss: 0.443862692330535, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 776, Loss: 0.37285667438992676, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 777, Loss: 0.3809867159511263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 778, Loss: 0.3563708321870646, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 779, Loss: 0.3634980727885654, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 780, Loss: 0.2542003710366106, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 781, Loss: 0.6272465851412018, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 782, Loss: 0.41037357872197383, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 783, Loss: 0.25337355603516926, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 784, Loss: 0.3040910866804017, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 785, Loss: 0.24936521681706517, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 786, Loss: 0.5715097455918123, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 787, Loss: 0.39420735302829973, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 788, Loss: 0.2575115320209241, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 789, Loss: 0.2851135370099151, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 790, Loss: 0.24700822490326077, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 791, Loss: 0.23491276587111318, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 792, Loss: 0.47744740384768075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 793, Loss: 0.5128450238544898, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 794, Loss: 0.3047358136645062, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 795, Loss: 0.27698376040984224, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 796, Loss: 0.3578841948012853, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 797, Loss: 0.53923158169323, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 798, Loss: 0.23959520049873842, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 799, Loss: 0.4324776751492406, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 800, Loss: 0.2883754095087115, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 801, Loss: 0.3824900944943436, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 802, Loss: 0.32715662263777545, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 803, Loss: 0.2714698292841161, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 804, Loss: 0.3116192019946911, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 805, Loss: 0.2520498818502806, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 806, Loss: 0.27404908401761363, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 807, Loss: 0.4535454733599981, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 808, Loss: 0.2715188928303349, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 809, Loss: 0.23671229365791033, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 810, Loss: 0.280449113429988, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 811, Loss: 0.5504412407173634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 812, Loss: 0.3203958913886177, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 813, Loss: 0.256402415941764, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 814, Loss: 0.2970800008802408, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 815, Loss: 0.4231241351503009, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 816, Loss: 0.40476147942523777, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 817, Loss: 0.2964969849546441, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 818, Loss: 0.3039876748495433, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 819, Loss: 0.295329279693247, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 820, Loss: 0.2729095109244336, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 821, Loss: 0.25525332437441783, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 822, Loss: 0.2834143982937467, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 823, Loss: 0.42144468248691047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 824, Loss: 0.27104493834414445, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 825, Loss: 0.25751620036810857, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 826, Loss: 0.4177876883459826, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 827, Loss: 0.315776409706319, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 828, Loss: 0.31534628418572824, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 829, Loss: 0.2778787502262244, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 830, Loss: 0.26704437393881625, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 831, Loss: 0.260296239585063, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 832, Loss: 0.24377841013618, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 833, Loss: 0.341850553092247, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 834, Loss: 0.31755234022211676, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 835, Loss: 0.6064331401904559, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 836, Loss: 0.30789111677477987, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 837, Loss: 0.44354384487798376, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 838, Loss: 0.2760894780367324, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 839, Loss: 0.3064877044131488, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 840, Loss: 0.29005501627311897, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 841, Loss: 0.3268160725597259, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 842, Loss: 0.2682852429855516, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 843, Loss: 0.4281111938881933, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 844, Loss: 0.2489000925159658, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 845, Loss: 0.3441384724560399, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 846, Loss: 0.2770995070995126, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 847, Loss: 0.2852562414277694, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 848, Loss: 0.36192886634134036, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 849, Loss: 0.2917268157117533, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 850, Loss: 0.3577834554763954, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 851, Loss: 0.3721128239919189, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 852, Loss: 0.2777945120952259, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 853, Loss: 0.29243046104782533, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 854, Loss: 0.29247569956577557, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 855, Loss: 0.24163406658698283, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 856, Loss: 0.35233355168598945, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 857, Loss: 0.65933059641985, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 858, Loss: 0.48937450198352805, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 859, Loss: 0.28401483803508265, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 860, Loss: 0.2539906362192795, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 861, Loss: 0.5914183862291106, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 862, Loss: 0.2838179144301871, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 863, Loss: 0.3134628012875961, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 864, Loss: 0.244452986659505, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 865, Loss: 0.28018258819451985, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 866, Loss: 0.23789159771264615, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 867, Loss: 0.31406128530954014, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 868, Loss: 0.2728100786806301, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 869, Loss: 0.2952392559590543, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 870, Loss: 0.2955761608876023, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 871, Loss: 0.2731847896530049, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 872, Loss: 0.3825253576742512, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 873, Loss: 0.36473765469383024, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 874, Loss: 0.2406814817473176, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 875, Loss: 0.26958355379840093, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 876, Loss: 0.3438627897704145, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 877, Loss: 0.24087834677155412, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 878, Loss: 0.244652912493243, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 879, Loss: 0.34332230519804097, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 880, Loss: 0.4082309553986453, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 881, Loss: 0.40751297926864705, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 882, Loss: 0.38062110895750556, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 883, Loss: 0.25220546371092223, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 884, Loss: 0.26685308837083144, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 885, Loss: 0.408321918548181, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 886, Loss: 0.26681400065746075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 887, Loss: 0.31213584771349057, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 888, Loss: 0.48371268647120136, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 889, Loss: 0.38224232770261435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 890, Loss: 0.2814369260483026, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 891, Loss: 0.2637582409942933, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 892, Loss: 0.25100953374861357, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 893, Loss: 0.25581829711142323, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 894, Loss: 0.24111244629043005, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 895, Loss: 0.39226542788866736, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 896, Loss: 0.3821418161263473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 897, Loss: 0.22665947713942447, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 898, Loss: 0.2540347768408705, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 899, Loss: 0.2458963146725599, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 900, Loss: 0.3452801319381792, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 901, Loss: 0.3461751144390002, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 902, Loss: 0.39098402148124434, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 903, Loss: 0.4566327334566444, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 904, Loss: 0.3565498907231789, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 905, Loss: 0.314610328584187, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 906, Loss: 0.295723954342498, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 907, Loss: 0.34705949828478433, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 908, Loss: 0.3634606823857319, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 909, Loss: 0.36405298645028006, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 910, Loss: 0.6521401793087688, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 911, Loss: 0.2635349404251761, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 912, Loss: 0.2815940282840716, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 913, Loss: 0.29993197920019504, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 914, Loss: 0.2293776926273828, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 915, Loss: 0.4352170375815504, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 916, Loss: 0.2586914541781923, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 917, Loss: 0.5962547993896964, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 918, Loss: 0.2618821076489337, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 919, Loss: 0.2758913713033364, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 920, Loss: 0.3519272210039196, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 921, Loss: 0.28928770763131684, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 922, Loss: 0.2706917121386539, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 923, Loss: 0.3171594555967986, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 924, Loss: 0.40937835135225475, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 925, Loss: 0.6355702642511203, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 926, Loss: 0.49927862431210773, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 927, Loss: 0.4945856257423376, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 928, Loss: 0.3746217019261604, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 929, Loss: 0.32606460774652496, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 930, Loss: 0.45671333952810145, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 931, Loss: 0.43542573415694635, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 932, Loss: 0.46576792022462843, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 933, Loss: 0.228600094935152, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 934, Loss: 0.3474190783095825, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 935, Loss: 0.37072253352576995, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 936, Loss: 0.27280238631954334, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 937, Loss: 0.3111272791097035, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 938, Loss: 0.5868748217488692, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 939, Loss: 0.27148015016759997, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 940, Loss: 0.3702715968120725, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 941, Loss: 0.3235578637649875, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 942, Loss: 0.4503555670624555, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 943, Loss: 0.2790210988381263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 944, Loss: 0.32746968592070375, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 945, Loss: 0.3367717576554681, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 946, Loss: 0.309979990792818, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 947, Loss: 0.24187521590512823, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 948, Loss: 0.30787817240339743, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 949, Loss: 0.3960878937116211, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 950, Loss: 0.2735065384311203, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 951, Loss: 0.26307121088114754, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 952, Loss: 0.30205526044607844, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 953, Loss: 0.4379162507223704, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 954, Loss: 0.2668862462052133, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 955, Loss: 0.28529445899953015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 956, Loss: 0.32848835697063633, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 957, Loss: 0.2906391198305703, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 958, Loss: 0.2861910698678641, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 959, Loss: 0.2713786300092143, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 960, Loss: 0.2473870138274306, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 961, Loss: 0.2644485121668633, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 962, Loss: 0.3189020844674121, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 963, Loss: 0.3081850907165149, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 964, Loss: 0.26731086918413666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 965, Loss: 0.3844388311265217, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 966, Loss: 0.3688151785355471, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 967, Loss: 0.3292815450720848, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 968, Loss: 0.24137457943582824, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 969, Loss: 0.2996119599472943, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 970, Loss: 0.26377189658447103, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 971, Loss: 0.2660603919483712, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 972, Loss: 0.47716145318837405, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 973, Loss: 0.29082594108553883, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 974, Loss: 0.5079783868781037, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 975, Loss: 0.2505717644463961, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 976, Loss: 0.2618461502497922, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 977, Loss: 0.27861842902816913, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 978, Loss: 0.3545341795427901, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 979, Loss: 0.3056863142942913, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 980, Loss: 0.3090820048492104, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 981, Loss: 0.27313577618451, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 982, Loss: 0.24155970439235175, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 983, Loss: 0.2425436188165082, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 984, Loss: 0.3837475390888402, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 985, Loss: 0.23601137848242837, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 986, Loss: 0.41864418639949763, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 987, Loss: 0.38952153970150155, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 988, Loss: 0.34270244953925216, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 989, Loss: 0.475102641115105, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 990, Loss: 0.5582257659813192, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 991, Loss: 0.37782033484556343, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 992, Loss: 0.23628990548923448, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 993, Loss: 0.3352394451417131, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 994, Loss: 0.25784243217953473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 995, Loss: 0.3595599983094737, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 996, Loss: 0.5201684890688241, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 997, Loss: 0.26452302400046346, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 998, Loss: 0.35575969935736373, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 999, Loss: 0.28435339738382764, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1000, Loss: 0.36491514214991655, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1001, Loss: 0.36877649294251846, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1002, Loss: 0.32698985081122023, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1003, Loss: 0.23998830742050642, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1004, Loss: 0.240213160892449, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1005, Loss: 0.5242593967060303, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1006, Loss: 0.26624274076471827, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1007, Loss: 0.30991133090426193, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1008, Loss: 0.23477278738335894, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1009, Loss: 0.28223515839106017, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1010, Loss: 0.441182762690553, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1011, Loss: 0.3023628306456499, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1012, Loss: 0.3905128097715388, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1013, Loss: 0.29738120788635514, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1014, Loss: 0.40758082416288804, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1015, Loss: 0.30532832786194125, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1016, Loss: 0.24411727669909708, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1017, Loss: 0.35983950129344267, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1018, Loss: 0.3486661458130814, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1019, Loss: 0.2805364089810753, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1020, Loss: 0.28231928966686914, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1021, Loss: 0.3436155654742554, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1022, Loss: 0.41874153795790015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1023, Loss: 0.2847806609838846, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1024, Loss: 0.3285845131013513, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1025, Loss: 0.2643142381218662, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1026, Loss: 0.3169440809697248, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1027, Loss: 0.27535067710622263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1028, Loss: 0.27178722994710747, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1029, Loss: 0.4146562612885334, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1030, Loss: 0.370599095848016, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1031, Loss: 0.3270233608986222, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1032, Loss: 0.38268262006465004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1033, Loss: 0.42721526367329654, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1034, Loss: 0.26626345741025104, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1035, Loss: 0.36839806814371945, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1036, Loss: 0.4425048578276681, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1037, Loss: 0.31089939453446785, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1038, Loss: 0.4074537696360402, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1039, Loss: 0.26817610816249426, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1040, Loss: 0.24351044006598715, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1041, Loss: 0.26510726496857706, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1042, Loss: 0.27320272822404995, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1043, Loss: 0.31422139395346504, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1044, Loss: 0.26010110887009286, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1045, Loss: 0.3890402443915347, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1046, Loss: 0.25087626318274014, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1047, Loss: 0.36697384433185043, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1048, Loss: 0.29238708229161375, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1049, Loss: 0.37456351358085316, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1050, Loss: 0.28585168098978514, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1051, Loss: 0.30841371325994704, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1052, Loss: 0.4216835295001318, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1053, Loss: 0.3100275830838047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1054, Loss: 0.3880318226094034, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1055, Loss: 0.2477509029828271, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1056, Loss: 0.30095445383654396, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1057, Loss: 0.3705780443114137, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1058, Loss: 0.569538019615444, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1059, Loss: 0.42226930842366217, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1060, Loss: 0.3049107795531283, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1061, Loss: 0.2593224173738076, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1062, Loss: 0.33940993977799333, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1063, Loss: 0.2743612138109195, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1064, Loss: 0.4114041609861435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1065, Loss: 0.41025737269104945, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1066, Loss: 0.2771364341704281, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1067, Loss: 0.27328390465628866, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1068, Loss: 0.3536768695881444, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1069, Loss: 0.30114379567944066, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1070, Loss: 0.30050036868313645, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1071, Loss: 0.28177046466391015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1072, Loss: 0.24152880607530897, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1073, Loss: 0.2854504233208498, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1074, Loss: 0.24775215303252254, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1075, Loss: 0.33117299746593515, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1076, Loss: 0.24729171866762617, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1077, Loss: 0.34709379341424473, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1078, Loss: 0.26131793782589013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1079, Loss: 0.4415069009155711, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1080, Loss: 0.24395035831473305, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1081, Loss: 0.29181019648809386, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1082, Loss: 0.24536455341904417, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1083, Loss: 0.2345291856240645, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1084, Loss: 0.26399002327225507, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1085, Loss: 0.2997711174247433, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1086, Loss: 0.3172580694647035, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1087, Loss: 0.4945599913586829, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1088, Loss: 0.5536176160839147, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1089, Loss: 0.23558019070390604, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1090, Loss: 0.38121861214897285, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1091, Loss: 0.3748526176702419, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1092, Loss: 0.5658515817548515, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1093, Loss: 0.3686916092405482, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1094, Loss: 0.48649561128623325, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1095, Loss: 0.3015790250677737, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1096, Loss: 0.42518639641790756, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1097, Loss: 0.31014250932812315, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1098, Loss: 0.31998732860567974, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1099, Loss: 0.2517142817906224, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1100, Loss: 0.24670082906119656, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1101, Loss: 0.5753354825620919, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1102, Loss: 0.38277712662666896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1103, Loss: 0.4307910588846575, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1104, Loss: 0.36349984003452973, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1105, Loss: 0.26146839649852666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1106, Loss: 0.2724848364180958, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1107, Loss: 0.3252912427263156, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1108, Loss: 0.2626667614813277, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1109, Loss: 0.36460406463039596, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1110, Loss: 0.26085714805906324, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1111, Loss: 0.6018581090055443, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1112, Loss: 0.2825148202653443, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1113, Loss: 0.2652892438184212, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1114, Loss: 0.35164161037105157, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1115, Loss: 0.24479065180014242, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1116, Loss: 0.5241895908769022, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1117, Loss: 0.3641294849015979, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1118, Loss: 0.3122161488230262, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1119, Loss: 0.4801221906876959, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1120, Loss: 0.29168752996831215, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1121, Loss: 0.2855873673559096, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1122, Loss: 0.29525146562054827, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1123, Loss: 0.35683430810273675, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1124, Loss: 0.2837105151639715, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1125, Loss: 0.3131561238946103, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1126, Loss: 0.4385703816685683, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1127, Loss: 0.39636276153611333, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1128, Loss: 0.310288333390112, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1129, Loss: 0.25450723179152024, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1130, Loss: 0.26788032585602223, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1131, Loss: 0.7039672505299829, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1132, Loss: 0.24250394707667544, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1133, Loss: 0.25800944955074206, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1134, Loss: 0.3524861704236757, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1135, Loss: 0.36547548215394415, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1136, Loss: 0.3766821935756261, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1137, Loss: 0.5104712984375313, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1138, Loss: 0.47041532139564673, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1139, Loss: 0.27808171365456824, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1140, Loss: 0.3327551707733583, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1141, Loss: 0.5199667048464299, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1142, Loss: 0.3662083043608433, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1143, Loss: 0.35834250681040514, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1144, Loss: 0.31948838912122846, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1145, Loss: 0.2855254602552422, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1146, Loss: 0.3817470920100896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1147, Loss: 0.2621701276035904, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1148, Loss: 0.2442834293003395, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1149, Loss: 0.3587318291337873, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1150, Loss: 0.5361761127519312, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1151, Loss: 0.25926777975026377, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1152, Loss: 0.291691573544978, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1153, Loss: 0.25231113395107946, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1154, Loss: 0.31079115301875904, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1155, Loss: 0.34876856248661264, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1156, Loss: 0.3244354907474136, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1157, Loss: 0.26590081268238636, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1158, Loss: 0.36469066858028143, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1159, Loss: 0.35949308005230896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1160, Loss: 0.45621301997021246, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1161, Loss: 0.3377357420788863, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1162, Loss: 0.28563695948620255, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1163, Loss: 0.2455999355815424, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1164, Loss: 0.32080178884782984, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1165, Loss: 0.26864250638367504, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1166, Loss: 0.24578378853395405, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1167, Loss: 0.2433537723086618, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1168, Loss: 0.3400897990967146, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1169, Loss: 0.3139584185453361, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1170, Loss: 0.24784531152494635, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1171, Loss: 0.48782173135114554, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1172, Loss: 0.25304793254613583, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1173, Loss: 0.3084622359432059, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1174, Loss: 0.3185542940761603, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1175, Loss: 0.33901708710746625, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1176, Loss: 0.31852609844969626, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1177, Loss: 0.25555472500729426, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1178, Loss: 0.36380561482587714, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1179, Loss: 0.2670645974020798, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1180, Loss: 0.3590603966646312, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1181, Loss: 0.3129408750037218, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1182, Loss: 0.3497718487013055, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1183, Loss: 0.31799160916848596, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1184, Loss: 0.44755366527109797, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1185, Loss: 0.2726906061493901, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1186, Loss: 0.23240356278595894, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1187, Loss: 0.2624435484638109, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1188, Loss: 0.2711286494671038, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1189, Loss: 0.26620101146053854, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1190, Loss: 0.2591307679748523, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1191, Loss: 0.533659723783735, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1192, Loss: 0.24906944361792013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1193, Loss: 0.3726130431345299, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1194, Loss: 0.38509119899920874, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1195, Loss: 0.25807195073663963, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1196, Loss: 0.3430627029837426, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1197, Loss: 0.25467254704478887, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1198, Loss: 0.3332005166859572, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1199, Loss: 0.33314688967539086, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1200, Loss: 0.2514436216796797, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1201, Loss: 0.23135557946611532, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1202, Loss: 0.2512983774094084, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1203, Loss: 0.3840762456553173, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1204, Loss: 0.2966464973497733, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1205, Loss: 0.25495632038674043, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1206, Loss: 0.33094597460940056, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1207, Loss: 0.44149004499408745, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1208, Loss: 0.23059681450263048, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1209, Loss: 0.25664261155691886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1210, Loss: 0.2813365752012779, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1211, Loss: 0.4526094586345666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1212, Loss: 0.3547245596697164, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1213, Loss: 0.3778027250751886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1214, Loss: 0.2523979344533909, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1215, Loss: 0.3812159156298851, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1216, Loss: 0.2522514941118265, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1217, Loss: 0.25814308058233615, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1218, Loss: 0.6425918724676791, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1219, Loss: 0.26824415721816763, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1220, Loss: 0.34452823325211346, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1221, Loss: 0.4574699468672774, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1222, Loss: 0.6262196874654329, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1223, Loss: 0.3044156778529613, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1224, Loss: 0.2602759907364935, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1225, Loss: 0.3634493910725037, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1226, Loss: 0.28819088730120157, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1227, Loss: 0.3361595123998036, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1228, Loss: 0.2506874469553351, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1229, Loss: 0.5019757345811536, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1230, Loss: 0.24066043236509466, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1231, Loss: 0.34697083674979123, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1232, Loss: 0.26109651677609613, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1233, Loss: 0.3142845375221632, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1234, Loss: 0.3402406222239511, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1235, Loss: 0.2475796073616901, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1236, Loss: 0.480045157599864, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1237, Loss: 0.2608053796445876, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1238, Loss: 0.4760284616358622, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1239, Loss: 0.3522278548990141, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1240, Loss: 0.4122156580986125, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1241, Loss: 0.3421189354872819, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1242, Loss: 0.37893816649693446, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1243, Loss: 0.2361624632667668, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1244, Loss: 0.2401780458222192, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1245, Loss: 0.4069022693444983, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1246, Loss: 0.28065970934643086, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1247, Loss: 0.291881955618644, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1248, Loss: 0.3479371080413828, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1249, Loss: 0.2388128199241996, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1250, Loss: 0.33918127497882855, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1251, Loss: 0.30206422280982836, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1252, Loss: 0.41276511763565005, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1253, Loss: 0.2600314421529524, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1254, Loss: 0.3491358893731243, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1255, Loss: 0.24389945134422447, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1256, Loss: 0.4052490304379113, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1257, Loss: 0.3759480034716771, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1258, Loss: 0.23365737350934768, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1259, Loss: 0.42001503179290645, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1260, Loss: 0.2697087065536047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1261, Loss: 0.389194316281324, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1262, Loss: 0.5045890453152293, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1263, Loss: 0.37249464200806004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1264, Loss: 0.2742210378254566, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1265, Loss: 0.27874182105940465, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1266, Loss: 0.24546391738819745, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1267, Loss: 0.29496358001437245, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1268, Loss: 0.3800732446490137, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1269, Loss: 0.30365687752893505, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1270, Loss: 0.24060119332340762, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1271, Loss: 0.3223586504977396, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1272, Loss: 0.2541699431680928, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1273, Loss: 0.29458956669202263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1274, Loss: 0.43424833117184514, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1275, Loss: 0.2641073729825495, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1276, Loss: 0.4128487229895555, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1277, Loss: 0.2869590934269165, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1278, Loss: 0.30605305662018223, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1279, Loss: 0.3482480764110587, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1280, Loss: 0.3820258264275793, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1281, Loss: 0.24992815367780155, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1282, Loss: 0.2709834880044104, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1283, Loss: 0.27038405373587043, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1284, Loss: 0.27218459332673584, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1285, Loss: 0.25628453762363823, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1286, Loss: 0.2972683952355487, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1287, Loss: 0.3034652042247873, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1288, Loss: 0.6291826525799372, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1289, Loss: 0.26225733945397056, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1290, Loss: 0.3526196971776082, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1291, Loss: 0.3998092311706028, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1292, Loss: 0.25086710449359684, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1293, Loss: 0.45444665353660474, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1294, Loss: 0.28582079119366094, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1295, Loss: 0.2649998602030775, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1296, Loss: 0.276748256189963, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1297, Loss: 0.30126845703748295, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1298, Loss: 0.560448840884372, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1299, Loss: 0.5180776488952238, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1300, Loss: 0.24300862957961905, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1301, Loss: 0.3603200475006923, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1302, Loss: 0.30464420946429915, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1303, Loss: 0.2584789147402089, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1304, Loss: 0.2298463007973046, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1305, Loss: 0.38960920973846136, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1306, Loss: 0.28144331128940064, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1307, Loss: 0.3669758658582368, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1308, Loss: 0.2623528482883338, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1309, Loss: 0.39701560408624814, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1310, Loss: 0.26871206329299496, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1311, Loss: 0.26181418137334056, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1312, Loss: 0.3803751208204271, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1313, Loss: 0.35559626071968387, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1314, Loss: 0.3974073953460382, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1315, Loss: 0.26280257381503636, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1316, Loss: 0.3092417715822396, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1317, Loss: 0.2466749826897977, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1318, Loss: 0.23475349528638895, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1319, Loss: 0.25105014817208066, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1320, Loss: 0.3004189007195666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1321, Loss: 0.3636123669459925, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1322, Loss: 0.4162744694492096, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1323, Loss: 0.2773778437674826, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1324, Loss: 0.6688063828234925, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1325, Loss: 0.2659779516646166, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1326, Loss: 0.279661030724329, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1327, Loss: 0.36676900108315524, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1328, Loss: 0.2852066337047038, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1329, Loss: 0.2532527021337318, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1330, Loss: 0.23717077651632343, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1331, Loss: 0.23757960542900836, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1332, Loss: 0.2569789451437658, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1333, Loss: 0.42421527342537013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1334, Loss: 0.4051276535951107, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1335, Loss: 0.28053216469648, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1336, Loss: 0.3742340421287987, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1337, Loss: 0.2404312884427906, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1338, Loss: 0.3111399655289521, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1339, Loss: 0.5717711913612556, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1340, Loss: 0.3456593225950573, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1341, Loss: 0.3268914736510406, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1342, Loss: 0.5325037775833437, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1343, Loss: 0.3509499019544738, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1344, Loss: 0.29474077754918, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1345, Loss: 0.24833114297511769, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1346, Loss: 0.2290557400989019, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1347, Loss: 0.5334545094275263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1348, Loss: 0.36944935156621306, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1349, Loss: 0.6053554494689565, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1350, Loss: 0.2431285430759575, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1351, Loss: 0.2538590861236886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1352, Loss: 0.27299808963354777, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1353, Loss: 0.26603583721892615, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1354, Loss: 0.34132753461057874, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1355, Loss: 0.3113733109232283, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1356, Loss: 0.26074395590240756, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1357, Loss: 0.3475780848350927, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1358, Loss: 0.25882502397215673, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1359, Loss: 0.24579846105633285, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1360, Loss: 0.33994080629395124, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1361, Loss: 0.3843484111607468, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1362, Loss: 0.3043175571040636, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1363, Loss: 0.3110043638875074, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1364, Loss: 0.3172066357367681, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1365, Loss: 0.2937778843340009, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1366, Loss: 0.348061714306441, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1367, Loss: 0.2575856925797749, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1368, Loss: 0.4174617845180554, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1369, Loss: 0.25729733576022384, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1370, Loss: 0.41185249114301714, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1371, Loss: 0.42761494054106086, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1372, Loss: 0.2974245074135554, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1373, Loss: 0.33872653243676226, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1374, Loss: 0.2686599434653069, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1375, Loss: 0.24896686923036573, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1376, Loss: 0.25510898065492715, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1377, Loss: 0.4572575295759572, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1378, Loss: 0.4230772707239455, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1379, Loss: 0.2527902038219861, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1380, Loss: 0.4727487974433079, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1381, Loss: 0.36102390234708703, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1382, Loss: 0.3519729941752342, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1383, Loss: 0.2951844016385856, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1384, Loss: 0.26012555098642853, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1385, Loss: 0.23872625288687827, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1386, Loss: 0.3598254646779452, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1387, Loss: 0.2393781426392033, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1388, Loss: 0.30500340594680436, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1389, Loss: 0.27276774047721475, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1390, Loss: 0.39010656152485634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1391, Loss: 0.31160633934840193, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1392, Loss: 0.26118159247237205, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1393, Loss: 0.31716954217105753, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1394, Loss: 0.29704129423262493, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1395, Loss: 0.33152324697592017, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1396, Loss: 0.32765261704605164, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1397, Loss: 0.23169751643163727, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1398, Loss: 0.40594663018463384, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1399, Loss: 0.4312258757440661, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1400, Loss: 0.31710536624136465, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1401, Loss: 0.24684858440225, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1402, Loss: 0.3017072894183435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1403, Loss: 0.3082140269668966, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1404, Loss: 0.27867375304915387, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1405, Loss: 0.2566447583218176, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1406, Loss: 0.28626500524995524, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1407, Loss: 0.34957986032129906, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1408, Loss: 0.31865116439049096, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1409, Loss: 0.22666671954624465, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1410, Loss: 0.2836798819199659, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1411, Loss: 0.29083579039390595, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1412, Loss: 0.4527073167650154, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1413, Loss: 0.4754126079247043, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1414, Loss: 0.3814796868470587, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1415, Loss: 0.35949955640796183, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1416, Loss: 0.5165237039217779, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1417, Loss: 0.26382917358726826, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1418, Loss: 0.27492440864518886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1419, Loss: 0.3189201686419761, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1420, Loss: 0.28902856983462594, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1421, Loss: 0.2820371587812567, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1422, Loss: 0.27760826126971055, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1423, Loss: 0.3629313629469967, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1424, Loss: 0.3617268603910665, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1425, Loss: 0.5569539959330378, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1426, Loss: 0.23337689057139924, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1427, Loss: 0.2710892557389809, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1428, Loss: 0.40418229420067775, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1429, Loss: 0.2934992480809989, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1430, Loss: 0.3669391369002051, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1431, Loss: 0.2878166065586035, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1432, Loss: 0.6316160924948194, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1433, Loss: 0.2963108105545913, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1434, Loss: 0.27666812367164106, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1435, Loss: 0.26054173324218277, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1436, Loss: 0.2803156645267699, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1437, Loss: 0.328827875812399, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1438, Loss: 0.2899642052171986, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1439, Loss: 0.2406407719591583, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1440, Loss: 0.4252035191706636, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1441, Loss: 0.5230597253955571, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1442, Loss: 0.44636587689097773, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1443, Loss: 0.2965533670345805, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1444, Loss: 0.2697170195402994, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1445, Loss: 0.25522810308522886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1446, Loss: 0.23657212797326416, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1447, Loss: 0.23849084752585747, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1448, Loss: 0.28794364157538754, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1449, Loss: 0.3530522435905288, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1450, Loss: 0.2750907795686067, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1451, Loss: 0.2383994198230684, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1452, Loss: 0.26439692502333456, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1453, Loss: 0.23926904143680572, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1454, Loss: 0.26005335857091216, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1455, Loss: 0.3720731819406199, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1456, Loss: 0.299192429168584, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1457, Loss: 0.32849028139518804, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1458, Loss: 0.31081895149811223, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1459, Loss: 0.2582776850408896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1460, Loss: 0.2481642028316342, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1461, Loss: 0.47737358336796665, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1462, Loss: 0.28876797660118314, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1463, Loss: 0.24888286456541198, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1464, Loss: 0.31102384921771176, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1465, Loss: 0.24951952149811285, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1466, Loss: 0.2590914339425702, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1467, Loss: 0.568846308166407, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1468, Loss: 0.2399124612837283, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1469, Loss: 0.3608058513428468, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1470, Loss: 0.4185953421307004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1471, Loss: 0.26185757279204475, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1472, Loss: 0.3344463713543557, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1473, Loss: 0.2461494645820238, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1474, Loss: 0.3205272564503384, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1475, Loss: 0.3107038265891348, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1476, Loss: 0.3081103284476219, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1477, Loss: 0.3483308916566013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1478, Loss: 0.25160578418953344, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1479, Loss: 0.36581722318851656, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1480, Loss: 0.30059233070898134, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1481, Loss: 0.27835032193444686, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1482, Loss: 0.36832144235434816, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1483, Loss: 0.25753338661827463, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1484, Loss: 0.5097028293861159, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1485, Loss: 0.3408245198406896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1486, Loss: 0.2880290161837063, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1487, Loss: 0.2959069827097996, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1488, Loss: 0.276323638618209, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1489, Loss: 0.25500595476720994, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1490, Loss: 0.23064975035341068, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1491, Loss: 0.2734762684980792, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1492, Loss: 0.27689147128782343, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1493, Loss: 0.41518229312552896, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1494, Loss: 0.26255288586450487, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1495, Loss: 0.2602524919144976, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1496, Loss: 0.24722281882083172, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1497, Loss: 0.3170581823277421, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1498, Loss: 0.2582369414415504, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1499, Loss: 0.491932321081767, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1500, Loss: 0.32787407013210373, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1501, Loss: 0.30507829881124204, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1502, Loss: 0.263568475718146, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1503, Loss: 0.33250516031328237, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1504, Loss: 0.30239112231642906, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1505, Loss: 0.315583450195589, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1506, Loss: 0.25388049267679513, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1507, Loss: 0.25500086871842986, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1508, Loss: 0.398624867347411, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1509, Loss: 0.2605313513100595, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1510, Loss: 0.2651656490000741, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1511, Loss: 0.32210294001033435, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1512, Loss: 0.9831466003547256, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1513, Loss: 0.25442910554453535, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1514, Loss: 0.25796351092129954, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1515, Loss: 0.3222011085843225, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1516, Loss: 0.32578140697689273, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1517, Loss: 0.2749629451749034, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1518, Loss: 0.37535849472299676, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1519, Loss: 0.39728271843728546, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1520, Loss: 0.45035569007132015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1521, Loss: 0.2825352937353728, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1522, Loss: 0.3626868500802848, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1523, Loss: 0.3893279709245506, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1524, Loss: 0.29825522007687066, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1525, Loss: 0.32969070638974624, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1526, Loss: 0.24941315104873035, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1527, Loss: 0.3229125820475437, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1528, Loss: 0.37065161277542114, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1529, Loss: 0.27877406361900325, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1530, Loss: 0.26831850533158147, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1531, Loss: 0.28518155826756575, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1532, Loss: 0.3184209204648317, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1533, Loss: 0.5743105425296943, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1534, Loss: 0.24251499774212182, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1535, Loss: 0.2703076601063282, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1536, Loss: 0.34794462956886196, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1537, Loss: 0.4111413428229619, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1538, Loss: 0.2640975401726658, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1539, Loss: 0.24194734748025407, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1540, Loss: 0.3158983462298127, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1541, Loss: 0.34016951718480587, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1542, Loss: 0.2771184908459144, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1543, Loss: 0.24082214015258782, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1544, Loss: 0.24661645051925987, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1545, Loss: 0.423181180561056, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1546, Loss: 0.23638329374987005, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1547, Loss: 0.31122892429256455, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1548, Loss: 0.2710020725112956, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1549, Loss: 0.3539482234139022, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1550, Loss: 0.2617484382521551, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1551, Loss: 0.479607128239968, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1552, Loss: 0.27097674984575254, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1553, Loss: 0.3287724629260895, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1554, Loss: 0.39425948475936323, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1555, Loss: 0.3119703354232524, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1556, Loss: 0.326843061118877, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1557, Loss: 0.33970026364361444, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1558, Loss: 0.24899771504793478, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1559, Loss: 0.27771812747711033, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1560, Loss: 0.353504537480843, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1561, Loss: 0.29037123230156464, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1562, Loss: 0.6430520927581709, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1563, Loss: 0.2626622783186426, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1564, Loss: 0.3003231213113077, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1565, Loss: 0.234629198454512, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1566, Loss: 0.25335525673971165, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1567, Loss: 0.2791543919253074, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1568, Loss: 0.23277378003097174, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1569, Loss: 0.43519035129067773, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1570, Loss: 0.3521120577256811, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1571, Loss: 0.2325284027251069, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1572, Loss: 0.2772228054627676, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1573, Loss: 0.2581476129976145, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1574, Loss: 0.2616302653017077, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1575, Loss: 0.2895610030986126, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1576, Loss: 0.36246986869844106, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1577, Loss: 0.2320708348292556, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1578, Loss: 0.38218792216646336, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1579, Loss: 0.5768122967427715, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1580, Loss: 0.48050859407964963, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1581, Loss: 0.38226853943747047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1582, Loss: 0.37152081946520066, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1583, Loss: 0.4641948223462921, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1584, Loss: 0.3112490583684965, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1585, Loss: 0.2391705049618692, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1586, Loss: 0.26562202234567933, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1587, Loss: 0.23622888075289672, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1588, Loss: 0.3128965281598146, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1589, Loss: 0.5039455006762319, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1590, Loss: 0.25533701093670735, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1591, Loss: 0.2387416603908569, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1592, Loss: 0.26801099008325246, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1593, Loss: 0.23338440590041026, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1594, Loss: 0.2798120174491008, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1595, Loss: 0.3325280244365859, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1596, Loss: 0.25168883365238115, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1597, Loss: 0.31381670099804004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1598, Loss: 0.2552587360244832, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1599, Loss: 0.3164505774689217, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1600, Loss: 0.2674440465688566, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1601, Loss: 0.3475442633730015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1602, Loss: 0.24339753264074854, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1603, Loss: 0.34247609114784594, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1604, Loss: 0.37003859865675026, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1605, Loss: 0.2727684244543501, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1606, Loss: 0.40989310956710634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1607, Loss: 0.2503630006407426, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1608, Loss: 0.34151032374379847, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1609, Loss: 0.26743466564315416, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1610, Loss: 0.2645885936359215, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1611, Loss: 0.27243040725726014, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1612, Loss: 0.3273371314323648, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1613, Loss: 0.38938053530676586, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1614, Loss: 0.29692843822415294, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1615, Loss: 0.29389698447837714, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1616, Loss: 0.454601300829647, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1617, Loss: 0.2801976939964905, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1618, Loss: 0.2832006559162248, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1619, Loss: 0.3111979529629001, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1620, Loss: 0.28638525368152745, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1621, Loss: 0.2613556236381116, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1622, Loss: 0.23626477211512165, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1623, Loss: 0.235333673567635, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1624, Loss: 0.32121943395487956, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1625, Loss: 0.412347873930214, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1626, Loss: 0.23886092205315707, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1627, Loss: 0.5401091259152733, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1628, Loss: 0.2815963440449777, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1629, Loss: 0.2322710863051226, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1630, Loss: 0.24434253221895713, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1631, Loss: 0.25976741019356936, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1632, Loss: 0.3436184183071189, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1633, Loss: 0.3043029768463298, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1634, Loss: 0.4213350631141902, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1635, Loss: 0.4400969004379379, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1636, Loss: 0.24340973236739144, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1637, Loss: 0.26470365994150236, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1638, Loss: 0.2550918571157124, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1639, Loss: 0.35339351659516627, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1640, Loss: 0.30437718126877134, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1641, Loss: 0.45577901307187874, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1642, Loss: 0.2633358916484002, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1643, Loss: 0.30634540298767166, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1644, Loss: 0.5680333882975362, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1645, Loss: 0.34661316219155025, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1646, Loss: 0.24906588533721288, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1647, Loss: 0.28156182290208187, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1648, Loss: 0.2731014144475398, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1649, Loss: 0.2866635686326491, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1650, Loss: 0.3347535832939111, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1651, Loss: 0.6596307362148917, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1652, Loss: 0.27121695150325004, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1653, Loss: 0.28699024270144075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1654, Loss: 0.41255441885271205, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1655, Loss: 0.39618843990936614, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1656, Loss: 0.4336541921667596, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1657, Loss: 0.266303834999545, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1658, Loss: 0.42596445449962406, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1659, Loss: 0.3347616142802474, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1660, Loss: 0.27402395294845405, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1661, Loss: 0.43246565507449886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1662, Loss: 0.3563385686392761, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1663, Loss: 0.42286171612424756, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1664, Loss: 0.3287260870337538, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1665, Loss: 0.29052202235083013, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1666, Loss: 0.4776283907339455, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1667, Loss: 0.33999247808760164, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1668, Loss: 0.3733908182925319, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1669, Loss: 0.3396883280944032, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1670, Loss: 0.2680938117762846, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1671, Loss: 0.31571389414192397, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1672, Loss: 0.5337600121126889, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1673, Loss: 0.2652179810719213, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1674, Loss: 0.3309198606346571, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1675, Loss: 0.28296878861077823, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1676, Loss: 0.38198680589166945, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1677, Loss: 0.5337833149921203, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1678, Loss: 0.3029179985450529, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1679, Loss: 0.24713314431137867, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1680, Loss: 0.4001975141223283, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1681, Loss: 0.2974247579892747, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1682, Loss: 0.24567147781878093, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1683, Loss: 0.33635402698359007, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1684, Loss: 0.3632288854942126, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1685, Loss: 0.3079802390946063, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1686, Loss: 0.8095450512836708, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1687, Loss: 0.2568128253881259, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1688, Loss: 0.24509372719235503, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1689, Loss: 0.5920939851318691, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1690, Loss: 0.2516106446807635, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1691, Loss: 0.4786143655139317, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1692, Loss: 0.430496439547585, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1693, Loss: 0.30711807766310717, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1694, Loss: 0.302364522093959, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1695, Loss: 0.2591323305338035, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1696, Loss: 0.4263037942025638, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1697, Loss: 0.4588437622088463, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1698, Loss: 0.29108646374855596, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1699, Loss: 0.3058241981012372, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1700, Loss: 0.246084582122546, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1701, Loss: 0.25829931770585196, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1702, Loss: 0.29636724710315177, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1703, Loss: 0.3176148292787067, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1704, Loss: 0.24224826787837125, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1705, Loss: 0.28986376603394126, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1706, Loss: 0.4017513186720816, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1707, Loss: 0.3379744505286417, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1708, Loss: 0.35972501430330867, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1709, Loss: 0.22866507786327192, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1710, Loss: 0.3080037816559201, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1711, Loss: 0.40128808437820407, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1712, Loss: 0.44922931244518005, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1713, Loss: 0.26542550868200937, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1714, Loss: 0.2949076981157054, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1715, Loss: 0.375318571870766, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1716, Loss: 0.29530586015363414, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1717, Loss: 0.24982328484550745, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1718, Loss: 0.40058399097055575, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1719, Loss: 0.2909219626947157, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1720, Loss: 0.3033411238480442, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1721, Loss: 0.3629428995757765, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1722, Loss: 0.31497079146527923, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1723, Loss: 0.29653035697207275, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1724, Loss: 0.2540185962082397, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1725, Loss: 0.2815310782369017, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1726, Loss: 0.32634774709503445, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1727, Loss: 0.3013582776847813, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1728, Loss: 0.3220755333484441, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1729, Loss: 0.3708841383056227, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1730, Loss: 0.30694409604156514, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1731, Loss: 0.2679996691548505, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1732, Loss: 0.336333592690252, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1733, Loss: 0.5212367252819841, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1734, Loss: 0.29712438036022804, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1735, Loss: 0.2799756583527649, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1736, Loss: 0.40931706502918086, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1737, Loss: 0.3019973049650586, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1738, Loss: 0.2733878448112873, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1739, Loss: 0.3123931041262643, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1740, Loss: 0.2677481190793984, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1741, Loss: 0.3131731906889892, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1742, Loss: 0.24801177950523137, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1743, Loss: 0.2715599214167353, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1744, Loss: 0.5409751958030706, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1745, Loss: 0.30970357117973274, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1746, Loss: 0.2706676878434993, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1747, Loss: 0.31539459211758775, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1748, Loss: 0.380287788816996, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1749, Loss: 0.31590538160700116, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1750, Loss: 0.41762029051393545, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1751, Loss: 0.3040083691906747, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1752, Loss: 0.25474257792795263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1753, Loss: 0.2562370896036291, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1754, Loss: 0.3261367033145708, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1755, Loss: 0.3590863286359961, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1756, Loss: 0.23814644213734373, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1757, Loss: 0.37952078358452956, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1758, Loss: 0.2666645484439912, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1759, Loss: 0.2758214357889826, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1760, Loss: 0.376257149723327, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1761, Loss: 0.22971053448244955, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1762, Loss: 0.5121009310259954, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1763, Loss: 0.4414848475436446, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1764, Loss: 0.37166306348299943, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1765, Loss: 0.2864122981019462, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1766, Loss: 0.23479780881496065, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1767, Loss: 0.23875635891407263, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1768, Loss: 0.28546428474062724, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1769, Loss: 0.35905576407142026, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1770, Loss: 0.3990398047791936, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1771, Loss: 0.42696539901455055, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1772, Loss: 0.27690100016183283, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1773, Loss: 0.3701811332713314, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1774, Loss: 0.3167589374326913, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1775, Loss: 0.27267135475691295, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1776, Loss: 0.25440815418852886, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1777, Loss: 0.3268502504310948, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1778, Loss: 0.37558706428303046, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1779, Loss: 0.3437525921234236, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1780, Loss: 0.24007801248016183, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1781, Loss: 0.28455135609732735, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1782, Loss: 0.340384047349257, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1783, Loss: 0.34209124972467153, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1784, Loss: 0.27568625395027346, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1785, Loss: 0.3961082816425444, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1786, Loss: 0.4368224484146666, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1787, Loss: 0.2474517188216425, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1788, Loss: 0.2812280826794394, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1789, Loss: 0.3535569194324655, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1790, Loss: 0.37557487008431445, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1791, Loss: 0.247794327103225, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1792, Loss: 0.35494880077350277, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1793, Loss: 0.4062792991433534, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1794, Loss: 0.3262632786348061, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1795, Loss: 0.25674615113579785, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1796, Loss: 0.5251724085023672, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1797, Loss: 0.2517852303789672, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1798, Loss: 0.2377849474656112, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1799, Loss: 0.25527207307956357, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1800, Loss: 0.3306288586047354, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1801, Loss: 0.36388406373764437, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1802, Loss: 0.5100799849963564, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1803, Loss: 0.2411764467963905, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1804, Loss: 0.31833301972879346, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1805, Loss: 0.4053607810289078, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1806, Loss: 0.49470342026274106, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1807, Loss: 0.3571740085103906, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1808, Loss: 0.4588956919538565, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1809, Loss: 0.3334963712420673, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1810, Loss: 0.2511588291819274, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1811, Loss: 0.3546733207231437, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1812, Loss: 0.2998688470655861, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1813, Loss: 0.28146864097588786, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1814, Loss: 0.3568873400176428, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1815, Loss: 0.26862343075199924, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1816, Loss: 0.3107338605867678, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1817, Loss: 0.3747773979068202, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1818, Loss: 0.3422397877279125, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1819, Loss: 0.23400392471170606, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1820, Loss: 0.30529204256859505, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1821, Loss: 0.29185532645468015, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1822, Loss: 0.25073282375198885, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1823, Loss: 0.2728734717799553, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1824, Loss: 0.2603615038596006, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1825, Loss: 0.33824250940775924, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1826, Loss: 0.6806075795686931, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1827, Loss: 0.2565768893177956, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1828, Loss: 0.43856449762590394, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1829, Loss: 0.5138693550769262, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1830, Loss: 0.3234366642664042, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1831, Loss: 0.2709456916873389, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1832, Loss: 0.3138519055663478, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1833, Loss: 0.25684077783672066, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1834, Loss: 0.2636458118161422, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1835, Loss: 0.29778187984958515, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1836, Loss: 0.26751321351389784, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1837, Loss: 0.24005014502333163, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1838, Loss: 0.2756473827240917, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1839, Loss: 0.2912529133589075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1840, Loss: 0.4207531258953993, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1841, Loss: 0.318210514366672, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1842, Loss: 0.29633213067732184, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1843, Loss: 0.26574687506248135, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1844, Loss: 0.271057087037183, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1845, Loss: 0.25006520020150025, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1846, Loss: 0.28228446899837634, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1847, Loss: 0.31808769970548256, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1848, Loss: 0.37566152867450253, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1849, Loss: 0.31437702655405414, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1850, Loss: 0.25983551616781053, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1851, Loss: 0.37216470065243246, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1852, Loss: 0.4912452445267973, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1853, Loss: 0.2656469545564587, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1854, Loss: 0.3361121793733419, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1855, Loss: 0.3036152471076621, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1856, Loss: 0.3450641359417712, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1857, Loss: 0.38133068640198753, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1858, Loss: 0.3896342154822047, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1859, Loss: 0.45605615742031685, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1860, Loss: 0.4648945890455203, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1861, Loss: 0.5073723639493684, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1862, Loss: 0.24516902396155027, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1863, Loss: 0.491451203954086, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1864, Loss: 0.3022785811342096, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1865, Loss: 0.36068325063659845, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1866, Loss: 0.2934062659468399, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1867, Loss: 0.26284673781400775, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1868, Loss: 0.2552040289860402, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1869, Loss: 0.29571228651271075, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1870, Loss: 0.29674748080531677, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1871, Loss: 0.29731770083855147, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1872, Loss: 0.3284156880370311, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1873, Loss: 0.35072498677088815, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1874, Loss: 0.2953939647655828, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Batch 1875, Loss: 0.5289924948096416, Batch Size: 32, Learning Rate: 3.6271648069723874e-05\n",
      "Epoch 15, Updated Learning Rate: 3.083090085926529e-05\n",
      "Epoch 15, Average Loss: 0.332318301546566, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1, Loss: 0.4889689841742021, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 2, Loss: 0.2877708415643097, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 3, Loss: 0.33893659775002083, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 4, Loss: 0.4744591573827823, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 5, Loss: 0.3113942214375849, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 6, Loss: 0.28690324945287626, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 7, Loss: 0.3237737353542306, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 8, Loss: 0.3913722548743395, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 9, Loss: 0.24696202295533, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 10, Loss: 0.41798566467348064, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 11, Loss: 0.2726932151994853, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 12, Loss: 0.449042015417793, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 13, Loss: 0.34626620973208116, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 14, Loss: 0.4211259153071527, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 15, Loss: 0.2789242982618275, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 16, Loss: 0.4475421127808009, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 17, Loss: 0.40091078912638034, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 18, Loss: 0.31558992826322635, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 19, Loss: 0.2616346965922933, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 20, Loss: 0.27863633035988844, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 21, Loss: 0.4416383171832111, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 22, Loss: 0.3412886843923356, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 23, Loss: 0.30582245596527136, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 24, Loss: 0.36767299850346813, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 25, Loss: 0.3360743315000403, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 26, Loss: 0.23578877610259916, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 27, Loss: 0.30328887334704574, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 28, Loss: 0.3340691685433886, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 29, Loss: 0.5769540110210679, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 30, Loss: 0.2695213121113413, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 31, Loss: 0.331268609580653, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 32, Loss: 0.29143160163760495, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 33, Loss: 0.29530912966126444, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 34, Loss: 0.24843267284927947, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 35, Loss: 0.2474334274927767, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 36, Loss: 0.36880837637099095, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 37, Loss: 0.24735952309753417, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 38, Loss: 0.2914496457034407, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 39, Loss: 0.310767523447924, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 40, Loss: 0.3759792288359718, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 41, Loss: 0.25279165404535114, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 42, Loss: 0.22939624697093683, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 43, Loss: 0.5833019724727526, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 44, Loss: 0.250557279800887, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 45, Loss: 0.27415019745001606, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 46, Loss: 0.4539653694832581, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 47, Loss: 0.2926632729774966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 48, Loss: 0.26452440840895863, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 49, Loss: 0.6147839187608001, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 50, Loss: 0.2590504954789061, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 51, Loss: 0.3299087676285751, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 52, Loss: 0.3432095836554615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 53, Loss: 0.3988276711508205, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 54, Loss: 0.3272732089151992, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 55, Loss: 0.274965836035377, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 56, Loss: 0.3478613144655209, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 57, Loss: 0.28873384134543567, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 58, Loss: 0.2719502407318794, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 59, Loss: 0.2612544375481911, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 60, Loss: 0.42983963580311557, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 61, Loss: 0.36159348244808764, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 62, Loss: 0.2836391769192502, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 63, Loss: 0.29916317899241746, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 64, Loss: 0.3666719790023922, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 65, Loss: 0.373006411975184, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 66, Loss: 0.2740432747692977, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 67, Loss: 0.3499124783503391, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 68, Loss: 0.2650003124961885, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 69, Loss: 0.2610007757963604, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 70, Loss: 0.5138047413724202, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 71, Loss: 0.5000101967484252, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 72, Loss: 0.2434217818597727, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 73, Loss: 0.28500843378090746, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 74, Loss: 0.2597062249340607, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 75, Loss: 0.32339737896906706, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 76, Loss: 0.24319239006088889, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 77, Loss: 0.281295536939443, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 78, Loss: 0.3597629584671208, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 79, Loss: 0.3275525774700799, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 80, Loss: 0.3682842944712358, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 81, Loss: 0.3907528119938559, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 82, Loss: 0.40121124947653275, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 83, Loss: 0.2574602845960356, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 84, Loss: 0.257353446133481, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 85, Loss: 0.3225794805380785, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 86, Loss: 0.2918925336704224, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 87, Loss: 0.32573541851307386, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 88, Loss: 0.2701437661947067, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 89, Loss: 0.27286170031283485, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 90, Loss: 0.2792120836864353, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 91, Loss: 0.2403426213767715, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 92, Loss: 0.24988438987420147, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 93, Loss: 0.3452426942884443, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 94, Loss: 0.2622489208303018, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 95, Loss: 0.4934904523839152, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 96, Loss: 0.307639258571841, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 97, Loss: 0.26030568945516286, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 98, Loss: 0.26960251151554526, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 99, Loss: 0.24046845475375206, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 100, Loss: 0.29339143882198815, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 101, Loss: 0.28933772806138824, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 102, Loss: 0.23749803163421088, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 103, Loss: 0.28277297673005913, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 104, Loss: 0.32827027657921615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 105, Loss: 0.2918804203324556, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 106, Loss: 0.23890557492600567, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 107, Loss: 0.28922452442431884, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 108, Loss: 0.3267988386083553, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 109, Loss: 0.3246475461680715, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 110, Loss: 0.2508926395401978, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 111, Loss: 0.30724575343815974, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 112, Loss: 0.3185072249145279, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 113, Loss: 0.28935787129986107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 114, Loss: 0.2626830102940992, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 115, Loss: 0.2950746732528836, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 116, Loss: 0.24845239602364738, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 117, Loss: 0.5277949165668333, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 118, Loss: 0.27208690242913525, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 119, Loss: 0.2884753734897408, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 120, Loss: 0.39510928986938054, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 121, Loss: 0.2532814901970199, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 122, Loss: 0.30447660844456625, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 123, Loss: 0.3909685493606145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 124, Loss: 0.22986785937517384, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 125, Loss: 0.43896925237106893, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 126, Loss: 0.25573175468151843, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 127, Loss: 0.3929759620837968, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 128, Loss: 0.33898711770020956, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 129, Loss: 0.4446719373151793, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 130, Loss: 0.324486116156435, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 131, Loss: 0.25880441290201966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 132, Loss: 0.34797522552858084, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 133, Loss: 0.4639926962752514, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 134, Loss: 0.3946937554561224, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 135, Loss: 0.26430967653756127, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 136, Loss: 0.4528441085476502, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 137, Loss: 0.2409436183801264, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 138, Loss: 0.31390860035190593, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 139, Loss: 0.28294105350671245, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 140, Loss: 0.4969132028842995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 141, Loss: 0.31081548145612503, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 142, Loss: 0.24133431007989264, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 143, Loss: 0.4231045606065073, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 144, Loss: 0.4064370348817433, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 145, Loss: 0.36044423167047257, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 146, Loss: 0.3610618610023957, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 147, Loss: 0.44811299296389007, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 148, Loss: 0.2871528552633494, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 149, Loss: 0.2929806636081599, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 150, Loss: 0.2659323184637377, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 151, Loss: 0.4522513645026501, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 152, Loss: 0.2753200308857357, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 153, Loss: 0.26307386030306334, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 154, Loss: 0.3368453251484316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 155, Loss: 0.22727669834961267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 156, Loss: 0.3397172329939259, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 157, Loss: 0.2500095896305417, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 158, Loss: 0.26172050972275057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 159, Loss: 0.2755827782818231, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 160, Loss: 0.3609769459500329, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 161, Loss: 0.30052628953669197, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 162, Loss: 0.2527536878887812, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 163, Loss: 0.3333570224090855, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 164, Loss: 0.44439041023192766, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 165, Loss: 0.2887368381026083, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 166, Loss: 0.3072884754624087, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 167, Loss: 0.29516059358533625, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 168, Loss: 0.4937091547388338, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 169, Loss: 0.3457854057937031, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 170, Loss: 0.2580171849723436, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 171, Loss: 0.27666616934763977, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 172, Loss: 0.296701588302155, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 173, Loss: 0.35807317400900357, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 174, Loss: 0.2875578909515355, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 175, Loss: 0.25042115812538035, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 176, Loss: 0.24398671434286628, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 177, Loss: 0.3386185747347398, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 178, Loss: 0.2505916822794039, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 179, Loss: 0.3017897167881103, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 180, Loss: 0.2981772295368552, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 181, Loss: 0.2589357325606576, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 182, Loss: 0.2796341074828899, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 183, Loss: 0.32389742179761144, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 184, Loss: 0.4262213343414565, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 185, Loss: 0.36604348336433645, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 186, Loss: 0.3004831810079341, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 187, Loss: 0.4640321284422906, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 188, Loss: 0.2928899918490464, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 189, Loss: 0.4377983939470361, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 190, Loss: 0.33882029458024215, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 191, Loss: 0.39823529605002034, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 192, Loss: 0.2816204923008679, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 193, Loss: 0.27955972953089725, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 194, Loss: 0.30764428105674, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 195, Loss: 0.294580029772754, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 196, Loss: 0.2858255123663547, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 197, Loss: 0.37547098411655133, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 198, Loss: 0.30186688190819677, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 199, Loss: 0.352141329098285, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 200, Loss: 0.36990333924943847, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 201, Loss: 0.29288019419318095, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 202, Loss: 0.33338397332734826, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 203, Loss: 0.3825439163303729, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 204, Loss: 0.5613558279466692, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 205, Loss: 0.2319657437803695, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 206, Loss: 0.27487029323276113, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 207, Loss: 0.4228506794122934, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 208, Loss: 0.3150568972663298, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 209, Loss: 0.429312901024261, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 210, Loss: 0.24901199876912555, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 211, Loss: 0.3024076169043982, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 212, Loss: 0.3876380046053579, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 213, Loss: 0.3597988498500417, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 214, Loss: 0.5325950583550262, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 215, Loss: 0.3315900884560465, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 216, Loss: 0.28902675206608885, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 217, Loss: 0.3817821585080243, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 218, Loss: 0.30184386079685727, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 219, Loss: 0.2856575210453103, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 220, Loss: 0.2652973961365358, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 221, Loss: 0.2611888930077145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 222, Loss: 0.3342449325706767, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 223, Loss: 0.3520364057741077, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 224, Loss: 0.30676490385774846, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 225, Loss: 0.2975471026978384, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 226, Loss: 0.5110977388607691, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 227, Loss: 0.47706224438901057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 228, Loss: 0.3285813001194162, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 229, Loss: 0.27605987897840456, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 230, Loss: 0.3764233135037976, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 231, Loss: 0.30772763149526927, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 232, Loss: 0.6481204587861737, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 233, Loss: 0.3006524242514416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 234, Loss: 0.37490549165165, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 235, Loss: 0.37801756944653164, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 236, Loss: 0.25718226196142824, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 237, Loss: 0.28373230225867396, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 238, Loss: 0.25236739288978344, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 239, Loss: 0.31529298226677105, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 240, Loss: 0.2531879159737837, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 241, Loss: 0.27436196715710587, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 242, Loss: 0.30871365777055065, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 243, Loss: 0.262516669663458, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 244, Loss: 0.28509672935099256, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 245, Loss: 0.2757572087579221, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 246, Loss: 0.27112988183646375, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 247, Loss: 0.2557404186205319, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 248, Loss: 0.25224716636631195, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 249, Loss: 0.2699412904396805, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 250, Loss: 0.7578929620000303, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 251, Loss: 0.3745040864458644, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 252, Loss: 0.4084284822810912, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 253, Loss: 0.26325385290417247, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 254, Loss: 0.25401523063875564, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 255, Loss: 0.3411224206630157, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 256, Loss: 0.2720806899404115, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 257, Loss: 0.39990814772207284, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 258, Loss: 0.45762406598274574, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 259, Loss: 0.23582543454345598, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 260, Loss: 0.338445023189168, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 261, Loss: 0.26613720203207736, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 262, Loss: 0.2501999898699896, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 263, Loss: 0.26238611658106664, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 264, Loss: 0.35802557582521893, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 265, Loss: 0.26395140499759856, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 266, Loss: 0.25329007761455485, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 267, Loss: 0.2748202560387028, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 268, Loss: 0.36543027007795625, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 269, Loss: 0.35765181162460846, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 270, Loss: 0.32948453226069785, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 271, Loss: 0.30675801478234493, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 272, Loss: 0.27006902844499214, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 273, Loss: 0.27073348270014047, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 274, Loss: 0.44596273026357514, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 275, Loss: 0.41554375769247753, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 276, Loss: 0.28572361220803016, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 277, Loss: 0.30259264135871033, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 278, Loss: 0.365625497982457, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 279, Loss: 0.3195029580425788, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 280, Loss: 0.29362449616248915, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 281, Loss: 0.26808275575418855, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 282, Loss: 0.2760008882642394, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 283, Loss: 0.32310766962224147, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 284, Loss: 0.3234670670613057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 285, Loss: 0.3078943715587015, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 286, Loss: 0.25046238738975973, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 287, Loss: 0.407309016492166, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 288, Loss: 0.44945762296413627, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 289, Loss: 0.2856273256541694, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 290, Loss: 0.29396644273589356, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 291, Loss: 0.4783137839153456, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 292, Loss: 0.25711614294216323, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 293, Loss: 0.2720645883396304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 294, Loss: 0.2965193570083287, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 295, Loss: 0.2444275656690966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 296, Loss: 0.312340493365442, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 297, Loss: 0.2805155090163788, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 298, Loss: 0.2823501493634848, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 299, Loss: 0.326727964987541, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 300, Loss: 0.31974255756072534, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 301, Loss: 0.30832925575640624, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 302, Loss: 0.33003834165000756, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 303, Loss: 0.48119190215058244, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 304, Loss: 0.3219402245536579, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 305, Loss: 0.2568497256046845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 306, Loss: 0.3044431357731323, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 307, Loss: 0.4369562713091363, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 308, Loss: 0.30994180222845963, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 309, Loss: 0.26416527614659535, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 310, Loss: 0.2423854354551588, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 311, Loss: 0.3493315421819452, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 312, Loss: 0.24845822768619408, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 313, Loss: 0.4145141152881907, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 314, Loss: 0.2441382646036081, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 315, Loss: 0.24123802339040593, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 316, Loss: 0.24854770549861682, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 317, Loss: 0.6504672796241167, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 318, Loss: 0.24510371743553622, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 319, Loss: 0.2538594476139402, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 320, Loss: 0.28325244481955375, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 321, Loss: 0.42985255242062614, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 322, Loss: 0.28994485333174946, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 323, Loss: 0.2455346745737613, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 324, Loss: 0.3883349168879322, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 325, Loss: 0.24423216000040418, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 326, Loss: 0.26241390223202965, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 327, Loss: 0.24721539693178585, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 328, Loss: 0.23943549917603613, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 329, Loss: 0.25708542480006946, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 330, Loss: 0.2690597630885113, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 331, Loss: 0.34887324422474864, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 332, Loss: 0.38107636444890924, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 333, Loss: 0.3399561839664299, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 334, Loss: 0.2682135179170554, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 335, Loss: 0.39433750902464904, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 336, Loss: 0.31676992415038224, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 337, Loss: 0.2366879149338349, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 338, Loss: 0.34145254809986525, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 339, Loss: 0.33208100790605943, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 340, Loss: 0.23943190746736712, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 341, Loss: 0.487909196540821, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 342, Loss: 0.24195474582911203, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 343, Loss: 0.2483563452641829, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 344, Loss: 0.23741746812827189, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 345, Loss: 0.2734932875007446, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 346, Loss: 0.3482183825588142, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 347, Loss: 0.2544681730093825, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 348, Loss: 0.27479552529239654, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 349, Loss: 0.3134956961481776, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 350, Loss: 0.6023462299199702, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 351, Loss: 0.30299358327039705, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 352, Loss: 0.25655104941264073, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 353, Loss: 0.2918808386136774, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 354, Loss: 0.24595490549781435, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 355, Loss: 0.31530622797163566, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 356, Loss: 0.37343204822317544, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 357, Loss: 0.3372569745290708, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 358, Loss: 0.3029039185068746, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 359, Loss: 0.4204565468143552, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 360, Loss: 0.26709966570927784, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 361, Loss: 0.3092962914172998, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 362, Loss: 0.2871483734338498, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 363, Loss: 0.3004253755082615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 364, Loss: 0.27888148455694517, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 365, Loss: 0.2745778026543605, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 366, Loss: 0.2929307314537225, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 367, Loss: 0.267056637699272, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 368, Loss: 0.34920073302834664, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 369, Loss: 0.4506633003386564, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 370, Loss: 0.3781312059292261, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 371, Loss: 0.33876405851001523, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 372, Loss: 0.4326058250778635, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 373, Loss: 0.3119168194408309, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 374, Loss: 0.29497032848406995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 375, Loss: 0.2663745593083467, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 376, Loss: 0.2377111888915987, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 377, Loss: 0.5626480089723097, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 378, Loss: 0.243918747622132, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 379, Loss: 0.25116805980030077, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 380, Loss: 0.6067417542612317, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 381, Loss: 0.36325523251998226, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 382, Loss: 0.4403158768595254, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 383, Loss: 0.744728737229302, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 384, Loss: 0.3479589179076132, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 385, Loss: 0.41419670542884, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 386, Loss: 0.2710837784429072, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 387, Loss: 0.46885105164767876, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 388, Loss: 0.49195249461813395, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 389, Loss: 0.3773404247747775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 390, Loss: 0.2715818310613233, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 391, Loss: 0.28331463441738935, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 392, Loss: 0.2654728218891905, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 393, Loss: 0.31422353077136483, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 394, Loss: 0.29156706958679907, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 395, Loss: 0.24069969584471135, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 396, Loss: 0.40132436992046244, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 397, Loss: 0.2419913791616036, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 398, Loss: 0.2602325723829225, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 399, Loss: 0.25839927619620096, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 400, Loss: 0.34182703590104513, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 401, Loss: 0.2902513337730092, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 402, Loss: 0.2837835314949822, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 403, Loss: 0.39607493741763733, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 404, Loss: 0.28619104298406295, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 405, Loss: 0.327217588176939, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 406, Loss: 0.5482848572695878, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 407, Loss: 0.25547128626408233, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 408, Loss: 0.2844548138442906, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 409, Loss: 0.2575803237453133, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 410, Loss: 0.28857148434616336, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 411, Loss: 0.29965867331298107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 412, Loss: 0.23468220991007924, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 413, Loss: 0.3701858970032704, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 414, Loss: 0.7807000568624267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 415, Loss: 0.4097701660132298, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 416, Loss: 0.3528937965009319, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 417, Loss: 0.2530286056598805, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 418, Loss: 0.24368853479023467, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 419, Loss: 0.3611536780136248, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 420, Loss: 0.4664643460726752, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 421, Loss: 0.28673184726531303, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 422, Loss: 0.25992223121562125, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 423, Loss: 0.2593379017955215, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 424, Loss: 0.267633789547037, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 425, Loss: 0.28897239408357045, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 426, Loss: 0.26287814036591406, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 427, Loss: 0.25669970348965676, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 428, Loss: 0.23012085069302995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 429, Loss: 0.2704298830282842, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 430, Loss: 0.34713932848690016, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 431, Loss: 0.2928363215990035, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 432, Loss: 0.4478560728524742, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 433, Loss: 0.2687802150192654, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 434, Loss: 0.2882504005624565, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 435, Loss: 0.34525906793276917, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 436, Loss: 0.3811898557376754, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 437, Loss: 0.27343954044853236, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 438, Loss: 0.3145786248163778, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 439, Loss: 0.31103880230439274, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 440, Loss: 0.2834673717139331, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 441, Loss: 0.3392556116154612, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 442, Loss: 0.2714653137948031, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 443, Loss: 0.33369999031751263, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 444, Loss: 0.36557526573914867, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 445, Loss: 0.24589356485290315, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 446, Loss: 0.7205377353330191, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 447, Loss: 0.30219043529512046, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 448, Loss: 0.31346221104440375, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 449, Loss: 0.2699845705709202, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 450, Loss: 0.5021619680032963, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 451, Loss: 0.25504685308766784, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 452, Loss: 0.28666982062215146, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 453, Loss: 0.2910611950350572, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 454, Loss: 0.5319786702917457, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 455, Loss: 0.23334585513896108, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 456, Loss: 0.24868828302493987, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 457, Loss: 0.32790571000341423, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 458, Loss: 0.31118047482194566, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 459, Loss: 0.26669025605189467, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 460, Loss: 0.28689960807739856, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 461, Loss: 0.2445598919854054, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 462, Loss: 0.3299702929879974, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 463, Loss: 0.25164888446721606, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 464, Loss: 0.2924755631491019, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 465, Loss: 0.5404647911992692, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 466, Loss: 0.2795797831038789, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 467, Loss: 0.3593633582090616, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 468, Loss: 0.2771643294068598, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 469, Loss: 0.238293179230378, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 470, Loss: 0.5898432488836582, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 471, Loss: 0.25839828240061785, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 472, Loss: 0.4515873382572022, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 473, Loss: 0.4024992650524286, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 474, Loss: 0.47987948633561794, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 475, Loss: 0.2989371864973816, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 476, Loss: 0.2815232202745009, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 477, Loss: 0.47767503606946304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 478, Loss: 0.27851090311054505, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 479, Loss: 0.4728660535197869, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 480, Loss: 0.4082121871311052, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 481, Loss: 0.26563406018175106, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 482, Loss: 0.23279896806862682, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 483, Loss: 0.30624897200617107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 484, Loss: 0.23531233841050916, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 485, Loss: 0.7133205230981945, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 486, Loss: 0.2510989486426199, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 487, Loss: 0.24868303666417907, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 488, Loss: 0.6680074643421502, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 489, Loss: 0.4994440391385012, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 490, Loss: 0.2472293276494569, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 491, Loss: 0.3926533182606838, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 492, Loss: 0.2538692395348347, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 493, Loss: 0.5673748357896016, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 494, Loss: 0.2909175877430174, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 495, Loss: 0.3375581156697734, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 496, Loss: 0.31579559118772843, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 497, Loss: 0.32374762542773683, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 498, Loss: 0.257928138021641, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 499, Loss: 0.26359004951455434, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 500, Loss: 0.2639183237477215, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 501, Loss: 0.31049196532369894, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 502, Loss: 0.3717894535528164, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 503, Loss: 0.5964785675729092, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 504, Loss: 0.24608396995349519, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 505, Loss: 0.2651445588665528, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 506, Loss: 0.28623874364360413, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 507, Loss: 0.36971214151417686, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 508, Loss: 0.3225754540214181, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 509, Loss: 0.43978005994018476, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 510, Loss: 0.2923065656535797, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 511, Loss: 0.37482586424513636, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 512, Loss: 0.29392530842947306, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 513, Loss: 0.29533134809929185, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 514, Loss: 0.341034371692011, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 515, Loss: 0.27609148475326917, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 516, Loss: 0.2734548005830387, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 517, Loss: 0.3330121367307148, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 518, Loss: 0.3481003059984066, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 519, Loss: 0.3585854708987395, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 520, Loss: 0.5203012017770997, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 521, Loss: 0.34477058880087824, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 522, Loss: 0.29817759335890337, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 523, Loss: 0.29024489498912326, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 524, Loss: 0.3596020262480745, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 525, Loss: 0.3771138893560197, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 526, Loss: 0.4948455473233888, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 527, Loss: 0.26809410234346615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 528, Loss: 0.2996385297485484, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 529, Loss: 0.29048398867142505, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 530, Loss: 0.3262514737207354, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 531, Loss: 0.4511114402990458, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 532, Loss: 0.30798527094062017, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 533, Loss: 0.25733435625900125, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 534, Loss: 0.4441628718034592, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 535, Loss: 0.24550874224310248, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 536, Loss: 0.4777054495541747, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 537, Loss: 0.3944042957071604, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 538, Loss: 0.4081402467855156, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 539, Loss: 0.33647404242061474, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 540, Loss: 0.2737197745711589, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 541, Loss: 0.30113600493629145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 542, Loss: 0.2650456634187748, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 543, Loss: 0.30435841448640283, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 544, Loss: 0.23313885509325952, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 545, Loss: 0.2865821252193943, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 546, Loss: 0.3699946536723441, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 547, Loss: 0.2465395069092058, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 548, Loss: 0.3099327635814689, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 549, Loss: 0.2717538351396481, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 550, Loss: 0.3000845249008196, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 551, Loss: 0.38547767631274044, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 552, Loss: 0.4551829974378039, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 553, Loss: 0.24111776721962705, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 554, Loss: 0.30453367774768814, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 555, Loss: 0.2546465844650942, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 556, Loss: 0.46023370890182125, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 557, Loss: 0.33251738790839425, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 558, Loss: 0.37444389994985383, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 559, Loss: 0.2660982433514073, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 560, Loss: 0.32630076182528467, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 561, Loss: 0.33487225066454934, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 562, Loss: 0.3304354088915533, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 563, Loss: 0.3261216265810243, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 564, Loss: 0.25890451157574473, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 565, Loss: 0.3238433993879562, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 566, Loss: 0.2577938103174407, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 567, Loss: 0.3154814968467883, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 568, Loss: 0.3891260383473506, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 569, Loss: 0.35954990958868815, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 570, Loss: 0.28876866328346723, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 571, Loss: 0.23577273432310344, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 572, Loss: 0.30257359003195816, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 573, Loss: 0.2702182690302156, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 574, Loss: 0.6374396777134913, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 575, Loss: 0.5441277951157216, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 576, Loss: 0.2323266158203391, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 577, Loss: 0.2635341723998649, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 578, Loss: 0.2575403713931359, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 579, Loss: 0.2736086356124051, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 580, Loss: 0.44641964562988196, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 581, Loss: 0.27868145565099234, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 582, Loss: 0.43695552478220107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 583, Loss: 0.3298743951913713, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 584, Loss: 0.6932249492752294, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 585, Loss: 0.2977035910193954, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 586, Loss: 0.337521838629959, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 587, Loss: 0.33484650515231945, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 588, Loss: 0.5185534402433858, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 589, Loss: 0.2921720313342807, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 590, Loss: 0.24032747218879966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 591, Loss: 0.42228483173550624, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 592, Loss: 0.3029832331513853, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 593, Loss: 0.26171455467724153, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 594, Loss: 0.46051875416032373, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 595, Loss: 0.2869796068064468, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 596, Loss: 0.2743677550224514, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 597, Loss: 0.3926447730675019, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 598, Loss: 0.27576050528158325, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 599, Loss: 0.28526980787762113, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 600, Loss: 0.2952743019310567, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 601, Loss: 0.3871275080609978, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 602, Loss: 0.2566647609912066, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 603, Loss: 0.3178430454420995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 604, Loss: 0.29363102310218847, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 605, Loss: 0.36294766352389995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 606, Loss: 0.24565856460743266, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 607, Loss: 0.3110572377455202, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 608, Loss: 0.24961871671983576, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 609, Loss: 0.2606190875330644, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 610, Loss: 0.2960909068532897, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 611, Loss: 0.245561277473257, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 612, Loss: 0.25640192997423056, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 613, Loss: 0.24783853860488533, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 614, Loss: 0.3077334976490272, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 615, Loss: 0.34883062794883946, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 616, Loss: 0.5380169327709834, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 617, Loss: 0.238827751495288, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 618, Loss: 0.25124198667727304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 619, Loss: 0.27186352770788885, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 620, Loss: 0.28399472701218204, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 621, Loss: 0.29725183300046465, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 622, Loss: 0.2509796688346047, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 623, Loss: 0.2866532807690202, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 624, Loss: 0.2558557820194793, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 625, Loss: 0.26171319293316525, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 626, Loss: 0.29230788593040474, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 627, Loss: 0.27544893911939233, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 628, Loss: 0.2896074541732952, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 629, Loss: 0.2673388633967763, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 630, Loss: 0.293015676356636, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 631, Loss: 0.28254344125856323, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 632, Loss: 0.2593946226186048, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 633, Loss: 0.35506429861450417, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 634, Loss: 0.26610892522053886, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 635, Loss: 0.5413894509122977, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 636, Loss: 0.3225107112021073, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 637, Loss: 0.29731508053441813, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 638, Loss: 0.31373312808885967, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 639, Loss: 0.42838566006817846, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 640, Loss: 0.27852351566137523, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 641, Loss: 0.27137734308858164, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 642, Loss: 0.27261473947162435, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 643, Loss: 0.3052240651974636, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 644, Loss: 0.3868163419908843, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 645, Loss: 0.4279377595043334, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 646, Loss: 0.3965674118874867, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 647, Loss: 0.24706725399256235, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 648, Loss: 0.30265084827045974, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 649, Loss: 0.2601260488752151, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 650, Loss: 0.3407803305485957, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 651, Loss: 0.4740591166401814, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 652, Loss: 0.23072561866468938, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 653, Loss: 0.32681424863087977, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 654, Loss: 0.2358154430665948, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 655, Loss: 0.25440731559333424, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 656, Loss: 0.5191043379023011, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 657, Loss: 0.284993164192402, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 658, Loss: 0.6672352326824711, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 659, Loss: 0.3008899026414753, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 660, Loss: 0.2525486998490343, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 661, Loss: 0.25458595171447335, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 662, Loss: 0.2533195097106523, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 663, Loss: 0.2540052308400883, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 664, Loss: 0.288205948990197, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 665, Loss: 0.30661259073272673, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 666, Loss: 0.37345989236279387, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 667, Loss: 0.37217915334147067, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 668, Loss: 0.3063322558822713, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 669, Loss: 0.30470639809575306, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 670, Loss: 0.23911951986909957, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 671, Loss: 0.2799687666196078, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 672, Loss: 0.28612755700930453, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 673, Loss: 0.28002428055000883, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 674, Loss: 0.32265540984304236, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 675, Loss: 0.3430147041164213, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 676, Loss: 0.3357261318641152, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 677, Loss: 0.30412191963452373, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 678, Loss: 0.30191962362417823, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 679, Loss: 0.2292321006806801, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 680, Loss: 0.2726039789053204, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 681, Loss: 0.4410031827272114, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 682, Loss: 0.4439046039006773, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 683, Loss: 0.4731946139481461, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 684, Loss: 0.3411666079414153, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 685, Loss: 0.2844348470554324, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 686, Loss: 0.38126327951904126, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 687, Loss: 0.2433525778636534, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 688, Loss: 0.2985456820009339, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 689, Loss: 0.2993918310832753, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 690, Loss: 0.335237495545888, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 691, Loss: 0.28965393850326127, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 692, Loss: 0.35996535856944306, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 693, Loss: 0.3220826223879657, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 694, Loss: 0.43296957798125824, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 695, Loss: 0.4005305698212779, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 696, Loss: 0.2773753747527552, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 697, Loss: 0.39452607012819696, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 698, Loss: 0.2966748257605484, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 699, Loss: 0.24084865848752543, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 700, Loss: 0.35366516237223705, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 701, Loss: 0.408077734376476, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 702, Loss: 0.3740666011352886, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 703, Loss: 0.28170073789097927, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 704, Loss: 0.3453735359689901, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 705, Loss: 0.27772125651635815, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 706, Loss: 0.5048686108508068, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 707, Loss: 0.2471511557962547, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 708, Loss: 0.3010660707569551, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 709, Loss: 0.37198820816754624, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 710, Loss: 0.268448467220808, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 711, Loss: 0.47867978265504074, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 712, Loss: 0.4314433656210775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 713, Loss: 0.27816122121454134, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 714, Loss: 0.455922308534518, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 715, Loss: 0.24078935769791593, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 716, Loss: 0.36632004407960145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 717, Loss: 0.27181792087162304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 718, Loss: 0.7687920929134722, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 719, Loss: 0.24449315644376657, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 720, Loss: 0.3211152837162033, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 721, Loss: 0.3469960647716215, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 722, Loss: 0.3578237650811996, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 723, Loss: 0.3107670632776416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 724, Loss: 0.2705818008873061, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 725, Loss: 0.23269102215651694, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 726, Loss: 0.23428385347303676, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 727, Loss: 0.3554593667170758, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 728, Loss: 0.2457031548769398, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 729, Loss: 0.4719690314183016, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 730, Loss: 0.4088789172995762, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 731, Loss: 0.2619280398313202, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 732, Loss: 0.2511376847183618, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 733, Loss: 0.32537508516033, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 734, Loss: 0.3656643553862299, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 735, Loss: 0.2623281053778441, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 736, Loss: 0.2852101480435065, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 737, Loss: 0.25588382083423233, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 738, Loss: 0.32893698668781773, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 739, Loss: 0.4801955294273045, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 740, Loss: 0.2562077902166357, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 741, Loss: 0.3358853013224874, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 742, Loss: 0.3371361399349184, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 743, Loss: 0.36771583880295167, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 744, Loss: 0.23938118996115168, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 745, Loss: 0.3227986926375511, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 746, Loss: 0.5181174734183225, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 747, Loss: 0.6656411366342084, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 748, Loss: 0.3827341780623116, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 749, Loss: 0.3684005062336623, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 750, Loss: 0.2671574469875198, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 751, Loss: 0.37842858645884786, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 752, Loss: 0.33728448465383537, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 753, Loss: 0.29530003143129835, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 754, Loss: 0.3137338629224725, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 755, Loss: 0.35608197603839725, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 756, Loss: 0.3438111586526253, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 757, Loss: 0.25851655024985615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 758, Loss: 0.4340712213964053, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 759, Loss: 0.3602821368220005, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 760, Loss: 0.25230607241143427, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 761, Loss: 0.2551553240161357, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 762, Loss: 0.24855069978762595, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 763, Loss: 0.23501343841593475, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 764, Loss: 0.23143651325633427, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 765, Loss: 0.35254842983108897, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 766, Loss: 0.28405383970740417, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 767, Loss: 0.4660840487196081, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 768, Loss: 0.3658850627085585, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 769, Loss: 0.49605037487643405, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 770, Loss: 0.3144994625735379, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 771, Loss: 0.2812277684813722, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 772, Loss: 0.25842396470107254, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 773, Loss: 0.2673950205651044, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 774, Loss: 0.27232100865127495, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 775, Loss: 0.326700712970047, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 776, Loss: 0.28882152705507574, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 777, Loss: 0.3017447278511453, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 778, Loss: 0.32734941942525286, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 779, Loss: 0.34453534800605146, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 780, Loss: 0.2301525785474279, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 781, Loss: 0.6433182797326965, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 782, Loss: 0.35932351951177083, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 783, Loss: 0.25656085579260923, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 784, Loss: 0.27315788362672666, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 785, Loss: 0.24828647328036854, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 786, Loss: 0.44451210769344673, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 787, Loss: 0.41452471710439764, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 788, Loss: 0.23264659134603866, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 789, Loss: 0.2769695726850532, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 790, Loss: 0.3210935324029854, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 791, Loss: 0.32572492760878113, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 792, Loss: 0.3711177136659125, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 793, Loss: 0.6397328911769689, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 794, Loss: 0.2785028457337066, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 795, Loss: 0.26049383743967747, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 796, Loss: 0.29060147513561496, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 797, Loss: 0.35398993324335215, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 798, Loss: 0.23808586777118684, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 799, Loss: 0.29211205861637957, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 800, Loss: 0.38969279258038697, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 801, Loss: 0.28630503098087057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 802, Loss: 0.3960331286015649, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 803, Loss: 0.31603019688920686, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 804, Loss: 0.42857769403661905, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 805, Loss: 0.3853712000626774, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 806, Loss: 0.2677135296765307, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 807, Loss: 0.7489221153494549, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 808, Loss: 0.2763725694377302, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 809, Loss: 0.2470279793341154, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 810, Loss: 0.2637141779341076, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 811, Loss: 0.5422019611958458, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 812, Loss: 0.3836248262964188, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 813, Loss: 0.30361691537581303, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 814, Loss: 0.3123478546912011, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 815, Loss: 0.2586835202989148, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 816, Loss: 0.34462629640434994, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 817, Loss: 0.3632905744897135, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 818, Loss: 0.3094322992914549, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 819, Loss: 0.25902531257804207, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 820, Loss: 0.2988384275682288, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 821, Loss: 0.32536134673318595, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 822, Loss: 0.23963609956786308, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 823, Loss: 0.30964379442441686, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 824, Loss: 0.2769334519650088, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 825, Loss: 0.3003098191426648, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 826, Loss: 0.5030091740828494, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 827, Loss: 0.29618760556737656, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 828, Loss: 0.30093794802331764, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 829, Loss: 0.32385921859965555, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 830, Loss: 0.3958993984524411, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 831, Loss: 0.29551818368331384, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 832, Loss: 0.25236937364267215, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 833, Loss: 0.2908676620091911, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 834, Loss: 0.26532029061343665, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 835, Loss: 0.3678805348527203, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 836, Loss: 0.34036116597134847, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 837, Loss: 0.3054831622536363, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 838, Loss: 0.3174502924008162, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 839, Loss: 0.40449776274062565, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 840, Loss: 0.36828024672831783, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 841, Loss: 0.2636290604248789, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 842, Loss: 0.24867249028175897, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 843, Loss: 0.30045705778078874, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 844, Loss: 0.2838027847228568, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 845, Loss: 0.2818484932826214, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 846, Loss: 0.30479579896974146, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 847, Loss: 0.25248372689910586, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 848, Loss: 0.26900475007525637, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 849, Loss: 0.30057507532979433, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 850, Loss: 0.469009698325327, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 851, Loss: 0.26800685752906567, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 852, Loss: 0.3612589718582485, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 853, Loss: 0.34636690143049786, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 854, Loss: 0.39333403032434416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 855, Loss: 0.47823934954861635, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 856, Loss: 0.33248628127786894, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 857, Loss: 0.4649973521535986, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 858, Loss: 0.4177344164246295, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 859, Loss: 0.3704845449890826, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 860, Loss: 0.27140253222936767, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 861, Loss: 0.3075774307951008, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 862, Loss: 0.2630677882920681, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 863, Loss: 0.26879299073654744, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 864, Loss: 0.25754487188424846, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 865, Loss: 0.32407162216698726, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 866, Loss: 0.25367783308121294, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 867, Loss: 0.2853659731856698, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 868, Loss: 0.24045632395459607, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 869, Loss: 0.29609827691467305, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 870, Loss: 0.3184591374513482, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 871, Loss: 0.27700018144576966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 872, Loss: 0.24627929202677118, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 873, Loss: 0.28913238058750473, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 874, Loss: 0.2357237998640779, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 875, Loss: 0.3487349996754156, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 876, Loss: 0.2994189835622373, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 877, Loss: 0.27176855455729304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 878, Loss: 0.25058765907684005, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 879, Loss: 0.31665141811351477, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 880, Loss: 0.476803525071099, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 881, Loss: 0.3417567501718271, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 882, Loss: 0.33438250878735454, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 883, Loss: 0.24492899182567548, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 884, Loss: 0.2534172093141612, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 885, Loss: 0.3583767121088982, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 886, Loss: 0.27280684301122854, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 887, Loss: 0.34055662871609227, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 888, Loss: 0.3294526974852493, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 889, Loss: 0.27473318528968416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 890, Loss: 0.24488995485758788, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 891, Loss: 0.32439342019992184, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 892, Loss: 0.3006148034832882, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 893, Loss: 0.2588896969592997, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 894, Loss: 0.2498055754045226, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 895, Loss: 0.48282198064326387, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 896, Loss: 0.3561997583561328, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 897, Loss: 0.31179415550207307, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 898, Loss: 0.2870197960635316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 899, Loss: 0.22978312955824864, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 900, Loss: 0.5109309742550466, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 901, Loss: 0.26113840309067593, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 902, Loss: 0.37155995956292776, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 903, Loss: 0.4695299896102863, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 904, Loss: 0.30243019323486353, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 905, Loss: 0.30183631227635244, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 906, Loss: 0.344913827155574, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 907, Loss: 0.27611985937730676, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 908, Loss: 0.28957402624525425, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 909, Loss: 0.3135546523405637, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 910, Loss: 0.5919570444276601, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 911, Loss: 0.3098484234807706, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 912, Loss: 0.27275396306096966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 913, Loss: 0.23959373522443106, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 914, Loss: 0.23250557066983327, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 915, Loss: 0.5600702981294663, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 916, Loss: 0.24844118339421264, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 917, Loss: 0.45049712930792374, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 918, Loss: 0.2853885557530693, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 919, Loss: 0.2894579009989978, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 920, Loss: 0.3236453152063308, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 921, Loss: 0.30719219008110416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 922, Loss: 0.26112337050129575, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 923, Loss: 0.3133572706266639, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 924, Loss: 0.4335330071048784, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 925, Loss: 0.6899603296613711, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 926, Loss: 0.5240731712421864, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 927, Loss: 0.41857819697045573, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 928, Loss: 0.40142072498012316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 929, Loss: 0.276744352011745, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 930, Loss: 0.34190077738937374, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 931, Loss: 0.2938077815008452, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 932, Loss: 0.2867608989592442, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 933, Loss: 0.22998188217081886, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 934, Loss: 0.35635573408745913, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 935, Loss: 0.39851503884254846, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 936, Loss: 0.2645141423765471, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 937, Loss: 0.27762372654378775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 938, Loss: 0.6473817067502279, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 939, Loss: 0.24171742685405773, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 940, Loss: 0.24049870759697414, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 941, Loss: 0.24421808731303665, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 942, Loss: 0.23378342963367477, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 943, Loss: 0.3764810135156634, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 944, Loss: 0.25324181888603026, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 945, Loss: 0.38665009168128867, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 946, Loss: 0.25380885858403246, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 947, Loss: 0.2911750288817287, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 948, Loss: 0.27475714880552854, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 949, Loss: 0.3778414028349177, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 950, Loss: 0.2642500995182264, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 951, Loss: 0.2545395418601518, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 952, Loss: 0.3235777810421373, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 953, Loss: 0.3970534816752855, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 954, Loss: 0.28701376168708276, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 955, Loss: 0.26605150814162737, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 956, Loss: 0.24471498072909575, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 957, Loss: 0.3528076325557571, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 958, Loss: 0.3810610935109503, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 959, Loss: 0.4003906102438713, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 960, Loss: 0.23506520162752276, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 961, Loss: 0.33401398754971656, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 962, Loss: 0.26700669943799504, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 963, Loss: 0.35370179338144164, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 964, Loss: 0.3078116343223405, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 965, Loss: 0.27866039167069095, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 966, Loss: 0.39150851777472717, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 967, Loss: 0.3358191843996916, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 968, Loss: 0.327637193972187, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 969, Loss: 0.38058122765337804, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 970, Loss: 0.24346412638521997, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 971, Loss: 0.260340289307733, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 972, Loss: 0.31896770629928417, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 973, Loss: 0.3373399178929023, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 974, Loss: 0.5973643155799412, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 975, Loss: 0.28871009008625, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 976, Loss: 0.4396072048774986, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 977, Loss: 0.25156967987955964, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 978, Loss: 0.28508123254974993, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 979, Loss: 0.31127711017712445, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 980, Loss: 0.32936739538675786, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 981, Loss: 0.344969301186051, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 982, Loss: 0.2396756462364149, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 983, Loss: 0.2579630358406126, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 984, Loss: 0.3250281354116108, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 985, Loss: 0.23797625782912613, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 986, Loss: 0.37142257632019754, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 987, Loss: 0.29735687657491294, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 988, Loss: 0.3131737397496497, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 989, Loss: 0.2856469336729089, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 990, Loss: 0.41428370771590894, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 991, Loss: 0.40911744181873344, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 992, Loss: 0.24849879897575372, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 993, Loss: 0.24243057051124972, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 994, Loss: 0.26936975094078014, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 995, Loss: 0.2979424998612894, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 996, Loss: 0.47445046694787096, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 997, Loss: 0.23762157821306848, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 998, Loss: 0.25563508485695896, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 999, Loss: 0.29076451530733677, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1000, Loss: 0.31542085408587145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1001, Loss: 0.2938788586939664, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1002, Loss: 0.5100796690320514, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1003, Loss: 0.24602988378727136, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1004, Loss: 0.2367495130028645, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1005, Loss: 0.31327350004792454, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1006, Loss: 0.2674461858852468, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1007, Loss: 0.29135457302809425, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1008, Loss: 0.26730640260941607, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1009, Loss: 0.32152670002280365, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1010, Loss: 0.2511861788024929, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1011, Loss: 0.25873351839474845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1012, Loss: 0.33678687750376046, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1013, Loss: 0.3443233401106389, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1014, Loss: 0.2809884016651432, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1015, Loss: 0.33433949337794633, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1016, Loss: 0.2630494681841007, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1017, Loss: 0.49185556702077193, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1018, Loss: 0.2895150216011952, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1019, Loss: 0.2809681984987438, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1020, Loss: 0.23496902552413615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1021, Loss: 0.4154989477873243, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1022, Loss: 0.5433196593119529, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1023, Loss: 0.2407643022089556, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1024, Loss: 0.36423376237892313, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1025, Loss: 0.2722311755523369, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1026, Loss: 0.2668421624297707, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1027, Loss: 0.28235991730931875, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1028, Loss: 0.3508835450274516, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1029, Loss: 0.30235348544793295, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1030, Loss: 0.25831490262080326, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1031, Loss: 0.5460732791587041, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1032, Loss: 0.4010846993532337, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1033, Loss: 0.4226612591852944, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1034, Loss: 0.2807411578856943, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1035, Loss: 0.37440864687103925, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1036, Loss: 0.3204551264780823, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1037, Loss: 0.3277780233979601, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1038, Loss: 0.4448457592023719, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1039, Loss: 0.4392579390123287, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1040, Loss: 0.4958779128198549, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1041, Loss: 0.3085081999036704, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1042, Loss: 0.290327213456716, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1043, Loss: 0.30052310363560775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1044, Loss: 0.23498682056531842, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1045, Loss: 0.49770462832884454, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1046, Loss: 0.3598951710194689, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1047, Loss: 0.4056385681084578, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1048, Loss: 0.26380592366219546, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1049, Loss: 0.4469963602989173, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1050, Loss: 0.4091086446084663, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1051, Loss: 0.36881104243848295, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1052, Loss: 0.530604970833966, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1053, Loss: 0.3769434817476139, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1054, Loss: 0.401575645571079, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1055, Loss: 0.24156675143396739, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1056, Loss: 0.35657989139223734, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1057, Loss: 0.26438911142227267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1058, Loss: 0.3005111677806449, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1059, Loss: 0.310858964312336, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1060, Loss: 0.26837585605882325, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1061, Loss: 0.25841647099979453, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1062, Loss: 0.484721759545777, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1063, Loss: 0.24552197636240145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1064, Loss: 0.33275405734063046, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1065, Loss: 0.37617981062943767, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1066, Loss: 0.26460350254892606, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1067, Loss: 0.2547532204624721, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1068, Loss: 0.33714281661662193, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1069, Loss: 0.31050501150225324, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1070, Loss: 0.23457751595455728, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1071, Loss: 0.25590966541551674, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1072, Loss: 0.2679458762507006, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1073, Loss: 0.42145046963751254, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1074, Loss: 0.2570261117368219, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1075, Loss: 0.355224197021237, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1076, Loss: 0.2746547574495661, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1077, Loss: 0.42328448605389085, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1078, Loss: 0.28069320118825836, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1079, Loss: 0.33398882021804555, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1080, Loss: 0.2605297067860298, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1081, Loss: 0.40776215809415806, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1082, Loss: 0.30015993690528775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1083, Loss: 0.26269495424340916, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1084, Loss: 0.3040220386608259, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1085, Loss: 0.4747162055866827, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1086, Loss: 0.3397069394279709, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1087, Loss: 0.5100268950499185, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1088, Loss: 0.41095330261434393, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1089, Loss: 0.3313096475710753, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1090, Loss: 0.3756963089209339, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1091, Loss: 0.26894998001016945, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1092, Loss: 0.6192117122871378, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1093, Loss: 0.32258506058941316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1094, Loss: 0.6507966737401882, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1095, Loss: 0.27773258569090137, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1096, Loss: 0.392506671569788, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1097, Loss: 0.364223628911878, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1098, Loss: 0.4362010599764865, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1099, Loss: 0.26697893788076943, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1100, Loss: 0.24714885477912954, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1101, Loss: 0.552094967467596, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1102, Loss: 0.4552525851322102, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1103, Loss: 0.34808645519273096, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1104, Loss: 0.3644653561354685, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1105, Loss: 0.28088804396543127, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1106, Loss: 0.32729984750661534, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1107, Loss: 0.35361422539684845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1108, Loss: 0.29234649638964927, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1109, Loss: 0.33033674095344834, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1110, Loss: 0.26182971043013126, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1111, Loss: 0.6861592760474194, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1112, Loss: 0.25084898907953346, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1113, Loss: 0.23748760296305085, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1114, Loss: 0.34941251456101313, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1115, Loss: 0.295980298629518, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1116, Loss: 0.42126423982711114, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1117, Loss: 0.4856698884509052, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1118, Loss: 0.48069573404736204, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1119, Loss: 0.5670148722727905, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1120, Loss: 0.2712410432840819, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1121, Loss: 0.3144587651326934, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1122, Loss: 0.26273387273115995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1123, Loss: 0.2913693253337908, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1124, Loss: 0.352529839623438, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1125, Loss: 0.27147856465117187, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1126, Loss: 0.44097812286181526, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1127, Loss: 0.3028357943846199, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1128, Loss: 0.3464705652514871, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1129, Loss: 0.2971320497149213, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1130, Loss: 0.2495531446523639, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1131, Loss: 0.4962227746432797, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1132, Loss: 0.269786318039753, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1133, Loss: 0.27172552290915647, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1134, Loss: 0.3752923977593227, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1135, Loss: 0.3395076884848207, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1136, Loss: 0.45677091580968054, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1137, Loss: 0.3824835919625766, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1138, Loss: 0.26639437407927324, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1139, Loss: 0.2822435588646015, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1140, Loss: 0.3199866111106249, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1141, Loss: 0.39194531858402576, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1142, Loss: 0.42479978676925667, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1143, Loss: 0.2833774013433631, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1144, Loss: 0.3758743887405182, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1145, Loss: 0.3947428310893331, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1146, Loss: 0.3503190669590468, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1147, Loss: 0.38859315128363336, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1148, Loss: 0.23826689484054964, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1149, Loss: 0.3207263037968837, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1150, Loss: 0.4174159781917782, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1151, Loss: 0.31815364710015404, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1152, Loss: 0.40877186094353685, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1153, Loss: 0.27944933690731466, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1154, Loss: 0.294833528155621, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1155, Loss: 0.30017041430541447, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1156, Loss: 0.24675183231598657, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1157, Loss: 0.2498667719944862, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1158, Loss: 0.44008218704957236, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1159, Loss: 0.2724883718786535, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1160, Loss: 0.6524218731482934, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1161, Loss: 0.3640674023739439, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1162, Loss: 0.3157481335847957, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1163, Loss: 0.24884884289113546, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1164, Loss: 0.32403034875131864, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1165, Loss: 0.3740680029205493, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1166, Loss: 0.2529593699996778, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1167, Loss: 0.23344731164971597, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1168, Loss: 0.25444500554937205, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1169, Loss: 0.3224378799962154, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1170, Loss: 0.2527443651377425, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1171, Loss: 0.49948488957560083, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1172, Loss: 0.2662108928041285, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1173, Loss: 0.3384511443770539, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1174, Loss: 0.41639105081457495, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1175, Loss: 0.31442228737852684, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1176, Loss: 0.26541515739972826, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1177, Loss: 0.24332434245408524, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1178, Loss: 0.3257596916319277, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1179, Loss: 0.2695688940008266, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1180, Loss: 0.2849650529558687, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1181, Loss: 0.23345331854845405, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1182, Loss: 0.4000152042100494, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1183, Loss: 0.24147484335866923, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1184, Loss: 0.39190770538841824, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1185, Loss: 0.23040499872663975, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1186, Loss: 0.24572554425826326, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1187, Loss: 0.35172480493950764, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1188, Loss: 0.3137839725004267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1189, Loss: 0.2746090430428313, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1190, Loss: 0.28247929475061107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1191, Loss: 0.3915521034338426, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1192, Loss: 0.3184855359666335, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1193, Loss: 0.5426348864014687, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1194, Loss: 0.2591752888031003, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1195, Loss: 0.27084842976101053, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1196, Loss: 0.41352139862598336, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1197, Loss: 0.2485531735925218, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1198, Loss: 0.3314184512016771, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1199, Loss: 0.3318171560368398, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1200, Loss: 0.2831510858005719, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1201, Loss: 0.2611439511135955, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1202, Loss: 0.25415311037664473, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1203, Loss: 0.48523228153384235, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1204, Loss: 0.27023969177773693, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1205, Loss: 0.3064187192345408, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1206, Loss: 0.3208662279473864, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1207, Loss: 0.2691417650631944, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1208, Loss: 0.2563132107634195, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1209, Loss: 0.23304973767304699, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1210, Loss: 0.31519892396339444, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1211, Loss: 0.5277502496072057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1212, Loss: 0.421845397921351, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1213, Loss: 0.30321543581222177, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1214, Loss: 0.2402523148038928, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1215, Loss: 0.3382998124674553, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1216, Loss: 0.2806158135567291, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1217, Loss: 0.3086782909265657, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1218, Loss: 0.32493488449082386, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1219, Loss: 0.2476075587189836, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1220, Loss: 0.3167517313089729, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1221, Loss: 0.5206592440038832, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1222, Loss: 0.5667662697120831, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1223, Loss: 0.2787265309142273, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1224, Loss: 0.27432521035838575, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1225, Loss: 0.28155220081218413, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1226, Loss: 0.32153905985997494, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1227, Loss: 0.3988790503332728, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1228, Loss: 0.2316744357929575, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1229, Loss: 0.43970652753356987, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1230, Loss: 0.3169250068015055, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1231, Loss: 0.30736539544461094, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1232, Loss: 0.25816336956423497, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1233, Loss: 0.24614301049191809, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1234, Loss: 0.5075324936249263, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1235, Loss: 0.29433938488270883, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1236, Loss: 0.5241835296756822, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1237, Loss: 0.26188302176622075, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1238, Loss: 0.4386299174736745, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1239, Loss: 0.31291395287389057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1240, Loss: 0.3668753918758188, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1241, Loss: 0.264570041347133, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1242, Loss: 0.27859519456182374, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1243, Loss: 0.30206750978629765, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1244, Loss: 0.26131939282928884, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1245, Loss: 0.4988857504171566, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1246, Loss: 0.3578726816647604, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1247, Loss: 0.31192813116022067, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1248, Loss: 0.44058874615919275, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1249, Loss: 0.2763604936818122, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1250, Loss: 0.37542586843977166, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1251, Loss: 0.25880137000315234, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1252, Loss: 0.24756135400779564, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1253, Loss: 0.30675883564532935, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1254, Loss: 0.2771691840292394, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1255, Loss: 0.30542454823450726, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1256, Loss: 0.3827374732690507, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1257, Loss: 0.27767628966399516, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1258, Loss: 0.2541745663735106, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1259, Loss: 0.3392158285520238, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1260, Loss: 0.25752332360109487, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1261, Loss: 0.23926127770592806, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1262, Loss: 0.37811687328706506, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1263, Loss: 0.380408403838127, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1264, Loss: 0.25996611216391313, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1265, Loss: 0.31974678927838124, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1266, Loss: 0.27099342233653445, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1267, Loss: 0.31140445363141, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1268, Loss: 0.41652350295196267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1269, Loss: 0.29343942933827255, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1270, Loss: 0.2525760366870735, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1271, Loss: 0.25798649885811714, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1272, Loss: 0.2546570598555207, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1273, Loss: 0.29325534266562103, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1274, Loss: 0.40263282994306493, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1275, Loss: 0.2621066867047577, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1276, Loss: 0.5162349101009936, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1277, Loss: 0.23877820315886383, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1278, Loss: 0.3430801933088623, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1279, Loss: 0.2627207110101473, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1280, Loss: 0.3692124724715327, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1281, Loss: 0.28161610247060953, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1282, Loss: 0.31252336244195394, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1283, Loss: 0.23283727781910243, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1284, Loss: 0.35337978670850817, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1285, Loss: 0.3038248863219709, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1286, Loss: 0.2432845462262565, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1287, Loss: 0.31899088809621273, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1288, Loss: 0.49526677785912565, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1289, Loss: 0.2488064530933663, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1290, Loss: 0.42041307541398076, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1291, Loss: 0.3653762706818463, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1292, Loss: 0.2756242374405941, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1293, Loss: 0.4097967969922119, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1294, Loss: 0.2885486189557643, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1295, Loss: 0.29585868756783784, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1296, Loss: 0.26090663511942513, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1297, Loss: 0.2706821668409917, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1298, Loss: 0.6915920278067142, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1299, Loss: 0.5113083811396075, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1300, Loss: 0.3047457816129301, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1301, Loss: 0.3862076521264486, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1302, Loss: 0.3381825080057764, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1303, Loss: 0.33403400533167815, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1304, Loss: 0.314994147663366, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1305, Loss: 0.33667860446416076, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1306, Loss: 0.2687266537647684, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1307, Loss: 0.46055593466307376, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1308, Loss: 0.2725412261176127, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1309, Loss: 0.38249468386911656, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1310, Loss: 0.34998806935932686, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1311, Loss: 0.267002595477025, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1312, Loss: 0.31474679091279506, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1313, Loss: 0.3322324848135037, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1314, Loss: 0.45890784014623504, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1315, Loss: 0.29260319341379315, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1316, Loss: 0.3206707894694064, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1317, Loss: 0.2431655602054712, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1318, Loss: 0.23821164332268163, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1319, Loss: 0.32129281522899544, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1320, Loss: 0.24305681502049303, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1321, Loss: 0.3438211758494889, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1322, Loss: 0.5123180173629047, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1323, Loss: 0.2759608509991901, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1324, Loss: 0.5956776805271826, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1325, Loss: 0.28887359602171786, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1326, Loss: 0.2946375303983817, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1327, Loss: 0.3601127552392013, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1328, Loss: 0.32572773962118823, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1329, Loss: 0.27503978328517314, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1330, Loss: 0.23934825543694466, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1331, Loss: 0.23667278935689962, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1332, Loss: 0.2978186538013391, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1333, Loss: 0.35512912034585686, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1334, Loss: 0.3862381484787555, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1335, Loss: 0.3065155519031386, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1336, Loss: 0.6640262140964628, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1337, Loss: 0.3048753659339092, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1338, Loss: 0.32118522142848854, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1339, Loss: 0.2996294232715414, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1340, Loss: 0.4628029881120036, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1341, Loss: 0.3740426982532807, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1342, Loss: 0.4122059684135624, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1343, Loss: 0.2410474642279126, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1344, Loss: 0.2594422506397983, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1345, Loss: 0.25560788947382107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1346, Loss: 0.23799309606468272, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1347, Loss: 0.36524915332570335, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1348, Loss: 0.393268733160206, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1349, Loss: 0.4339345533569776, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1350, Loss: 0.2713490214802836, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1351, Loss: 0.25452963853081645, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1352, Loss: 0.3153703063664041, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1353, Loss: 0.2915764901284137, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1354, Loss: 0.3642442816528401, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1355, Loss: 0.2826768955803888, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1356, Loss: 0.26986411379058944, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1357, Loss: 0.372652492085058, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1358, Loss: 0.3075349898852416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1359, Loss: 0.29124629635153015, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1360, Loss: 0.3517368290535522, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1361, Loss: 0.3505083113183971, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1362, Loss: 0.3202625221252514, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1363, Loss: 0.39612194825410924, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1364, Loss: 0.3113532512887527, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1365, Loss: 0.24707369268732615, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1366, Loss: 0.28636443746895845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1367, Loss: 0.3159825658079221, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1368, Loss: 0.5849616734216938, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1369, Loss: 0.26032422698549057, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1370, Loss: 0.30866759564745183, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1371, Loss: 0.44550782846937387, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1372, Loss: 0.2671783550492404, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1373, Loss: 0.547710404105627, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1374, Loss: 0.3578732221839316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1375, Loss: 0.23754004083008007, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1376, Loss: 0.3138702425471797, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1377, Loss: 0.539724425775093, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1378, Loss: 0.37077538121924347, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1379, Loss: 0.24970010777122437, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1380, Loss: 0.4284126138947293, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1381, Loss: 0.5712408601968009, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1382, Loss: 0.30581531766891773, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1383, Loss: 0.32670929895689804, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1384, Loss: 0.2904056672360022, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1385, Loss: 0.28246556838054027, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1386, Loss: 0.2967187419960118, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1387, Loss: 0.2426111515072398, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1388, Loss: 0.25353799533567634, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1389, Loss: 0.3635811507809674, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1390, Loss: 0.33128959313028083, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1391, Loss: 0.35525020429171694, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1392, Loss: 0.24584737024957704, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1393, Loss: 0.36442468604002487, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1394, Loss: 0.4369085607084977, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1395, Loss: 0.368077169220629, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1396, Loss: 0.311634082114509, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1397, Loss: 0.2473107856115664, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1398, Loss: 0.3456547131469624, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1399, Loss: 0.25459601509133833, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1400, Loss: 0.352431841457715, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1401, Loss: 0.23655316155645462, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1402, Loss: 0.2664847087479933, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1403, Loss: 0.24677649783567246, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1404, Loss: 0.40062231732406406, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1405, Loss: 0.26167031230953836, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1406, Loss: 0.34361398862161285, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1407, Loss: 0.2920136064474235, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1408, Loss: 0.3380198022640212, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1409, Loss: 0.23727460498098404, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1410, Loss: 0.43767156348643044, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1411, Loss: 0.2603440313608989, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1412, Loss: 0.4327059977809739, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1413, Loss: 0.28048797791185487, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1414, Loss: 0.32105313419242953, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1415, Loss: 0.2486162234130077, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1416, Loss: 0.32865773801275777, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1417, Loss: 0.2570596818172384, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1418, Loss: 0.3470522422009244, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1419, Loss: 0.26409260475221386, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1420, Loss: 0.28893904510985363, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1421, Loss: 0.2436492406235542, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1422, Loss: 0.3760937701179396, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1423, Loss: 0.3827925773323075, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1424, Loss: 0.3227270147731932, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1425, Loss: 0.5014292486442304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1426, Loss: 0.23139331864836363, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1427, Loss: 0.26012257995778254, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1428, Loss: 0.4001321448660526, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1429, Loss: 0.3115584748923269, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1430, Loss: 0.35421756304835733, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1431, Loss: 0.23549241871881418, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1432, Loss: 0.45880793674885845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1433, Loss: 0.3417698038835005, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1434, Loss: 0.2673508787925347, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1435, Loss: 0.2870080084215602, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1436, Loss: 0.30533480688201914, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1437, Loss: 0.296133403775409, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1438, Loss: 0.27719798512192817, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1439, Loss: 0.26525150931991487, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1440, Loss: 0.36145846698354267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1441, Loss: 0.38177757228784126, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1442, Loss: 0.629968525829247, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1443, Loss: 0.3038568582926203, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1444, Loss: 0.2897238967667285, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1445, Loss: 0.3628051950976642, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1446, Loss: 0.2522014084759488, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1447, Loss: 0.2522442747263166, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1448, Loss: 0.2847189330843346, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1449, Loss: 0.2738027690629037, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1450, Loss: 0.24297852060389385, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1451, Loss: 0.26701188233106227, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1452, Loss: 0.2776899422083302, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1453, Loss: 0.23496599664935164, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1454, Loss: 0.22745827900796187, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1455, Loss: 0.26757656931407, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1456, Loss: 0.2624261183502459, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1457, Loss: 0.33334562294127673, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1458, Loss: 0.2574127293238669, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1459, Loss: 0.3834720874523314, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1460, Loss: 0.24056039114053818, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1461, Loss: 0.4020714287688772, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1462, Loss: 0.4120366618146546, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1463, Loss: 0.2356217167381324, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1464, Loss: 0.3084063377123593, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1465, Loss: 0.24748176224389573, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1466, Loss: 0.3441829704793878, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1467, Loss: 0.3620016171139053, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1468, Loss: 0.3607000259550811, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1469, Loss: 0.2725534453595929, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1470, Loss: 0.41308046740651233, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1471, Loss: 0.2710674160018259, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1472, Loss: 0.36048106426574117, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1473, Loss: 0.27277065821527274, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1474, Loss: 0.35462896160163304, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1475, Loss: 0.30131559297405763, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1476, Loss: 0.2999268412014007, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1477, Loss: 0.3867964684360775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1478, Loss: 0.36856583353373845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1479, Loss: 0.265825598971026, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1480, Loss: 0.4487070238060934, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1481, Loss: 0.4577900676309421, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1482, Loss: 0.30018780440028303, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1483, Loss: 0.3285051415479, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1484, Loss: 0.41835553202468256, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1485, Loss: 0.35988151602440865, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1486, Loss: 0.2632382634578847, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1487, Loss: 0.2757135365905874, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1488, Loss: 0.24614438481296622, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1489, Loss: 0.24755270899000106, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1490, Loss: 0.27719165630623277, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1491, Loss: 0.2785867097564125, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1492, Loss: 0.33809719018737905, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1493, Loss: 0.3370474735946772, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1494, Loss: 0.3331714533950154, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1495, Loss: 0.3203160291518319, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1496, Loss: 0.23892631522165703, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1497, Loss: 0.5110244740020529, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1498, Loss: 0.2776354287658711, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1499, Loss: 0.3805057354136127, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1500, Loss: 0.3470101191167239, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1501, Loss: 0.2801058355483044, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1502, Loss: 0.30616645409857673, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1503, Loss: 0.2731728299933932, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1504, Loss: 0.25970441176211045, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1505, Loss: 0.2959260965845766, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1506, Loss: 0.2649585133833043, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1507, Loss: 0.24511785433617353, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1508, Loss: 0.33908226323478974, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1509, Loss: 0.36595311603159414, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1510, Loss: 0.26533688450355053, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1511, Loss: 0.2945193080482147, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1512, Loss: 0.8896874013180777, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1513, Loss: 0.35488754630971947, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1514, Loss: 0.2340217232564754, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1515, Loss: 0.3592613556867956, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1516, Loss: 0.403621655452166, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1517, Loss: 0.33782659992335395, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1518, Loss: 0.39986229972461307, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1519, Loss: 0.3322911731283377, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1520, Loss: 0.3737968402129408, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1521, Loss: 0.33644502549836997, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1522, Loss: 0.33089096744584184, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1523, Loss: 0.27635377828685337, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1524, Loss: 0.23020153212231503, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1525, Loss: 0.2840916196852988, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1526, Loss: 0.29044943054983896, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1527, Loss: 0.27877455995467876, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1528, Loss: 0.3451770208809125, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1529, Loss: 0.3784561797792162, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1530, Loss: 0.254929941829629, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1531, Loss: 0.25795626074641165, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1532, Loss: 0.3166356272851586, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1533, Loss: 0.4619743037858286, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1534, Loss: 0.2345165564697462, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1535, Loss: 0.2781502065250424, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1536, Loss: 0.35708154035717776, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1537, Loss: 0.33121237591659236, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1538, Loss: 0.2627304711203734, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1539, Loss: 0.24638582863712405, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1540, Loss: 0.277467358175021, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1541, Loss: 0.3597937984526516, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1542, Loss: 0.2509047272546991, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1543, Loss: 0.3255476718831756, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1544, Loss: 0.28522770469373476, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1545, Loss: 0.3337632959426079, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1546, Loss: 0.2586190351276768, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1547, Loss: 0.27412357881231225, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1548, Loss: 0.2861743862742546, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1549, Loss: 0.2486625761026748, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1550, Loss: 0.2536550159793223, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1551, Loss: 0.42023880492591037, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1552, Loss: 0.24228180528195717, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1553, Loss: 0.4846989611179775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1554, Loss: 0.40474378696591085, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1555, Loss: 0.38737646763312594, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1556, Loss: 0.29296468553970983, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1557, Loss: 0.2796204453592776, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1558, Loss: 0.25529532506818725, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1559, Loss: 0.2983251485647984, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1560, Loss: 0.5229266227915254, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1561, Loss: 0.2642515693414666, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1562, Loss: 0.6726761761239362, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1563, Loss: 0.2588702593235038, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1564, Loss: 0.25739817863035214, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1565, Loss: 0.32314556166379504, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1566, Loss: 0.271623921147847, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1567, Loss: 0.31305561009692384, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1568, Loss: 0.2634663284373291, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1569, Loss: 0.5185781437646246, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1570, Loss: 0.28586576855330764, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1571, Loss: 0.24737219308710767, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1572, Loss: 0.294112110871976, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1573, Loss: 0.3123736430981693, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1574, Loss: 0.24060029404857292, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1575, Loss: 0.4198661931722453, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1576, Loss: 0.4438401840515138, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1577, Loss: 0.24538892113108737, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1578, Loss: 0.39899934068167175, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1579, Loss: 0.6704844363027663, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1580, Loss: 0.3232916757111914, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1581, Loss: 0.3081045612300714, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1582, Loss: 0.3467445498070679, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1583, Loss: 0.4504596609021784, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1584, Loss: 0.339942712482185, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1585, Loss: 0.2585099692587538, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1586, Loss: 0.3117604152195669, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1587, Loss: 0.23370930596816117, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1588, Loss: 0.2880913195387925, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1589, Loss: 0.29255964149131897, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1590, Loss: 0.25789631395715407, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1591, Loss: 0.24241621617103837, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1592, Loss: 0.39608311746061065, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1593, Loss: 0.23379195812965772, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1594, Loss: 0.262304131786018, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1595, Loss: 0.2694155952635755, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1596, Loss: 0.25588150016813593, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1597, Loss: 0.2560533684522102, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1598, Loss: 0.40534196907257264, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1599, Loss: 0.24382646178586986, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1600, Loss: 0.25209276244071877, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1601, Loss: 0.32801876110858147, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1602, Loss: 0.28499813242404753, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1603, Loss: 0.40408411572878256, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1604, Loss: 0.388238357373416, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1605, Loss: 0.3940634159731051, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1606, Loss: 0.3599164317940513, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1607, Loss: 0.2751247649345885, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1608, Loss: 0.25579046280191936, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1609, Loss: 0.23927235599746885, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1610, Loss: 0.373964443657574, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1611, Loss: 0.36131418614613553, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1612, Loss: 0.26211078984446184, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1613, Loss: 0.2925726571538775, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1614, Loss: 0.30424186531548303, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1615, Loss: 0.254988910283783, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1616, Loss: 0.4067830884190316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1617, Loss: 0.25667478282035283, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1618, Loss: 0.24464307086501325, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1619, Loss: 0.2754865463501734, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1620, Loss: 0.29004942139036716, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1621, Loss: 0.24151310248759078, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1622, Loss: 0.37243162147549763, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1623, Loss: 0.28994628629915326, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1624, Loss: 0.3999302417621046, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1625, Loss: 0.3974586803652612, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1626, Loss: 0.25946554105419134, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1627, Loss: 0.4263134883109634, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1628, Loss: 0.30456055571238705, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1629, Loss: 0.3777005468901504, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1630, Loss: 0.30465584721933964, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1631, Loss: 0.3832436527155513, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1632, Loss: 0.38407191823388276, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1633, Loss: 0.351652092446338, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1634, Loss: 0.4455714680605054, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1635, Loss: 0.2771559143496946, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1636, Loss: 0.2763436595342312, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1637, Loss: 0.3009317812119211, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1638, Loss: 0.23473926032517967, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1639, Loss: 0.34334508857829116, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1640, Loss: 0.27440638326760003, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1641, Loss: 0.5440215650885023, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1642, Loss: 0.2895347495303093, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1643, Loss: 0.29708134985986817, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1644, Loss: 0.49106356891323955, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1645, Loss: 0.3188863600399549, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1646, Loss: 0.3623294753046881, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1647, Loss: 0.23787951584527267, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1648, Loss: 0.27814632195557143, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1649, Loss: 0.2696694670408703, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1650, Loss: 0.42310917750676913, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1651, Loss: 0.5254638342453689, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1652, Loss: 0.3097254067450135, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1653, Loss: 0.3583154416482268, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1654, Loss: 0.37961886421620095, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1655, Loss: 0.25685128471141916, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1656, Loss: 0.3705034192185236, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1657, Loss: 0.26218521581276166, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1658, Loss: 0.3088058139678473, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1659, Loss: 0.40396763797674634, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1660, Loss: 0.28388565472414623, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1661, Loss: 0.380745624979008, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1662, Loss: 0.33049441802429164, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1663, Loss: 0.3424761395628298, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1664, Loss: 0.24735547299149835, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1665, Loss: 0.3111563463330554, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1666, Loss: 0.4362809952751209, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1667, Loss: 0.3182817038734092, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1668, Loss: 0.4175241309102675, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1669, Loss: 0.32366413328526533, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1670, Loss: 0.26284811486345006, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1671, Loss: 0.33868990464800103, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1672, Loss: 0.4158018097444708, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1673, Loss: 0.2623904275535256, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1674, Loss: 0.3162625023868562, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1675, Loss: 0.32132145220879804, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1676, Loss: 0.3680906211387107, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1677, Loss: 0.34503787761550286, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1678, Loss: 0.52737555460983, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1679, Loss: 0.2549164792981249, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1680, Loss: 0.31224149469275264, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1681, Loss: 0.2770285752483419, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1682, Loss: 0.24812903345701, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1683, Loss: 0.2754227211718132, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1684, Loss: 0.35007596176439554, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1685, Loss: 0.35294693257705945, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1686, Loss: 0.5161495031175027, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1687, Loss: 0.29294118495108823, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1688, Loss: 0.25425694092666, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1689, Loss: 0.5542775355761711, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1690, Loss: 0.24137008542106375, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1691, Loss: 0.350436650353356, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1692, Loss: 0.3068426734136882, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1693, Loss: 0.24124303027139485, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1694, Loss: 0.34075079406453596, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1695, Loss: 0.2943349233712959, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1696, Loss: 0.38773978636763207, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1697, Loss: 0.48499293201180793, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1698, Loss: 0.27336872364483655, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1699, Loss: 0.2730710777576175, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1700, Loss: 0.30633338505656915, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1701, Loss: 0.2520010652931247, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1702, Loss: 0.25288675367269714, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1703, Loss: 0.29700277332775393, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1704, Loss: 0.2382529485252842, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1705, Loss: 0.32440111254115667, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1706, Loss: 0.3242948187874971, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1707, Loss: 0.3596666816868437, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1708, Loss: 0.3269580821125189, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1709, Loss: 0.2305541051195841, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1710, Loss: 0.3555231531597102, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1711, Loss: 0.41256501544057045, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1712, Loss: 0.3964790483335261, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1713, Loss: 0.5103320244045938, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1714, Loss: 0.29596230420579, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1715, Loss: 0.34387806335399035, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1716, Loss: 0.3753345197237751, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1717, Loss: 0.25402332272212946, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1718, Loss: 0.49356613999080895, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1719, Loss: 0.394410561551664, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1720, Loss: 0.33311690863783366, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1721, Loss: 0.39417096325478346, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1722, Loss: 0.4895491448327496, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1723, Loss: 0.2624402360592754, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1724, Loss: 0.27030535790644356, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1725, Loss: 0.3683164509304675, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1726, Loss: 0.24887902857976113, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1727, Loss: 0.4011366484329674, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1728, Loss: 0.30251571154126333, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1729, Loss: 0.42880526666378316, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1730, Loss: 0.27391089938276, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1731, Loss: 0.26713540366940935, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1732, Loss: 0.2895945091014056, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1733, Loss: 0.49492089703030073, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1734, Loss: 0.24214469258951662, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1735, Loss: 0.2572342444013629, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1736, Loss: 0.44406111923619096, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1737, Loss: 0.35823431708137876, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1738, Loss: 0.23851537649703866, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1739, Loss: 0.45262706883245973, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1740, Loss: 0.2357361966416898, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1741, Loss: 0.27983711378009846, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1742, Loss: 0.2616291222948819, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1743, Loss: 0.3211432455193663, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1744, Loss: 0.38235355670136034, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1745, Loss: 0.34146866724718067, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1746, Loss: 0.23770694021017333, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1747, Loss: 0.28107375125780376, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1748, Loss: 0.3591963458513311, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1749, Loss: 0.2970788938053594, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1750, Loss: 0.3966035771337749, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1751, Loss: 0.2710189807433635, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1752, Loss: 0.2777840600885883, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1753, Loss: 0.24637794153535045, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1754, Loss: 0.32997792883346344, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1755, Loss: 0.2530532894019741, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1756, Loss: 0.26243541164834533, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1757, Loss: 0.43321830993712784, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1758, Loss: 0.27189228467553145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1759, Loss: 0.40285171980523427, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1760, Loss: 0.3966729207331132, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1761, Loss: 0.29535470772807604, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1762, Loss: 0.5227273171726421, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1763, Loss: 0.4336719453954596, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1764, Loss: 0.34886709681699696, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1765, Loss: 0.4666038636170854, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1766, Loss: 0.2645841007456779, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1767, Loss: 0.4167373849089041, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1768, Loss: 0.32384453226091364, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1769, Loss: 0.3118647762637442, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1770, Loss: 0.34251696992001845, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1771, Loss: 0.45516609278710973, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1772, Loss: 0.25710398283198477, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1773, Loss: 0.30157867612098166, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1774, Loss: 0.4131981660597759, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1775, Loss: 0.31104082007026446, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1776, Loss: 0.2403234521388544, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1777, Loss: 0.301893701290145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1778, Loss: 0.27300505418066645, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1779, Loss: 0.30205805907616035, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1780, Loss: 0.22935301076752265, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1781, Loss: 0.2727430519309431, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1782, Loss: 0.4401655876094208, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1783, Loss: 0.3863030416605626, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1784, Loss: 0.24643639208026313, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1785, Loss: 0.26114589703387653, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1786, Loss: 0.4307567195295234, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1787, Loss: 0.34335351626594063, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1788, Loss: 0.32437180297224427, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1789, Loss: 0.3640665690102275, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1790, Loss: 0.4716339998567375, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1791, Loss: 0.3589965575434931, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1792, Loss: 0.3140608754712248, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1793, Loss: 0.3447728288651224, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1794, Loss: 0.2785941618830717, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1795, Loss: 0.26710786960504, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1796, Loss: 0.45094474473540425, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1797, Loss: 0.2569412229104463, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1798, Loss: 0.24731459690240226, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1799, Loss: 0.2597921263625234, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1800, Loss: 0.25750382770540026, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1801, Loss: 0.33685534724403954, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1802, Loss: 0.3761767848132136, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1803, Loss: 0.3135853808216129, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1804, Loss: 0.41259272373300204, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1805, Loss: 0.29872552132612995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1806, Loss: 0.30288547078759553, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1807, Loss: 0.5109560467246353, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1808, Loss: 0.5170681470513564, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1809, Loss: 0.2618415751300969, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1810, Loss: 0.3155942416398452, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1811, Loss: 0.2625828536031, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1812, Loss: 0.33499643857232075, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1813, Loss: 0.25077806133856717, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1814, Loss: 0.2773480869774292, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1815, Loss: 0.24944630515457347, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1816, Loss: 0.3236269016737895, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1817, Loss: 0.28775281801224006, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1818, Loss: 0.4095380501925232, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1819, Loss: 0.2436635087935742, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1820, Loss: 0.3330557600184479, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1821, Loss: 0.3108026901546054, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1822, Loss: 0.25299537775714787, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1823, Loss: 0.2757947252502433, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1824, Loss: 0.25555957951371033, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1825, Loss: 0.2604185156394245, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1826, Loss: 0.46999331503295605, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1827, Loss: 0.2559040413693381, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1828, Loss: 0.3747443629573527, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1829, Loss: 0.33043123433465765, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1830, Loss: 0.29426234017729336, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1831, Loss: 0.24697285472677727, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1832, Loss: 0.28257104198014565, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1833, Loss: 0.2383027994709627, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1834, Loss: 0.33658621456074533, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1835, Loss: 0.35484714725601135, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1836, Loss: 0.2668608760703995, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1837, Loss: 0.2898989768649317, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1838, Loss: 0.34559656923620224, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1839, Loss: 0.28408441994227324, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1840, Loss: 0.4398222805517145, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1841, Loss: 0.31245061061806345, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1842, Loss: 0.31018261218273335, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1843, Loss: 0.3780594142629631, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1844, Loss: 0.3061772559979989, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1845, Loss: 0.270776925554115, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1846, Loss: 0.3151663726875287, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1847, Loss: 0.36473047786488044, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1848, Loss: 0.2583609578481433, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1849, Loss: 0.33973367816942024, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1850, Loss: 0.3191074413273538, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1851, Loss: 0.2801906050610973, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1852, Loss: 0.44194353961949695, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1853, Loss: 0.24691860048933922, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1854, Loss: 0.2696704773341996, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1855, Loss: 0.33636875820920503, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1856, Loss: 0.3862411711243484, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1857, Loss: 0.37545316362506265, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1858, Loss: 0.3278173153166211, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1859, Loss: 0.35057757274002466, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1860, Loss: 0.48566234149568105, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1861, Loss: 0.33967724522099474, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1862, Loss: 0.24015821448707833, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1863, Loss: 0.4904599305527097, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1864, Loss: 0.3982261452147693, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1865, Loss: 0.3929528310468498, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1866, Loss: 0.29563445497278906, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1867, Loss: 0.29835118290611623, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1868, Loss: 0.2411452572591232, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1869, Loss: 0.4877105342949344, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1870, Loss: 0.2320284715644176, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1871, Loss: 0.2724183584851255, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1872, Loss: 0.30570293888676103, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1873, Loss: 0.3680992192365413, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1874, Loss: 0.3187794381224632, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Batch 1875, Loss: 0.40558206856023277, Batch Size: 32, Learning Rate: 3.083090085926529e-05\n",
      "Epoch 16, Updated Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 16, Average Loss: 0.32995331689058227, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1, Loss: 0.5905548334793274, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 2, Loss: 0.35620501577465535, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 3, Loss: 0.2742338560845393, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 4, Loss: 0.45128114484569504, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 5, Loss: 0.30088017738342576, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 6, Loss: 0.31995310843424196, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 7, Loss: 0.2727891701191913, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 8, Loss: 0.4407581598055823, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 9, Loss: 0.2712866543099852, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 10, Loss: 0.3976169881475722, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 11, Loss: 0.3152868067639154, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 12, Loss: 0.5768340611950068, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 13, Loss: 0.2617727478064, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 14, Loss: 0.26980318017548605, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 15, Loss: 0.2554536992223347, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 16, Loss: 0.36656226919487056, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 17, Loss: 0.2640978108346351, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 18, Loss: 0.31495518014900725, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 19, Loss: 0.2534151771892268, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 20, Loss: 0.27857297220076815, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 21, Loss: 0.3976230487525467, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 22, Loss: 0.3156323163969304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 23, Loss: 0.282879266521866, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 24, Loss: 0.5125068227132603, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 25, Loss: 0.37703853268173226, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 26, Loss: 0.23683135775334943, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 27, Loss: 0.3191145710870768, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 28, Loss: 0.30347672156978045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 29, Loss: 0.5382423952782859, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 30, Loss: 0.264465775289026, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 31, Loss: 0.33883181265170953, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 32, Loss: 0.28477035267608686, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 33, Loss: 0.30211200795963317, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 34, Loss: 0.27138804269105765, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 35, Loss: 0.23611041968848634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 36, Loss: 0.29080675330709915, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 37, Loss: 0.3215638579349538, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 38, Loss: 0.2856378786304518, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 39, Loss: 0.2899460998691292, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 40, Loss: 0.40054681860856045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 41, Loss: 0.2415476022447867, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 42, Loss: 0.27624797908181187, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 43, Loss: 0.6943486318835745, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 44, Loss: 0.3301579484525086, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 45, Loss: 0.26822400724432593, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 46, Loss: 0.3364223298482737, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 47, Loss: 0.3195157698855354, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 48, Loss: 0.338154619603791, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 49, Loss: 0.5192532895245306, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 50, Loss: 0.2903272234853876, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 51, Loss: 0.47569840801020224, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 52, Loss: 0.29897492794562985, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 53, Loss: 0.3220132025467039, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 54, Loss: 0.356541849538789, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 55, Loss: 0.28305677333793866, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 56, Loss: 0.4457388525299886, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 57, Loss: 0.2556040014630789, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 58, Loss: 0.3179928812573274, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 59, Loss: 0.2316064592623384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 60, Loss: 0.33007549122545937, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 61, Loss: 0.23755715264867686, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 62, Loss: 0.30348735639452623, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 63, Loss: 0.4037449357584729, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 64, Loss: 0.30772300782600936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 65, Loss: 0.3128877008692216, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 66, Loss: 0.2868752561974115, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 67, Loss: 0.4068232854910987, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 68, Loss: 0.2786274014372011, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 69, Loss: 0.2555031793192483, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 70, Loss: 0.5963574287815907, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 71, Loss: 0.6234270766722035, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 72, Loss: 0.23876540161202311, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 73, Loss: 0.23644892478727925, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 74, Loss: 0.28895857093437577, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 75, Loss: 0.3916048590255068, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 76, Loss: 0.268222842650116, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 77, Loss: 0.24996601879678565, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 78, Loss: 0.274985016537469, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 79, Loss: 0.3143469882333057, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 80, Loss: 0.41118579536666167, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 81, Loss: 0.35465859843130454, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 82, Loss: 0.5483363213334627, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 83, Loss: 0.2764685594213827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 84, Loss: 0.24972721700262931, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 85, Loss: 0.3536967005816251, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 86, Loss: 0.3308652674962028, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 87, Loss: 0.28658763463746223, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 88, Loss: 0.30528540828367956, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 89, Loss: 0.2944119526334818, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 90, Loss: 0.27687910860880366, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 91, Loss: 0.24833494789218863, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 92, Loss: 0.263566341002822, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 93, Loss: 0.27626861058688235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 94, Loss: 0.23992596421499698, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 95, Loss: 0.37346882844919704, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 96, Loss: 0.304780413203399, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 97, Loss: 0.2947259528342787, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 98, Loss: 0.3441540724965228, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 99, Loss: 0.27087024128866205, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 100, Loss: 0.2572626746325552, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 101, Loss: 0.2789296491765364, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 102, Loss: 0.23743723346250883, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 103, Loss: 0.24290918284493737, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 104, Loss: 0.3140372202745457, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 105, Loss: 0.26566808596207614, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 106, Loss: 0.2656491029223863, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 107, Loss: 0.27149388499089266, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 108, Loss: 0.27305622690815373, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 109, Loss: 0.3295457744525102, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 110, Loss: 0.26635973947595465, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 111, Loss: 0.310680942924428, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 112, Loss: 0.2774524401297107, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 113, Loss: 0.3027309045695677, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 114, Loss: 0.2950401863559505, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 115, Loss: 0.344602953997466, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 116, Loss: 0.28519365173028094, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 117, Loss: 0.4019251530237662, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 118, Loss: 0.28722593717431105, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 119, Loss: 0.34388190803032126, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 120, Loss: 0.2865978055314512, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 121, Loss: 0.3106552998525196, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 122, Loss: 0.3134557032814481, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 123, Loss: 0.5523315023355645, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 124, Loss: 0.2949531025080727, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 125, Loss: 0.32732791384234006, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 126, Loss: 0.24094671855370742, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 127, Loss: 0.30062850174341715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 128, Loss: 0.2997862502528489, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 129, Loss: 0.45662848887225127, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 130, Loss: 0.41307627734635033, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 131, Loss: 0.3316505286066145, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 132, Loss: 0.40291044219724526, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 133, Loss: 0.3808880897252079, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 134, Loss: 0.31049090568062143, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 135, Loss: 0.27555315057329094, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 136, Loss: 0.4593826715994349, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 137, Loss: 0.23110127994735233, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 138, Loss: 0.32430818535622563, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 139, Loss: 0.2505746636722529, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 140, Loss: 0.3463311093054946, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 141, Loss: 0.2955690898051562, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 142, Loss: 0.2528287968431757, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 143, Loss: 0.287126880458813, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 144, Loss: 0.2979047690599288, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 145, Loss: 0.3842459117658378, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 146, Loss: 0.32471663569901804, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 147, Loss: 0.38624403411149066, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 148, Loss: 0.2617094347341059, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 149, Loss: 0.25574990457869623, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 150, Loss: 0.32814041379064895, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 151, Loss: 0.37728603726553955, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 152, Loss: 0.3416434064828098, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 153, Loss: 0.37268797901128525, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 154, Loss: 0.3496327308898747, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 155, Loss: 0.26808044782045304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 156, Loss: 0.33673016757863483, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 157, Loss: 0.24703150987756856, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 158, Loss: 0.26524234019850723, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 159, Loss: 0.23797458484877088, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 160, Loss: 0.4236350687778839, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 161, Loss: 0.25658859548546337, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 162, Loss: 0.2687254731549199, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 163, Loss: 0.2556753184817657, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 164, Loss: 0.4276163803171513, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 165, Loss: 0.29769719141965406, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 166, Loss: 0.3978989151108345, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 167, Loss: 0.26073732508406827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 168, Loss: 0.30292215409790385, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 169, Loss: 0.36831071901955337, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 170, Loss: 0.276763321596489, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 171, Loss: 0.2639181857155194, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 172, Loss: 0.26158619371482184, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 173, Loss: 0.27089648769019936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 174, Loss: 0.3956623500362607, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 175, Loss: 0.25206764780853086, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 176, Loss: 0.24961151290219455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 177, Loss: 0.5379696154286604, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 178, Loss: 0.24861959637882183, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 179, Loss: 0.301598216658866, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 180, Loss: 0.3051155288979843, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 181, Loss: 0.516790950940044, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 182, Loss: 0.2734892804289318, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 183, Loss: 0.2970951384007087, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 184, Loss: 0.308098771617691, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 185, Loss: 0.24871985537532332, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 186, Loss: 0.24761253301509045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 187, Loss: 0.4888092167219409, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 188, Loss: 0.33509045444433994, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 189, Loss: 0.4133057917821529, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 190, Loss: 0.2800412711605776, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 191, Loss: 0.32589058839146706, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 192, Loss: 0.27240989774450836, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 193, Loss: 0.3205419698918687, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 194, Loss: 0.34788689430380243, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 195, Loss: 0.24095687590073347, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 196, Loss: 0.33110164130823916, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 197, Loss: 0.27923033595921964, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 198, Loss: 0.28011521501750664, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 199, Loss: 0.270666834625889, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 200, Loss: 0.3809480002317767, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 201, Loss: 0.30563094606868313, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 202, Loss: 0.29517174678839014, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 203, Loss: 0.33966026635371926, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 204, Loss: 0.3426050147012415, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 205, Loss: 0.2918544490006846, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 206, Loss: 0.2389213473518176, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 207, Loss: 0.4914252154872747, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 208, Loss: 0.3140773397088886, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 209, Loss: 0.5237799261087839, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 210, Loss: 0.27867782157892873, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 211, Loss: 0.28595802258693925, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 212, Loss: 0.29105462794002146, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 213, Loss: 0.3186869530006116, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 214, Loss: 0.4103512481436631, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 215, Loss: 0.3553780555511002, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 216, Loss: 0.23148927070012987, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 217, Loss: 0.3377909889440962, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 218, Loss: 0.40852747726096794, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 219, Loss: 0.2629478800707578, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 220, Loss: 0.2936112494365097, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 221, Loss: 0.23079327272709046, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 222, Loss: 0.30654054066609016, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 223, Loss: 0.356554103206876, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 224, Loss: 0.2833703157907618, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 225, Loss: 0.476174071210712, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 226, Loss: 0.6716369704028404, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 227, Loss: 0.5429300645344239, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 228, Loss: 0.4192282096272539, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 229, Loss: 0.38112315126485596, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 230, Loss: 0.44446733734761934, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 231, Loss: 0.40127493304230144, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 232, Loss: 0.630676866380731, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 233, Loss: 0.33119816829797344, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 234, Loss: 0.3539752004622302, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 235, Loss: 0.2758784770018904, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 236, Loss: 0.2816161013428137, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 237, Loss: 0.30195730423662526, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 238, Loss: 0.30330214012898743, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 239, Loss: 0.30871795213016284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 240, Loss: 0.25433046384591224, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 241, Loss: 0.2720606406804827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 242, Loss: 0.24472236868297406, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 243, Loss: 0.31190994788641246, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 244, Loss: 0.25254868447090667, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 245, Loss: 0.2759528736504682, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 246, Loss: 0.3655341720611738, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 247, Loss: 0.3021726245309916, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 248, Loss: 0.38693200133320993, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 249, Loss: 0.30348393264450835, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 250, Loss: 0.44925077286515636, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 251, Loss: 0.3701094332612003, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 252, Loss: 0.48705329867583763, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 253, Loss: 0.2483545269396892, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 254, Loss: 0.22958794034683536, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 255, Loss: 0.24612844848659485, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 256, Loss: 0.34064701329667446, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 257, Loss: 0.4079031662766715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 258, Loss: 0.36785858269476085, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 259, Loss: 0.2742783033471042, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 260, Loss: 0.24414340555991065, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 261, Loss: 0.2755712432021612, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 262, Loss: 0.2741565363785734, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 263, Loss: 0.31490079894032774, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 264, Loss: 0.31140938818522534, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 265, Loss: 0.27605116488033243, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 266, Loss: 0.31649790031372105, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 267, Loss: 0.3226194456537572, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 268, Loss: 0.25897108343348585, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 269, Loss: 0.3651180441918095, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 270, Loss: 0.30952414569448494, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 271, Loss: 0.31194583584929453, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 272, Loss: 0.26942812577471414, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 273, Loss: 0.2942819290348101, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 274, Loss: 0.31321576692909936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 275, Loss: 0.4500551835004231, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 276, Loss: 0.3038086597386531, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 277, Loss: 0.2433163037221401, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 278, Loss: 0.41768174025813287, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 279, Loss: 0.37801309848802933, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 280, Loss: 0.2675578284948542, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 281, Loss: 0.2606601756344826, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 282, Loss: 0.26378730596878264, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 283, Loss: 0.31970515230677177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 284, Loss: 0.26119443581980917, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 285, Loss: 0.37449589647222165, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 286, Loss: 0.3066160462479719, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 287, Loss: 0.3129564613195343, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 288, Loss: 0.3777908744431082, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 289, Loss: 0.2888617110053977, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 290, Loss: 0.3186643040640783, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 291, Loss: 0.4634773490686554, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 292, Loss: 0.3559011948168965, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 293, Loss: 0.2832180447275719, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 294, Loss: 0.26130500515181926, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 295, Loss: 0.23685905039747912, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 296, Loss: 0.2822578804929996, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 297, Loss: 0.28231898777706177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 298, Loss: 0.3239957953160717, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 299, Loss: 0.2601891104966189, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 300, Loss: 0.2690130832030946, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 301, Loss: 0.2580735199639819, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 302, Loss: 0.2551600562744675, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 303, Loss: 0.36644863232862723, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 304, Loss: 0.25369232423235605, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 305, Loss: 0.265564476978704, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 306, Loss: 0.45866776702764656, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 307, Loss: 0.3798039387431027, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 308, Loss: 0.2350978401537359, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 309, Loss: 0.2523801476101556, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 310, Loss: 0.29529468551649624, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 311, Loss: 0.2844220721842078, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 312, Loss: 0.26810673774306226, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 313, Loss: 0.3863635064854813, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 314, Loss: 0.2560355984553601, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 315, Loss: 0.24284872918675754, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 316, Loss: 0.24341976132466364, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 317, Loss: 0.5214398721072304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 318, Loss: 0.2644805874661118, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 319, Loss: 0.3649351771133731, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 320, Loss: 0.3382134348893609, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 321, Loss: 0.5191702152378238, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 322, Loss: 0.2866005271497997, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 323, Loss: 0.32808899233088745, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 324, Loss: 0.2566555261952426, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 325, Loss: 0.35015824740645085, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 326, Loss: 0.2455839502009255, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 327, Loss: 0.27393998623970545, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 328, Loss: 0.22904874268079733, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 329, Loss: 0.2999064723925165, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 330, Loss: 0.34575136430140463, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 331, Loss: 0.39301772078995917, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 332, Loss: 0.29836081692684985, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 333, Loss: 0.3003238546557472, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 334, Loss: 0.27588619464315284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 335, Loss: 0.2597463669210621, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 336, Loss: 0.3361040561407318, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 337, Loss: 0.2522234771056014, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 338, Loss: 0.3591179096533593, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 339, Loss: 0.3692579097516341, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 340, Loss: 0.25563125674393405, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 341, Loss: 0.5903558263627142, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 342, Loss: 0.26965910221775446, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 343, Loss: 0.2690034329659835, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 344, Loss: 0.26099651668959445, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 345, Loss: 0.3414523737056165, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 346, Loss: 0.2790151755684284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 347, Loss: 0.3168333193675361, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 348, Loss: 0.318990126491948, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 349, Loss: 0.3626702165291308, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 350, Loss: 0.3790819681773623, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 351, Loss: 0.24682834556052313, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 352, Loss: 0.2367494486865392, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 353, Loss: 0.2823200632850509, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 354, Loss: 0.27886670940267716, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 355, Loss: 0.24771169567650125, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 356, Loss: 0.45251751444921284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 357, Loss: 0.26336929358629524, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 358, Loss: 0.25979610664262437, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 359, Loss: 0.2811492433007522, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 360, Loss: 0.2511133105976977, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 361, Loss: 0.23455253670517792, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 362, Loss: 0.2617696158970662, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 363, Loss: 0.36736204529886146, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 364, Loss: 0.31985553668949407, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 365, Loss: 0.25007648039361385, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 366, Loss: 0.2868137177758971, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 367, Loss: 0.3153631230309113, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 368, Loss: 0.2652391251551477, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 369, Loss: 0.44481492829401614, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 370, Loss: 0.32154243481767153, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 371, Loss: 0.43742805038262433, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 372, Loss: 0.35949091802395117, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 373, Loss: 0.35950112277544977, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 374, Loss: 0.2809692334811704, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 375, Loss: 0.3047499375593891, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 376, Loss: 0.2696434890263709, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 377, Loss: 0.565641239382712, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 378, Loss: 0.25668435140072715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 379, Loss: 0.2587015503082064, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 380, Loss: 0.39560479648172264, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 381, Loss: 0.3134440859072168, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 382, Loss: 0.45073182126595224, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 383, Loss: 0.5766375935798091, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 384, Loss: 0.4525540406081067, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 385, Loss: 0.342849894898209, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 386, Loss: 0.2903047420114112, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 387, Loss: 0.4537568141060334, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 388, Loss: 0.4795140808543249, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 389, Loss: 0.38066872294448834, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 390, Loss: 0.24357272173345035, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 391, Loss: 0.28216332827738455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 392, Loss: 0.26278372464962235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 393, Loss: 0.38255557240632965, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 394, Loss: 0.3044554185281854, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 395, Loss: 0.2609253917499641, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 396, Loss: 0.3180110711849412, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 397, Loss: 0.3709726851347455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 398, Loss: 0.29216945545794104, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 399, Loss: 0.24795865927507488, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 400, Loss: 0.36503817072468625, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 401, Loss: 0.4033879797229363, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 402, Loss: 0.30238988506713926, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 403, Loss: 0.3243682926571128, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 404, Loss: 0.27972043920666695, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 405, Loss: 0.506216361214338, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 406, Loss: 0.43778384208186905, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 407, Loss: 0.34040636052324297, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 408, Loss: 0.3149278039256055, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 409, Loss: 0.264490924995584, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 410, Loss: 0.31701168744421615, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 411, Loss: 0.34345913520487364, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 412, Loss: 0.23633368665205795, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 413, Loss: 0.3058357965160191, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 414, Loss: 0.7870503422597148, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 415, Loss: 0.4257664696684346, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 416, Loss: 0.3569839557863241, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 417, Loss: 0.28446332193654256, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 418, Loss: 0.3078035043576026, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 419, Loss: 0.3149299621116507, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 420, Loss: 0.3502033864750529, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 421, Loss: 0.36872808555492276, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 422, Loss: 0.2883580689934375, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 423, Loss: 0.2727431374055102, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 424, Loss: 0.2903570984809996, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 425, Loss: 0.28963065827040985, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 426, Loss: 0.3094114444013145, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 427, Loss: 0.28299750813232355, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 428, Loss: 0.26572400022844755, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 429, Loss: 0.4662435333867625, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 430, Loss: 0.3046952799677827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 431, Loss: 0.2786393420408606, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 432, Loss: 0.42616427456349215, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 433, Loss: 0.23591153142812316, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 434, Loss: 0.2972866163028392, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 435, Loss: 0.30637847208880875, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 436, Loss: 0.4096886770956661, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 437, Loss: 0.3169349953664597, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 438, Loss: 0.3098841490068366, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 439, Loss: 0.2581645965883058, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 440, Loss: 0.2866933851282981, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 441, Loss: 0.31051760300385317, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 442, Loss: 0.26307839287085927, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 443, Loss: 0.29297546450633694, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 444, Loss: 0.2461153757760345, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 445, Loss: 0.275080951673317, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 446, Loss: 0.35736074597445516, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 447, Loss: 0.3161281732707418, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 448, Loss: 0.2624304614698964, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 449, Loss: 0.2330709661281635, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 450, Loss: 0.2957873185926345, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 451, Loss: 0.2836032460105722, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 452, Loss: 0.3705862074542471, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 453, Loss: 0.2825331103550437, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 454, Loss: 0.4262350134793247, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 455, Loss: 0.26080239024728585, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 456, Loss: 0.27508206173795896, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 457, Loss: 0.30020012237519045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 458, Loss: 0.3069291546026922, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 459, Loss: 0.31105250893383823, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 460, Loss: 0.26436999761232804, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 461, Loss: 0.25793612663199317, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 462, Loss: 0.283198997077803, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 463, Loss: 0.2470457031201628, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 464, Loss: 0.3457305744220476, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 465, Loss: 0.4610146345570459, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 466, Loss: 0.2694363463942667, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 467, Loss: 0.3286412414069625, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 468, Loss: 0.29080515864347223, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 469, Loss: 0.2613312408050168, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 470, Loss: 0.3805356982889988, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 471, Loss: 0.3108827656598384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 472, Loss: 0.4192409521588307, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 473, Loss: 0.28238868741088397, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 474, Loss: 0.40653126814022567, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 475, Loss: 0.30644755960763437, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 476, Loss: 0.25428059955946747, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 477, Loss: 0.4364083132661928, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 478, Loss: 0.3759914178711643, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 479, Loss: 0.4655659257525481, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 480, Loss: 0.2946192138347563, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 481, Loss: 0.27027794747739414, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 482, Loss: 0.2420937493872537, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 483, Loss: 0.2898261254409218, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 484, Loss: 0.2337348719874666, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 485, Loss: 0.5391826919057499, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 486, Loss: 0.30195700635901407, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 487, Loss: 0.2892578471967499, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 488, Loss: 0.6601085260899161, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 489, Loss: 0.42885366981276474, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 490, Loss: 0.3430922476250462, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 491, Loss: 0.2548163936910384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 492, Loss: 0.3609424828456197, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 493, Loss: 0.37695151731011634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 494, Loss: 0.2909997569793476, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 495, Loss: 0.3886600069770648, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 496, Loss: 0.26125486499274303, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 497, Loss: 0.23166522194533795, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 498, Loss: 0.2992647748109527, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 499, Loss: 0.25291008169746865, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 500, Loss: 0.2741690888651778, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 501, Loss: 0.270584761231062, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 502, Loss: 0.2877769468611652, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 503, Loss: 0.5491272112004578, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 504, Loss: 0.2575211711533063, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 505, Loss: 0.2843124443184015, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 506, Loss: 0.25288115770187036, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 507, Loss: 0.3158010714945419, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 508, Loss: 0.3631085879118764, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 509, Loss: 0.42138786808731576, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 510, Loss: 0.4163559552670316, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 511, Loss: 0.38600758770827887, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 512, Loss: 0.27721947709089045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 513, Loss: 0.25944418051653595, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 514, Loss: 0.4940157965397195, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 515, Loss: 0.2640473281078762, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 516, Loss: 0.23369439861680597, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 517, Loss: 0.3457842445380235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 518, Loss: 0.2498904462389207, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 519, Loss: 0.26502851222394636, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 520, Loss: 0.5773622495405517, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 521, Loss: 0.2716788750563413, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 522, Loss: 0.3392523202431001, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 523, Loss: 0.2505245620970616, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 524, Loss: 0.4251530481238456, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 525, Loss: 0.28829060889897345, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 526, Loss: 0.4381502548403116, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 527, Loss: 0.24412488360406828, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 528, Loss: 0.3215004529416133, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 529, Loss: 0.29681016625696177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 530, Loss: 0.3173926897949634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 531, Loss: 0.35566018627586826, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 532, Loss: 0.24263167519762507, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 533, Loss: 0.38046163563078134, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 534, Loss: 0.3629767944761223, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 535, Loss: 0.3490809274907124, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 536, Loss: 0.48379881737656344, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 537, Loss: 0.37393801694167417, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 538, Loss: 0.43202953668187755, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 539, Loss: 0.32944997408576115, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 540, Loss: 0.27841377846178106, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 541, Loss: 0.41774372809862437, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 542, Loss: 0.256332403477689, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 543, Loss: 0.28559923654030706, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 544, Loss: 0.25297275547664044, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 545, Loss: 0.2539008740720716, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 546, Loss: 0.3779116470977528, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 547, Loss: 0.2508202150854847, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 548, Loss: 0.4160655381875818, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 549, Loss: 0.3123215085976765, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 550, Loss: 0.31625986819836027, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 551, Loss: 0.2961665670205844, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 552, Loss: 0.4448154436820747, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 553, Loss: 0.2376226890548549, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 554, Loss: 0.30752470045646985, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 555, Loss: 0.2854279287104185, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 556, Loss: 0.40476735671031416, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 557, Loss: 0.2544408641209356, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 558, Loss: 0.2996138967729224, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 559, Loss: 0.35800655043416074, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 560, Loss: 0.30901657632704066, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 561, Loss: 0.24532503271492723, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 562, Loss: 0.25579281299913187, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 563, Loss: 0.32922131948063893, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 564, Loss: 0.2291225104100393, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 565, Loss: 0.3615581877249967, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 566, Loss: 0.27404808194782715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 567, Loss: 0.23894288348655573, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 568, Loss: 0.3700363433188294, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 569, Loss: 0.3035607191196854, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 570, Loss: 0.2721898275825749, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 571, Loss: 0.26890143200001226, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 572, Loss: 0.276920161913079, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 573, Loss: 0.2554994122231751, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 574, Loss: 0.4200342020498856, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 575, Loss: 0.3388731218365244, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 576, Loss: 0.23190351614144308, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 577, Loss: 0.3615241207889466, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 578, Loss: 0.255096007466641, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 579, Loss: 0.3724875309572374, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 580, Loss: 0.46134624221848175, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 581, Loss: 0.3668926141059097, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 582, Loss: 0.33021505079330515, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 583, Loss: 0.27718872993472304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 584, Loss: 0.5057629725525059, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 585, Loss: 0.24965673847477937, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 586, Loss: 0.3474090223496946, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 587, Loss: 0.29458326928033485, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 588, Loss: 0.3918898838781827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 589, Loss: 0.3231887699074439, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 590, Loss: 0.2489134265067524, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 591, Loss: 0.45610100877961907, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 592, Loss: 0.34518312021130293, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 593, Loss: 0.2916600234983231, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 594, Loss: 0.3144719707225091, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 595, Loss: 0.24351692635180086, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 596, Loss: 0.275681489935234, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 597, Loss: 0.3393852846553049, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 598, Loss: 0.3695628061890165, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 599, Loss: 0.252293977283786, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 600, Loss: 0.29617133054726974, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 601, Loss: 0.3951724956591864, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 602, Loss: 0.27675579170916187, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 603, Loss: 0.2695942463506835, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 604, Loss: 0.258648697085895, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 605, Loss: 0.4324601710719768, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 606, Loss: 0.258628095452872, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 607, Loss: 0.3159809730772245, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 608, Loss: 0.24328185745600822, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 609, Loss: 0.33120616861406993, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 610, Loss: 0.41302306290819246, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 611, Loss: 0.2598951291095493, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 612, Loss: 0.2659753973526235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 613, Loss: 0.2897333868087055, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 614, Loss: 0.5525281841511908, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 615, Loss: 0.4006644842808963, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 616, Loss: 0.3461920952725989, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 617, Loss: 0.31957671177197494, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 618, Loss: 0.25247594720567546, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 619, Loss: 0.2707801028165336, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 620, Loss: 0.3546753362220491, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 621, Loss: 0.3016241335081855, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 622, Loss: 0.25719501511167603, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 623, Loss: 0.3248414395542555, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 624, Loss: 0.36200224391432256, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 625, Loss: 0.2693642668683995, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 626, Loss: 0.2395075542139957, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 627, Loss: 0.44621588043716676, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 628, Loss: 0.25341208221446854, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 629, Loss: 0.28433880711079956, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 630, Loss: 0.2960034427385061, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 631, Loss: 0.33532282264844093, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 632, Loss: 0.28277861176605523, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 633, Loss: 0.254392850474504, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 634, Loss: 0.23590082242176647, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 635, Loss: 0.4947791020676853, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 636, Loss: 0.3705623067623318, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 637, Loss: 0.25209099975195454, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 638, Loss: 0.29512833377719416, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 639, Loss: 0.32135499067257256, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 640, Loss: 0.23301957918800792, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 641, Loss: 0.29690750826250534, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 642, Loss: 0.2676476213283173, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 643, Loss: 0.3182691709430745, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 644, Loss: 0.3625206058611595, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 645, Loss: 0.33651191407601744, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 646, Loss: 0.2739573402494438, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 647, Loss: 0.23878710555902752, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 648, Loss: 0.3172255045897854, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 649, Loss: 0.23792648177340758, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 650, Loss: 0.31976406532814916, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 651, Loss: 0.26135431948699295, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 652, Loss: 0.22912984428727964, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 653, Loss: 0.24952275393169382, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 654, Loss: 0.2509074021739429, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 655, Loss: 0.2916583375764973, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 656, Loss: 0.5925274082393047, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 657, Loss: 0.23350384394720514, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 658, Loss: 0.34282871062122483, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 659, Loss: 0.3329582878543244, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 660, Loss: 0.3132430426328039, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 661, Loss: 0.36146466599474325, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 662, Loss: 0.25517628801052883, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 663, Loss: 0.3323162977944299, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 664, Loss: 0.3135301289766235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 665, Loss: 0.27430670480880626, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 666, Loss: 0.296328389780425, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 667, Loss: 0.33095139959537023, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 668, Loss: 0.41149108894017183, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 669, Loss: 0.2804132376340934, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 670, Loss: 0.2446223635366222, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 671, Loss: 0.28530545352572845, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 672, Loss: 0.297913683832404, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 673, Loss: 0.23215918679234226, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 674, Loss: 0.3162833541625209, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 675, Loss: 0.32652546190639165, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 676, Loss: 0.3420068525887179, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 677, Loss: 0.29265802492083365, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 678, Loss: 0.3910228956663323, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 679, Loss: 0.30378274149615636, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 680, Loss: 0.282439140572251, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 681, Loss: 0.5296433690438292, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 682, Loss: 0.3623984356759872, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 683, Loss: 0.38406275237251153, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 684, Loss: 0.2596586521113365, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 685, Loss: 0.4519274446469296, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 686, Loss: 0.41405620487296446, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 687, Loss: 0.24562067690919265, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 688, Loss: 0.5224723927201662, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 689, Loss: 0.2626337677709905, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 690, Loss: 0.4211478582092124, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 691, Loss: 0.26209632215865447, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 692, Loss: 0.36743842717698894, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 693, Loss: 0.26506143480597444, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 694, Loss: 0.3160612977474624, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 695, Loss: 0.43434936465031426, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 696, Loss: 0.2579374102739791, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 697, Loss: 0.3722589029756621, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 698, Loss: 0.34411904940227567, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 699, Loss: 0.298368916002009, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 700, Loss: 0.36569318646242543, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 701, Loss: 0.3813608191645841, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 702, Loss: 0.27047699083774474, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 703, Loss: 0.42406276798862164, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 704, Loss: 0.432647772573402, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 705, Loss: 0.25111153935025954, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 706, Loss: 0.2664426708882034, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 707, Loss: 0.3176333097313533, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 708, Loss: 0.33886460782795386, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 709, Loss: 0.3395480589261769, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 710, Loss: 0.24328144759166695, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 711, Loss: 0.38035985023294694, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 712, Loss: 0.3484240601758094, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 713, Loss: 0.25403294851129665, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 714, Loss: 0.6040679837741874, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 715, Loss: 0.2437278777278884, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 716, Loss: 0.40115694532287544, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 717, Loss: 0.2566656418533939, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 718, Loss: 0.6861809676576749, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 719, Loss: 0.24610926878999484, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 720, Loss: 0.29164366093030103, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 721, Loss: 0.42834157481849083, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 722, Loss: 0.2952030388864047, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 723, Loss: 0.2866945014768475, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 724, Loss: 0.26672425335493655, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 725, Loss: 0.2968989691697648, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 726, Loss: 0.24823090901849562, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 727, Loss: 0.36846107376539233, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 728, Loss: 0.24344043895309536, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 729, Loss: 0.47317076311963935, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 730, Loss: 0.33336383128403263, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 731, Loss: 0.2875306545891214, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 732, Loss: 0.3037202040783136, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 733, Loss: 0.2987950591065845, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 734, Loss: 0.34063685763328005, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 735, Loss: 0.27290664523648595, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 736, Loss: 0.2558524937979395, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 737, Loss: 0.23964990981784212, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 738, Loss: 0.3378675093990746, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 739, Loss: 0.5650548284615634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 740, Loss: 0.28958402145458223, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 741, Loss: 0.2978627424669138, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 742, Loss: 0.4163301815120771, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 743, Loss: 0.4583442641024249, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 744, Loss: 0.23197706081909378, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 745, Loss: 0.36039965494056403, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 746, Loss: 0.5330806147762448, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 747, Loss: 0.5598432777915118, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 748, Loss: 0.37518139024266284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 749, Loss: 0.3673259875216898, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 750, Loss: 0.24149606369554796, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 751, Loss: 0.2684669508039844, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 752, Loss: 0.3923908382651704, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 753, Loss: 0.36770968616276445, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 754, Loss: 0.46423050343382277, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 755, Loss: 0.3225549184318271, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 756, Loss: 0.30701801361056624, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 757, Loss: 0.31193558709136604, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 758, Loss: 0.3287225275785123, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 759, Loss: 0.2583930744533003, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 760, Loss: 0.23518655758549267, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 761, Loss: 0.31600160421695384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 762, Loss: 0.30882431166420937, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 763, Loss: 0.25231850201355605, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 764, Loss: 0.23055753244472207, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 765, Loss: 0.2676351131432759, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 766, Loss: 0.3405497162895416, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 767, Loss: 0.43628382717551967, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 768, Loss: 0.2940230580331936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 769, Loss: 0.4892438337284759, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 770, Loss: 0.28761124404017224, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 771, Loss: 0.26831475757879913, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 772, Loss: 0.3252131008774409, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 773, Loss: 0.2576292765547179, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 774, Loss: 0.2630555517773088, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 775, Loss: 0.29105103858511255, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 776, Loss: 0.27298629518797046, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 777, Loss: 0.3033144027277298, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 778, Loss: 0.39325614045985724, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 779, Loss: 0.3149517409929131, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 780, Loss: 0.3359945889718606, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 781, Loss: 0.6684983416205935, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 782, Loss: 0.3937951314156801, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 783, Loss: 0.2528420811463227, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 784, Loss: 0.23303843229097387, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 785, Loss: 0.2360574325818195, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 786, Loss: 0.3409583881426514, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 787, Loss: 0.3522391371062949, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 788, Loss: 0.24639833502728115, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 789, Loss: 0.36375819932115516, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 790, Loss: 0.2721966388324141, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 791, Loss: 0.24625778548692712, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 792, Loss: 0.4423882678595584, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 793, Loss: 0.6494498942899191, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 794, Loss: 0.2851514293009387, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 795, Loss: 0.24515767784821577, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 796, Loss: 0.42805460980084886, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 797, Loss: 0.2746617084945566, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 798, Loss: 0.2667539259344751, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 799, Loss: 0.5575664765973654, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 800, Loss: 0.28184946655089915, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 801, Loss: 0.2683327701319418, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 802, Loss: 0.3494115522095776, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 803, Loss: 0.3155250158159255, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 804, Loss: 0.2918577700735032, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 805, Loss: 0.34775716569523674, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 806, Loss: 0.2834629250944575, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 807, Loss: 0.40680578816186647, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 808, Loss: 0.3270521068608469, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 809, Loss: 0.2481567403119652, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 810, Loss: 0.25628078655863, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 811, Loss: 0.5610242854123393, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 812, Loss: 0.4749071005712199, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 813, Loss: 0.29488480625675695, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 814, Loss: 0.26379318335911145, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 815, Loss: 0.27860688736632133, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 816, Loss: 0.3170953165500514, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 817, Loss: 0.25102368271430664, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 818, Loss: 0.3443361439138045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 819, Loss: 0.3111554035363365, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 820, Loss: 0.31849559615452294, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 821, Loss: 0.31850639828608324, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 822, Loss: 0.2712816968998017, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 823, Loss: 0.33851287107211014, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 824, Loss: 0.31532457391001606, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 825, Loss: 0.24126769138290555, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 826, Loss: 0.36613105971647475, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 827, Loss: 0.28324108646029017, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 828, Loss: 0.3826539492813304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 829, Loss: 0.3318981228337633, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 830, Loss: 0.24543546832999968, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 831, Loss: 0.29240234143614496, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 832, Loss: 0.30923510159084266, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 833, Loss: 0.31287923447241495, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 834, Loss: 0.3452116309071436, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 835, Loss: 0.36736971195671114, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 836, Loss: 0.2973945711552118, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 837, Loss: 0.46415827523390235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 838, Loss: 0.2760281018812406, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 839, Loss: 0.27452668590334195, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 840, Loss: 0.3965409459973389, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 841, Loss: 0.2890752796061956, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 842, Loss: 0.23983509096066435, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 843, Loss: 0.4627195033180046, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 844, Loss: 0.2581512921690939, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 845, Loss: 0.2708286099335522, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 846, Loss: 0.4243045116522338, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 847, Loss: 0.26835393341561653, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 848, Loss: 0.28615482178177154, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 849, Loss: 0.4074869729450059, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 850, Loss: 0.4661902329941412, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 851, Loss: 0.2992485558791817, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 852, Loss: 0.2515012139421529, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 853, Loss: 0.4001823150361863, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 854, Loss: 0.26041373712668064, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 855, Loss: 0.33056152232832275, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 856, Loss: 0.45095262447476187, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 857, Loss: 0.438329756455037, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 858, Loss: 0.40005963223724883, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 859, Loss: 0.3826394242766441, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 860, Loss: 0.2349786276644926, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 861, Loss: 0.38846226560683783, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 862, Loss: 0.2632988248624772, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 863, Loss: 0.2637369610599252, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 864, Loss: 0.35494234275474557, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 865, Loss: 0.4591215448165088, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 866, Loss: 0.28296484774153613, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 867, Loss: 0.2547975237115534, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 868, Loss: 0.23761315411098702, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 869, Loss: 0.25718069617224665, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 870, Loss: 0.26814491506618104, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 871, Loss: 0.24652626441457606, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 872, Loss: 0.5051224975901893, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 873, Loss: 0.26212180692072207, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 874, Loss: 0.25664454418238025, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 875, Loss: 0.34065723320924945, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 876, Loss: 0.28956090415901936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 877, Loss: 0.23628557607312492, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 878, Loss: 0.24179173413189067, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 879, Loss: 0.2739205362040167, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 880, Loss: 0.38810610314244126, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 881, Loss: 0.28700764796315753, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 882, Loss: 0.2754018730037056, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 883, Loss: 0.24913637633637742, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 884, Loss: 0.27145692774484087, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 885, Loss: 0.26038553928627406, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 886, Loss: 0.26780421808077604, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 887, Loss: 0.3320841774383455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 888, Loss: 0.5326903526790643, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 889, Loss: 0.43741271078978916, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 890, Loss: 0.235917098012513, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 891, Loss: 0.2889430662015031, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 892, Loss: 0.366590760836294, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 893, Loss: 0.26211951233474134, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 894, Loss: 0.36486660567780166, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 895, Loss: 0.4111833176888774, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 896, Loss: 0.36116046753014447, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 897, Loss: 0.24183138566728068, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 898, Loss: 0.23276492999992757, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 899, Loss: 0.24517547804645995, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 900, Loss: 0.36743460474192424, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 901, Loss: 0.397954650990607, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 902, Loss: 0.3511475808718584, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 903, Loss: 0.4419405596086168, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 904, Loss: 0.2818109339323387, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 905, Loss: 0.43807105902720767, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 906, Loss: 0.3477028277528384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 907, Loss: 0.24302902281394897, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 908, Loss: 0.31179138122038835, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 909, Loss: 0.31640901879303385, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 910, Loss: 0.47293326391430635, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 911, Loss: 0.3042763038123839, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 912, Loss: 0.32864087557675536, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 913, Loss: 0.3055359922791545, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 914, Loss: 0.22848702746191327, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 915, Loss: 0.5714954698576291, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 916, Loss: 0.2720980555749028, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 917, Loss: 0.4496243832644464, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 918, Loss: 0.25936354353822816, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 919, Loss: 0.38123784963034624, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 920, Loss: 0.27351458966993125, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 921, Loss: 0.2662485420080694, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 922, Loss: 0.24914565853479023, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 923, Loss: 0.2876353544996282, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 924, Loss: 0.39764924552373127, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 925, Loss: 0.7946825410021781, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 926, Loss: 0.5187102610889828, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 927, Loss: 0.3539349290623112, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 928, Loss: 0.3244368510681077, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 929, Loss: 0.2411817718701368, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 930, Loss: 0.3005219272622107, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 931, Loss: 0.2838594652736296, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 932, Loss: 0.3727105526885355, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 933, Loss: 0.2977223346194366, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 934, Loss: 0.3509146130030261, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 935, Loss: 0.48505527031154316, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 936, Loss: 0.2687485023348309, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 937, Loss: 0.2947331268183976, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 938, Loss: 0.43098528997764185, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 939, Loss: 0.25091871553565315, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 940, Loss: 0.27554031196504863, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 941, Loss: 0.27446299620130665, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 942, Loss: 0.26357241094582806, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 943, Loss: 0.4079815786099548, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 944, Loss: 0.2958952553925043, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 945, Loss: 0.33229466097413063, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 946, Loss: 0.25364355838082975, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 947, Loss: 0.25031649089369806, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 948, Loss: 0.3210419471565251, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 949, Loss: 0.40301414550097725, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 950, Loss: 0.2836990086460248, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 951, Loss: 0.2931554676782223, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 952, Loss: 0.329374029810502, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 953, Loss: 0.4758212184155064, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 954, Loss: 0.27370728479536965, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 955, Loss: 0.30244741777058837, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 956, Loss: 0.24445374229801642, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 957, Loss: 0.35655923906774134, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 958, Loss: 0.3169145457915943, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 959, Loss: 0.2735484986140363, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 960, Loss: 0.3373308288261507, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 961, Loss: 0.2490087960390814, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 962, Loss: 0.30291461351758264, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 963, Loss: 0.2621987594339542, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 964, Loss: 0.30616106359024836, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 965, Loss: 0.42954831673403093, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 966, Loss: 0.37416414623156996, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 967, Loss: 0.2903975201334139, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 968, Loss: 0.2579388346755481, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 969, Loss: 0.23819950587708277, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 970, Loss: 0.276084073471669, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 971, Loss: 0.2569464226881599, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 972, Loss: 0.28315706097678683, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 973, Loss: 0.27715541857938747, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 974, Loss: 0.6021692570219433, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 975, Loss: 0.3027296135621143, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 976, Loss: 0.32499706707126097, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 977, Loss: 0.3054917761729047, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 978, Loss: 0.2542504363389552, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 979, Loss: 0.2633024299565313, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 980, Loss: 0.3183812559370122, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 981, Loss: 0.2695055487522357, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 982, Loss: 0.2905513508439121, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 983, Loss: 0.28144317050686274, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 984, Loss: 0.29402202305869346, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 985, Loss: 0.25504236187670437, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 986, Loss: 0.4081631566412149, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 987, Loss: 0.3000548300447147, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 988, Loss: 0.40541192040571034, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 989, Loss: 0.42811915301936365, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 990, Loss: 0.41369327254093125, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 991, Loss: 0.3538201077329065, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 992, Loss: 0.23859548884303827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 993, Loss: 0.2668444666339895, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 994, Loss: 0.2937672660208308, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 995, Loss: 0.29310822610816234, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 996, Loss: 0.4682042620523419, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 997, Loss: 0.2907716414291473, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 998, Loss: 0.2737047218149577, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 999, Loss: 0.3233957205396781, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1000, Loss: 0.3053909813789846, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1001, Loss: 0.3965897997181367, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1002, Loss: 0.307254377757581, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1003, Loss: 0.2710244851731576, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1004, Loss: 0.32190034368703396, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1005, Loss: 0.30811788729565603, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1006, Loss: 0.27458320726107677, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1007, Loss: 0.3216248159884027, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1008, Loss: 0.2476745875674633, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1009, Loss: 0.36794437669061897, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1010, Loss: 0.4253719525045463, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1011, Loss: 0.27635141078912856, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1012, Loss: 0.3456242173442738, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1013, Loss: 0.32387491875497904, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1014, Loss: 0.28847241992429573, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1015, Loss: 0.3545274610105067, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1016, Loss: 0.2432240956449361, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1017, Loss: 0.4310176757868174, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1018, Loss: 0.33415250658382145, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1019, Loss: 0.44753511344576113, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1020, Loss: 0.2771958614921632, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1021, Loss: 0.4645986816085188, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1022, Loss: 0.49230336851988066, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1023, Loss: 0.2527985620682585, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1024, Loss: 0.3759984474512419, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1025, Loss: 0.23964643231086555, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1026, Loss: 0.2793499142711043, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1027, Loss: 0.34627807274483785, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1028, Loss: 0.4202391721774318, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1029, Loss: 0.2907709640311363, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1030, Loss: 0.3003611214998474, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1031, Loss: 0.36741223881296003, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1032, Loss: 0.3151809721547446, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1033, Loss: 0.5464870671673867, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1034, Loss: 0.24472675651225784, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1035, Loss: 0.42001758418565965, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1036, Loss: 0.3021334780143825, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1037, Loss: 0.3022114618087915, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1038, Loss: 0.2642681855702871, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1039, Loss: 0.35024004739737885, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1040, Loss: 0.3310305697257331, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1041, Loss: 0.3305043357500721, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1042, Loss: 0.2639431371824633, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1043, Loss: 0.3129744249481735, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1044, Loss: 0.27041895472345423, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1045, Loss: 0.307674887642969, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1046, Loss: 0.4629299752179754, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1047, Loss: 0.38615775918089695, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1048, Loss: 0.2351592493189853, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1049, Loss: 0.2651959699367268, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1050, Loss: 0.36380129717566384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1051, Loss: 0.2824869055502488, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1052, Loss: 0.36805135417553725, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1053, Loss: 0.3073256373485319, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1054, Loss: 0.35595665571338175, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1055, Loss: 0.24534028175278183, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1056, Loss: 0.35194942176523847, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1057, Loss: 0.2659955096342143, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1058, Loss: 0.3162625682920254, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1059, Loss: 0.2727677098041923, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1060, Loss: 0.28877224322579953, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1061, Loss: 0.24645440118183445, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1062, Loss: 0.39255397785089957, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1063, Loss: 0.23247877289773874, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1064, Loss: 0.39255697509844545, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1065, Loss: 0.5003250267610354, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1066, Loss: 0.2419637761826675, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1067, Loss: 0.24864493993980052, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1068, Loss: 0.33001681298060975, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1069, Loss: 0.2626742692922386, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1070, Loss: 0.2387520380240038, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1071, Loss: 0.3143002463306922, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1072, Loss: 0.23597727353766607, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1073, Loss: 0.38773663684425996, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1074, Loss: 0.26847640744676704, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1075, Loss: 0.3195646402468203, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1076, Loss: 0.2537092857018469, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1077, Loss: 0.24755397787988573, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1078, Loss: 0.25344248341582304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1079, Loss: 0.266884697071433, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1080, Loss: 0.29849665621353605, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1081, Loss: 0.3723815404300011, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1082, Loss: 0.2517033387498249, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1083, Loss: 0.24119826325859894, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1084, Loss: 0.33739676728411266, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1085, Loss: 0.4049399552028815, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1086, Loss: 0.29572484735940074, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1087, Loss: 0.44421488632369266, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1088, Loss: 0.5592173733630345, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1089, Loss: 0.26079118272592294, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1090, Loss: 0.32230556033725166, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1091, Loss: 0.27493052148297226, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1092, Loss: 0.4563951407281741, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1093, Loss: 0.3226308045703277, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1094, Loss: 0.5560663556269656, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1095, Loss: 0.2511928161641308, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1096, Loss: 0.4081478908415834, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1097, Loss: 0.3147673633462198, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1098, Loss: 0.30404000606995235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1099, Loss: 0.2298100912344613, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1100, Loss: 0.25231973046876893, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1101, Loss: 0.5495318849485018, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1102, Loss: 0.33520346792619043, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1103, Loss: 0.29551764801012237, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1104, Loss: 0.46940745373422865, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1105, Loss: 0.23352376416418733, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1106, Loss: 0.35150115326953074, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1107, Loss: 0.42272236308643474, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1108, Loss: 0.23575863553188958, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1109, Loss: 0.3080930847953559, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1110, Loss: 0.30524629447467433, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1111, Loss: 0.657577420824323, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1112, Loss: 0.30039743042773487, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1113, Loss: 0.25121267010217857, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1114, Loss: 0.35318903142924657, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1115, Loss: 0.24362121378034315, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1116, Loss: 0.4305225115244006, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1117, Loss: 0.4163992058567419, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1118, Loss: 0.2850958186029303, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1119, Loss: 0.47323358046398617, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1120, Loss: 0.327587633486259, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1121, Loss: 0.2626582928665936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1122, Loss: 0.26633544440293605, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1123, Loss: 0.3878688562084207, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1124, Loss: 0.2569281363442448, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1125, Loss: 0.2941740124406005, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1126, Loss: 0.2712669908072627, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1127, Loss: 0.349977863564068, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1128, Loss: 0.25249480031460625, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1129, Loss: 0.24727242722246767, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1130, Loss: 0.24451646925642473, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1131, Loss: 0.7332319459723979, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1132, Loss: 0.24934972660060628, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1133, Loss: 0.2630848360602725, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1134, Loss: 0.3467882778941298, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1135, Loss: 0.29393122086533463, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1136, Loss: 0.4211338858819097, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1137, Loss: 0.6193137217874873, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1138, Loss: 0.4515346453858499, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1139, Loss: 0.26202817413274854, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1140, Loss: 0.2648770445871556, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1141, Loss: 0.4787171455897361, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1142, Loss: 0.39611506103797145, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1143, Loss: 0.27823062848198554, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1144, Loss: 0.2870888070050251, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1145, Loss: 0.3885807955179339, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1146, Loss: 0.3724543705958766, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1147, Loss: 0.3678466463533755, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1148, Loss: 0.2976109165963602, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1149, Loss: 0.32619967479209644, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1150, Loss: 0.44425953454853906, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1151, Loss: 0.5184865166180521, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1152, Loss: 0.32197476090613253, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1153, Loss: 0.27151584801840445, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1154, Loss: 0.3171053089943401, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1155, Loss: 0.40300347602112596, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1156, Loss: 0.24640036093903758, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1157, Loss: 0.2896282892731349, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1158, Loss: 0.3903439159085198, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1159, Loss: 0.26454247644580076, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1160, Loss: 0.4864685017749795, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1161, Loss: 0.40511760892226006, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1162, Loss: 0.4293984436721666, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1163, Loss: 0.26997819400763373, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1164, Loss: 0.27249646353698276, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1165, Loss: 0.2975623869533409, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1166, Loss: 0.275537610521398, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1167, Loss: 0.2298004803330637, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1168, Loss: 0.26705379164486637, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1169, Loss: 0.3354126650043712, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1170, Loss: 0.2667036243546949, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1171, Loss: 0.4317987573918831, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1172, Loss: 0.26602829074276624, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1173, Loss: 0.36263515628851517, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1174, Loss: 0.29503681253440656, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1175, Loss: 0.24815065527997926, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1176, Loss: 0.2586338920167558, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1177, Loss: 0.27803934818084597, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1178, Loss: 0.2977740352390288, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1179, Loss: 0.4593032023001044, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1180, Loss: 0.2983550453567876, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1181, Loss: 0.2574846828042808, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1182, Loss: 0.3530832221922893, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1183, Loss: 0.37555019011158725, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1184, Loss: 0.3754862328287809, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1185, Loss: 0.2321893129302691, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1186, Loss: 0.33267772963723, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1187, Loss: 0.37213948402982855, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1188, Loss: 0.32567169944269386, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1189, Loss: 0.25610219762569225, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1190, Loss: 0.25764093195262494, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1191, Loss: 0.32744632875534996, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1192, Loss: 0.3135717598583303, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1193, Loss: 0.36749779377938585, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1194, Loss: 0.32561243871088497, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1195, Loss: 0.2511631311066951, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1196, Loss: 0.39923906244123175, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1197, Loss: 0.23925383743847511, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1198, Loss: 0.2706660059236691, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1199, Loss: 0.4285263679785602, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1200, Loss: 0.24855664696633595, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1201, Loss: 0.25246636871228945, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1202, Loss: 0.23248643510141245, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1203, Loss: 0.46073764036953135, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1204, Loss: 0.24475900324788924, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1205, Loss: 0.22937121560645113, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1206, Loss: 0.26699420551021197, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1207, Loss: 0.33052418698893593, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1208, Loss: 0.3105540003725734, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1209, Loss: 0.24123591889105006, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1210, Loss: 0.2581121451726649, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1211, Loss: 0.51881004306624, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1212, Loss: 0.32704046085586747, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1213, Loss: 0.3329377873126782, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1214, Loss: 0.2912546348223993, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1215, Loss: 0.2495694712497853, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1216, Loss: 0.271670044472493, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1217, Loss: 0.29794293869621946, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1218, Loss: 0.7352199857530155, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1219, Loss: 0.2409443637418007, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1220, Loss: 0.3442287373461734, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1221, Loss: 0.3678880022598469, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1222, Loss: 0.4344820823649135, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1223, Loss: 0.2701166508515383, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1224, Loss: 0.32367274199062623, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1225, Loss: 0.2651154581498911, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1226, Loss: 0.2902192521567901, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1227, Loss: 0.5447501406105896, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1228, Loss: 0.33553827059695274, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1229, Loss: 0.5716281160117699, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1230, Loss: 0.3105509212080301, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1231, Loss: 0.26252961527312735, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1232, Loss: 0.2442335457988674, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1233, Loss: 0.2545213696429214, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1234, Loss: 0.4013158660639054, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1235, Loss: 0.2899378095657269, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1236, Loss: 0.38378155698102034, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1237, Loss: 0.2479336736413448, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1238, Loss: 0.3321525933352657, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1239, Loss: 0.34692195636734746, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1240, Loss: 0.2892352502943718, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1241, Loss: 0.2572761885552405, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1242, Loss: 0.3026623374305405, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1243, Loss: 0.2984255483363767, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1244, Loss: 0.26322882835789185, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1245, Loss: 0.26155585347436705, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1246, Loss: 0.30157586812036075, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1247, Loss: 0.29470140460762184, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1248, Loss: 0.34666409351898547, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1249, Loss: 0.25013670474400634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1250, Loss: 0.32223914910967955, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1251, Loss: 0.3155153513985302, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1252, Loss: 0.24486365948508404, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1253, Loss: 0.26698842712821913, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1254, Loss: 0.28040714336526285, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1255, Loss: 0.30130791437128823, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1256, Loss: 0.3387038760647239, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1257, Loss: 0.3289660658636784, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1258, Loss: 0.2550276730925517, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1259, Loss: 0.2819703477803142, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1260, Loss: 0.2833731131234757, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1261, Loss: 0.2642536841115953, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1262, Loss: 0.3875110276543599, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1263, Loss: 0.3277517926001381, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1264, Loss: 0.37102382872903356, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1265, Loss: 0.30853789689228267, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1266, Loss: 0.26377787578588596, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1267, Loss: 0.25737303059306327, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1268, Loss: 0.366767479630184, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1269, Loss: 0.29011900013807673, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1270, Loss: 0.24393200858109676, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1271, Loss: 0.32215613621397576, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1272, Loss: 0.23933764581842967, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1273, Loss: 0.3336969985616022, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1274, Loss: 0.493185731728374, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1275, Loss: 0.24368729768584832, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1276, Loss: 0.4295297725194437, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1277, Loss: 0.29683873168842045, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1278, Loss: 0.4474595753270989, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1279, Loss: 0.30602292168439343, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1280, Loss: 0.3092614833440412, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1281, Loss: 0.29828243573440094, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1282, Loss: 0.24069975377019076, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1283, Loss: 0.2547985058794502, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1284, Loss: 0.2541969237982811, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1285, Loss: 0.2793739911480701, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1286, Loss: 0.31308380888556436, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1287, Loss: 0.2652893052286539, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1288, Loss: 0.5273167713107929, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1289, Loss: 0.23460756274707392, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1290, Loss: 0.33819841045444055, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1291, Loss: 0.32307983478359925, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1292, Loss: 0.2992992038765946, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1293, Loss: 0.5084918696358202, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1294, Loss: 0.2837236895368529, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1295, Loss: 0.24209351423261802, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1296, Loss: 0.24648762974083258, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1297, Loss: 0.2588172139411667, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1298, Loss: 0.44152518082378056, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1299, Loss: 0.2917169554470962, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1300, Loss: 0.2743593908219349, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1301, Loss: 0.40708113536674284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1302, Loss: 0.28400938792873964, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1303, Loss: 0.31506424057707433, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1304, Loss: 0.2457507388769524, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1305, Loss: 0.3545352916470177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1306, Loss: 0.3251135911729601, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1307, Loss: 0.4414618930664251, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1308, Loss: 0.24937217446128782, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1309, Loss: 0.3929170360332952, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1310, Loss: 0.3387657414085811, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1311, Loss: 0.2762193187084198, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1312, Loss: 0.38113479792919924, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1313, Loss: 0.3693180596834581, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1314, Loss: 0.4087465608213363, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1315, Loss: 0.25496077249230015, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1316, Loss: 0.26371499967110357, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1317, Loss: 0.24748951297775498, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1318, Loss: 0.2382433502463576, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1319, Loss: 0.2538819664349383, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1320, Loss: 0.30822937058686634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1321, Loss: 0.4513697848123134, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1322, Loss: 0.39553797387291256, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1323, Loss: 0.270266375536616, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1324, Loss: 0.424266238273762, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1325, Loss: 0.31912922048042697, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1326, Loss: 0.25265230167052394, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1327, Loss: 0.3653801790960898, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1328, Loss: 0.380778399019765, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1329, Loss: 0.2678872449146352, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1330, Loss: 0.2600315094049079, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1331, Loss: 0.30985987047445623, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1332, Loss: 0.27029596380578386, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1333, Loss: 0.48002431164533643, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1334, Loss: 0.4221393668544573, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1335, Loss: 0.26443123018521086, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1336, Loss: 0.695510727620457, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1337, Loss: 0.3636829769017741, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1338, Loss: 0.26391698339154307, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1339, Loss: 0.37190438711157314, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1340, Loss: 0.3686005461061452, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1341, Loss: 0.2616362785280027, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1342, Loss: 0.3419539211706615, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1343, Loss: 0.2608434178108119, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1344, Loss: 0.2974887584649966, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1345, Loss: 0.3046906483512351, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1346, Loss: 0.24616374935885843, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1347, Loss: 0.4211040250621088, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1348, Loss: 0.3215428556711833, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1349, Loss: 0.6396730749440545, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1350, Loss: 0.25177990982859866, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1351, Loss: 0.2723682107715455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1352, Loss: 0.33171040633891324, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1353, Loss: 0.27629274529665043, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1354, Loss: 0.332427776958783, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1355, Loss: 0.32949314790413375, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1356, Loss: 0.33554830283351744, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1357, Loss: 0.33053791604974775, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1358, Loss: 0.3313969568110566, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1359, Loss: 0.27182196658315433, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1360, Loss: 0.325977685694715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1361, Loss: 0.35885763297430384, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1362, Loss: 0.24217049318716524, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1363, Loss: 0.36228763129948593, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1364, Loss: 0.30398189038229917, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1365, Loss: 0.265543615522033, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1366, Loss: 0.2972382503867459, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1367, Loss: 0.3323525922559647, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1368, Loss: 0.353964086590981, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1369, Loss: 0.24917514468388305, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1370, Loss: 0.3550484364894852, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1371, Loss: 0.5655011262226387, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1372, Loss: 0.27987531725141, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1373, Loss: 0.34358300409695797, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1374, Loss: 0.4043193722435168, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1375, Loss: 0.3086366637053707, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1376, Loss: 0.289618492355859, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1377, Loss: 0.43836788192090786, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1378, Loss: 0.38664002309196743, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1379, Loss: 0.25248389849318975, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1380, Loss: 0.3986257689208188, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1381, Loss: 0.4577487778777496, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1382, Loss: 0.33618775605043727, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1383, Loss: 0.44415658500972854, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1384, Loss: 0.2331972006993144, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1385, Loss: 0.24933243931084392, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1386, Loss: 0.37292024348831354, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1387, Loss: 0.2572120499319708, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1388, Loss: 0.33705653370494676, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1389, Loss: 0.36151681100308775, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1390, Loss: 0.4027153528411783, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1391, Loss: 0.26768797881974693, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1392, Loss: 0.32460856287048107, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1393, Loss: 0.3158040169799335, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1394, Loss: 0.3179913570140569, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1395, Loss: 0.2926193094680037, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1396, Loss: 0.3790507584196237, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1397, Loss: 0.258831005788729, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1398, Loss: 0.29538016368349035, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1399, Loss: 0.3139986159262257, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1400, Loss: 0.35872168942926247, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1401, Loss: 0.3246147672343226, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1402, Loss: 0.2893505455357299, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1403, Loss: 0.2609768825368508, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1404, Loss: 0.36289289960354526, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1405, Loss: 0.24262547558584316, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1406, Loss: 0.257506002992754, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1407, Loss: 0.28230719369572005, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1408, Loss: 0.3392970654411578, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1409, Loss: 0.2518012436640166, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1410, Loss: 0.3526154878371674, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1411, Loss: 0.2847160903184359, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1412, Loss: 0.40994331654586347, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1413, Loss: 0.40346066552523263, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1414, Loss: 0.38404857381098956, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1415, Loss: 0.38700519558975166, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1416, Loss: 0.4661621665313379, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1417, Loss: 0.23932356164140933, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1418, Loss: 0.2656952776242958, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1419, Loss: 0.295036726669546, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1420, Loss: 0.2405675417561447, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1421, Loss: 0.25663063524658586, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1422, Loss: 0.40312356638347147, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1423, Loss: 0.3035699446293162, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1424, Loss: 0.38335569565972366, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1425, Loss: 0.38980144368719516, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1426, Loss: 0.24302624726127905, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1427, Loss: 0.28166842558348915, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1428, Loss: 0.46100525135443005, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1429, Loss: 0.36419398426793664, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1430, Loss: 0.27241384710097283, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1431, Loss: 0.24461179232932548, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1432, Loss: 0.3691292265651268, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1433, Loss: 0.430995789257656, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1434, Loss: 0.3586273379540786, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1435, Loss: 0.25388199462350475, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1436, Loss: 0.3483895577826118, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1437, Loss: 0.3405678684040243, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1438, Loss: 0.23199637308374305, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1439, Loss: 0.2687516693887407, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1440, Loss: 0.49301024028874696, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1441, Loss: 0.42804052582776825, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1442, Loss: 0.4350910595984546, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1443, Loss: 0.32700929335438, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1444, Loss: 0.25516722103606526, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1445, Loss: 0.3245558208131289, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1446, Loss: 0.24737886354404467, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1447, Loss: 0.238559375519471, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1448, Loss: 0.2621479739504122, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1449, Loss: 0.2848657415485459, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1450, Loss: 0.238893287464001, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1451, Loss: 0.2718685816880829, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1452, Loss: 0.3282310006870321, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1453, Loss: 0.2376720522334962, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1454, Loss: 0.32356427127537, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1455, Loss: 0.31272100615980264, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1456, Loss: 0.2407138477130255, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1457, Loss: 0.23927287452060267, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1458, Loss: 0.279691438894221, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1459, Loss: 0.35498513362915013, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1460, Loss: 0.24374552148513362, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1461, Loss: 0.3882656146680177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1462, Loss: 0.27084837918612603, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1463, Loss: 0.28776585373892477, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1464, Loss: 0.29081815138158956, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1465, Loss: 0.24921337766066853, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1466, Loss: 0.24781437941131068, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1467, Loss: 0.32615625640189605, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1468, Loss: 0.29244913667188394, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1469, Loss: 0.4163911510362994, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1470, Loss: 0.42793846805324975, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1471, Loss: 0.2633738883518767, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1472, Loss: 0.4034004715389602, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1473, Loss: 0.26015528874517585, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1474, Loss: 0.2692634839136572, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1475, Loss: 0.3842002410585919, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1476, Loss: 0.2979010188259006, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1477, Loss: 0.47554215364207586, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1478, Loss: 0.26324208300162555, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1479, Loss: 0.4249964977059967, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1480, Loss: 0.29629296800402494, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1481, Loss: 0.27065301567464284, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1482, Loss: 0.26399759277798235, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1483, Loss: 0.32232729100665414, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1484, Loss: 0.3807213952738184, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1485, Loss: 0.35731259575345664, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1486, Loss: 0.2348528690358797, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1487, Loss: 0.2894796086606181, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1488, Loss: 0.32057607088549, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1489, Loss: 0.2365739159638841, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1490, Loss: 0.2792203223300907, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1491, Loss: 0.28713634222342327, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1492, Loss: 0.2578994772274914, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1493, Loss: 0.3238949103329323, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1494, Loss: 0.28393421535842644, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1495, Loss: 0.24534976957496596, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1496, Loss: 0.2959171301339173, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1497, Loss: 0.5729022640747599, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1498, Loss: 0.2703498712570248, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1499, Loss: 0.3272048299287832, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1500, Loss: 0.3541707155542453, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1501, Loss: 0.293906166531026, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1502, Loss: 0.3120900604768715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1503, Loss: 0.26500966667116177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1504, Loss: 0.24422612557923745, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1505, Loss: 0.27882751787654103, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1506, Loss: 0.2518148447827633, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1507, Loss: 0.27100645139825913, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1508, Loss: 0.282644518984007, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1509, Loss: 0.28827405080405855, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1510, Loss: 0.3090060239910056, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1511, Loss: 0.3174986661391857, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1512, Loss: 0.7849198257352993, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1513, Loss: 0.27953527464833, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1514, Loss: 0.3041500889057859, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1515, Loss: 0.40587979757243675, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1516, Loss: 0.39271357613275415, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1517, Loss: 0.3264591758393718, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1518, Loss: 0.39640945830207835, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1519, Loss: 0.2904994050175509, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1520, Loss: 0.4631590066152039, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1521, Loss: 0.35523750283660505, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1522, Loss: 0.4557858915281058, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1523, Loss: 0.39154742219459004, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1524, Loss: 0.23755652547492628, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1525, Loss: 0.2905660802799428, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1526, Loss: 0.2628156748660971, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1527, Loss: 0.37005641089522495, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1528, Loss: 0.35542680058791715, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1529, Loss: 0.23741355096940048, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1530, Loss: 0.2585058196289989, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1531, Loss: 0.2790609374207535, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1532, Loss: 0.24742899044661826, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1533, Loss: 0.32051199359005755, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1534, Loss: 0.24770836538299162, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1535, Loss: 0.261698399185238, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1536, Loss: 0.248606701664003, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1537, Loss: 0.3515328946845465, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1538, Loss: 0.23600525724169064, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1539, Loss: 0.23587309908482504, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1540, Loss: 0.28621464201536617, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1541, Loss: 0.3952948519629429, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1542, Loss: 0.2696324041288792, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1543, Loss: 0.2637681170894448, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1544, Loss: 0.3070382356306412, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1545, Loss: 0.3640472107873087, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1546, Loss: 0.2944305540635852, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1547, Loss: 0.23826546949155958, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1548, Loss: 0.26173542786308274, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1549, Loss: 0.3181142468638695, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1550, Loss: 0.2584424956630868, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1551, Loss: 0.4727913801952931, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1552, Loss: 0.3050535651810127, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1553, Loss: 0.2858859563547318, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1554, Loss: 0.3670870992815922, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1555, Loss: 0.3556770719603817, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1556, Loss: 0.328106130561245, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1557, Loss: 0.3105730976176851, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1558, Loss: 0.26570195776919786, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1559, Loss: 0.26771440200087465, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1560, Loss: 0.4698341939012831, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1561, Loss: 0.35162631868341676, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1562, Loss: 0.5895214437321692, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1563, Loss: 0.26017765086442196, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1564, Loss: 0.237221229582614, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1565, Loss: 0.23729297397411134, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1566, Loss: 0.2673218746936967, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1567, Loss: 0.3167250854556528, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1568, Loss: 0.27050319529055133, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1569, Loss: 0.5648420140305053, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1570, Loss: 0.29053446131943067, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1571, Loss: 0.23335092241362623, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1572, Loss: 0.2778272038288448, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1573, Loss: 0.26417198252757645, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1574, Loss: 0.26272781609476187, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1575, Loss: 0.27323954959904806, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1576, Loss: 0.3183985600639391, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1577, Loss: 0.23842091397316892, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1578, Loss: 0.24824360172055504, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1579, Loss: 0.5587686235931982, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1580, Loss: 0.43588683594793487, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1581, Loss: 0.33519338989559494, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1582, Loss: 0.41174489577037465, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1583, Loss: 0.42282673908196283, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1584, Loss: 0.32342416317962647, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1585, Loss: 0.25350214596361137, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1586, Loss: 0.3107235311345399, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1587, Loss: 0.23016944546222076, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1588, Loss: 0.289688795145599, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1589, Loss: 0.34167693919646563, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1590, Loss: 0.32182167098496633, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1591, Loss: 0.2436189558289151, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1592, Loss: 0.3377915259176153, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1593, Loss: 0.2901011641521091, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1594, Loss: 0.32585392318864764, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1595, Loss: 0.25524280682188183, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1596, Loss: 0.2508436613880842, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1597, Loss: 0.2783671451994702, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1598, Loss: 0.258538905700953, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1599, Loss: 0.2618632066252634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1600, Loss: 0.33702191715028856, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1601, Loss: 0.4014818870838701, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1602, Loss: 0.30556875237330805, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1603, Loss: 0.4586850929483353, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1604, Loss: 0.3378790204168705, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1605, Loss: 0.2877085426822291, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1606, Loss: 0.2789518894592636, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1607, Loss: 0.2668954276399735, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1608, Loss: 0.2597569686022615, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1609, Loss: 0.26057415125991085, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1610, Loss: 0.27041704291874175, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1611, Loss: 0.2880060454985793, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1612, Loss: 0.2549047506452155, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1613, Loss: 0.3394768143990692, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1614, Loss: 0.27506486983653233, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1615, Loss: 0.28757406819461195, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1616, Loss: 0.4186733674490194, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1617, Loss: 0.2684597117091068, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1618, Loss: 0.502109413457244, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1619, Loss: 0.4044631911627826, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1620, Loss: 0.3081383103342375, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1621, Loss: 0.32487371795634884, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1622, Loss: 0.2487318129411032, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1623, Loss: 0.267844113467812, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1624, Loss: 0.3367285570323334, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1625, Loss: 0.351637624988752, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1626, Loss: 0.2724210886516744, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1627, Loss: 0.31613882038049634, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1628, Loss: 0.25433389775709275, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1629, Loss: 0.318644095497463, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1630, Loss: 0.24889151337711263, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1631, Loss: 0.2719713205974461, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1632, Loss: 0.27864832305111153, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1633, Loss: 0.24962785139502797, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1634, Loss: 0.3844737337808385, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1635, Loss: 0.2501209815539905, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1636, Loss: 0.38538157713604015, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1637, Loss: 0.2810076279476556, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1638, Loss: 0.2983347856994307, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1639, Loss: 0.23759394901578618, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1640, Loss: 0.23892130881188112, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1641, Loss: 0.442562919278082, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1642, Loss: 0.2999808330593193, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1643, Loss: 0.268737932872377, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1644, Loss: 0.4279752785007679, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1645, Loss: 0.33123422311073386, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1646, Loss: 0.3481689375298844, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1647, Loss: 0.25621156244551824, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1648, Loss: 0.25194387392307616, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1649, Loss: 0.23916646162923794, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1650, Loss: 0.47729096641967905, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1651, Loss: 0.42198137398685626, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1652, Loss: 0.34731190236441956, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1653, Loss: 0.28177495848612316, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1654, Loss: 0.2457627527919073, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1655, Loss: 0.2826310157506931, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1656, Loss: 0.5018683900267201, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1657, Loss: 0.36239260824358777, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1658, Loss: 0.4474445010467965, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1659, Loss: 0.26223297057180583, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1660, Loss: 0.26636145494074903, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1661, Loss: 0.5411313818413455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1662, Loss: 0.34279636025862936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1663, Loss: 0.33472557495563615, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1664, Loss: 0.23969246229599594, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1665, Loss: 0.2528129016161418, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1666, Loss: 0.36827968236275177, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1667, Loss: 0.2918128325731358, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1668, Loss: 0.4001636413816517, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1669, Loss: 0.3673553349329484, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1670, Loss: 0.28365488760671626, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1671, Loss: 0.292724778339017, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1672, Loss: 0.3131540670385935, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1673, Loss: 0.24387523852593102, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1674, Loss: 0.29192889968619695, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1675, Loss: 0.2678398590991862, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1676, Loss: 0.3619618612378776, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1677, Loss: 0.32563297552875536, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1678, Loss: 0.47616727889620114, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1679, Loss: 0.27908798449201017, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1680, Loss: 0.3106751832930628, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1681, Loss: 0.2653066853270928, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1682, Loss: 0.2616083829688998, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1683, Loss: 0.3284124751182415, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1684, Loss: 0.2484094830557598, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1685, Loss: 0.3459154625788772, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1686, Loss: 0.6493611276581095, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1687, Loss: 0.2767486553638675, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1688, Loss: 0.2612882921858205, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1689, Loss: 0.4012896806257029, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1690, Loss: 0.2615856315440826, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1691, Loss: 0.3480791280873545, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1692, Loss: 0.30447064584920414, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1693, Loss: 0.30058809970698874, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1694, Loss: 0.34628475250151936, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1695, Loss: 0.2382742808822094, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1696, Loss: 0.41200042726980124, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1697, Loss: 0.47180009629113406, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1698, Loss: 0.2794991985546472, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1699, Loss: 0.30545440987057393, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1700, Loss: 0.2835944690794872, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1701, Loss: 0.2549516702313328, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1702, Loss: 0.27722837915421145, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1703, Loss: 0.3461988703342193, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1704, Loss: 0.2534429062762974, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1705, Loss: 0.29027414895611736, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1706, Loss: 0.26088067422135097, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1707, Loss: 0.2970995086648455, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1708, Loss: 0.3039308594750121, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1709, Loss: 0.23216432152379843, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1710, Loss: 0.23379663578623633, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1711, Loss: 0.41554477547594815, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1712, Loss: 0.4454146507379656, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1713, Loss: 0.3914667866955882, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1714, Loss: 0.32787796195053953, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1715, Loss: 0.36137394812869683, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1716, Loss: 0.4146966410233819, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1717, Loss: 0.31205659693655324, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1718, Loss: 0.4946384251066938, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1719, Loss: 0.3055745632712292, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1720, Loss: 0.39117717038895305, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1721, Loss: 0.3368447174203111, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1722, Loss: 0.42957740030356995, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1723, Loss: 0.40863883556722047, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1724, Loss: 0.24875375871289346, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1725, Loss: 0.3594879081273969, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1726, Loss: 0.24271398997236698, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1727, Loss: 0.7068655688117348, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1728, Loss: 0.35902899538716804, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1729, Loss: 0.3323675005334286, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1730, Loss: 0.35339561648107337, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1731, Loss: 0.2502087385155736, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1732, Loss: 0.2845579712109273, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1733, Loss: 0.3689171403523598, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1734, Loss: 0.27356949155871957, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1735, Loss: 0.35885612883165974, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1736, Loss: 0.32326026225884974, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1737, Loss: 0.34874403068381044, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1738, Loss: 0.23650707243096142, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1739, Loss: 0.3112518298509352, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1740, Loss: 0.3178054476844024, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1741, Loss: 0.27922009306005746, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1742, Loss: 0.2499866236098503, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1743, Loss: 0.329421238540782, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1744, Loss: 0.41035892473699387, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1745, Loss: 0.34445897328413205, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1746, Loss: 0.26406796595251, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1747, Loss: 0.27546203912943473, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1748, Loss: 0.542100571576982, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1749, Loss: 0.2802966136817353, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1750, Loss: 0.4937121795606383, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1751, Loss: 0.2924165687698468, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1752, Loss: 0.28507858700569444, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1753, Loss: 0.2893056029976382, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1754, Loss: 0.3256684949394216, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1755, Loss: 0.31302198136627335, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1756, Loss: 0.2974781646104886, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1757, Loss: 0.4068941816043894, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1758, Loss: 0.30435509681483963, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1759, Loss: 0.3440522368086497, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1760, Loss: 0.40512779468624827, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1761, Loss: 0.28642008727638263, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1762, Loss: 0.7283401069746783, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1763, Loss: 0.6470626138284983, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1764, Loss: 0.348006619977053, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1765, Loss: 0.31283775357434995, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1766, Loss: 0.241952728226987, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1767, Loss: 0.33389658389506643, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1768, Loss: 0.2573461635730663, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1769, Loss: 0.3259457016691907, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1770, Loss: 0.4563511277777553, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1771, Loss: 0.3918065174855554, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1772, Loss: 0.30185950219963265, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1773, Loss: 0.28262080951166735, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1774, Loss: 0.4732987697141442, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1775, Loss: 0.29890790973167336, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1776, Loss: 0.280198718753578, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1777, Loss: 0.33414672344319196, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1778, Loss: 0.3304663678627017, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1779, Loss: 0.272078327101002, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1780, Loss: 0.23630208260316987, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1781, Loss: 0.2930256134478597, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1782, Loss: 0.5250865446367476, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1783, Loss: 0.27633910521981986, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1784, Loss: 0.25762659068735316, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1785, Loss: 0.28762406485259323, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1786, Loss: 0.35899019038962326, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1787, Loss: 0.2627699521584159, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1788, Loss: 0.23973590943585357, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1789, Loss: 0.2709907742122193, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1790, Loss: 0.4093093088510733, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1791, Loss: 0.29570198802964337, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1792, Loss: 0.26146316252097734, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1793, Loss: 0.3550648439502182, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1794, Loss: 0.24466944103955407, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1795, Loss: 0.2794466744544712, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1796, Loss: 0.4477223846045826, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1797, Loss: 0.26836890369098926, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1798, Loss: 0.2468558059658276, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1799, Loss: 0.26662558382645807, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1800, Loss: 0.26069313799453253, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1801, Loss: 0.38354429956071806, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1802, Loss: 0.28661172609493285, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1803, Loss: 0.24151351446899888, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1804, Loss: 0.3194279064749268, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1805, Loss: 0.3208137502687606, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1806, Loss: 0.3236116940809247, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1807, Loss: 0.4116804145705103, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1808, Loss: 0.6064684788730674, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1809, Loss: 0.24096429326302546, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1810, Loss: 0.3085745030304143, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1811, Loss: 0.2758364788185676, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1812, Loss: 0.2849485157175015, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1813, Loss: 0.2837310823436067, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1814, Loss: 0.35454667622680647, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1815, Loss: 0.279848659796309, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1816, Loss: 0.2681292978089286, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1817, Loss: 0.24138070711806522, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1818, Loss: 0.3491323898873797, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1819, Loss: 0.23221714448684022, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1820, Loss: 0.298409591945249, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1821, Loss: 0.25761759946773816, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1822, Loss: 0.2675677546753745, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1823, Loss: 0.4006821033943866, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1824, Loss: 0.2408921904266436, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1825, Loss: 0.3124731017747059, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1826, Loss: 0.4002696720640944, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1827, Loss: 0.29915560947271824, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1828, Loss: 0.38502709381452327, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1829, Loss: 0.450889446759304, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1830, Loss: 0.3069356257379289, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1831, Loss: 0.30324731530157617, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1832, Loss: 0.32879980988767005, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1833, Loss: 0.2380560214791831, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1834, Loss: 0.29495444468042603, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1835, Loss: 0.28486055953634914, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1836, Loss: 0.27810174655469566, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1837, Loss: 0.3240015191684814, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1838, Loss: 0.284147202359851, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1839, Loss: 0.3009632995515079, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1840, Loss: 0.45224954987440724, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1841, Loss: 0.31268642516207185, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1842, Loss: 0.31070053458292746, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1843, Loss: 0.23346837289920686, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1844, Loss: 0.2824565462477804, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1845, Loss: 0.2531504300776543, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1846, Loss: 0.25833954316067037, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1847, Loss: 0.2775350427409861, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1848, Loss: 0.24561003411736024, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1849, Loss: 0.6295072017187961, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1850, Loss: 0.27146773868612983, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1851, Loss: 0.29731506305022454, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1852, Loss: 0.6093807361561621, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1853, Loss: 0.28231138965889, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1854, Loss: 0.25664384440078347, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1855, Loss: 0.2627839058443004, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1856, Loss: 0.38149126944686856, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1857, Loss: 0.27368477011681425, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1858, Loss: 0.4075549855466612, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1859, Loss: 0.4027622504072609, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1860, Loss: 0.46581709546908345, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1861, Loss: 0.417592287190194, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1862, Loss: 0.23589226663993298, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1863, Loss: 0.4180466725345148, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1864, Loss: 0.3173593581717783, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1865, Loss: 0.4975286579524208, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1866, Loss: 0.2518278526157816, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1867, Loss: 0.27595245022141024, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1868, Loss: 0.24566005661746043, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1869, Loss: 0.4813686253677576, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1870, Loss: 0.26582884012092667, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1871, Loss: 0.284601117355883, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1872, Loss: 0.3420900755911993, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1873, Loss: 0.2939325984665782, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1874, Loss: 0.255969714210015, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Batch 1875, Loss: 0.3673784456295149, Batch Size: 32, Learning Rate: 2.6206265730375498e-05\n",
      "Epoch 17, Updated Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 17, Average Loss: 0.325695952970683, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1, Loss: 0.7178765367190725, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 2, Loss: 0.2608818626361304, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 3, Loss: 0.2867654014508924, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 4, Loss: 0.3167578389902588, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 5, Loss: 0.34737024131234634, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 6, Loss: 0.25509975796407086, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 7, Loss: 0.2681379273073433, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 8, Loss: 0.41077171083902253, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 9, Loss: 0.2513102544959807, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 10, Loss: 0.4215179319039542, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 11, Loss: 0.24622138318365405, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 12, Loss: 0.6627405704311687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 13, Loss: 0.24375829492582543, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 14, Loss: 0.2850743014558111, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 15, Loss: 0.28082193120239973, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 16, Loss: 0.4337647519020965, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 17, Loss: 0.35399424854316924, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 18, Loss: 0.28612450958028957, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 19, Loss: 0.27287196473274566, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 20, Loss: 0.26103734231497033, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 21, Loss: 0.5589096415486199, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 22, Loss: 0.43923816818646466, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 23, Loss: 0.3784115864703914, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 24, Loss: 0.36731433504961686, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 25, Loss: 0.381785524688636, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 26, Loss: 0.26392632965916357, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 27, Loss: 0.2974588703517563, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 28, Loss: 0.29531084627566523, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 29, Loss: 0.5725951842709665, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 30, Loss: 0.27956562737516116, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 31, Loss: 0.27786567416819363, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 32, Loss: 0.29650125435801755, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 33, Loss: 0.27245779974040185, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 34, Loss: 0.28551309612771636, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 35, Loss: 0.2277076329521201, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 36, Loss: 0.2740500133263615, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 37, Loss: 0.3249968776150775, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 38, Loss: 0.3163364414521809, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 39, Loss: 0.30506028080315545, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 40, Loss: 0.27296713516181614, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 41, Loss: 0.2479390357025896, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 42, Loss: 0.26570363927786345, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 43, Loss: 0.5096297899066922, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 44, Loss: 0.2615782386643945, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 45, Loss: 0.2444226477352176, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 46, Loss: 0.2946226129806639, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 47, Loss: 0.28815986088347484, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 48, Loss: 0.28072880430013814, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 49, Loss: 0.5533469033324157, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 50, Loss: 0.30285170699069575, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 51, Loss: 0.40752574432719085, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 52, Loss: 0.3139237577836668, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 53, Loss: 0.29491197014252424, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 54, Loss: 0.4233890497303979, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 55, Loss: 0.28120709598018656, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 56, Loss: 0.390389836001651, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 57, Loss: 0.2775809554018718, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 58, Loss: 0.374599913533162, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 59, Loss: 0.2437798520666, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 60, Loss: 0.385205603006484, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 61, Loss: 0.2559975686670348, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 62, Loss: 0.30883536610577866, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 63, Loss: 0.3744832790539314, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 64, Loss: 0.2826057460555996, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 65, Loss: 0.25746912140123834, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 66, Loss: 0.3017458594399006, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 67, Loss: 0.41737157298698424, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 68, Loss: 0.30771036070829383, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 69, Loss: 0.37189895003804363, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 70, Loss: 0.47220420191364876, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 71, Loss: 0.6593011656920283, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 72, Loss: 0.24109230141761942, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 73, Loss: 0.2538213211849336, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 74, Loss: 0.43498285057268227, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 75, Loss: 0.31534769777700494, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 76, Loss: 0.23279151642589235, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 77, Loss: 0.2647550094852318, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 78, Loss: 0.42480033837705033, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 79, Loss: 0.26825196526605266, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 80, Loss: 0.43704390603213783, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 81, Loss: 0.3104309835223563, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 82, Loss: 0.5743249969599193, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 83, Loss: 0.3000909774873022, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 84, Loss: 0.3142903220333394, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 85, Loss: 0.3521165518707362, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 86, Loss: 0.26457219485175976, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 87, Loss: 0.3881521020644363, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 88, Loss: 0.27627324190269864, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 89, Loss: 0.3166551889826148, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 90, Loss: 0.26417150309750626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 91, Loss: 0.23084506581673908, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 92, Loss: 0.26621333961511123, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 93, Loss: 0.25209585737424123, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 94, Loss: 0.28016206131575067, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 95, Loss: 0.6637897998456801, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 96, Loss: 0.3738467776763565, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 97, Loss: 0.27835495919290687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 98, Loss: 0.2909573547303781, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 99, Loss: 0.2589783985261298, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 100, Loss: 0.37245629861866414, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 101, Loss: 0.33049869286909495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 102, Loss: 0.29968561921102, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 103, Loss: 0.24513490218188966, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 104, Loss: 0.27505719654545235, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 105, Loss: 0.2826272134498198, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 106, Loss: 0.25944063660450684, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 107, Loss: 0.24247281594339007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 108, Loss: 0.360279203176623, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 109, Loss: 0.27196856664882724, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 110, Loss: 0.2717133985615453, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 111, Loss: 0.29671423944184094, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 112, Loss: 0.3331174136239293, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 113, Loss: 0.3186802239594874, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 114, Loss: 0.3472359353702769, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 115, Loss: 0.25133700844176154, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 116, Loss: 0.3049334911460282, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 117, Loss: 0.7657895049975206, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 118, Loss: 0.24387416294089123, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 119, Loss: 0.3322824318819015, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 120, Loss: 0.40224080982382626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 121, Loss: 0.2850988775084159, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 122, Loss: 0.33072278713187964, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 123, Loss: 0.3467324669940525, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 124, Loss: 0.30973388481587916, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 125, Loss: 0.33206169383306455, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 126, Loss: 0.24599245932469352, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 127, Loss: 0.37822443224019286, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 128, Loss: 0.36250556361025965, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 129, Loss: 0.27810444578795995, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 130, Loss: 0.3107440972089529, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 131, Loss: 0.30353145589853275, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 132, Loss: 0.3953888236953494, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 133, Loss: 0.3790833526508698, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 134, Loss: 0.27097516364071705, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 135, Loss: 0.27487688768172674, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 136, Loss: 0.3626915249112007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 137, Loss: 0.2525067691798338, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 138, Loss: 0.2913094008624894, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 139, Loss: 0.25182420710746606, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 140, Loss: 0.4995337028015533, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 141, Loss: 0.33236239025608977, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 142, Loss: 0.24862626512837505, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 143, Loss: 0.3201331682031645, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 144, Loss: 0.2800318404890322, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 145, Loss: 0.2517214959008971, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 146, Loss: 0.3271547884827954, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 147, Loss: 0.5254916900702733, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 148, Loss: 0.25916628051084, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 149, Loss: 0.25846359785003403, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 150, Loss: 0.31531083641585506, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 151, Loss: 0.372994579857878, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 152, Loss: 0.2711519411266221, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 153, Loss: 0.2656271644854931, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 154, Loss: 0.33106574863589594, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 155, Loss: 0.27909762709791747, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 156, Loss: 0.3215727979506475, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 157, Loss: 0.2381150320869468, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 158, Loss: 0.27145225890805574, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 159, Loss: 0.25656978936077673, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 160, Loss: 0.28980446821390404, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 161, Loss: 0.2522107875457232, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 162, Loss: 0.2994802793399914, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 163, Loss: 0.24475469835871194, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 164, Loss: 0.32018465481863456, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 165, Loss: 0.24162156824040049, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 166, Loss: 0.37833555169414335, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 167, Loss: 0.2944444459092712, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 168, Loss: 0.32549946261204266, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 169, Loss: 0.31207967409977705, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 170, Loss: 0.2468532628286226, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 171, Loss: 0.41726115556881327, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 172, Loss: 0.3270026223310699, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 173, Loss: 0.2815533015703463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 174, Loss: 0.2924661426261112, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 175, Loss: 0.25722212066859135, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 176, Loss: 0.267545854387801, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 177, Loss: 0.3825644401700412, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 178, Loss: 0.24591228282032915, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 179, Loss: 0.3282849492410708, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 180, Loss: 0.40569710631469647, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 181, Loss: 0.4278826192557614, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 182, Loss: 0.3032075577536209, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 183, Loss: 0.3740217384013448, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 184, Loss: 0.33874870468353885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 185, Loss: 0.3125324029878713, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 186, Loss: 0.2702056298939054, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 187, Loss: 0.44841049323859, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 188, Loss: 0.36413358701744, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 189, Loss: 0.4676196877678209, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 190, Loss: 0.25524346358744043, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 191, Loss: 0.48838065258052665, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 192, Loss: 0.3804571972599374, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 193, Loss: 0.2536256969734363, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 194, Loss: 0.30914591000942093, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 195, Loss: 0.3013158628964622, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 196, Loss: 0.274720616163116, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 197, Loss: 0.35637753288649704, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 198, Loss: 0.2905456626685709, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 199, Loss: 0.3749055352792199, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 200, Loss: 0.30481514298640683, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 201, Loss: 0.3158588678944795, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 202, Loss: 0.26597918430412076, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 203, Loss: 0.34781649264515613, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 204, Loss: 0.5057044310877479, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 205, Loss: 0.25277096166032603, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 206, Loss: 0.3082077738942869, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 207, Loss: 0.629314188154481, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 208, Loss: 0.30802022246008304, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 209, Loss: 0.5226445404671449, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 210, Loss: 0.3022318623877463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 211, Loss: 0.3327522923343741, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 212, Loss: 0.29794124838961067, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 213, Loss: 0.33981269026775096, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 214, Loss: 0.3936013854275304, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 215, Loss: 0.318009074171945, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 216, Loss: 0.2736846257680072, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 217, Loss: 0.38138233292628443, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 218, Loss: 0.29956166374505766, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 219, Loss: 0.24877365142171026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 220, Loss: 0.2621563407067089, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 221, Loss: 0.24649035482436824, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 222, Loss: 0.26404666944074, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 223, Loss: 0.3721101950566028, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 224, Loss: 0.2638985350761617, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 225, Loss: 0.40614753285911964, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 226, Loss: 0.4808438632471658, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 227, Loss: 0.4014406672563964, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 228, Loss: 0.47590093497193287, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 229, Loss: 0.2813826296919115, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 230, Loss: 0.275771622075697, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 231, Loss: 0.3127605830146374, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 232, Loss: 0.5013619181347131, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 233, Loss: 0.3514344454509901, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 234, Loss: 0.3049154159715609, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 235, Loss: 0.4354649140759835, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 236, Loss: 0.2581238240678967, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 237, Loss: 0.2867792379489326, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 238, Loss: 0.2322924133432338, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 239, Loss: 0.27981988729803914, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 240, Loss: 0.2865794426012909, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 241, Loss: 0.2517650120299893, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 242, Loss: 0.2520958828516666, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 243, Loss: 0.2854524645171971, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 244, Loss: 0.2673460293968184, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 245, Loss: 0.38115222004573024, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 246, Loss: 0.4454891807369008, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 247, Loss: 0.299943118439826, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 248, Loss: 0.3238138685405311, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 249, Loss: 0.26094501780496915, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 250, Loss: 0.6307548768305618, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 251, Loss: 0.31742578137027483, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 252, Loss: 0.43797250713084374, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 253, Loss: 0.27588540912223486, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 254, Loss: 0.2549327370916861, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 255, Loss: 0.3248319309806007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 256, Loss: 0.3229869144651085, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 257, Loss: 0.3330397844070613, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 258, Loss: 0.32528790007678354, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 259, Loss: 0.28390044544919746, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 260, Loss: 0.33363547762211476, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 261, Loss: 0.28166487960278563, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 262, Loss: 0.3431425808167935, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 263, Loss: 0.3301552139429965, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 264, Loss: 0.3765390751741444, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 265, Loss: 0.2592568308288993, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 266, Loss: 0.33948562940986776, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 267, Loss: 0.32729542433073855, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 268, Loss: 0.31714492356068125, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 269, Loss: 0.3184706058230873, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 270, Loss: 0.28785790732539684, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 271, Loss: 0.48946581572705583, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 272, Loss: 0.283884397959843, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 273, Loss: 0.30569543740869365, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 274, Loss: 0.27965578128984997, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 275, Loss: 0.4070650281892255, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 276, Loss: 0.2927121461502928, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 277, Loss: 0.28310961600637924, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 278, Loss: 0.27860035283200857, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 279, Loss: 0.2735340976668459, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 280, Loss: 0.29546077255410813, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 281, Loss: 0.24066592966092457, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 282, Loss: 0.24046685693022027, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 283, Loss: 0.306777275396993, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 284, Loss: 0.26375590040164915, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 285, Loss: 0.259437152675357, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 286, Loss: 0.2811045639664893, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 287, Loss: 0.2536265258169274, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 288, Loss: 0.41161017271636646, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 289, Loss: 0.25216833133788297, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 290, Loss: 0.25535169962801835, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 291, Loss: 0.39540903158308605, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 292, Loss: 0.27240580530921327, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 293, Loss: 0.24409903891226772, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 294, Loss: 0.2398402017236085, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 295, Loss: 0.28919218541080305, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 296, Loss: 0.3104690245897258, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 297, Loss: 0.23399989229603657, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 298, Loss: 0.36883999606084517, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 299, Loss: 0.2639642586631402, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 300, Loss: 0.2878547965683951, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 301, Loss: 0.3148271184610716, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 302, Loss: 0.2709751236343384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 303, Loss: 0.5247832154375504, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 304, Loss: 0.30981540082707426, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 305, Loss: 0.24332827215646796, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 306, Loss: 0.26653441096789476, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 307, Loss: 0.2942455626661885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 308, Loss: 0.2517957143293599, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 309, Loss: 0.2634646115742448, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 310, Loss: 0.2753111451399099, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 311, Loss: 0.25515910377231854, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 312, Loss: 0.23871842173533334, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 313, Loss: 0.4919044804880427, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 314, Loss: 0.2984548648096752, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 315, Loss: 0.2978844206622056, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 316, Loss: 0.2358706924759802, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 317, Loss: 0.3775631924103938, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 318, Loss: 0.2740121542337848, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 319, Loss: 0.24749589449675113, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 320, Loss: 0.296577152230063, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 321, Loss: 0.39708438955886677, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 322, Loss: 0.37124332497498425, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 323, Loss: 0.44677496909185044, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 324, Loss: 0.4056273967377082, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 325, Loss: 0.2540430165651664, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 326, Loss: 0.2972917265282401, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 327, Loss: 0.2679476952123004, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 328, Loss: 0.24231543633322086, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 329, Loss: 0.27893667881690737, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 330, Loss: 0.2729682594894536, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 331, Loss: 0.38701735480725874, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 332, Loss: 0.36752467210733664, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 333, Loss: 0.4256735934262257, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 334, Loss: 0.24246718441011306, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 335, Loss: 0.31669375932501787, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 336, Loss: 0.31866189186345184, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 337, Loss: 0.23681565995892256, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 338, Loss: 0.3779507705659983, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 339, Loss: 0.29817934868936324, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 340, Loss: 0.2633932414653054, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 341, Loss: 0.4330072618369084, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 342, Loss: 0.38440248486388845, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 343, Loss: 0.29697595604819305, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 344, Loss: 0.25731592288478944, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 345, Loss: 0.3125413461881956, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 346, Loss: 0.278530289729631, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 347, Loss: 0.315660566329674, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 348, Loss: 0.3716006722248626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 349, Loss: 0.27316350193224714, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 350, Loss: 0.39864699187611125, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 351, Loss: 0.34580105108447584, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 352, Loss: 0.3485194260754781, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 353, Loss: 0.3438648418577201, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 354, Loss: 0.25571848628671306, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 355, Loss: 0.3053827820088128, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 356, Loss: 0.2927250808212902, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 357, Loss: 0.2569379310079637, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 358, Loss: 0.2784738641093688, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 359, Loss: 0.3011861286516706, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 360, Loss: 0.2989976212590475, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 361, Loss: 0.264783646091075, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 362, Loss: 0.35442293450112194, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 363, Loss: 0.307038991142501, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 364, Loss: 0.2527785910970335, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 365, Loss: 0.2959371992475583, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 366, Loss: 0.28704212154114794, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 367, Loss: 0.2500782744838255, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 368, Loss: 0.2798967754720477, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 369, Loss: 0.4372952741234153, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 370, Loss: 0.26215254880045474, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 371, Loss: 0.3180558554807552, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 372, Loss: 0.37191503364858597, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 373, Loss: 0.40051899594471885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 374, Loss: 0.29838308635664845, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 375, Loss: 0.24556499030655607, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 376, Loss: 0.233103969348913, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 377, Loss: 0.4772532528066026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 378, Loss: 0.27822108734912626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 379, Loss: 0.3027823024365581, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 380, Loss: 0.2802944413856891, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 381, Loss: 0.2938469551935279, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 382, Loss: 0.5591636873560153, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 383, Loss: 0.6287024151530276, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 384, Loss: 0.39270905239747256, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 385, Loss: 0.37187309588686973, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 386, Loss: 0.35918826129084463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 387, Loss: 0.34765564668077664, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 388, Loss: 0.4892216538547666, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 389, Loss: 0.39169537983311775, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 390, Loss: 0.272971610461025, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 391, Loss: 0.3035121800334951, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 392, Loss: 0.2848571795183209, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 393, Loss: 0.27656308432554105, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 394, Loss: 0.3020276852383996, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 395, Loss: 0.24137187794708687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 396, Loss: 0.314739484247326, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 397, Loss: 0.25666687353756495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 398, Loss: 0.25507652273920606, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 399, Loss: 0.2713759601854396, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 400, Loss: 0.25788439092656157, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 401, Loss: 0.2995543396103004, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 402, Loss: 0.25159327025699035, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 403, Loss: 0.3094110855110928, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 404, Loss: 0.2622674897942495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 405, Loss: 0.3534786799564464, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 406, Loss: 0.38174023119540723, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 407, Loss: 0.2667901334462026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 408, Loss: 0.2610156728574485, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 409, Loss: 0.2562136355815346, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 410, Loss: 0.40906325201236016, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 411, Loss: 0.257359705021508, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 412, Loss: 0.25034907952182334, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 413, Loss: 0.3107454318653463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 414, Loss: 0.6616737860104049, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 415, Loss: 0.430643760397759, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 416, Loss: 0.27953360142970624, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 417, Loss: 0.2428518285343796, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 418, Loss: 0.2602348501882853, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 419, Loss: 0.37316318913050683, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 420, Loss: 0.3076301027423507, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 421, Loss: 0.3076436868862945, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 422, Loss: 0.2730503244440925, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 423, Loss: 0.2887264325241174, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 424, Loss: 0.29199822629068317, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 425, Loss: 0.34040843242060403, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 426, Loss: 0.26980624000821984, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 427, Loss: 0.27097685491344087, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 428, Loss: 0.2756913646874366, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 429, Loss: 0.31920523921439425, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 430, Loss: 0.41343819296055007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 431, Loss: 0.28728696880294496, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 432, Loss: 0.4111065531239396, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 433, Loss: 0.23137524238569032, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 434, Loss: 0.33157781470846687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 435, Loss: 0.2750596789300548, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 436, Loss: 0.2940462783973688, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 437, Loss: 0.4034919157435657, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 438, Loss: 0.32995719071805746, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 439, Loss: 0.2870736769404188, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 440, Loss: 0.3307174770987534, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 441, Loss: 0.2878549985723268, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 442, Loss: 0.29046474858700033, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 443, Loss: 0.5152460252982655, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 444, Loss: 0.28436564718091295, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 445, Loss: 0.26525634666224285, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 446, Loss: 0.37520666738734876, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 447, Loss: 0.26903327745900674, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 448, Loss: 0.31828092159650867, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 449, Loss: 0.25268651099459954, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 450, Loss: 0.4280362494313805, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 451, Loss: 0.2692301480343606, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 452, Loss: 0.2990948265559082, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 453, Loss: 0.29181966111093294, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 454, Loss: 0.3361544827629487, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 455, Loss: 0.2328664091205491, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 456, Loss: 0.2897228952913149, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 457, Loss: 0.25177183039067963, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 458, Loss: 0.335070994959838, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 459, Loss: 0.32322641779478933, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 460, Loss: 0.2537431136030292, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 461, Loss: 0.24563808212395222, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 462, Loss: 0.24842191268729397, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 463, Loss: 0.24221494228286905, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 464, Loss: 0.26441255777503103, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 465, Loss: 0.4398648160614677, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 466, Loss: 0.26216445067127775, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 467, Loss: 0.2756266072845541, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 468, Loss: 0.3162529366269081, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 469, Loss: 0.2325654697455303, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 470, Loss: 0.3686427288168208, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 471, Loss: 0.2617514704313055, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 472, Loss: 0.44805367139655716, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 473, Loss: 0.42945255463655196, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 474, Loss: 0.44487645928670244, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 475, Loss: 0.24788703387441235, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 476, Loss: 0.27367181339228913, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 477, Loss: 0.4341600718720018, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 478, Loss: 0.41863954035919715, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 479, Loss: 0.3575188737276622, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 480, Loss: 0.3742454083430601, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 481, Loss: 0.27879831279039835, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 482, Loss: 0.2890347677044751, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 483, Loss: 0.27063705133053007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 484, Loss: 0.2316785552238027, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 485, Loss: 0.5758256471730294, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 486, Loss: 0.30701472576407696, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 487, Loss: 0.2929528872639613, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 488, Loss: 0.6010646137584135, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 489, Loss: 0.4278085589323892, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 490, Loss: 0.29054960409418823, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 491, Loss: 0.3562975402720593, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 492, Loss: 0.45523883232449225, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 493, Loss: 0.577371164362535, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 494, Loss: 0.3256768483939426, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 495, Loss: 0.25270546996978244, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 496, Loss: 0.2596480428655851, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 497, Loss: 0.24854429426865082, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 498, Loss: 0.3026704091514765, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 499, Loss: 0.2741385824781515, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 500, Loss: 0.28763609655065037, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 501, Loss: 0.2535061176305576, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 502, Loss: 0.3675098332937023, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 503, Loss: 0.561245102459263, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 504, Loss: 0.2583261944364012, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 505, Loss: 0.31130028403022253, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 506, Loss: 0.34260109280588913, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 507, Loss: 0.3202613031359224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 508, Loss: 0.31129035311576025, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 509, Loss: 0.301275903892661, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 510, Loss: 0.2561041289093587, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 511, Loss: 0.33663730287233384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 512, Loss: 0.3290134405429198, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 513, Loss: 0.268124686473611, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 514, Loss: 0.34238786941513843, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 515, Loss: 0.2481158904014462, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 516, Loss: 0.23693831284568742, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 517, Loss: 0.2765194102802667, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 518, Loss: 0.44163485688991033, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 519, Loss: 0.2815592030496762, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 520, Loss: 0.2791158112363743, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 521, Loss: 0.31960999405173324, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 522, Loss: 0.28647396786775636, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 523, Loss: 0.3706823883566628, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 524, Loss: 0.37130102557484956, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 525, Loss: 0.2860933020759612, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 526, Loss: 0.31596345429504824, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 527, Loss: 0.3648851286581708, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 528, Loss: 0.28001460862628413, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 529, Loss: 0.2847375037822138, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 530, Loss: 0.30589418020775483, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 531, Loss: 0.42673728673696254, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 532, Loss: 0.23747754740072383, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 533, Loss: 0.500813395846963, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 534, Loss: 0.31496901573907166, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 535, Loss: 0.2560002064793555, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 536, Loss: 0.43114914322000775, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 537, Loss: 0.3242685798630348, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 538, Loss: 0.3448844410305328, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 539, Loss: 0.3253293010310684, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 540, Loss: 0.3212964116910235, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 541, Loss: 0.261978955151307, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 542, Loss: 0.24659089687737729, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 543, Loss: 0.33784226965812275, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 544, Loss: 0.2412710328911529, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 545, Loss: 0.25354718018246586, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 546, Loss: 0.4401663038470038, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 547, Loss: 0.2435391165471324, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 548, Loss: 0.31157680981511277, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 549, Loss: 0.26114266098824723, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 550, Loss: 0.2633040200581232, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 551, Loss: 0.476684148414821, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 552, Loss: 0.514567664296645, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 553, Loss: 0.26004186147858555, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 554, Loss: 0.40651569589523195, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 555, Loss: 0.3164050259260329, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 556, Loss: 0.332353381241815, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 557, Loss: 0.267543020563878, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 558, Loss: 0.30148491147219586, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 559, Loss: 0.24104954461652792, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 560, Loss: 0.25961907629856473, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 561, Loss: 0.3610607966651297, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 562, Loss: 0.2625572117944051, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 563, Loss: 0.2698767923956645, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 564, Loss: 0.24710338567172818, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 565, Loss: 0.39332318404068933, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 566, Loss: 0.3235606994301269, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 567, Loss: 0.2855894637977758, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 568, Loss: 0.48024868981984126, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 569, Loss: 0.3135517385245901, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 570, Loss: 0.29708650637606315, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 571, Loss: 0.2639703160885972, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 572, Loss: 0.25262510972650876, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 573, Loss: 0.26970315316721993, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 574, Loss: 0.5320794017016919, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 575, Loss: 0.39574734807538803, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 576, Loss: 0.23079841543332774, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 577, Loss: 0.2596883628438255, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 578, Loss: 0.30736755369877783, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 579, Loss: 0.34021124562448946, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 580, Loss: 0.34855286932842944, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 581, Loss: 0.3083618786005407, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 582, Loss: 0.4700509419870219, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 583, Loss: 0.30120090643498865, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 584, Loss: 0.42146436613310156, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 585, Loss: 0.29751362796695374, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 586, Loss: 0.3533083796395221, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 587, Loss: 0.2853922504650072, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 588, Loss: 0.45621739576272824, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 589, Loss: 0.33229385366813613, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 590, Loss: 0.2463533440713716, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 591, Loss: 0.34225468362301753, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 592, Loss: 0.24651644290488214, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 593, Loss: 0.29884299751186816, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 594, Loss: 0.46968171901926814, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 595, Loss: 0.27682895458027823, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 596, Loss: 0.23933333070244228, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 597, Loss: 0.3323727418670781, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 598, Loss: 0.33072264904528464, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 599, Loss: 0.26638524466118624, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 600, Loss: 0.32923523447188113, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 601, Loss: 0.3789844153666734, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 602, Loss: 0.24967474116718572, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 603, Loss: 0.24240677647171355, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 604, Loss: 0.27732312435099626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 605, Loss: 0.5209455559761819, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 606, Loss: 0.25959731893237026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 607, Loss: 0.25883155376086203, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 608, Loss: 0.26076187562780573, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 609, Loss: 0.3630219876534344, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 610, Loss: 0.32115557724521016, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 611, Loss: 0.39196555391639276, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 612, Loss: 0.2770636051449669, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 613, Loss: 0.26357584352407076, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 614, Loss: 0.2420555535323886, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 615, Loss: 0.31208226895623037, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 616, Loss: 0.34236813001366656, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 617, Loss: 0.37195140737002014, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 618, Loss: 0.23591733762626366, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 619, Loss: 0.2389555083409134, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 620, Loss: 0.25359446539407254, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 621, Loss: 0.2847906171155158, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 622, Loss: 0.27959067130138826, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 623, Loss: 0.35513515088935843, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 624, Loss: 0.27239753302538056, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 625, Loss: 0.2564778559133027, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 626, Loss: 0.24427491570264687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 627, Loss: 0.3580922945887911, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 628, Loss: 0.3522178121860924, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 629, Loss: 0.23962008783704297, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 630, Loss: 0.26385910718025546, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 631, Loss: 0.279573530203138, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 632, Loss: 0.309159010204234, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 633, Loss: 0.30433598805614803, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 634, Loss: 0.27064481815624125, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 635, Loss: 0.4501772189427966, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 636, Loss: 0.3211104783361314, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 637, Loss: 0.2362037383462376, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 638, Loss: 0.2996367868276276, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 639, Loss: 0.5768505340423756, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 640, Loss: 0.2464883374573337, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 641, Loss: 0.3346287923326623, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 642, Loss: 0.3704867621041306, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 643, Loss: 0.2684193070223846, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 644, Loss: 0.35786528104751586, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 645, Loss: 0.2576123362267084, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 646, Loss: 0.44256921213780265, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 647, Loss: 0.24108897377572186, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 648, Loss: 0.2642340934789419, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 649, Loss: 0.27954904777963063, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 650, Loss: 0.2594734378773691, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 651, Loss: 0.3282879898651915, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 652, Loss: 0.2327406752375839, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 653, Loss: 0.24285518636137082, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 654, Loss: 0.27735581807206366, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 655, Loss: 0.26842888827159966, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 656, Loss: 0.39116375122885105, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 657, Loss: 0.2577081069793038, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 658, Loss: 0.39323227158047314, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 659, Loss: 0.2852719571956895, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 660, Loss: 0.2373368819682473, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 661, Loss: 0.29030620838620347, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 662, Loss: 0.2903437764156632, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 663, Loss: 0.5777759792061248, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 664, Loss: 0.2723297036706308, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 665, Loss: 0.24754686899444137, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 666, Loss: 0.37203763644329935, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 667, Loss: 0.424319118171763, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 668, Loss: 0.37616788444570837, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 669, Loss: 0.30645888321517534, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 670, Loss: 0.24328814609910757, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 671, Loss: 0.348269138714584, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 672, Loss: 0.3105692175330361, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 673, Loss: 0.24058223677722665, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 674, Loss: 0.3389387546400481, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 675, Loss: 0.295501954505483, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 676, Loss: 0.26729568919896757, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 677, Loss: 0.24079881866110472, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 678, Loss: 0.34785908419046885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 679, Loss: 0.28750930745439063, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 680, Loss: 0.23837070298424723, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 681, Loss: 0.31308018181919095, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 682, Loss: 0.5243007909602837, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 683, Loss: 0.378389669998044, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 684, Loss: 0.34400383098942294, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 685, Loss: 0.4228405459374933, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 686, Loss: 0.48610676161030153, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 687, Loss: 0.23940145654705047, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 688, Loss: 0.506782833146141, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 689, Loss: 0.2870958854811549, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 690, Loss: 0.36611186262616136, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 691, Loss: 0.2614724674261423, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 692, Loss: 0.420578994495588, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 693, Loss: 0.3124372834529924, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 694, Loss: 0.3758219043295456, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 695, Loss: 0.5539835377522708, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 696, Loss: 0.3223103350329495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 697, Loss: 0.5271456171738426, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 698, Loss: 0.3648682407594639, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 699, Loss: 0.2615353156661332, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 700, Loss: 0.2699445595598723, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 701, Loss: 0.35285167378829335, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 702, Loss: 0.2572714275339761, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 703, Loss: 0.394042293490298, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 704, Loss: 0.4551919119620673, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 705, Loss: 0.29045913523444056, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 706, Loss: 0.5760152340981105, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 707, Loss: 0.2946459699969029, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 708, Loss: 0.3392851229830182, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 709, Loss: 0.3456251431129944, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 710, Loss: 0.2483854818398073, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 711, Loss: 0.521176197334908, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 712, Loss: 0.49902519327521294, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 713, Loss: 0.2578071468970502, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 714, Loss: 0.3846382765873459, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 715, Loss: 0.2777879540046543, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 716, Loss: 0.3697222067232986, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 717, Loss: 0.29309116515681094, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 718, Loss: 0.520249709185901, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 719, Loss: 0.3570412516790327, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 720, Loss: 0.4043035913488008, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 721, Loss: 0.349542161791656, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 722, Loss: 0.30017274647676556, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 723, Loss: 0.4025942071636466, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 724, Loss: 0.2360268503497119, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 725, Loss: 0.35937116579969464, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 726, Loss: 0.2607335496661788, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 727, Loss: 0.33781134860685647, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 728, Loss: 0.24331195625903695, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 729, Loss: 0.3097922215562503, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 730, Loss: 0.33091238326357014, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 731, Loss: 0.2417042677227351, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 732, Loss: 0.3040616091135977, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 733, Loss: 0.3243563984025521, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 734, Loss: 0.44180510009347407, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 735, Loss: 0.30357538356181013, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 736, Loss: 0.25619484020555616, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 737, Loss: 0.25621515210678286, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 738, Loss: 0.42391827090618484, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 739, Loss: 0.3417791813843541, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 740, Loss: 0.2494122413245735, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 741, Loss: 0.2906639996812224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 742, Loss: 0.29169258871034126, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 743, Loss: 0.31908258831228486, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 744, Loss: 0.2311517169263959, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 745, Loss: 0.31883328348063883, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 746, Loss: 0.5240152480676349, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 747, Loss: 0.3422602326108421, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 748, Loss: 0.2882304305824022, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 749, Loss: 0.3081967439673691, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 750, Loss: 0.32713791992108965, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 751, Loss: 0.24946271920950333, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 752, Loss: 0.3392892457190082, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 753, Loss: 0.3153543152152519, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 754, Loss: 0.4441800199362175, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 755, Loss: 0.36307342156873246, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 756, Loss: 0.35995273540878514, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 757, Loss: 0.30804867958827803, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 758, Loss: 0.37910027266734136, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 759, Loss: 0.3400103261511781, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 760, Loss: 0.31814239700491515, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 761, Loss: 0.2505280246786449, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 762, Loss: 0.3028837534404227, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 763, Loss: 0.2572227867816408, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 764, Loss: 0.3062069742885135, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 765, Loss: 0.3829687989704481, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 766, Loss: 0.24548248552015992, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 767, Loss: 0.48947998407589294, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 768, Loss: 0.3441534143075559, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 769, Loss: 0.5274405244129151, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 770, Loss: 0.3622332541235078, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 771, Loss: 0.3250163930208767, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 772, Loss: 0.28401890555721837, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 773, Loss: 0.23899115451739025, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 774, Loss: 0.28494893986030057, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 775, Loss: 0.29925170827534037, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 776, Loss: 0.3109965667599054, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 777, Loss: 0.24307684016765702, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 778, Loss: 0.23394993059766958, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 779, Loss: 0.33578399853996965, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 780, Loss: 0.2845697561926918, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 781, Loss: 0.5125674788374202, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 782, Loss: 0.35056893188445537, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 783, Loss: 0.2770533741860083, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 784, Loss: 0.2587319446091253, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 785, Loss: 0.3447376075149372, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 786, Loss: 0.3156471002450514, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 787, Loss: 0.2946293964486875, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 788, Loss: 0.23960609541380398, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 789, Loss: 0.36726802916951395, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 790, Loss: 0.3804628896878275, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 791, Loss: 0.2475425679046711, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 792, Loss: 0.2924085652490665, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 793, Loss: 0.5602660242091455, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 794, Loss: 0.30349969732630866, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 795, Loss: 0.27048649949607717, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 796, Loss: 0.3222464382149638, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 797, Loss: 0.382478300071974, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 798, Loss: 0.2392795623015522, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 799, Loss: 0.2769812168303914, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 800, Loss: 0.2637884240926929, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 801, Loss: 0.331519587809469, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 802, Loss: 0.34052028074831237, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 803, Loss: 0.2521880300542146, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 804, Loss: 0.3116746294944928, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 805, Loss: 0.34349116169231636, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 806, Loss: 0.29933124082085766, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 807, Loss: 0.498396078180324, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 808, Loss: 0.27438358351561254, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 809, Loss: 0.27094691786447483, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 810, Loss: 0.23534476059280302, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 811, Loss: 0.40424340579569995, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 812, Loss: 0.3090771607055194, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 813, Loss: 0.2599184333196201, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 814, Loss: 0.2580654839849225, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 815, Loss: 0.37594723035763333, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 816, Loss: 0.3072100662315565, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 817, Loss: 0.27341584002047403, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 818, Loss: 0.36323422684582785, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 819, Loss: 0.2986108964931052, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 820, Loss: 0.2992810397135567, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 821, Loss: 0.3109483767532775, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 822, Loss: 0.2665570461414115, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 823, Loss: 0.3567250699500013, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 824, Loss: 0.33104290495928895, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 825, Loss: 0.25292699691863624, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 826, Loss: 0.40013043417689265, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 827, Loss: 0.31861699319099496, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 828, Loss: 0.3240749859835591, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 829, Loss: 0.37714111544334017, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 830, Loss: 0.2598074546263226, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 831, Loss: 0.33332542004830706, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 832, Loss: 0.3223745336153612, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 833, Loss: 0.3023502909545611, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 834, Loss: 0.2491367824640295, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 835, Loss: 0.33863946626445696, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 836, Loss: 0.26785434318682677, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 837, Loss: 0.38026165530738726, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 838, Loss: 0.3343905606305311, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 839, Loss: 0.2731092481231337, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 840, Loss: 0.2698507921929765, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 841, Loss: 0.31902142549490065, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 842, Loss: 0.3089053492937853, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 843, Loss: 0.42156848214484954, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 844, Loss: 0.2867352840154669, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 845, Loss: 0.35468583315399405, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 846, Loss: 0.32701733619712725, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 847, Loss: 0.24092344405806368, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 848, Loss: 0.3179826717932, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 849, Loss: 0.2513906998209781, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 850, Loss: 0.4599135162489842, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 851, Loss: 0.24599343190023235, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 852, Loss: 0.3254434038117575, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 853, Loss: 0.543522481675977, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 854, Loss: 0.3457929882663625, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 855, Loss: 0.3859328382846589, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 856, Loss: 0.3086120726999291, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 857, Loss: 0.5896789698939554, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 858, Loss: 0.33624646945691355, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 859, Loss: 0.30036752357759205, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 860, Loss: 0.2529832689027112, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 861, Loss: 0.3423069192790502, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 862, Loss: 0.24279670750229704, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 863, Loss: 0.25373587468421, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 864, Loss: 0.2885522874377007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 865, Loss: 0.29759369617405806, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 866, Loss: 0.32122546154511794, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 867, Loss: 0.26495215098713587, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 868, Loss: 0.24458343416097816, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 869, Loss: 0.30253132524450516, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 870, Loss: 0.3089014956669689, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 871, Loss: 0.2657152831455884, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 872, Loss: 0.3161994965138788, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 873, Loss: 0.39370685574248265, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 874, Loss: 0.2631301818365411, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 875, Loss: 0.25787107453489444, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 876, Loss: 0.32433713813763393, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 877, Loss: 0.23982269426169234, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 878, Loss: 0.2869314537858385, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 879, Loss: 0.28860283241864726, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 880, Loss: 0.34803030428853515, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 881, Loss: 0.26095560187072786, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 882, Loss: 0.34370843581501936, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 883, Loss: 0.23456207513076882, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 884, Loss: 0.2894887808856999, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 885, Loss: 0.27238210780326955, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 886, Loss: 0.2840688513041918, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 887, Loss: 0.45005930676908745, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 888, Loss: 0.3130337154004682, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 889, Loss: 0.2842298414281218, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 890, Loss: 0.2600278987188981, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 891, Loss: 0.2543643971658576, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 892, Loss: 0.27155011740756213, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 893, Loss: 0.2437062374345315, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 894, Loss: 0.2531060062562316, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 895, Loss: 0.28420062651141215, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 896, Loss: 0.32548291589389433, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 897, Loss: 0.23774004228246923, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 898, Loss: 0.2411457068401076, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 899, Loss: 0.24558408729208883, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 900, Loss: 0.354249884289621, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 901, Loss: 0.27234332449834625, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 902, Loss: 0.44214205694652786, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 903, Loss: 0.5064498701224356, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 904, Loss: 0.2603885016317754, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 905, Loss: 0.4771139016184315, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 906, Loss: 0.3220945493685149, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 907, Loss: 0.45065870210130765, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 908, Loss: 0.3233802787450323, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 909, Loss: 0.3432359007291329, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 910, Loss: 0.38532811728567007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 911, Loss: 0.31320822717040975, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 912, Loss: 0.36564468776424885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 913, Loss: 0.24503418454376058, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 914, Loss: 0.2299771279861572, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 915, Loss: 0.3418423094637935, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 916, Loss: 0.264611245517079, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 917, Loss: 0.3891845519542819, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 918, Loss: 0.2478082490932812, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 919, Loss: 0.32365056010559295, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 920, Loss: 0.30300003712912427, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 921, Loss: 0.29379906906093733, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 922, Loss: 0.24729892870487857, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 923, Loss: 0.2662262022141283, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 924, Loss: 0.242246025406755, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 925, Loss: 0.6824289044080238, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 926, Loss: 0.38574369098576744, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 927, Loss: 0.4071956436422304, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 928, Loss: 0.3657028420001235, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 929, Loss: 0.26639599158889543, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 930, Loss: 0.3056125010719618, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 931, Loss: 0.3404941046714408, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 932, Loss: 0.4060594517817826, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 933, Loss: 0.2328249008082855, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 934, Loss: 0.3671199227651396, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 935, Loss: 0.46060423081976687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 936, Loss: 0.2897877702360665, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 937, Loss: 0.3106302258103357, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 938, Loss: 0.48526153738417577, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 939, Loss: 0.27190093652781977, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 940, Loss: 0.30490390201212003, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 941, Loss: 0.3326376460498315, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 942, Loss: 0.3273555151142137, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 943, Loss: 0.42504400455812275, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 944, Loss: 0.3182196407840926, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 945, Loss: 0.37965886161526374, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 946, Loss: 0.2510689771053895, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 947, Loss: 0.3523205799881831, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 948, Loss: 0.24820343817589396, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 949, Loss: 0.2979721869833414, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 950, Loss: 0.3212428343146289, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 951, Loss: 0.285909235177459, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 952, Loss: 0.38381450033917774, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 953, Loss: 0.38933304234680904, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 954, Loss: 0.27946660566198284, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 955, Loss: 0.2847568940053685, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 956, Loss: 0.2420733783543383, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 957, Loss: 0.374785958635409, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 958, Loss: 0.4023077974835605, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 959, Loss: 0.32802997405048395, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 960, Loss: 0.2527988817533828, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 961, Loss: 0.29383106815473625, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 962, Loss: 0.26748145970904574, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 963, Loss: 0.32334168334094027, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 964, Loss: 0.31065807826258685, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 965, Loss: 0.27184275582786155, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 966, Loss: 0.2888592456390141, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 967, Loss: 0.32205227386274704, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 968, Loss: 0.2395688389351432, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 969, Loss: 0.25642458926794104, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 970, Loss: 0.2804841442993301, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 971, Loss: 0.29398406358270013, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 972, Loss: 0.3000045996295613, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 973, Loss: 0.25014100870382405, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 974, Loss: 0.5382153755346316, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 975, Loss: 0.2981210102138532, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 976, Loss: 0.3425818106355172, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 977, Loss: 0.24005622377806982, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 978, Loss: 0.33826622462162925, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 979, Loss: 0.2383221769493495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 980, Loss: 0.2425220300898286, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 981, Loss: 0.2998625702701033, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 982, Loss: 0.28973047977185284, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 983, Loss: 0.2807071125313021, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 984, Loss: 0.32352381926488705, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 985, Loss: 0.5099585345082385, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 986, Loss: 0.33248761775171687, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 987, Loss: 0.26335811451033553, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 988, Loss: 0.3101105263658234, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 989, Loss: 0.2590846450523062, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 990, Loss: 0.44221980567455055, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 991, Loss: 0.4340800367922341, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 992, Loss: 0.24811765491641297, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 993, Loss: 0.3797149684839004, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 994, Loss: 0.25651587925698754, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 995, Loss: 0.2904651831065659, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 996, Loss: 0.4875046228064437, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 997, Loss: 0.23444396934966139, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 998, Loss: 0.23174390325261596, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 999, Loss: 0.39433641904558114, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1000, Loss: 0.2732316959203009, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1001, Loss: 0.34559503623330406, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1002, Loss: 0.33329454962349137, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1003, Loss: 0.2347424140209308, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1004, Loss: 0.2496671705128589, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1005, Loss: 0.4030243395517036, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1006, Loss: 0.30480288214708573, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1007, Loss: 0.2734216525640658, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1008, Loss: 0.2402093024813358, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1009, Loss: 0.28924037189557233, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1010, Loss: 0.335417678516778, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1011, Loss: 0.31148564455803446, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1012, Loss: 0.437962424696432, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1013, Loss: 0.28195526525727127, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1014, Loss: 0.44895146707201056, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1015, Loss: 0.2643060450864265, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1016, Loss: 0.28831481380327323, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1017, Loss: 0.40727084521347323, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1018, Loss: 0.43172605578126505, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1019, Loss: 0.33363895244993036, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1020, Loss: 0.29865982598841684, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1021, Loss: 0.3830380132199866, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1022, Loss: 0.35337598548449667, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1023, Loss: 0.27788747443286865, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1024, Loss: 0.30993301354579794, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1025, Loss: 0.2513787803250946, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1026, Loss: 0.31666060819790365, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1027, Loss: 0.301388607022311, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1028, Loss: 0.32431983702853884, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1029, Loss: 0.2998088346212928, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1030, Loss: 0.30248529800480556, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1031, Loss: 0.30159335660475806, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1032, Loss: 0.26506356689031246, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1033, Loss: 0.35550480604665885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1034, Loss: 0.25015123230477493, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1035, Loss: 0.416133989946578, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1036, Loss: 0.3036884681811022, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1037, Loss: 0.2867006209458277, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1038, Loss: 0.28469528942763117, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1039, Loss: 0.2639205461500729, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1040, Loss: 0.3021125351501709, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1041, Loss: 0.2787248795521164, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1042, Loss: 0.24144822294742338, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1043, Loss: 0.30175451001705506, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1044, Loss: 0.3554078947050131, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1045, Loss: 0.4229116162018679, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1046, Loss: 0.2940747097498413, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1047, Loss: 0.3670888297877863, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1048, Loss: 0.36557129364070196, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1049, Loss: 0.30905001902830886, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1050, Loss: 0.43359901267673573, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1051, Loss: 0.27341707543086124, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1052, Loss: 0.31086065797393203, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1053, Loss: 0.34617823621323096, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1054, Loss: 0.3922571787973447, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1055, Loss: 0.25170057825560355, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1056, Loss: 0.32362085630119103, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1057, Loss: 0.2652315559709375, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1058, Loss: 0.40636485486183505, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1059, Loss: 0.33414808400647, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1060, Loss: 0.3400759304178287, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1061, Loss: 0.25354850613938845, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1062, Loss: 0.44446809835352363, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1063, Loss: 0.26611394103089075, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1064, Loss: 0.3076023893403833, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1065, Loss: 0.36505736554438717, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1066, Loss: 0.3925197282836921, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1067, Loss: 0.23850572944168258, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1068, Loss: 0.28774446936825854, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1069, Loss: 0.29463889620324224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1070, Loss: 0.2562222536787985, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1071, Loss: 0.3158706786276072, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1072, Loss: 0.23561033468910786, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1073, Loss: 0.395227286702159, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1074, Loss: 0.2640931073361725, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1075, Loss: 0.4091566651437991, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1076, Loss: 0.3196150239767037, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1077, Loss: 0.26986298385188767, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1078, Loss: 0.2748510865233462, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1079, Loss: 0.4101172118552605, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1080, Loss: 0.25805426240611223, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1081, Loss: 0.27121336061731094, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1082, Loss: 0.23552327470927378, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1083, Loss: 0.24828738744735798, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1084, Loss: 0.2739804297926772, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1085, Loss: 0.41961324338060463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1086, Loss: 0.36060904502405333, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1087, Loss: 0.5850227115950641, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1088, Loss: 0.5312227834161909, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1089, Loss: 0.23953437357896135, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1090, Loss: 0.5486403603350322, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1091, Loss: 0.24168917976537097, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1092, Loss: 0.39535524986205206, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1093, Loss: 0.2840833747462781, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1094, Loss: 0.45160123277542763, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1095, Loss: 0.2465593194942099, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1096, Loss: 0.34342112744892983, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1097, Loss: 0.3448303371743139, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1098, Loss: 0.3007285849584302, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1099, Loss: 0.28399848671208505, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1100, Loss: 0.25139602670388095, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1101, Loss: 0.5597480418419702, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1102, Loss: 0.3397888225994945, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1103, Loss: 0.49112643139948464, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1104, Loss: 0.3322127276581745, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1105, Loss: 0.23906017476800384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1106, Loss: 0.39590963078145064, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1107, Loss: 0.30580732833231494, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1108, Loss: 0.26217795033525376, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1109, Loss: 0.3587735923640062, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1110, Loss: 0.31276153649826693, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1111, Loss: 0.7286560976748239, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1112, Loss: 0.2594721679005904, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1113, Loss: 0.2725832124557229, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1114, Loss: 0.292741284379715, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1115, Loss: 0.25415789337141337, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1116, Loss: 0.6071871804368352, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1117, Loss: 0.3629178285924355, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1118, Loss: 0.35150216766980025, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1119, Loss: 0.45578128177804134, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1120, Loss: 0.25710449491968307, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1121, Loss: 0.29304271675950666, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1122, Loss: 0.25465608137512685, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1123, Loss: 0.3067903522718889, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1124, Loss: 0.27163004098555876, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1125, Loss: 0.2637960374700285, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1126, Loss: 0.24518620490863485, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1127, Loss: 0.4859218770465534, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1128, Loss: 0.3456110123474254, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1129, Loss: 0.3059589498976023, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1130, Loss: 0.2515282922286358, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1131, Loss: 0.48492829771697776, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1132, Loss: 0.2593297488115976, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1133, Loss: 0.30017950465527565, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1134, Loss: 0.31437784553516485, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1135, Loss: 0.37487990040281094, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1136, Loss: 0.2708513824907535, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1137, Loss: 0.6209125677351292, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1138, Loss: 0.39211911953907863, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1139, Loss: 0.25764089523821343, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1140, Loss: 0.29522673575441805, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1141, Loss: 0.3142307652699035, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1142, Loss: 0.33942714105383315, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1143, Loss: 0.3059101076623758, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1144, Loss: 0.35967451690274177, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1145, Loss: 0.32110999610080965, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1146, Loss: 0.35504213235423987, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1147, Loss: 0.41597091113007156, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1148, Loss: 0.2409489574550413, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1149, Loss: 0.3565207789241567, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1150, Loss: 0.37152566265611586, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1151, Loss: 0.28707663897742725, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1152, Loss: 0.4877843915490272, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1153, Loss: 0.26887269784652384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1154, Loss: 0.2779152188095354, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1155, Loss: 0.3609403795957579, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1156, Loss: 0.3285289886194844, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1157, Loss: 0.27082561465486865, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1158, Loss: 0.3590222149369722, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1159, Loss: 0.35899938615160776, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1160, Loss: 0.6181143400986137, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1161, Loss: 0.3338850214990372, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1162, Loss: 0.43475712204935313, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1163, Loss: 0.35658672428002447, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1164, Loss: 0.38868363803314543, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1165, Loss: 0.3159264971112462, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1166, Loss: 0.24276940233859173, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1167, Loss: 0.2367918909128448, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1168, Loss: 0.35147341037225033, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1169, Loss: 0.26931741785899094, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1170, Loss: 0.2442540887068983, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1171, Loss: 0.6186825500849512, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1172, Loss: 0.3108257587896506, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1173, Loss: 0.36872532180634626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1174, Loss: 0.26931155221373637, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1175, Loss: 0.29999292718909126, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1176, Loss: 0.3619424552822932, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1177, Loss: 0.2854572128156277, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1178, Loss: 0.2404916535248024, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1179, Loss: 0.3048614292757736, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1180, Loss: 0.3003052908201183, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1181, Loss: 0.23179163902704034, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1182, Loss: 0.5195417570888007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1183, Loss: 0.2845864740641323, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1184, Loss: 0.4794258667391473, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1185, Loss: 0.2877250235551266, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1186, Loss: 0.2319782319471412, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1187, Loss: 0.29394795868869866, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1188, Loss: 0.40206379788808067, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1189, Loss: 0.27101232122861957, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1190, Loss: 0.3565608817227103, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1191, Loss: 0.39804374974716267, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1192, Loss: 0.28219368925549093, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1193, Loss: 0.5606004684194983, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1194, Loss: 0.28978339110012885, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1195, Loss: 0.2687304631615057, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1196, Loss: 0.3176024268923813, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1197, Loss: 0.2386085040215073, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1198, Loss: 0.3291649325973144, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1199, Loss: 0.3148132281094346, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1200, Loss: 0.23979858984203967, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1201, Loss: 0.24124375069650156, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1202, Loss: 0.29110926509493124, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1203, Loss: 0.2749574112356292, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1204, Loss: 0.30366735944800527, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1205, Loss: 0.2530681149629557, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1206, Loss: 0.2627079565904471, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1207, Loss: 0.3245709594932489, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1208, Loss: 0.29009832182661255, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1209, Loss: 0.2410264952536483, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1210, Loss: 0.26844000864091605, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1211, Loss: 0.4693374156787921, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1212, Loss: 0.48843661587371656, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1213, Loss: 0.32996256041868743, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1214, Loss: 0.2861231920546484, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1215, Loss: 0.302671558552955, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1216, Loss: 0.24472031715219195, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1217, Loss: 0.27718445250616, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1218, Loss: 0.3922468705846633, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1219, Loss: 0.25187017549268526, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1220, Loss: 0.2838531568220566, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1221, Loss: 0.35979224846738556, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1222, Loss: 0.3996252685456426, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1223, Loss: 0.2639144722141924, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1224, Loss: 0.3019099732509434, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1225, Loss: 0.25087336501179736, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1226, Loss: 0.24451799817787587, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1227, Loss: 0.26615072597339295, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1228, Loss: 0.2543914632994303, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1229, Loss: 0.4320126704120792, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1230, Loss: 0.25855328823380763, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1231, Loss: 0.43278789467819795, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1232, Loss: 0.26325788218531837, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1233, Loss: 0.29355189998903713, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1234, Loss: 0.35104606435298785, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1235, Loss: 0.25801036171657565, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1236, Loss: 0.3958818035287752, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1237, Loss: 0.2633529054582951, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1238, Loss: 0.3297083014870087, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1239, Loss: 0.3126796346337357, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1240, Loss: 0.31419514956262384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1241, Loss: 0.2681417476947565, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1242, Loss: 0.35699283549988436, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1243, Loss: 0.3451978655873433, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1244, Loss: 0.2319383177980474, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1245, Loss: 0.24792990400180812, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1246, Loss: 0.2670517059152879, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1247, Loss: 0.312557040584558, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1248, Loss: 0.3399382809090852, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1249, Loss: 0.23592147565195032, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1250, Loss: 0.29252305382852734, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1251, Loss: 0.2430354739702879, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1252, Loss: 0.3347632042371788, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1253, Loss: 0.28442697563483704, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1254, Loss: 0.25049929524127207, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1255, Loss: 0.2389125015754321, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1256, Loss: 0.42670418343398486, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1257, Loss: 0.30617184601175296, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1258, Loss: 0.3369891127401696, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1259, Loss: 0.2889960045739684, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1260, Loss: 0.2722821694065637, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1261, Loss: 0.3024116792655587, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1262, Loss: 0.5445615645851107, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1263, Loss: 0.2862369189966859, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1264, Loss: 0.2785139938031513, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1265, Loss: 0.25015291338952905, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1266, Loss: 0.2730897639691714, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1267, Loss: 0.26661740666181655, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1268, Loss: 0.35790409449063565, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1269, Loss: 0.29791673030731247, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1270, Loss: 0.24776857329391702, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1271, Loss: 0.38725569398472137, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1272, Loss: 0.31819245424884435, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1273, Loss: 0.3574944214346113, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1274, Loss: 0.4063954315176913, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1275, Loss: 0.31381674688333966, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1276, Loss: 0.46937280951450777, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1277, Loss: 0.27714963948061616, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1278, Loss: 0.3095886381244056, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1279, Loss: 0.35282605153274665, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1280, Loss: 0.37418217575450863, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1281, Loss: 0.28400504191394427, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1282, Loss: 0.29855908567233747, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1283, Loss: 0.23276663108631013, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1284, Loss: 0.2865585834331437, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1285, Loss: 0.25802219607783317, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1286, Loss: 0.316269224515742, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1287, Loss: 0.31108184660698696, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1288, Loss: 0.5643501755623734, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1289, Loss: 0.2527297566927553, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1290, Loss: 0.37884112850142904, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1291, Loss: 0.42241385862352, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1292, Loss: 0.2959795171591435, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1293, Loss: 0.4466322960545862, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1294, Loss: 0.255240821636513, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1295, Loss: 0.25626857773009903, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1296, Loss: 0.3215848288351722, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1297, Loss: 0.32049022847815223, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1298, Loss: 0.6682345551036775, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1299, Loss: 0.3479619583071834, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1300, Loss: 0.2879760738074238, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1301, Loss: 0.2881789537638461, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1302, Loss: 0.31441791485888604, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1303, Loss: 0.2670019790501205, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1304, Loss: 0.24592239634711505, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1305, Loss: 0.32735268525671923, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1306, Loss: 0.2530505991331242, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1307, Loss: 0.4267629085207222, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1308, Loss: 0.2531004777249649, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1309, Loss: 0.3275245814300628, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1310, Loss: 0.306644168038544, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1311, Loss: 0.2642168833091734, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1312, Loss: 0.31655055353507866, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1313, Loss: 0.37056681215175113, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1314, Loss: 0.47898163794381554, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1315, Loss: 0.2766645457178536, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1316, Loss: 0.23466601201621695, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1317, Loss: 0.29836254102539644, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1318, Loss: 0.2568161827484676, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1319, Loss: 0.257288295021833, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1320, Loss: 0.2475148030265474, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1321, Loss: 0.4098685930934472, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1322, Loss: 0.38241587934741017, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1323, Loss: 0.2840204869161305, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1324, Loss: 0.5997617191863828, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1325, Loss: 0.29245507185412556, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1326, Loss: 0.25814560545162046, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1327, Loss: 0.3643923223267621, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1328, Loss: 0.36923892955247384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1329, Loss: 0.3451496442830081, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1330, Loss: 0.2958495218942161, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1331, Loss: 0.24189161729920727, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1332, Loss: 0.31188213185358127, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1333, Loss: 0.48459890668671585, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1334, Loss: 0.4033819113181367, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1335, Loss: 0.2915608438505912, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1336, Loss: 0.717932874591583, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1337, Loss: 0.2384842612256927, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1338, Loss: 0.40927186867766274, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1339, Loss: 0.5025138854003447, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1340, Loss: 0.4310382728810901, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1341, Loss: 0.31015012660892854, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1342, Loss: 0.33856535819109157, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1343, Loss: 0.2850813504477524, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1344, Loss: 0.3832343067734241, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1345, Loss: 0.3538405591976459, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1346, Loss: 0.24218896071087856, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1347, Loss: 0.38758518432908984, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1348, Loss: 0.3567707136320899, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1349, Loss: 0.5207029000713208, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1350, Loss: 0.24748265650273635, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1351, Loss: 0.23851679070602777, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1352, Loss: 0.30085730747472944, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1353, Loss: 0.26959100491184185, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1354, Loss: 0.28519234245123426, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1355, Loss: 0.24276267811180122, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1356, Loss: 0.2977626502933767, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1357, Loss: 0.3305626719213359, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1358, Loss: 0.260022774391382, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1359, Loss: 0.24987556142762912, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1360, Loss: 0.35085105348197765, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1361, Loss: 0.2912284304008144, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1362, Loss: 0.3289292713545532, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1363, Loss: 0.32999852851528355, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1364, Loss: 0.3010374511767072, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1365, Loss: 0.27180758291445617, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1366, Loss: 0.2495695260811635, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1367, Loss: 0.24543432342231325, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1368, Loss: 0.4450590698700546, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1369, Loss: 0.36936201420911774, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1370, Loss: 0.34625214061811527, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1371, Loss: 0.34544878033391074, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1372, Loss: 0.264620802034821, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1373, Loss: 0.38335353411963535, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1374, Loss: 0.3788434191548842, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1375, Loss: 0.25924186416635703, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1376, Loss: 0.2608528665018718, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1377, Loss: 0.41053282177472794, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1378, Loss: 0.3183468122260872, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1379, Loss: 0.23267646863109887, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1380, Loss: 0.37816651525159817, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1381, Loss: 0.33589647095632974, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1382, Loss: 0.3060157798174723, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1383, Loss: 0.2538740196940949, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1384, Loss: 0.24512039098296615, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1385, Loss: 0.23932647071416582, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1386, Loss: 0.2934934729033552, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1387, Loss: 0.23637492019920167, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1388, Loss: 0.31535531109645765, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1389, Loss: 0.35074914090506326, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1390, Loss: 0.4756411955367167, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1391, Loss: 0.3370601398306739, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1392, Loss: 0.27425062803997735, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1393, Loss: 0.359586106737654, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1394, Loss: 0.41939636886301834, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1395, Loss: 0.4228487715795463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1396, Loss: 0.3094864493448259, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1397, Loss: 0.2430616839120489, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1398, Loss: 0.34855017969098767, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1399, Loss: 0.3157278692226451, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1400, Loss: 0.3848735493264086, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1401, Loss: 0.25034184402211423, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1402, Loss: 0.2930415656341455, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1403, Loss: 0.2424655077042698, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1404, Loss: 0.3499792398466951, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1405, Loss: 0.2525262302149157, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1406, Loss: 0.31476242768909046, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1407, Loss: 0.2941330180601624, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1408, Loss: 0.29762999668145157, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1409, Loss: 0.26012078706077296, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1410, Loss: 0.3045212332256648, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1411, Loss: 0.25622521827762673, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1412, Loss: 0.45316871336937026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1413, Loss: 0.3617095910579893, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1414, Loss: 0.40925162287232963, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1415, Loss: 0.31722513846197276, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1416, Loss: 0.434495958548882, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1417, Loss: 0.2514250615251742, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1418, Loss: 0.36912607535111386, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1419, Loss: 0.31757878847853915, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1420, Loss: 0.24282404691116224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1421, Loss: 0.2552522482064145, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1422, Loss: 0.271413934662626, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1423, Loss: 0.2861123968225121, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1424, Loss: 0.3724403493242237, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1425, Loss: 0.4547062092207198, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1426, Loss: 0.23435541327434856, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1427, Loss: 0.2790931239096361, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1428, Loss: 0.43483698462471243, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1429, Loss: 0.30900184378413237, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1430, Loss: 0.37496714529552405, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1431, Loss: 0.25289357980852856, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1432, Loss: 0.3476225504484068, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1433, Loss: 0.275348485969219, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1434, Loss: 0.2559401909562257, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1435, Loss: 0.2763318948511142, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1436, Loss: 0.26211296494408803, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1437, Loss: 0.2844310873566176, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1438, Loss: 0.24045083299421532, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1439, Loss: 0.23483682964256022, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1440, Loss: 0.37921424841437407, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1441, Loss: 0.5404318846554234, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1442, Loss: 0.37977949425091007, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1443, Loss: 0.2900102749752041, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1444, Loss: 0.2988886987193379, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1445, Loss: 0.30990047315539276, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1446, Loss: 0.24458529287577083, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1447, Loss: 0.2660085899276815, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1448, Loss: 0.28173631008614014, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1449, Loss: 0.2883635910013669, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1450, Loss: 0.2637528647980917, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1451, Loss: 0.2813720930359297, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1452, Loss: 0.3119894127328455, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1453, Loss: 0.23335682534553795, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1454, Loss: 0.23782769455106, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1455, Loss: 0.2533782553743318, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1456, Loss: 0.23571876454063675, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1457, Loss: 0.3362593387511496, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1458, Loss: 0.2599878074186917, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1459, Loss: 0.2978318543848231, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1460, Loss: 0.2569811581571728, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1461, Loss: 0.5140089746601377, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1462, Loss: 0.4652030723093501, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1463, Loss: 0.2799858106355472, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1464, Loss: 0.28482706483270315, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1465, Loss: 0.2998025368019893, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1466, Loss: 0.4258341629090895, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1467, Loss: 0.4307176284630858, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1468, Loss: 0.32521407887745957, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1469, Loss: 0.3834539871419226, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1470, Loss: 0.32519127620748883, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1471, Loss: 0.2767267553108972, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1472, Loss: 0.32156440602592984, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1473, Loss: 0.23180032334877923, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1474, Loss: 0.2573473529517679, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1475, Loss: 0.5096410484830382, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1476, Loss: 0.34623489220860754, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1477, Loss: 0.4175850029510279, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1478, Loss: 0.3047830663782728, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1479, Loss: 0.294772051779659, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1480, Loss: 0.5459132394808657, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1481, Loss: 0.36947708390461076, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1482, Loss: 0.26429234103711013, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1483, Loss: 0.35890555348231207, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1484, Loss: 0.37683836260076997, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1485, Loss: 0.38197941169002814, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1486, Loss: 0.24812127509307502, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1487, Loss: 0.25757966492919293, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1488, Loss: 0.32165305985948495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1489, Loss: 0.2748383820343898, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1490, Loss: 0.3223493187602224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1491, Loss: 0.25186105784572327, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1492, Loss: 0.26449456515759656, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1493, Loss: 0.28586508601609134, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1494, Loss: 0.2876343106502196, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1495, Loss: 0.27813976915119876, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1496, Loss: 0.2391291196724791, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1497, Loss: 0.3740864807485714, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1498, Loss: 0.24396735824154314, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1499, Loss: 0.3200495229411031, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1500, Loss: 0.35231167906481625, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1501, Loss: 0.3663789209326549, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1502, Loss: 0.2993205636859776, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1503, Loss: 0.2908377926864538, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1504, Loss: 0.23508189128217044, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1505, Loss: 0.364375373366454, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1506, Loss: 0.2648096424890285, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1507, Loss: 0.2570465686042646, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1508, Loss: 0.3319884970068617, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1509, Loss: 0.35355788141541156, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1510, Loss: 0.35220497756912283, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1511, Loss: 0.32752849319850175, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1512, Loss: 0.9451250364359252, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1513, Loss: 0.34941853787390786, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1514, Loss: 0.31174163353138307, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1515, Loss: 0.33696684697645524, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1516, Loss: 0.31043713255145344, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1517, Loss: 0.29577611247785646, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1518, Loss: 0.31792401336312603, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1519, Loss: 0.33679652424066947, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1520, Loss: 0.3215861433486122, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1521, Loss: 0.4460170765478053, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1522, Loss: 0.35587749358088855, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1523, Loss: 0.3223624916395721, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1524, Loss: 0.2455912294025192, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1525, Loss: 0.29360625250125705, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1526, Loss: 0.26457983833741633, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1527, Loss: 0.40630139624009143, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1528, Loss: 0.34373319290482673, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1529, Loss: 0.29847950042679383, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1530, Loss: 0.25361026884518584, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1531, Loss: 0.25768247727014787, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1532, Loss: 0.2386695135316194, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1533, Loss: 0.5227337350712129, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1534, Loss: 0.2510514145475157, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1535, Loss: 0.25050561597564, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1536, Loss: 0.36374044436476094, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1537, Loss: 0.4223716363124005, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1538, Loss: 0.240918954614472, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1539, Loss: 0.2539237394828128, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1540, Loss: 0.2906183027587645, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1541, Loss: 0.3056857856071719, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1542, Loss: 0.29710589177942154, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1543, Loss: 0.2865781963934594, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1544, Loss: 0.3403396896596184, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1545, Loss: 0.3825733527569257, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1546, Loss: 0.31282312041962107, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1547, Loss: 0.5020250423497363, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1548, Loss: 0.2626270757672409, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1549, Loss: 0.3525931023273633, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1550, Loss: 0.24406774626158576, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1551, Loss: 0.6154323636696752, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1552, Loss: 0.23167728296924034, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1553, Loss: 0.3935251117046583, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1554, Loss: 0.45653604527695435, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1555, Loss: 0.5789348715145253, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1556, Loss: 0.2796269423743704, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1557, Loss: 0.2997181750119361, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1558, Loss: 0.2613131518819424, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1559, Loss: 0.2779573176029239, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1560, Loss: 0.4229992588786875, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1561, Loss: 0.3811029115487564, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1562, Loss: 0.49280512950101724, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1563, Loss: 0.2600335269394049, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1564, Loss: 0.2733342804830087, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1565, Loss: 0.29808770063770873, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1566, Loss: 0.2762537195746406, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1567, Loss: 0.3322355450692106, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1568, Loss: 0.2564441403191323, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1569, Loss: 0.3727211106173432, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1570, Loss: 0.3598980647779917, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1571, Loss: 0.2457472425648657, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1572, Loss: 0.26931498420433564, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1573, Loss: 0.31645575282469607, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1574, Loss: 0.26310921599131826, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1575, Loss: 0.408121901814648, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1576, Loss: 0.4023606556827378, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1577, Loss: 0.22808220925553488, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1578, Loss: 0.30940585153926814, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1579, Loss: 0.6953156572297987, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1580, Loss: 0.4412519558268977, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1581, Loss: 0.3307908133815853, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1582, Loss: 0.3806589759854144, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1583, Loss: 0.4443815466753114, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1584, Loss: 0.3347913032135232, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1585, Loss: 0.23531054494683976, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1586, Loss: 0.3047147913611018, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1587, Loss: 0.2428108960553555, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1588, Loss: 0.28551607706467275, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1589, Loss: 0.4136135237974199, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1590, Loss: 0.3095542566496655, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1591, Loss: 0.3060573530354024, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1592, Loss: 0.27850551213018654, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1593, Loss: 0.24998312272018774, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1594, Loss: 0.27416708790042493, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1595, Loss: 0.2723868906371507, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1596, Loss: 0.25142338783470575, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1597, Loss: 0.5203649601743128, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1598, Loss: 0.30365950699943955, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1599, Loss: 0.23873150557975809, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1600, Loss: 0.3077005130276802, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1601, Loss: 0.49596379755926123, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1602, Loss: 0.2764534506167041, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1603, Loss: 0.333224933010853, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1604, Loss: 0.35907701530064595, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1605, Loss: 0.4135504501980291, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1606, Loss: 0.3216404830753844, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1607, Loss: 0.44288080065281393, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1608, Loss: 0.3014303134654827, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1609, Loss: 0.3238554968310709, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1610, Loss: 0.2896098121343189, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1611, Loss: 0.27459290228325467, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1612, Loss: 0.27828840940151783, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1613, Loss: 0.34038405864440846, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1614, Loss: 0.245967174750512, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1615, Loss: 0.3567889207289546, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1616, Loss: 0.4499917960814754, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1617, Loss: 0.28516315488359706, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1618, Loss: 0.3831645093617755, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1619, Loss: 0.2694422263130129, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1620, Loss: 0.348646281369278, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1621, Loss: 0.25204990886383355, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1622, Loss: 0.2393571140596427, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1623, Loss: 0.2416617720900403, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1624, Loss: 0.32022034011499717, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1625, Loss: 0.3182419058260763, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1626, Loss: 0.24956879291845088, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1627, Loss: 0.3610095162458494, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1628, Loss: 0.23345083475770265, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1629, Loss: 0.24477298238220016, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1630, Loss: 0.25562661119133095, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1631, Loss: 0.26179631410296633, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1632, Loss: 0.3297195124384089, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1633, Loss: 0.25720460383245336, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1634, Loss: 0.4586034535222513, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1635, Loss: 0.33507626830950754, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1636, Loss: 0.29748761387647676, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1637, Loss: 0.30609841103750685, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1638, Loss: 0.28167839077865986, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1639, Loss: 0.2794975407321413, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1640, Loss: 0.2810210308026188, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1641, Loss: 0.3092166111485894, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1642, Loss: 0.2951050152653012, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1643, Loss: 0.2523843966778733, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1644, Loss: 0.4975579626675437, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1645, Loss: 0.2810750525548487, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1646, Loss: 0.2565201114661812, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1647, Loss: 0.2478313130240632, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1648, Loss: 0.3201476728182779, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1649, Loss: 0.28026557574456423, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1650, Loss: 0.4881616907195848, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1651, Loss: 0.5010668628488596, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1652, Loss: 0.23676500899829145, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1653, Loss: 0.34022327616292525, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1654, Loss: 0.2744314618298805, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1655, Loss: 0.2741888649868802, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1656, Loss: 0.37772262593227024, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1657, Loss: 0.2909131028721854, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1658, Loss: 0.3581275890480452, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1659, Loss: 0.30638107520728164, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1660, Loss: 0.2903321992060019, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1661, Loss: 0.3343865277799121, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1662, Loss: 0.3365648891932122, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1663, Loss: 0.265606431776821, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1664, Loss: 0.25029660390039166, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1665, Loss: 0.26958145945585155, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1666, Loss: 0.3809542934092627, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1667, Loss: 0.3184154549132208, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1668, Loss: 0.3724578201639027, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1669, Loss: 0.2658104857003382, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1670, Loss: 0.23246209166174162, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1671, Loss: 0.37922028482236136, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1672, Loss: 0.3233663467167968, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1673, Loss: 0.2903783445756068, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1674, Loss: 0.31779205319746023, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1675, Loss: 0.3229886718612034, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1676, Loss: 0.40245077404027535, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1677, Loss: 0.5042829399238795, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1678, Loss: 0.6311795071604664, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1679, Loss: 0.276225213434292, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1680, Loss: 0.412071719237757, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1681, Loss: 0.24148906180008026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1682, Loss: 0.3046401040089166, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1683, Loss: 0.30040950549359824, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1684, Loss: 0.35254520768795117, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1685, Loss: 0.4862525574768025, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1686, Loss: 0.8674488485198721, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1687, Loss: 0.25328898177377523, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1688, Loss: 0.27462916570157436, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1689, Loss: 0.42177468136260304, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1690, Loss: 0.2563898228071084, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1691, Loss: 0.33186754886787434, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1692, Loss: 0.3454629480341609, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1693, Loss: 0.24424451632587638, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1694, Loss: 0.28781767090582205, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1695, Loss: 0.23487386820348155, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1696, Loss: 0.40906987444614584, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1697, Loss: 0.5099545799503246, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1698, Loss: 0.323591741286017, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1699, Loss: 0.2561030765762878, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1700, Loss: 0.24752333293794698, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1701, Loss: 0.3129399112837959, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1702, Loss: 0.3740969961780426, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1703, Loss: 0.28382895998224755, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1704, Loss: 0.2937257349468753, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1705, Loss: 0.2490664781798718, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1706, Loss: 0.2991322718042969, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1707, Loss: 0.36170231408933967, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1708, Loss: 0.30632717718947516, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1709, Loss: 0.245556170669389, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1710, Loss: 0.2767057536858703, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1711, Loss: 0.37369141462381295, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1712, Loss: 0.3864903659067298, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1713, Loss: 0.4031239731064477, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1714, Loss: 0.35605279939046774, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1715, Loss: 0.2966207442623195, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1716, Loss: 0.38535459432982044, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1717, Loss: 0.2714061551363815, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1718, Loss: 0.39621524833452854, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1719, Loss: 0.27757025266432084, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1720, Loss: 0.3555249989624173, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1721, Loss: 0.32827236483799505, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1722, Loss: 0.3447776790594463, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1723, Loss: 0.25335614123521166, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1724, Loss: 0.24458913212587224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1725, Loss: 0.3170005380473694, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1726, Loss: 0.2554622247478341, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1727, Loss: 0.43956562322688697, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1728, Loss: 0.29012086326514497, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1729, Loss: 0.3911794156586421, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1730, Loss: 0.24950680999712704, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1731, Loss: 0.2715185005666327, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1732, Loss: 0.2575769908397162, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1733, Loss: 0.5100989025506715, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1734, Loss: 0.25531272547866823, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1735, Loss: 0.3330515541984085, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1736, Loss: 0.3782049176801494, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1737, Loss: 0.3338357869163218, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1738, Loss: 0.23296284343940252, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1739, Loss: 0.26709846998459086, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1740, Loss: 0.27130336523480797, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1741, Loss: 0.2581380099006372, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1742, Loss: 0.2463792235404756, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1743, Loss: 0.32118476404166585, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1744, Loss: 0.40351447736209317, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1745, Loss: 0.4257514420981676, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1746, Loss: 0.2718554768068612, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1747, Loss: 0.33686042805122307, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1748, Loss: 0.37003323408074795, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1749, Loss: 0.26094191822906, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1750, Loss: 0.43814970272133175, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1751, Loss: 0.31416840344257096, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1752, Loss: 0.29765714425671297, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1753, Loss: 0.25661586260275127, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1754, Loss: 0.24516034883221283, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1755, Loss: 0.25396793158289344, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1756, Loss: 0.23989103481853588, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1757, Loss: 0.3159602564932456, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1758, Loss: 0.302581373526518, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1759, Loss: 0.34635630394900574, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1760, Loss: 0.2546220992696974, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1761, Loss: 0.24230746870266834, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1762, Loss: 0.5223017029098191, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1763, Loss: 0.6762741311958516, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1764, Loss: 0.24970193544586963, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1765, Loss: 0.25719749301846623, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1766, Loss: 0.23618395001304227, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1767, Loss: 0.30099011622516897, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1768, Loss: 0.41888027291053764, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1769, Loss: 0.42895707225646834, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1770, Loss: 0.3218661448264535, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1771, Loss: 0.3190917686187191, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1772, Loss: 0.26146869101235026, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1773, Loss: 0.3473681476978022, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1774, Loss: 0.2657305991919224, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1775, Loss: 0.32177329193782567, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1776, Loss: 0.3623028870337175, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1777, Loss: 0.2600255374704631, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1778, Loss: 0.3279546784456145, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1779, Loss: 0.27355277659510546, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1780, Loss: 0.22753553243292948, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1781, Loss: 0.27339435689210273, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1782, Loss: 0.44139864108475546, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1783, Loss: 0.33948063033497133, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1784, Loss: 0.25628441656543016, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1785, Loss: 0.2976611232726053, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1786, Loss: 0.30100029859467986, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1787, Loss: 0.2651018294582342, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1788, Loss: 0.2645955270937384, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1789, Loss: 0.3377475506461645, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1790, Loss: 0.49832948702085345, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1791, Loss: 0.3021168272276914, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1792, Loss: 0.4189802406411849, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1793, Loss: 0.2799662203021657, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1794, Loss: 0.3691002709208621, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1795, Loss: 0.291796171076495, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1796, Loss: 0.4761756284190827, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1797, Loss: 0.252052546716183, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1798, Loss: 0.2397473743467926, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1799, Loss: 0.2601159658000496, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1800, Loss: 0.2896323473224432, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1801, Loss: 0.2811782592791601, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1802, Loss: 0.30007491988530166, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1803, Loss: 0.2723859499384005, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1804, Loss: 0.30643698529329383, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1805, Loss: 0.2772805390672862, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1806, Loss: 0.3619825849789325, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1807, Loss: 0.46627282055097435, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1808, Loss: 0.5162360695216455, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1809, Loss: 0.27126011632827274, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1810, Loss: 0.29418701895130167, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1811, Loss: 0.2654655913656673, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1812, Loss: 0.26455675525235506, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1813, Loss: 0.260441672025585, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1814, Loss: 0.2956178536263417, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1815, Loss: 0.24063170389139854, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1816, Loss: 0.35032763159089536, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1817, Loss: 0.35476295140580916, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1818, Loss: 0.38635025937166456, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1819, Loss: 0.253820940459526, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1820, Loss: 0.3899226314882721, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1821, Loss: 0.31689253520682875, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1822, Loss: 0.28341089032145084, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1823, Loss: 0.3010640056561472, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1824, Loss: 0.23669983209552703, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1825, Loss: 0.2749172312364613, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1826, Loss: 0.38103812855494706, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1827, Loss: 0.3058435687093935, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1828, Loss: 0.26956450479577077, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1829, Loss: 0.423506913916858, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1830, Loss: 0.3608752695527343, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1831, Loss: 0.23605651278158574, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1832, Loss: 0.24667836675332858, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1833, Loss: 0.312717665413431, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1834, Loss: 0.24557025671817306, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1835, Loss: 0.3535558579335782, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1836, Loss: 0.29678238331774576, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1837, Loss: 0.2608630559295928, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1838, Loss: 0.31208067740325407, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1839, Loss: 0.2640480252514029, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1840, Loss: 0.390090166140068, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1841, Loss: 0.28018883562010055, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1842, Loss: 0.3262678142552304, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1843, Loss: 0.2765793911351757, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1844, Loss: 0.23671098140409413, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1845, Loss: 0.23968468342418667, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1846, Loss: 0.29023989601415967, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1847, Loss: 0.37927366039025556, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1848, Loss: 0.23602993305856104, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1849, Loss: 0.2901633634863452, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1850, Loss: 0.2689822242454449, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1851, Loss: 0.27423097478908676, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1852, Loss: 0.5879603216922362, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1853, Loss: 0.2631037565045528, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1854, Loss: 0.3315128098355005, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1855, Loss: 0.33313477902606065, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1856, Loss: 0.2908257760981879, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1857, Loss: 0.5138975091052417, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1858, Loss: 0.2742429629386392, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1859, Loss: 0.31932177791913274, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1860, Loss: 0.5003768515412413, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1861, Loss: 0.32738452008301744, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1862, Loss: 0.23584297650246164, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1863, Loss: 0.35377960386106544, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1864, Loss: 0.5020834951361962, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1865, Loss: 0.32377305406991913, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1866, Loss: 0.24961787320540552, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1867, Loss: 0.4696850242446995, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1868, Loss: 0.26868363128092376, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1869, Loss: 0.27519413060278497, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1870, Loss: 0.24398905837325552, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1871, Loss: 0.2391835624811448, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1872, Loss: 0.29913262118409173, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1873, Loss: 0.3155227213137466, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1874, Loss: 0.26093714860870865, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Batch 1875, Loss: 0.4329562751158317, Batch Size: 32, Learning Rate: 2.2275325870819172e-05\n",
      "Epoch 18, Updated Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 18, Average Loss: 0.32438658149143085, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1, Loss: 0.3908930335798336, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 2, Loss: 0.3164707440882419, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 3, Loss: 0.24673001033023859, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 4, Loss: 0.42682056351036457, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 5, Loss: 0.3477346273800153, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 6, Loss: 0.27813579452118287, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 7, Loss: 0.3132960167271547, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 8, Loss: 0.4633193382385187, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 9, Loss: 0.2438919417410193, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 10, Loss: 0.3491720300145511, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 11, Loss: 0.31816545624903736, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 12, Loss: 0.48717948668305633, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 13, Loss: 0.2695981064084486, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 14, Loss: 0.31152281861761216, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 15, Loss: 0.2867960179350008, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 16, Loss: 0.28214735961214044, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 17, Loss: 0.3229184247619639, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 18, Loss: 0.3040908697864067, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 19, Loss: 0.2502707085556031, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 20, Loss: 0.25260007791207745, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 21, Loss: 0.42086587801907155, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 22, Loss: 0.4666606392387415, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 23, Loss: 0.3175919834183497, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 24, Loss: 0.3724210427332103, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 25, Loss: 0.3885972505457348, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 26, Loss: 0.2519680187552818, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 27, Loss: 0.3542308406696233, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 28, Loss: 0.25719598486601614, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 29, Loss: 0.5832242596965382, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 30, Loss: 0.3169512737497449, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 31, Loss: 0.3406031942496306, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 32, Loss: 0.23974288599892074, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 33, Loss: 0.2710500522759232, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 34, Loss: 0.25519754795658717, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 35, Loss: 0.2575057959212583, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 36, Loss: 0.3159143399381403, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 37, Loss: 0.2805782017887761, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 38, Loss: 0.3204135387999528, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 39, Loss: 0.2869791443831154, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 40, Loss: 0.4115702172095175, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 41, Loss: 0.28610952988409505, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 42, Loss: 0.2483142437369119, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 43, Loss: 0.4621585276129727, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 44, Loss: 0.265051378321057, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 45, Loss: 0.2553752285408649, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 46, Loss: 0.32101967352877286, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 47, Loss: 0.27789704960041417, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 48, Loss: 0.31737878793942553, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 49, Loss: 0.5423984033981972, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 50, Loss: 0.28333706226306093, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 51, Loss: 0.4123596843343075, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 52, Loss: 0.271649766088317, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 53, Loss: 0.28084345774203656, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 54, Loss: 0.3262508459602716, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 55, Loss: 0.25470132561572645, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 56, Loss: 0.31999927482120616, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 57, Loss: 0.25850128147037077, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 58, Loss: 0.321561356066029, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 59, Loss: 0.26774141397697, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 60, Loss: 0.4024908126180734, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 61, Loss: 0.24940757851714318, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 62, Loss: 0.32264727056926074, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 63, Loss: 0.34969704053475775, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 64, Loss: 0.27502820374574805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 65, Loss: 0.2711596416586718, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 66, Loss: 0.31153553513280635, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 67, Loss: 0.41923382842110724, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 68, Loss: 0.24586325464319417, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 69, Loss: 0.3032771954391787, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 70, Loss: 0.531081420620613, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 71, Loss: 0.6382161038989396, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 72, Loss: 0.27725452941744966, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 73, Loss: 0.23239090403870305, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 74, Loss: 0.4751310816668717, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 75, Loss: 0.3219334170810359, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 76, Loss: 0.23879704468357718, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 77, Loss: 0.298479689815096, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 78, Loss: 0.42623517719711845, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 79, Loss: 0.24257766289488122, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 80, Loss: 0.4014009776910691, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 81, Loss: 0.3601580208752827, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 82, Loss: 0.2590618861907178, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 83, Loss: 0.3304920562412442, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 84, Loss: 0.2691181503232552, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 85, Loss: 0.29285677186405557, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 86, Loss: 0.24972865567657845, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 87, Loss: 0.33661726413793513, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 88, Loss: 0.2346319725515832, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 89, Loss: 0.28256656499057736, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 90, Loss: 0.24729777919897328, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 91, Loss: 0.26617594068860206, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 92, Loss: 0.236485807404925, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 93, Loss: 0.3034865124497767, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 94, Loss: 0.24402467345681025, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 95, Loss: 0.49847191220047, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 96, Loss: 0.35830582817473133, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 97, Loss: 0.2690197435099255, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 98, Loss: 0.25531269328636846, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 99, Loss: 0.24780496832890453, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 100, Loss: 0.27811713233879287, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 101, Loss: 0.33411762825680297, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 102, Loss: 0.23905502296197598, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 103, Loss: 0.3360402229421331, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 104, Loss: 0.26303914690256325, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 105, Loss: 0.36882995143417946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 106, Loss: 0.2673969102202993, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 107, Loss: 0.2782393857909801, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 108, Loss: 0.24580636732694017, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 109, Loss: 0.3247553803833218, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 110, Loss: 0.30740285813635837, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 111, Loss: 0.3417013090036879, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 112, Loss: 0.43454548031579576, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 113, Loss: 0.29430541279697947, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 114, Loss: 0.29011353484999525, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 115, Loss: 0.2345756486586941, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 116, Loss: 0.26031207051866845, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 117, Loss: 0.6255816691078451, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 118, Loss: 0.2839201093201443, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 119, Loss: 0.3411584379552538, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 120, Loss: 0.2903814949046385, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 121, Loss: 0.24770483611581529, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 122, Loss: 0.2850636868550637, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 123, Loss: 0.5060562312773407, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 124, Loss: 0.32003910264614854, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 125, Loss: 0.35393492743247373, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 126, Loss: 0.273855138484191, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 127, Loss: 0.25990988574925283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 128, Loss: 0.3298910088125441, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 129, Loss: 0.5257815007187706, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 130, Loss: 0.2687955924624803, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 131, Loss: 0.32051620277914156, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 132, Loss: 0.2942990797514261, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 133, Loss: 0.33038502896944966, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 134, Loss: 0.39132327672682465, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 135, Loss: 0.2484501727818863, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 136, Loss: 0.36711844996575116, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 137, Loss: 0.24024937198737156, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 138, Loss: 0.337072831302688, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 139, Loss: 0.28173042909037904, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 140, Loss: 0.2873831149194981, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 141, Loss: 0.24429305385725564, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 142, Loss: 0.23317961070794815, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 143, Loss: 0.29853056058172084, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 144, Loss: 0.3895200057499986, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 145, Loss: 0.29372575169877047, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 146, Loss: 0.30906690695648276, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 147, Loss: 0.4191539215960487, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 148, Loss: 0.24385262246588332, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 149, Loss: 0.2581418242074156, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 150, Loss: 0.2744154687273195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 151, Loss: 0.28232999632872896, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 152, Loss: 0.2556618712599103, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 153, Loss: 0.2765462785820787, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 154, Loss: 0.2985975126272163, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 155, Loss: 0.2832329586634316, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 156, Loss: 0.286050201157846, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 157, Loss: 0.2464976891253561, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 158, Loss: 0.23992768366816322, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 159, Loss: 0.24690850409366608, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 160, Loss: 0.4906328558376625, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 161, Loss: 0.23961915410818954, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 162, Loss: 0.29911438244769806, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 163, Loss: 0.2398923527424572, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 164, Loss: 0.4037626660002261, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 165, Loss: 0.24937343542326804, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 166, Loss: 0.385676106421998, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 167, Loss: 0.30416540508508294, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 168, Loss: 0.3509830003447819, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 169, Loss: 0.264931276017672, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 170, Loss: 0.2556190155454742, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 171, Loss: 0.3238697078533033, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 172, Loss: 0.2592228295691516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 173, Loss: 0.3493492077709971, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 174, Loss: 0.30980725020146505, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 175, Loss: 0.26269061048701914, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 176, Loss: 0.3063708824085751, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 177, Loss: 0.38624131631034453, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 178, Loss: 0.2805296017584614, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 179, Loss: 0.34155861250034814, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 180, Loss: 0.4202992033006636, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 181, Loss: 0.4731587383888029, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 182, Loss: 0.25181844052137003, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 183, Loss: 0.35089520095569476, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 184, Loss: 0.2844823152377347, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 185, Loss: 0.2429956728979833, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 186, Loss: 0.30527807972878834, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 187, Loss: 0.5839266002350882, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 188, Loss: 0.2877335153926055, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 189, Loss: 0.46810901329722454, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 190, Loss: 0.252908018904881, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 191, Loss: 0.31342596907748177, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 192, Loss: 0.34878559237718454, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 193, Loss: 0.2477055118376905, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 194, Loss: 0.26870957316632726, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 195, Loss: 0.2686604479840334, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 196, Loss: 0.35786842059149127, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 197, Loss: 0.4422278656422661, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 198, Loss: 0.37011684686989266, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 199, Loss: 0.293741253647591, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 200, Loss: 0.26058402556220783, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 201, Loss: 0.4322042409016862, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 202, Loss: 0.28345121935227413, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 203, Loss: 0.2581276711974868, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 204, Loss: 0.43894765818300585, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 205, Loss: 0.26283973325759996, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 206, Loss: 0.31034705435407045, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 207, Loss: 0.5599673936647576, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 208, Loss: 0.2676459865521991, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 209, Loss: 0.5539078811087085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 210, Loss: 0.3243733937843931, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 211, Loss: 0.31517086144949547, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 212, Loss: 0.37355929015450084, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 213, Loss: 0.2901664211990769, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 214, Loss: 0.3247538490103582, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 215, Loss: 0.25562120712216996, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 216, Loss: 0.3193228335814195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 217, Loss: 0.3091362156737535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 218, Loss: 0.29964141480384937, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 219, Loss: 0.2740370494241071, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 220, Loss: 0.25021874455467374, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 221, Loss: 0.24966307465301815, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 222, Loss: 0.296952923051786, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 223, Loss: 0.35235922426423694, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 224, Loss: 0.2600226261419727, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 225, Loss: 0.3478202305623811, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 226, Loss: 0.3542429561849006, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 227, Loss: 0.3759419572594448, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 228, Loss: 0.37582159975164015, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 229, Loss: 0.2926269099291086, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 230, Loss: 0.28119517226140617, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 231, Loss: 0.2784279069275984, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 232, Loss: 0.6361924467966034, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 233, Loss: 0.3141341889716385, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 234, Loss: 0.31906663192249485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 235, Loss: 0.3978531992075678, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 236, Loss: 0.27570110038151163, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 237, Loss: 0.2681587724272975, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 238, Loss: 0.2500545172665502, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 239, Loss: 0.368624492034649, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 240, Loss: 0.3000420469697746, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 241, Loss: 0.24129291407759484, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 242, Loss: 0.29063683272269547, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 243, Loss: 0.2467123910992401, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 244, Loss: 0.25967542394581516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 245, Loss: 0.3609437924797283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 246, Loss: 0.36409661293693407, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 247, Loss: 0.47429918779199143, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 248, Loss: 0.246070692473767, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 249, Loss: 0.31282359464875653, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 250, Loss: 0.39673579345040866, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 251, Loss: 0.27725322480482806, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 252, Loss: 0.42577522856706, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 253, Loss: 0.24020106324897197, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 254, Loss: 0.2639063912532905, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 255, Loss: 0.276267945711863, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 256, Loss: 0.23998928234801897, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 257, Loss: 0.4780806347322221, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 258, Loss: 0.3732246544704172, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 259, Loss: 0.39802076149342197, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 260, Loss: 0.30527184953441155, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 261, Loss: 0.3162861985652477, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 262, Loss: 0.2929991939763213, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 263, Loss: 0.32377671544812314, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 264, Loss: 0.34256324735118526, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 265, Loss: 0.30596125618589054, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 266, Loss: 0.284187860309872, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 267, Loss: 0.29660961957211673, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 268, Loss: 0.2787321369092538, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 269, Loss: 0.3516985024903468, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 270, Loss: 0.30088779516568287, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 271, Loss: 0.39365313531193147, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 272, Loss: 0.2688063422307579, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 273, Loss: 0.2700776751488495, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 274, Loss: 0.2761849492036652, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 275, Loss: 0.35281024046273185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 276, Loss: 0.307272053443203, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 277, Loss: 0.2423216087121178, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 278, Loss: 0.3302075483659319, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 279, Loss: 0.3409324418607699, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 280, Loss: 0.2555191531747502, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 281, Loss: 0.24312575615265392, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 282, Loss: 0.26515105375696013, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 283, Loss: 0.3819298159394752, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 284, Loss: 0.281540442257797, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 285, Loss: 0.3036434041062196, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 286, Loss: 0.34295487045769624, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 287, Loss: 0.3211712495083424, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 288, Loss: 0.5285786086682724, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 289, Loss: 0.3475934976954682, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 290, Loss: 0.44647156832137147, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 291, Loss: 0.44215759767697305, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 292, Loss: 0.2637011150812583, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 293, Loss: 0.24095016120815457, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 294, Loss: 0.2508172573909657, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 295, Loss: 0.24314404979104187, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 296, Loss: 0.3579346519767879, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 297, Loss: 0.26642142506085076, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 298, Loss: 0.3060757751511828, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 299, Loss: 0.26643425011707017, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 300, Loss: 0.34622573278489455, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 301, Loss: 0.325439329673936, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 302, Loss: 0.2586466400714073, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 303, Loss: 0.4279190509257369, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 304, Loss: 0.33820935181062584, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 305, Loss: 0.2480264199985185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 306, Loss: 0.27425983248869884, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 307, Loss: 0.32464508639557865, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 308, Loss: 0.27433811805821035, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 309, Loss: 0.3404818469645946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 310, Loss: 0.23918925824667206, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 311, Loss: 0.3023209073461751, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 312, Loss: 0.2530930510115312, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 313, Loss: 0.3223616208831889, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 314, Loss: 0.2496850114761597, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 315, Loss: 0.23835178886998562, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 316, Loss: 0.2550744868961496, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 317, Loss: 0.5959491220944895, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 318, Loss: 0.2964108486933516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 319, Loss: 0.2808521007425566, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 320, Loss: 0.3853372258366749, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 321, Loss: 0.40484249896515856, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 322, Loss: 0.42700849648803013, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 323, Loss: 0.254102797395197, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 324, Loss: 0.2924981770493301, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 325, Loss: 0.2738140618900012, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 326, Loss: 0.29609836823311864, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 327, Loss: 0.2767499729841704, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 328, Loss: 0.23824663702381976, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 329, Loss: 0.2842665685789963, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 330, Loss: 0.26483687988684895, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 331, Loss: 0.43319629702842744, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 332, Loss: 0.4948406696977233, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 333, Loss: 0.2664919890159158, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 334, Loss: 0.2579402707110761, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 335, Loss: 0.3240244556328943, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 336, Loss: 0.4074607685445296, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 337, Loss: 0.25514056638850513, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 338, Loss: 0.3513184702363056, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 339, Loss: 0.277820819173434, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 340, Loss: 0.23858927408674002, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 341, Loss: 0.48001613911483687, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 342, Loss: 0.2428080052661192, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 343, Loss: 0.30200711454419793, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 344, Loss: 0.32975373675086195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 345, Loss: 0.3008144435087443, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 346, Loss: 0.299135397238527, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 347, Loss: 0.23600954319589953, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 348, Loss: 0.32041136909449996, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 349, Loss: 0.27970370989953525, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 350, Loss: 0.5070032712510082, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 351, Loss: 0.26857661503031327, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 352, Loss: 0.26067970532107426, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 353, Loss: 0.346915391221509, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 354, Loss: 0.29882266801715107, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 355, Loss: 0.27891515953715595, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 356, Loss: 0.5120640747038657, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 357, Loss: 0.2725012140374162, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 358, Loss: 0.28394302389508874, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 359, Loss: 0.3222046248681095, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 360, Loss: 0.30113226639024077, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 361, Loss: 0.2625712522914074, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 362, Loss: 0.34608604777988206, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 363, Loss: 0.2563550843132899, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 364, Loss: 0.25277770933785726, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 365, Loss: 0.26367642270614405, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 366, Loss: 0.2665063129051005, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 367, Loss: 0.2566843080084061, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 368, Loss: 0.3167354712736627, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 369, Loss: 0.637999890881522, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 370, Loss: 0.325800917608179, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 371, Loss: 0.42400070249521077, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 372, Loss: 0.2970895997610468, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 373, Loss: 0.3797230652154113, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 374, Loss: 0.255867744913855, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 375, Loss: 0.24241439763232625, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 376, Loss: 0.2685696536308217, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 377, Loss: 0.44419694453496716, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 378, Loss: 0.2842192883291763, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 379, Loss: 0.24900735997372286, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 380, Loss: 0.5346224667767879, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 381, Loss: 0.37594353522221124, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 382, Loss: 0.5001781615016796, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 383, Loss: 0.6218624151068529, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 384, Loss: 0.3402975626064158, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 385, Loss: 0.3975906533331946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 386, Loss: 0.27029796779975257, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 387, Loss: 0.25002351830477565, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 388, Loss: 0.3068227027015087, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 389, Loss: 0.34678316733109427, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 390, Loss: 0.238165780194232, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 391, Loss: 0.2593122871922879, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 392, Loss: 0.2795657085241985, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 393, Loss: 0.31054377713462106, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 394, Loss: 0.44788590365522774, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 395, Loss: 0.3243282126250868, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 396, Loss: 0.363943083242536, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 397, Loss: 0.3226304349939419, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 398, Loss: 0.2857702900247952, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 399, Loss: 0.24925298327489173, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 400, Loss: 0.2653767234990775, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 401, Loss: 0.3291798317397714, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 402, Loss: 0.24712775879523485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 403, Loss: 0.2675276230893714, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 404, Loss: 0.2995864110833793, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 405, Loss: 0.3325325437294957, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 406, Loss: 0.35615137583614304, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 407, Loss: 0.2735431450802546, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 408, Loss: 0.28783429714508896, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 409, Loss: 0.2607731926231526, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 410, Loss: 0.3008954155492241, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 411, Loss: 0.2656448735318187, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 412, Loss: 0.25066223380741426, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 413, Loss: 0.46044857826371, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 414, Loss: 0.47613018870028545, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 415, Loss: 0.42513441873837887, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 416, Loss: 0.33513306101412005, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 417, Loss: 0.24503054874137847, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 418, Loss: 0.2919200243436862, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 419, Loss: 0.4008838682712771, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 420, Loss: 0.32007286656070927, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 421, Loss: 0.3202841919982241, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 422, Loss: 0.2841739678082827, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 423, Loss: 0.3229266358306825, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 424, Loss: 0.24584246108126892, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 425, Loss: 0.33848621828442527, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 426, Loss: 0.24748397503771574, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 427, Loss: 0.277090652648666, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 428, Loss: 0.24880021170076835, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 429, Loss: 0.38326690423132886, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 430, Loss: 0.33400064564971654, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 431, Loss: 0.25279573362451335, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 432, Loss: 0.37740266963080926, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 433, Loss: 0.3189374746348558, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 434, Loss: 0.3835010298636359, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 435, Loss: 0.2958753457969634, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 436, Loss: 0.31401639734313086, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 437, Loss: 0.2642750959667858, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 438, Loss: 0.358654774142651, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 439, Loss: 0.27657853234106194, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 440, Loss: 0.26024142685906615, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 441, Loss: 0.3142007917175519, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 442, Loss: 0.2389370998060752, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 443, Loss: 0.35646483484298486, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 444, Loss: 0.26231453435859214, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 445, Loss: 0.267739597128273, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 446, Loss: 0.3322229282262439, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 447, Loss: 0.30106466354165706, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 448, Loss: 0.26307810264133086, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 449, Loss: 0.24821843869504057, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 450, Loss: 0.38042815933942176, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 451, Loss: 0.30854818205049206, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 452, Loss: 0.25152647592050853, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 453, Loss: 0.2940129951026621, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 454, Loss: 0.4692342303972883, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 455, Loss: 0.25201133312116397, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 456, Loss: 0.26528793691039126, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 457, Loss: 0.26943551333130533, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 458, Loss: 0.25700877059403676, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 459, Loss: 0.5408216458492032, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 460, Loss: 0.2825251323402313, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 461, Loss: 0.24589884304327358, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 462, Loss: 0.29035544957991155, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 463, Loss: 0.2325904790433634, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 464, Loss: 0.3083098261264279, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 465, Loss: 0.42927026450381295, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 466, Loss: 0.24695963233572885, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 467, Loss: 0.34312266659180624, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 468, Loss: 0.24919926566471906, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 469, Loss: 0.31895512117082087, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 470, Loss: 0.47418618020474235, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 471, Loss: 0.27896415368195715, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 472, Loss: 0.36265084833535643, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 473, Loss: 0.39826066752824796, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 474, Loss: 0.41065785409973665, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 475, Loss: 0.3368964923329668, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 476, Loss: 0.2521055207609477, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 477, Loss: 0.2880854707635858, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 478, Loss: 0.314749217709273, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 479, Loss: 0.4318626165294037, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 480, Loss: 0.27106905735793246, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 481, Loss: 0.2501424893627532, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 482, Loss: 0.26405448930929865, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 483, Loss: 0.45399578720556966, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 484, Loss: 0.22990824481056163, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 485, Loss: 0.5840778215831163, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 486, Loss: 0.37562008071543085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 487, Loss: 0.3015437719082058, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 488, Loss: 0.5960627957959623, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 489, Loss: 0.331783685414906, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 490, Loss: 0.2648442054213707, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 491, Loss: 0.27120329252582354, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 492, Loss: 0.4962859760221091, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 493, Loss: 0.3495268147204328, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 494, Loss: 0.29822914112107435, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 495, Loss: 0.2884404389454376, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 496, Loss: 0.27887317107466664, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 497, Loss: 0.3105679603784218, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 498, Loss: 0.3118483476840953, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 499, Loss: 0.3410143765014568, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 500, Loss: 0.28417825411186554, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 501, Loss: 0.3170594488031734, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 502, Loss: 0.307629602224889, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 503, Loss: 0.5555947613338655, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 504, Loss: 0.32900713195341996, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 505, Loss: 0.29317106268711185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 506, Loss: 0.2706171169172464, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 507, Loss: 0.2530428172807544, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 508, Loss: 0.3488121372052444, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 509, Loss: 0.616176388343247, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 510, Loss: 0.3356994882791173, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 511, Loss: 0.314644544859447, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 512, Loss: 0.29116091539996763, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 513, Loss: 0.28128382926785, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 514, Loss: 0.567485015941156, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 515, Loss: 0.29732624712330374, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 516, Loss: 0.25074593952455115, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 517, Loss: 0.28009628121888613, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 518, Loss: 0.29179528906777963, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 519, Loss: 0.3009233079642716, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 520, Loss: 0.3129759439315895, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 521, Loss: 0.4296542592134741, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 522, Loss: 0.32331951987000507, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 523, Loss: 0.3294449328673927, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 524, Loss: 0.4294375110841514, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 525, Loss: 0.335211039523745, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 526, Loss: 0.47100495525107416, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 527, Loss: 0.2617428278894309, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 528, Loss: 0.24437184349131683, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 529, Loss: 0.27818831192671184, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 530, Loss: 0.3311209005682022, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 531, Loss: 0.35933356290625573, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 532, Loss: 0.29370182228826974, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 533, Loss: 0.31429170983859694, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 534, Loss: 0.27140110843100995, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 535, Loss: 0.3283861555392311, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 536, Loss: 0.4999355299665275, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 537, Loss: 0.3013319640512438, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 538, Loss: 0.4243106932179611, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 539, Loss: 0.28812796563064685, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 540, Loss: 0.2675864932353005, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 541, Loss: 0.3039756766737615, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 542, Loss: 0.24275313171210927, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 543, Loss: 0.4087118370417449, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 544, Loss: 0.255472084879425, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 545, Loss: 0.2689975613025668, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 546, Loss: 0.36209656444561233, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 547, Loss: 0.24948746754475443, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 548, Loss: 0.4292580671975651, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 549, Loss: 0.3147952295979712, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 550, Loss: 0.2971177634869736, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 551, Loss: 0.3475597432729163, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 552, Loss: 0.4957881866865947, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 553, Loss: 0.2541723047927312, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 554, Loss: 0.4001211742591798, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 555, Loss: 0.2944410676865922, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 556, Loss: 0.4609661381688938, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 557, Loss: 0.28645696276365024, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 558, Loss: 0.31779357441037687, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 559, Loss: 0.306056221192484, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 560, Loss: 0.2413730805739953, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 561, Loss: 0.37986817126494765, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 562, Loss: 0.31043488242501527, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 563, Loss: 0.2806011771588173, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 564, Loss: 0.26188195023205635, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 565, Loss: 0.4195467152759197, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 566, Loss: 0.2863608834034569, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 567, Loss: 0.3057370380214045, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 568, Loss: 0.33533507339354085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 569, Loss: 0.3247091943043202, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 570, Loss: 0.26876928644240367, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 571, Loss: 0.29522352965450527, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 572, Loss: 0.27098308262081, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 573, Loss: 0.2740977920027976, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 574, Loss: 0.5191846565520389, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 575, Loss: 0.35176547900328237, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 576, Loss: 0.23755102059810249, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 577, Loss: 0.26954608929724855, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 578, Loss: 0.24188627397955595, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 579, Loss: 0.4340645016535859, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 580, Loss: 0.5099992892118088, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 581, Loss: 0.37881614572515987, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 582, Loss: 0.50797019741927, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 583, Loss: 0.2579743896878918, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 584, Loss: 0.445300778746844, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 585, Loss: 0.26782335610633423, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 586, Loss: 0.3118815047511828, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 587, Loss: 0.29923573722669145, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 588, Loss: 0.3429788029087586, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 589, Loss: 0.2565233459583625, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 590, Loss: 0.23403333638261176, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 591, Loss: 0.552919388928753, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 592, Loss: 0.3445649430997425, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 593, Loss: 0.2828194819567875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 594, Loss: 0.27956551366192844, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 595, Loss: 0.2587972126990619, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 596, Loss: 0.3072758531506266, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 597, Loss: 0.35193376466576964, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 598, Loss: 0.32125461379681075, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 599, Loss: 0.23290712540227546, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 600, Loss: 0.2974074049584997, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 601, Loss: 0.36214269548592465, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 602, Loss: 0.2940869126184541, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 603, Loss: 0.27753237727797997, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 604, Loss: 0.2661797319489502, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 605, Loss: 0.41995467517907126, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 606, Loss: 0.26048307836377427, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 607, Loss: 0.29370437095336754, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 608, Loss: 0.24572651806958268, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 609, Loss: 0.34780206831999794, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 610, Loss: 0.2504189895681793, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 611, Loss: 0.2982639812683562, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 612, Loss: 0.2648512861169705, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 613, Loss: 0.2784075955998767, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 614, Loss: 0.301885152634713, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 615, Loss: 0.3502109282653129, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 616, Loss: 0.500376329624898, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 617, Loss: 0.2525232369657142, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 618, Loss: 0.238510784367098, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 619, Loss: 0.2537549974362255, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 620, Loss: 0.3091502029678674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 621, Loss: 0.30836869599793043, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 622, Loss: 0.31721510803890185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 623, Loss: 0.2769213087286281, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 624, Loss: 0.409814691274078, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 625, Loss: 0.29241864872390705, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 626, Loss: 0.24732739098287893, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 627, Loss: 0.30034704104503174, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 628, Loss: 0.3771851693778946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 629, Loss: 0.24962627829633688, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 630, Loss: 0.2535780285373241, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 631, Loss: 0.2800831403908021, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 632, Loss: 0.4093509041481119, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 633, Loss: 0.35057207594706175, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 634, Loss: 0.23741237731543594, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 635, Loss: 0.43770729441028333, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 636, Loss: 0.3959634005302096, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 637, Loss: 0.34509631706772037, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 638, Loss: 0.3053580118334517, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 639, Loss: 0.39270589411317597, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 640, Loss: 0.28772647065471774, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 641, Loss: 0.30246151102036334, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 642, Loss: 0.32267326773377725, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 643, Loss: 0.29842356086187954, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 644, Loss: 0.31452587655485226, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 645, Loss: 0.2863114907077587, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 646, Loss: 0.29462191982039265, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 647, Loss: 0.23362550850495564, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 648, Loss: 0.31626697284995453, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 649, Loss: 0.2829548860674049, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 650, Loss: 0.28641639604560093, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 651, Loss: 0.33592303700161025, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 652, Loss: 0.2732327335403234, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 653, Loss: 0.2540926065922484, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 654, Loss: 0.2698891889695811, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 655, Loss: 0.2432338543989072, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 656, Loss: 0.3664411579117095, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 657, Loss: 0.27145596552720325, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 658, Loss: 0.5446967244548777, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 659, Loss: 0.3580485022090485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 660, Loss: 0.28407544024388753, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 661, Loss: 0.36829467781705805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 662, Loss: 0.2404234628703572, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 663, Loss: 0.3548527579012194, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 664, Loss: 0.35420497327885786, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 665, Loss: 0.31801120013951073, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 666, Loss: 0.33123892181678405, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 667, Loss: 0.41253051664387796, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 668, Loss: 0.30923230491991616, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 669, Loss: 0.33287681937166386, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 670, Loss: 0.26719451753290047, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 671, Loss: 0.28076486316048027, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 672, Loss: 0.27506192669658047, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 673, Loss: 0.2877383332384268, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 674, Loss: 0.362401431272565, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 675, Loss: 0.36508292018991506, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 676, Loss: 0.36734972076067673, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 677, Loss: 0.24702071371422668, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 678, Loss: 0.34364155898758686, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 679, Loss: 0.24594044807946594, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 680, Loss: 0.27411963763447034, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 681, Loss: 0.294963506827308, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 682, Loss: 0.4231304544732123, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 683, Loss: 0.39467610273289316, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 684, Loss: 0.27058094427709456, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 685, Loss: 0.33664029550616714, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 686, Loss: 0.3369684917932051, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 687, Loss: 0.2619899762257056, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 688, Loss: 0.34760068714657955, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 689, Loss: 0.2868116866169926, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 690, Loss: 0.40937219535806374, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 691, Loss: 0.2532276397788905, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 692, Loss: 0.4439162587321442, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 693, Loss: 0.4057751421218875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 694, Loss: 0.2738320230632842, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 695, Loss: 0.39252947574920927, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 696, Loss: 0.31624988679717586, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 697, Loss: 0.35390924213831026, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 698, Loss: 0.2707723167571164, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 699, Loss: 0.32378213648022897, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 700, Loss: 0.28322831070117627, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 701, Loss: 0.34908672330238394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 702, Loss: 0.2877316722157338, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 703, Loss: 0.3069770782246612, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 704, Loss: 0.46675047525154745, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 705, Loss: 0.287749510651493, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 706, Loss: 0.28211356653496505, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 707, Loss: 0.24541942924150137, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 708, Loss: 0.3346951077693894, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 709, Loss: 0.3396508191709804, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 710, Loss: 0.23952102447540197, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 711, Loss: 0.6150190991898016, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 712, Loss: 0.4131381627911267, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 713, Loss: 0.41761554042440696, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 714, Loss: 0.3382140216744744, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 715, Loss: 0.2620053905211026, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 716, Loss: 0.3642408966886805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 717, Loss: 0.2724667337183746, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 718, Loss: 0.5208307649298001, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 719, Loss: 0.295466178750409, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 720, Loss: 0.3097617160694896, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 721, Loss: 0.3409105737865901, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 722, Loss: 0.24493880230172127, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 723, Loss: 0.34888528631802085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 724, Loss: 0.2861734371897934, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 725, Loss: 0.3243877391079575, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 726, Loss: 0.2784531984674133, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 727, Loss: 0.39966325178547113, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 728, Loss: 0.2549119980307352, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 729, Loss: 0.41195339203631565, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 730, Loss: 0.47171414697177083, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 731, Loss: 0.2779533056382187, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 732, Loss: 0.27447776251169526, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 733, Loss: 0.28573337935387266, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 734, Loss: 0.30193518921939877, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 735, Loss: 0.2413231969551754, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 736, Loss: 0.27561602410197233, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 737, Loss: 0.24723776406776493, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 738, Loss: 0.46162716344394944, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 739, Loss: 0.5725220196428196, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 740, Loss: 0.3097811305802883, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 741, Loss: 0.3098310406317983, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 742, Loss: 0.30850145437284876, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 743, Loss: 0.4808470467161151, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 744, Loss: 0.2299609687244489, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 745, Loss: 0.27818732840246674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 746, Loss: 0.5757151067021589, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 747, Loss: 0.5568032777869594, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 748, Loss: 0.30571825635359484, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 749, Loss: 0.3250382895024772, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 750, Loss: 0.2594030513010174, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 751, Loss: 0.2744422650976608, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 752, Loss: 0.3433435696593787, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 753, Loss: 0.3888713449965554, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 754, Loss: 0.3846965518169506, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 755, Loss: 0.2720681981013248, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 756, Loss: 0.2871170637361614, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 757, Loss: 0.26305955233295514, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 758, Loss: 0.41889462779929276, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 759, Loss: 0.30515427377878923, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 760, Loss: 0.2437962948868625, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 761, Loss: 0.3342367693596795, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 762, Loss: 0.25342669286261604, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 763, Loss: 0.2332549476545262, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 764, Loss: 0.23027511346781446, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 765, Loss: 0.2658475618667516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 766, Loss: 0.24799572289096422, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 767, Loss: 0.4180472826228467, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 768, Loss: 0.47367723540976236, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 769, Loss: 0.5572508846559412, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 770, Loss: 0.31954226341716674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 771, Loss: 0.3344170924050798, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 772, Loss: 0.2632940440536785, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 773, Loss: 0.25461731232903845, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 774, Loss: 0.25476831371522174, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 775, Loss: 0.45419245962281285, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 776, Loss: 0.2690205737378409, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 777, Loss: 0.27938840557590605, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 778, Loss: 0.30454471175632253, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 779, Loss: 0.32163689315648314, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 780, Loss: 0.2618737136346643, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 781, Loss: 0.3070954752500821, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 782, Loss: 0.4396242626574705, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 783, Loss: 0.25375180051758356, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 784, Loss: 0.3090681405916129, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 785, Loss: 0.2537482168647043, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 786, Loss: 0.45137169518266673, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 787, Loss: 0.31256490316524965, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 788, Loss: 0.23653852615092777, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 789, Loss: 0.2749180046909125, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 790, Loss: 0.26233940366951475, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 791, Loss: 0.23130334440015307, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 792, Loss: 0.35727708654493123, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 793, Loss: 0.4159841118314056, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 794, Loss: 0.3217652299416781, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 795, Loss: 0.2588467510427064, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 796, Loss: 0.41597486359639313, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 797, Loss: 0.2764373987395352, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 798, Loss: 0.2662078599439266, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 799, Loss: 0.3100153878472743, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 800, Loss: 0.32137457338551406, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 801, Loss: 0.27253040703662285, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 802, Loss: 0.271543863841759, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 803, Loss: 0.25211672746051156, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 804, Loss: 0.28504648687170714, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 805, Loss: 0.2723026026651363, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 806, Loss: 0.2866245826933251, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 807, Loss: 0.4578833800016687, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 808, Loss: 0.26226517542309635, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 809, Loss: 0.23436337727522127, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 810, Loss: 0.23172281311745355, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 811, Loss: 0.4996341703966325, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 812, Loss: 0.32661123424979305, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 813, Loss: 0.4203850966289656, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 814, Loss: 0.27058717758675005, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 815, Loss: 0.31217714033958166, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 816, Loss: 0.34675554781632595, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 817, Loss: 0.31972478539611393, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 818, Loss: 0.31223442734512374, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 819, Loss: 0.4075925162453885, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 820, Loss: 0.2696672149879637, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 821, Loss: 0.28302893363934795, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 822, Loss: 0.2727325179028407, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 823, Loss: 0.30453168854700496, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 824, Loss: 0.43389360847181657, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 825, Loss: 0.26214550097879796, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 826, Loss: 0.3745054721525554, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 827, Loss: 0.3342085799995729, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 828, Loss: 0.32111066474864347, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 829, Loss: 0.40054571714690934, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 830, Loss: 0.29249393396162104, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 831, Loss: 0.26889530603705963, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 832, Loss: 0.24780286480784258, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 833, Loss: 0.35110573188584976, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 834, Loss: 0.3482923559879557, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 835, Loss: 0.3954341500664141, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 836, Loss: 0.2869901086349592, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 837, Loss: 0.40483528274604474, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 838, Loss: 0.28990623515449665, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 839, Loss: 0.2685708303696955, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 840, Loss: 0.40549675806192165, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 841, Loss: 0.30862697753138085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 842, Loss: 0.2553998412561516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 843, Loss: 0.461392999159962, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 844, Loss: 0.3588010349306401, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 845, Loss: 0.28383320825208014, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 846, Loss: 0.3445901137682621, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 847, Loss: 0.3245715311230413, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 848, Loss: 0.2787687376920567, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 849, Loss: 0.3164729979985201, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 850, Loss: 0.4990853095568303, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 851, Loss: 0.2951211287942183, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 852, Loss: 0.30063712976789, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 853, Loss: 0.3071996427596432, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 854, Loss: 0.30691536303626554, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 855, Loss: 0.2578313781486025, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 856, Loss: 0.3083426756903122, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 857, Loss: 0.468910271067835, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 858, Loss: 0.3297894593650785, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 859, Loss: 0.3181107036500328, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 860, Loss: 0.24375646406914306, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 861, Loss: 0.49295297420382456, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 862, Loss: 0.2511532812408875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 863, Loss: 0.27117372048870797, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 864, Loss: 0.2443974139094072, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 865, Loss: 0.4029282051537786, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 866, Loss: 0.25542140300526966, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 867, Loss: 0.2592802841295566, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 868, Loss: 0.24604916765842255, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 869, Loss: 0.2549331090370684, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 870, Loss: 0.23932465416160442, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 871, Loss: 0.2709126685970102, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 872, Loss: 0.2877582327502333, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 873, Loss: 0.3361400412693678, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 874, Loss: 0.24011321604957112, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 875, Loss: 0.3102953774117972, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 876, Loss: 0.2947619926319282, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 877, Loss: 0.2364405278100081, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 878, Loss: 0.2745761360927186, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 879, Loss: 0.311301857327481, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 880, Loss: 0.4661983809399821, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 881, Loss: 0.28890409698979624, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 882, Loss: 0.2623844497613116, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 883, Loss: 0.23947777916434923, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 884, Loss: 0.2989731243088279, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 885, Loss: 0.3222956639649526, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 886, Loss: 0.24216139010672513, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 887, Loss: 0.3873240325957291, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 888, Loss: 0.29987387196646864, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 889, Loss: 0.28369960561458535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 890, Loss: 0.24181981524690382, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 891, Loss: 0.2979223580877197, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 892, Loss: 0.2558421855032253, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 893, Loss: 0.26147376949642864, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 894, Loss: 0.27021080467396447, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 895, Loss: 0.3284983877980607, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 896, Loss: 0.2961155037812352, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 897, Loss: 0.22828842296990104, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 898, Loss: 0.2497187697436107, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 899, Loss: 0.2721506750748259, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 900, Loss: 0.3524859545864617, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 901, Loss: 0.28339282862760584, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 902, Loss: 0.4280991393545531, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 903, Loss: 0.5086429057251065, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 904, Loss: 0.24852283716264703, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 905, Loss: 0.3885792017732478, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 906, Loss: 0.3756441313987949, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 907, Loss: 0.2406257586147386, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 908, Loss: 0.28804791721235634, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 909, Loss: 0.3286802504693451, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 910, Loss: 0.5192105177576805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 911, Loss: 0.3235875297836403, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 912, Loss: 0.4037093891309973, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 913, Loss: 0.265897007788173, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 914, Loss: 0.2398375065641852, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 915, Loss: 0.4086350025271529, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 916, Loss: 0.3401412636865295, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 917, Loss: 0.36327231644903124, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 918, Loss: 0.2349335663820312, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 919, Loss: 0.2881241337379446, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 920, Loss: 0.24038337136357307, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 921, Loss: 0.2656965054768562, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 922, Loss: 0.2671181182010402, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 923, Loss: 0.2626756210733011, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 924, Loss: 0.3033657890506246, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 925, Loss: 0.5936893746216183, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 926, Loss: 0.4858395692242428, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 927, Loss: 0.45095012249482713, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 928, Loss: 0.4178487026935255, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 929, Loss: 0.28030517809024674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 930, Loss: 0.2989288086922346, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 931, Loss: 0.35068235661297875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 932, Loss: 0.4318354973503211, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 933, Loss: 0.23253002164804137, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 934, Loss: 0.30338577041579406, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 935, Loss: 0.3341696610608535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 936, Loss: 0.2592172946992194, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 937, Loss: 0.31622584875571, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 938, Loss: 0.49680436990730514, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 939, Loss: 0.2791795774870834, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 940, Loss: 0.2717430934011491, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 941, Loss: 0.23246234790133433, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 942, Loss: 0.3191394833259671, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 943, Loss: 0.30393906371773316, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 944, Loss: 0.29174097348889383, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 945, Loss: 0.3213722063496441, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 946, Loss: 0.3461642504148079, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 947, Loss: 0.2580963171773346, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 948, Loss: 0.25584991211822994, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 949, Loss: 0.30384592643793257, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 950, Loss: 0.25639304141179875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 951, Loss: 0.28023874811277205, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 952, Loss: 0.37249448786461586, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 953, Loss: 0.3040125494325095, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 954, Loss: 0.2568993595663932, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 955, Loss: 0.28263253699229096, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 956, Loss: 0.25680744466175276, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 957, Loss: 0.27365507625577834, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 958, Loss: 0.29949820678653094, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 959, Loss: 0.35770143028383805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 960, Loss: 0.32037628193051126, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 961, Loss: 0.3307183924169702, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 962, Loss: 0.2640561104958374, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 963, Loss: 0.4226553855111739, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 964, Loss: 0.37743337983716974, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 965, Loss: 0.3577804572020995, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 966, Loss: 0.306608889722226, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 967, Loss: 0.33300542988281506, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 968, Loss: 0.24601283540870258, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 969, Loss: 0.3016909822295692, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 970, Loss: 0.26244075260328864, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 971, Loss: 0.28813292001147683, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 972, Loss: 0.28154735180175305, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 973, Loss: 0.340185067145543, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 974, Loss: 0.4078257664967226, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 975, Loss: 0.23940029658961837, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 976, Loss: 0.37616725460500733, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 977, Loss: 0.25349210850569787, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 978, Loss: 0.31394781018103624, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 979, Loss: 0.2780497771384219, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 980, Loss: 0.2897953238376455, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 981, Loss: 0.3028344216966926, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 982, Loss: 0.2465858891855678, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 983, Loss: 0.23606914408497162, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 984, Loss: 0.3089509403796964, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 985, Loss: 0.3980672389212947, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 986, Loss: 0.5041695194800123, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 987, Loss: 0.3248470636200361, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 988, Loss: 0.32962074893036625, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 989, Loss: 0.29750654347548, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 990, Loss: 0.6201527217121575, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 991, Loss: 0.32535812613147425, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 992, Loss: 0.26986507139685983, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 993, Loss: 0.26623546151591804, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 994, Loss: 0.3106987442965485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 995, Loss: 0.3122093670903836, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 996, Loss: 0.2802754958816754, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 997, Loss: 0.2732650520326754, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 998, Loss: 0.3282269197949693, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 999, Loss: 0.35658207762096883, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1000, Loss: 0.30990194677350014, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1001, Loss: 0.3761898454376119, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1002, Loss: 0.343130340857595, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1003, Loss: 0.23795836305984427, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1004, Loss: 0.24677334375250295, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1005, Loss: 0.4363378168148629, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1006, Loss: 0.2717744496702945, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1007, Loss: 0.327674817879821, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1008, Loss: 0.23797962320003097, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1009, Loss: 0.27427812359888465, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1010, Loss: 0.2488137075863759, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1011, Loss: 0.288469544430234, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1012, Loss: 0.31377015355479765, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1013, Loss: 0.337501781883408, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1014, Loss: 0.2530809094952548, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1015, Loss: 0.3847450437533656, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1016, Loss: 0.24507631571005672, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1017, Loss: 0.33123648080042456, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1018, Loss: 0.31585942177830734, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1019, Loss: 0.3157152375928353, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1020, Loss: 0.2566032808967277, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1021, Loss: 0.3994075043602521, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1022, Loss: 0.36371567453727394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1023, Loss: 0.25647222782835527, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1024, Loss: 0.3470881331993665, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1025, Loss: 0.3778583651602583, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1026, Loss: 0.29169878662657495, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1027, Loss: 0.2709259576695514, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1028, Loss: 0.3396566226275862, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1029, Loss: 0.27180386461606415, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1030, Loss: 0.28025337071583645, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1031, Loss: 0.3281761260129436, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1032, Loss: 0.3428286533187068, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1033, Loss: 0.4324305752954882, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1034, Loss: 0.25136601486668914, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1035, Loss: 0.4542629682446696, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1036, Loss: 0.3928541737612675, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1037, Loss: 0.45170675341303235, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1038, Loss: 0.2998157076512373, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1039, Loss: 0.2753370333831795, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1040, Loss: 0.32996725837984264, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1041, Loss: 0.24103451764658645, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1042, Loss: 0.2637136665090383, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1043, Loss: 0.43457082185513574, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1044, Loss: 0.41238555361553186, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1045, Loss: 0.4628871020499682, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1046, Loss: 0.24043993079280046, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1047, Loss: 0.515432394817014, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1048, Loss: 0.2575665009816046, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1049, Loss: 0.3030446883554928, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1050, Loss: 0.3648744743651679, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1051, Loss: 0.3032699037232798, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1052, Loss: 0.416107484171508, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1053, Loss: 0.29693324119618025, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1054, Loss: 0.31698271651131626, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1055, Loss: 0.2882505512620493, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1056, Loss: 0.2809696010945161, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1057, Loss: 0.2994421676117439, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1058, Loss: 0.3406900123481456, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1059, Loss: 0.3965374023517519, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1060, Loss: 0.25596959232630334, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1061, Loss: 0.43477618908168864, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1062, Loss: 0.3397075764116946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1063, Loss: 0.2812462864178871, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1064, Loss: 0.294698640834808, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1065, Loss: 0.3639203352247131, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1066, Loss: 0.2917652759631253, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1067, Loss: 0.26292941145504173, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1068, Loss: 0.23987219039843563, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1069, Loss: 0.25936478131585583, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1070, Loss: 0.3406555490960472, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1071, Loss: 0.28993512198263766, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1072, Loss: 0.27774786370070537, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1073, Loss: 0.3371268310901952, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1074, Loss: 0.3731504351448253, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1075, Loss: 0.34204817660345715, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1076, Loss: 0.2625679831353253, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1077, Loss: 0.262587442168692, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1078, Loss: 0.26533599766675053, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1079, Loss: 0.35179456259404773, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1080, Loss: 0.24761888016936612, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1081, Loss: 0.30132576810021156, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1082, Loss: 0.2375250656839228, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1083, Loss: 0.27794831803465686, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1084, Loss: 0.3069699490540147, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1085, Loss: 0.4481602688576153, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1086, Loss: 0.32989585675890537, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1087, Loss: 0.5881884408371816, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1088, Loss: 0.4679097803705413, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1089, Loss: 0.2477626459363942, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1090, Loss: 0.39645200646001655, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1091, Loss: 0.2741340602948293, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1092, Loss: 0.45781016342765357, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1093, Loss: 0.2962322327794068, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1094, Loss: 0.7276452366650693, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1095, Loss: 0.26629501844390013, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1096, Loss: 0.4013461711257471, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1097, Loss: 0.3030717060680083, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1098, Loss: 0.35314586787307667, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1099, Loss: 0.24348075119556517, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1100, Loss: 0.25458158921355556, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1101, Loss: 0.4300083327575327, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1102, Loss: 0.35706834787078917, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1103, Loss: 0.405857947574085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1104, Loss: 0.33524252043259806, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1105, Loss: 0.2378023975779635, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1106, Loss: 0.2814285425255852, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1107, Loss: 0.381120633712947, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1108, Loss: 0.2552283738946613, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1109, Loss: 0.25880143444378645, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1110, Loss: 0.23581660037259394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1111, Loss: 0.5226955360671535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1112, Loss: 0.3395040068856191, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1113, Loss: 0.2416151141693492, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1114, Loss: 0.3278974623796131, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1115, Loss: 0.2798558462819961, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1116, Loss: 0.48863678233808394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1117, Loss: 0.315929892016788, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1118, Loss: 0.37645997528111896, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1119, Loss: 0.4844306546356323, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1120, Loss: 0.2933778456652708, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1121, Loss: 0.2779509831870765, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1122, Loss: 0.35579680230642363, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1123, Loss: 0.365424612813533, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1124, Loss: 0.2633129876331964, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1125, Loss: 0.25491199472336123, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1126, Loss: 0.38698671730828477, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1127, Loss: 0.43317770730858346, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1128, Loss: 0.29015425160205366, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1129, Loss: 0.24418932610589814, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1130, Loss: 0.27217836946688734, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1131, Loss: 0.44807609657068836, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1132, Loss: 0.244171398109048, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1133, Loss: 0.3605143889231073, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1134, Loss: 0.32900040456116386, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1135, Loss: 0.38496109713490767, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1136, Loss: 0.33784942205156415, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1137, Loss: 0.7599242824305146, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1138, Loss: 0.31731684431877527, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1139, Loss: 0.2617302644288649, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1140, Loss: 0.29335119066418935, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1141, Loss: 0.36691686800476464, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1142, Loss: 0.4288960585600737, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1143, Loss: 0.29056872833388403, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1144, Loss: 0.34231017445434286, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1145, Loss: 0.35486799089412435, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1146, Loss: 0.4377644377829083, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1147, Loss: 0.2757766254798551, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1148, Loss: 0.2375686912372633, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1149, Loss: 0.3382281710295278, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1150, Loss: 0.4267590457088044, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1151, Loss: 0.3474374665545506, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1152, Loss: 0.3483673064861901, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1153, Loss: 0.26495894668183, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1154, Loss: 0.2425945215243615, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1155, Loss: 0.33016945044745116, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1156, Loss: 0.2805533779335401, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1157, Loss: 0.2610497781740901, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1158, Loss: 0.33564540531715203, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1159, Loss: 0.27930412523620773, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1160, Loss: 0.34854778430132244, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1161, Loss: 0.2623204343777246, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1162, Loss: 0.3764844118550495, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1163, Loss: 0.25438690286149523, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1164, Loss: 0.3378880277106086, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1165, Loss: 0.31022083907667247, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1166, Loss: 0.26677444259272093, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1167, Loss: 0.31238860839460614, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1168, Loss: 0.2657928883539707, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1169, Loss: 0.2581512336618329, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1170, Loss: 0.26829324271238153, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1171, Loss: 0.5347085664100952, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1172, Loss: 0.24907907051944855, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1173, Loss: 0.31360150771281087, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1174, Loss: 0.29785306289527536, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1175, Loss: 0.27373089638812964, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1176, Loss: 0.3120700501917843, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1177, Loss: 0.29998090842360214, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1178, Loss: 0.2719866518289674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1179, Loss: 0.3298137892682369, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1180, Loss: 0.3004734949559718, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1181, Loss: 0.32762162087713864, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1182, Loss: 0.64669197451126, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1183, Loss: 0.2936809537226078, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1184, Loss: 0.4629282356118248, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1185, Loss: 0.2496475962546153, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1186, Loss: 0.24957456293636646, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1187, Loss: 0.2932100204360348, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1188, Loss: 0.4343552703496541, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1189, Loss: 0.2674342054729961, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1190, Loss: 0.26350946588168167, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1191, Loss: 0.4820614316647519, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1192, Loss: 0.36933663493640356, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1193, Loss: 0.39932239211393816, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1194, Loss: 0.44284899109468934, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1195, Loss: 0.3160146753954729, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1196, Loss: 0.3232289973334848, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1197, Loss: 0.2631822505884327, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1198, Loss: 0.35969470967559136, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1199, Loss: 0.30789630634574616, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1200, Loss: 0.3373659580205451, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1201, Loss: 0.23965750266047042, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1202, Loss: 0.23293738643542236, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1203, Loss: 0.3857832521936243, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1204, Loss: 0.2434339136814875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1205, Loss: 0.25830372991768485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1206, Loss: 0.27642563629859046, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1207, Loss: 0.42035728205407064, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1208, Loss: 0.23986570445976851, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1209, Loss: 0.23076682464176432, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1210, Loss: 0.2812086827248814, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1211, Loss: 0.4079439911312872, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1212, Loss: 0.3293859155517747, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1213, Loss: 0.39511188932462205, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1214, Loss: 0.27336305156571367, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1215, Loss: 0.30055256737110453, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1216, Loss: 0.25027375115286277, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1217, Loss: 0.27790189180771263, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1218, Loss: 0.5157450823008282, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1219, Loss: 0.24529298200493163, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1220, Loss: 0.41640576843589516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1221, Loss: 0.5535901778098741, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1222, Loss: 0.43803177213230815, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1223, Loss: 0.2663309574866134, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1224, Loss: 0.2916235118497682, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1225, Loss: 0.2413131943127019, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1226, Loss: 0.3889614809356228, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1227, Loss: 0.36260692371526726, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1228, Loss: 0.23944146370751654, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1229, Loss: 0.28548108948256234, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1230, Loss: 0.24628489410757565, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1231, Loss: 0.2846556102752789, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1232, Loss: 0.2921251246759659, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1233, Loss: 0.3321456539161063, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1234, Loss: 0.3647596578029701, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1235, Loss: 0.3190813741715821, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1236, Loss: 0.34259125359694387, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1237, Loss: 0.2336145480727158, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1238, Loss: 0.3906160877600119, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1239, Loss: 0.39650921685439, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1240, Loss: 0.3513216436805583, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1241, Loss: 0.27559699637833424, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1242, Loss: 0.2814866101896142, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1243, Loss: 0.31417725406285796, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1244, Loss: 0.2356371332268108, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1245, Loss: 0.3548567652696283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1246, Loss: 0.24779804452266324, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1247, Loss: 0.3630439630100717, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1248, Loss: 0.3028219887332406, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1249, Loss: 0.2310871129171299, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1250, Loss: 0.3991960454215085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1251, Loss: 0.24630955320733475, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1252, Loss: 0.34934289167958854, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1253, Loss: 0.28692646306066877, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1254, Loss: 0.3055903782783908, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1255, Loss: 0.2911267284340747, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1256, Loss: 0.3676712523871246, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1257, Loss: 0.27380027076068214, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1258, Loss: 0.23921872018619073, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1259, Loss: 0.39716883397774155, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1260, Loss: 0.32145073086697096, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1261, Loss: 0.28525225947289373, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1262, Loss: 0.3080049616645776, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1263, Loss: 0.4171564179775448, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1264, Loss: 0.24621114803462704, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1265, Loss: 0.3620029288032498, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1266, Loss: 0.2589939646724826, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1267, Loss: 0.24208821793820276, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1268, Loss: 0.3835923454611996, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1269, Loss: 0.3026066299941809, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1270, Loss: 0.2653973851241025, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1271, Loss: 0.28115044667861744, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1272, Loss: 0.30652165999544045, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1273, Loss: 0.2467264377652794, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1274, Loss: 0.5099176769766173, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1275, Loss: 0.28612379816344824, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1276, Loss: 0.41503900022652335, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1277, Loss: 0.2832404939531944, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1278, Loss: 0.30268397667245983, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1279, Loss: 0.3151567533542627, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1280, Loss: 0.4219235104967122, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1281, Loss: 0.28948286534013573, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1282, Loss: 0.2711375837519449, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1283, Loss: 0.2644050169662671, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1284, Loss: 0.39216251813967984, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1285, Loss: 0.31113277910556103, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1286, Loss: 0.4045028762876851, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1287, Loss: 0.327680525572694, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1288, Loss: 0.6314609114286625, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1289, Loss: 0.2361345726417454, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1290, Loss: 0.37017899195761816, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1291, Loss: 0.5389720786779553, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1292, Loss: 0.3543020040832583, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1293, Loss: 0.47156787929965804, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1294, Loss: 0.3889385613456181, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1295, Loss: 0.2691830206673166, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1296, Loss: 0.25110552383527596, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1297, Loss: 0.306101036324674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1298, Loss: 0.398556380376797, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1299, Loss: 0.50885326453235, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1300, Loss: 0.2999816869316925, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1301, Loss: 0.3456263736867083, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1302, Loss: 0.28441786211188724, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1303, Loss: 0.2869506401737132, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1304, Loss: 0.30410471141942386, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1305, Loss: 0.35017050940539507, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1306, Loss: 0.3039464187995281, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1307, Loss: 0.40114768084285346, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1308, Loss: 0.32269689901680115, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1309, Loss: 0.3860030337468682, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1310, Loss: 0.3026493384015909, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1311, Loss: 0.2970384408314454, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1312, Loss: 0.3246752111568065, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1313, Loss: 0.313273237975421, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1314, Loss: 0.5078158410284876, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1315, Loss: 0.2757640626810277, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1316, Loss: 0.3850571175268773, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1317, Loss: 0.2418301997913136, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1318, Loss: 0.26085156070283266, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1319, Loss: 0.2401437031279208, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1320, Loss: 0.23807348036166479, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1321, Loss: 0.5157032946023141, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1322, Loss: 0.38847882016893664, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1323, Loss: 0.2429037125436966, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1324, Loss: 0.5042242769098088, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1325, Loss: 0.2486717337799293, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1326, Loss: 0.33422644917649436, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1327, Loss: 0.34368374256640516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1328, Loss: 0.393129167889495, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1329, Loss: 0.3151686219767871, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1330, Loss: 0.2758305833194453, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1331, Loss: 0.25920580112112185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1332, Loss: 0.3893525032186458, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1333, Loss: 0.35860484805837134, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1334, Loss: 0.3853633671683755, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1335, Loss: 0.2719412539045406, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1336, Loss: 0.7186427771792521, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1337, Loss: 0.31865850251274325, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1338, Loss: 0.39113140518952055, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1339, Loss: 0.504968976567886, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1340, Loss: 0.46516099777993664, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1341, Loss: 0.28018485961161765, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1342, Loss: 0.3117537126790627, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1343, Loss: 0.34604316755741515, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1344, Loss: 0.29594182436661154, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1345, Loss: 0.28345703600319483, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1346, Loss: 0.271360672213224, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1347, Loss: 0.37327029608222845, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1348, Loss: 0.28660983919977406, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1349, Loss: 0.6238507991473968, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1350, Loss: 0.24226733503847064, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1351, Loss: 0.25241901144528395, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1352, Loss: 0.317004144754066, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1353, Loss: 0.3095439836408085, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1354, Loss: 0.2630722596591167, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1355, Loss: 0.2426742995063933, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1356, Loss: 0.23718116070251297, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1357, Loss: 0.2647898091060941, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1358, Loss: 0.3095087038582301, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1359, Loss: 0.26971452895277476, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1360, Loss: 0.26708651819377605, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1361, Loss: 0.3323565283335708, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1362, Loss: 0.35427706448663304, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1363, Loss: 0.32223539429386205, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1364, Loss: 0.3388117015893685, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1365, Loss: 0.26230628299944847, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1366, Loss: 0.2413721797043557, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1367, Loss: 0.277305668849473, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1368, Loss: 0.5663956113130597, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1369, Loss: 0.2994282599853706, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1370, Loss: 0.43292264536192104, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1371, Loss: 0.4574562483039146, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1372, Loss: 0.25854268738515807, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1373, Loss: 0.30666290354708725, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1374, Loss: 0.3406789069096444, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1375, Loss: 0.2820813918117094, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1376, Loss: 0.25071305075368516, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1377, Loss: 0.4552628375392868, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1378, Loss: 0.30057430998011064, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1379, Loss: 0.2547752216401238, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1380, Loss: 0.46170777874912805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1381, Loss: 0.46802409826171265, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1382, Loss: 0.25084091740167214, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1383, Loss: 0.38918586728575166, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1384, Loss: 0.3019106233291361, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1385, Loss: 0.2555432852390255, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1386, Loss: 0.35963955174226203, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1387, Loss: 0.27833497146717695, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1388, Loss: 0.2630789391245487, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1389, Loss: 0.32582531929640135, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1390, Loss: 0.5030429532666383, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1391, Loss: 0.2853649152471392, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1392, Loss: 0.2916470650214803, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1393, Loss: 0.34676100661356546, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1394, Loss: 0.32639405206596295, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1395, Loss: 0.35089935160961244, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1396, Loss: 0.29468584079459914, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1397, Loss: 0.2552140757013447, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1398, Loss: 0.3456349431665394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1399, Loss: 0.44714983306375805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1400, Loss: 0.3864092147390823, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1401, Loss: 0.23729436849930086, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1402, Loss: 0.3594822903513525, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1403, Loss: 0.23787988234081264, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1404, Loss: 0.2588622344825421, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1405, Loss: 0.25034549908359993, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1406, Loss: 0.37494833023359797, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1407, Loss: 0.255545943727912, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1408, Loss: 0.28390410729491755, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1409, Loss: 0.24026048684730206, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1410, Loss: 0.34015590990372846, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1411, Loss: 0.2631678979130697, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1412, Loss: 0.3200778835452315, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1413, Loss: 0.555578258634095, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1414, Loss: 0.27470365225722904, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1415, Loss: 0.2551338302255925, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1416, Loss: 0.47549228186182924, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1417, Loss: 0.24082398450329937, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1418, Loss: 0.236535722905553, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1419, Loss: 0.25833054148131696, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1420, Loss: 0.2531503088500505, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1421, Loss: 0.2855416537317063, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1422, Loss: 0.34209147333115936, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1423, Loss: 0.4121040017524579, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1424, Loss: 0.30877782212337035, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1425, Loss: 0.5927194431304365, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1426, Loss: 0.25037411185922365, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1427, Loss: 0.38506418259848535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1428, Loss: 0.42494573726671936, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1429, Loss: 0.320807875993184, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1430, Loss: 0.28427478395445827, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1431, Loss: 0.2691114715027354, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1432, Loss: 0.5268459156414984, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1433, Loss: 0.3637049549996615, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1434, Loss: 0.2889573256550957, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1435, Loss: 0.30351501101003653, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1436, Loss: 0.293205631540498, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1437, Loss: 0.2894741108744071, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1438, Loss: 0.2476127951265193, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1439, Loss: 0.28933795311402366, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1440, Loss: 0.49562276475442124, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1441, Loss: 0.47441053157335966, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1442, Loss: 0.2854086004936414, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1443, Loss: 0.3913463628844696, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1444, Loss: 0.3171001000408763, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1445, Loss: 0.2710532847581743, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1446, Loss: 0.27105779809792324, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1447, Loss: 0.27520076967225454, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1448, Loss: 0.28109491173190354, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1449, Loss: 0.2787558737159409, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1450, Loss: 0.2845717859542647, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1451, Loss: 0.2575975996044262, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1452, Loss: 0.33876185236354706, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1453, Loss: 0.23551000561865693, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1454, Loss: 0.2412788231450482, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1455, Loss: 0.29276292813785226, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1456, Loss: 0.24698432076861834, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1457, Loss: 0.28905196720701176, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1458, Loss: 0.2574718255793451, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1459, Loss: 0.29096610234408526, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1460, Loss: 0.23455510538580954, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1461, Loss: 0.36742263007541875, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1462, Loss: 0.33594457841527553, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1463, Loss: 0.25880725193340115, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1464, Loss: 0.3261990275581747, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1465, Loss: 0.3035566912759892, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1466, Loss: 0.3189882470408133, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1467, Loss: 0.3489693855058923, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1468, Loss: 0.25289066113686687, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1469, Loss: 0.2755417433851385, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1470, Loss: 0.4981391452080773, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1471, Loss: 0.2610900945738812, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1472, Loss: 0.314140430623729, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1473, Loss: 0.26311792142835333, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1474, Loss: 0.36849618011345664, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1475, Loss: 0.32232873103879334, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1476, Loss: 0.38576265501396667, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1477, Loss: 0.2936791009272107, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1478, Loss: 0.3145863736949739, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1479, Loss: 0.3874593474366779, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1480, Loss: 0.469189178768569, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1481, Loss: 0.3392728600352153, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1482, Loss: 0.346256260123164, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1483, Loss: 0.2781366607813822, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1484, Loss: 0.36786017831114415, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1485, Loss: 0.344130551361589, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1486, Loss: 0.29399787659792603, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1487, Loss: 0.2718527053716861, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1488, Loss: 0.3467742081082201, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1489, Loss: 0.25502884527880876, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1490, Loss: 0.298791619463912, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1491, Loss: 0.2782722096275486, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1492, Loss: 0.2695399774981009, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1493, Loss: 0.25193615671743874, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1494, Loss: 0.32342715314644704, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1495, Loss: 0.2852532889777811, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1496, Loss: 0.27695534663260657, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1497, Loss: 0.5707341407989807, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1498, Loss: 0.3063264449803441, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1499, Loss: 0.37904501289372594, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1500, Loss: 0.33536834883232214, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1501, Loss: 0.33848349372995185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1502, Loss: 0.3861353001972632, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1503, Loss: 0.3227898904543634, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1504, Loss: 0.23353356134466394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1505, Loss: 0.35918769892575775, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1506, Loss: 0.24769628728204576, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1507, Loss: 0.247286494074223, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1508, Loss: 0.31211283655208283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1509, Loss: 0.31703468839411264, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1510, Loss: 0.24757982263242187, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1511, Loss: 0.2928060771790452, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1512, Loss: 0.6593926578796759, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1513, Loss: 0.2809431032108378, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1514, Loss: 0.28412363959490705, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1515, Loss: 0.26417554397496784, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1516, Loss: 0.38909830401366935, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1517, Loss: 0.3394139036582812, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1518, Loss: 0.3512308743621637, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1519, Loss: 0.2695307435021079, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1520, Loss: 0.3725461637698404, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1521, Loss: 0.4103878280885577, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1522, Loss: 0.3357640095650721, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1523, Loss: 0.361981036519618, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1524, Loss: 0.2960574880724304, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1525, Loss: 0.3164158430411185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1526, Loss: 0.2426942525907674, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1527, Loss: 0.3395200722393814, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1528, Loss: 0.3833734768394028, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1529, Loss: 0.2911026400498034, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1530, Loss: 0.2653693527587766, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1531, Loss: 0.24542778104467414, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1532, Loss: 0.2747974098095621, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1533, Loss: 0.4511722187183552, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1534, Loss: 0.2683045176939021, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1535, Loss: 0.25898843062617244, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1536, Loss: 0.2570272827855303, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1537, Loss: 0.3936824374608683, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1538, Loss: 0.26770525183373695, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1539, Loss: 0.24408764385143591, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1540, Loss: 0.3585255832458354, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1541, Loss: 0.34440285058627496, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1542, Loss: 0.27434987881877815, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1543, Loss: 0.2639904895895057, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1544, Loss: 0.32914430291009555, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1545, Loss: 0.29277207133221517, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1546, Loss: 0.23997009270477773, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1547, Loss: 0.35157800822259444, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1548, Loss: 0.3149428958625404, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1549, Loss: 0.3082285520249012, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1550, Loss: 0.2588650769085896, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1551, Loss: 0.6016221499172556, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1552, Loss: 0.243559101917855, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1553, Loss: 0.27828385938303135, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1554, Loss: 0.437995959754971, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1555, Loss: 0.375153012225789, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1556, Loss: 0.2936169408508338, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1557, Loss: 0.26772725267530956, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1558, Loss: 0.24928260097368002, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1559, Loss: 0.31639783634464386, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1560, Loss: 0.4444113560300218, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1561, Loss: 0.2650125370219043, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1562, Loss: 0.47966916581817776, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1563, Loss: 0.27411409861407054, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1564, Loss: 0.27888014134312133, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1565, Loss: 0.23243355845974728, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1566, Loss: 0.2608596042988038, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1567, Loss: 0.280751435416193, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1568, Loss: 0.23694949222065495, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1569, Loss: 0.3766822597590217, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1570, Loss: 0.32487218806624263, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1571, Loss: 0.2404015703125264, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1572, Loss: 0.24264775237114283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1573, Loss: 0.3401413406054704, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1574, Loss: 0.24899338495953283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1575, Loss: 0.3406772783430698, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1576, Loss: 0.3470085265015703, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1577, Loss: 0.2523161662651806, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1578, Loss: 0.3159678877854264, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1579, Loss: 0.5524270731968122, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1580, Loss: 0.356092292377398, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1581, Loss: 0.29120600812294417, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1582, Loss: 0.5106921896884302, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1583, Loss: 0.42031729549373775, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1584, Loss: 0.46108455657708103, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1585, Loss: 0.2417083055204694, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1586, Loss: 0.2436282328895352, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1587, Loss: 0.2590200296462579, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1588, Loss: 0.3008761621115483, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1589, Loss: 0.2440852763023247, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1590, Loss: 0.2994104173268455, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1591, Loss: 0.28183943784272575, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1592, Loss: 0.477271354734442, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1593, Loss: 0.24166817668922486, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1594, Loss: 0.26751478848704613, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1595, Loss: 0.24933679776445944, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1596, Loss: 0.25300705195554213, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1597, Loss: 0.2638764993499242, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1598, Loss: 0.33266263671073315, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1599, Loss: 0.25795999224450766, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1600, Loss: 0.29676848255319654, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1601, Loss: 0.3948332696402946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1602, Loss: 0.2877750499443621, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1603, Loss: 0.40514144244897243, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1604, Loss: 0.3413540185037283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1605, Loss: 0.45453590997541116, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1606, Loss: 0.29643753629359715, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1607, Loss: 0.2720880918072499, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1608, Loss: 0.27634295368538947, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1609, Loss: 0.24201491083525936, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1610, Loss: 0.2622756173553638, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1611, Loss: 0.3160174167184828, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1612, Loss: 0.32789666225466707, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1613, Loss: 0.3301602006768979, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1614, Loss: 0.31575499095564624, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1615, Loss: 0.33627083194699015, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1616, Loss: 0.5106459984354303, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1617, Loss: 0.24685861408572002, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1618, Loss: 0.2771917395489463, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1619, Loss: 0.35339590186782077, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1620, Loss: 0.2922075738295414, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1621, Loss: 0.2512217789270186, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1622, Loss: 0.3450126857643874, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1623, Loss: 0.23790856487159756, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1624, Loss: 0.35636211053844036, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1625, Loss: 0.31066918729313675, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1626, Loss: 0.26520367193249433, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1627, Loss: 0.40671239114103863, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1628, Loss: 0.2835774495918195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1629, Loss: 0.2548893891121687, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1630, Loss: 0.27034559204445374, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1631, Loss: 0.23906431618968188, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1632, Loss: 0.2817906228100853, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1633, Loss: 0.3191538601398195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1634, Loss: 0.4082204483180568, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1635, Loss: 0.2889135719610198, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1636, Loss: 0.3255533904625191, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1637, Loss: 0.28640751992119196, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1638, Loss: 0.29463277514646385, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1639, Loss: 0.2633571966237324, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1640, Loss: 0.3106883080086903, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1641, Loss: 0.34812227506988946, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1642, Loss: 0.28905972089443394, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1643, Loss: 0.31250351086495776, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1644, Loss: 0.37144377147895535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1645, Loss: 0.3037626418101961, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1646, Loss: 0.28374169855417214, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1647, Loss: 0.3041772874158037, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1648, Loss: 0.3002863914870063, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1649, Loss: 0.2957148270634913, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1650, Loss: 0.5243691670390712, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1651, Loss: 0.3974093350224606, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1652, Loss: 0.25932760603626004, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1653, Loss: 0.25901774938786143, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1654, Loss: 0.3292895396519076, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1655, Loss: 0.40180080525842937, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1656, Loss: 0.34221951145118334, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1657, Loss: 0.3147166937198076, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1658, Loss: 0.27349869516155706, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1659, Loss: 0.31424199535169817, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1660, Loss: 0.2723994457783221, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1661, Loss: 0.3279956446121975, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1662, Loss: 0.32445513942213355, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1663, Loss: 0.25710975983367385, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1664, Loss: 0.2745991066368557, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1665, Loss: 0.28024017989440586, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1666, Loss: 0.42893193507443134, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1667, Loss: 0.28013070678537355, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1668, Loss: 0.5319575389039084, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1669, Loss: 0.2745456335917071, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1670, Loss: 0.24285342494353857, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1671, Loss: 0.36978117492385865, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1672, Loss: 0.3827597024720459, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1673, Loss: 0.27594619770734485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1674, Loss: 0.4355518976276648, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1675, Loss: 0.32160834954855505, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1676, Loss: 0.4211619992357694, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1677, Loss: 0.34085020666597055, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1678, Loss: 0.3239703615890125, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1679, Loss: 0.23732985104049198, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1680, Loss: 0.3489043330749043, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1681, Loss: 0.32149210672768136, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1682, Loss: 0.24940028913418372, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1683, Loss: 0.32102684518536717, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1684, Loss: 0.2628284716250341, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1685, Loss: 0.38336708697762356, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1686, Loss: 0.5552367062442043, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1687, Loss: 0.2920916963104676, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1688, Loss: 0.23935196549780716, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1689, Loss: 0.5990889715079114, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1690, Loss: 0.30952812571077026, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1691, Loss: 0.3115376789761851, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1692, Loss: 0.4449181657511997, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1693, Loss: 0.24129178780308155, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1694, Loss: 0.3480111187902418, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1695, Loss: 0.24518421347426997, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1696, Loss: 0.418374592344536, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1697, Loss: 0.40090173797901485, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1698, Loss: 0.2819184874949725, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1699, Loss: 0.29720328374865324, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1700, Loss: 0.32219085075825044, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1701, Loss: 0.27790550339306663, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1702, Loss: 0.24618631550657244, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1703, Loss: 0.2486955544778744, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1704, Loss: 0.30666923860349227, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1705, Loss: 0.26537227294777976, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1706, Loss: 0.3479004156943668, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1707, Loss: 0.4243739521140185, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1708, Loss: 0.3556776035319539, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1709, Loss: 0.23734265852891923, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1710, Loss: 0.23661033737521553, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1711, Loss: 0.3477227496090284, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1712, Loss: 0.403741913259432, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1713, Loss: 0.3432005767261721, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1714, Loss: 0.32805354206887893, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1715, Loss: 0.3201498369373186, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1716, Loss: 0.38364136041947916, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1717, Loss: 0.24159362675317575, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1718, Loss: 0.47046365452811834, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1719, Loss: 0.33209753959016075, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1720, Loss: 0.3669133806361833, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1721, Loss: 0.3320909428881495, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1722, Loss: 0.3515558270733585, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1723, Loss: 0.3825955725468343, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1724, Loss: 0.2489260972765263, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1725, Loss: 0.3805382650934801, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1726, Loss: 0.2750920726241654, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1727, Loss: 0.44343168802170535, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1728, Loss: 0.3595418928868521, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1729, Loss: 0.4201409108235682, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1730, Loss: 0.3550209028731441, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1731, Loss: 0.25281208186371684, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1732, Loss: 0.2986495843101596, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1733, Loss: 0.4893800045800014, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1734, Loss: 0.233934452213294, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1735, Loss: 0.2748915799631794, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1736, Loss: 0.3372513221438403, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1737, Loss: 0.30444150306254675, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1738, Loss: 0.22976712779586816, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1739, Loss: 0.2711468140099684, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1740, Loss: 0.2643695629285859, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1741, Loss: 0.3146011948294684, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1742, Loss: 0.24199673555818502, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1743, Loss: 0.3567883658821279, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1744, Loss: 0.5055761753564957, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1745, Loss: 0.34722329284846487, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1746, Loss: 0.2859017079257133, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1747, Loss: 0.30882365868413275, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1748, Loss: 0.3533578015942497, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1749, Loss: 0.2651835419633031, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1750, Loss: 0.41099862720578956, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1751, Loss: 0.27932781974429777, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1752, Loss: 0.2967961729267229, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1753, Loss: 0.24260383627167345, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1754, Loss: 0.29483007975604847, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1755, Loss: 0.2366625762765428, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1756, Loss: 0.29658204581455305, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1757, Loss: 0.3711229105146924, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1758, Loss: 0.2645997543799044, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1759, Loss: 0.3509164396200921, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1760, Loss: 0.3709600657936847, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1761, Loss: 0.26094143397796804, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1762, Loss: 0.43791773490262553, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1763, Loss: 0.40807007006180984, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1764, Loss: 0.33500216641385416, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1765, Loss: 0.2837385991519803, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1766, Loss: 0.2391548385067475, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1767, Loss: 0.3553599134588809, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1768, Loss: 0.29195534520644567, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1769, Loss: 0.413917890498862, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1770, Loss: 0.520721825169131, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1771, Loss: 0.25850034364210694, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1772, Loss: 0.3660403270584833, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1773, Loss: 0.391342453635283, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1774, Loss: 0.25188302835080195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1775, Loss: 0.27425184927609425, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1776, Loss: 0.2542180769284731, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1777, Loss: 0.3367183036214582, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1778, Loss: 0.28320227425692956, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1779, Loss: 0.25552708472892915, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1780, Loss: 0.23107931576122712, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1781, Loss: 0.2782359549982273, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1782, Loss: 0.5293698434858058, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1783, Loss: 0.35589912880928043, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1784, Loss: 0.24659578025629186, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1785, Loss: 0.27644476319813055, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1786, Loss: 0.2438820875902969, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1787, Loss: 0.24106384798866617, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1788, Loss: 0.2908941540410986, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1789, Loss: 0.2517101533976483, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1790, Loss: 0.3638277264098847, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1791, Loss: 0.2555263119810667, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1792, Loss: 0.23396569566041855, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1793, Loss: 0.31468925467599956, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1794, Loss: 0.35245832063785787, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1795, Loss: 0.2530796496197407, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1796, Loss: 0.49741295779381234, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1797, Loss: 0.24827832204138764, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1798, Loss: 0.25229239763221073, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1799, Loss: 0.3870136437512203, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1800, Loss: 0.3188979308533724, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1801, Loss: 0.2598760900200893, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1802, Loss: 0.3523620903695901, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1803, Loss: 0.2347783986578278, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1804, Loss: 0.35867950347012933, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1805, Loss: 0.2584469247509607, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1806, Loss: 0.339768026393512, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1807, Loss: 0.5208741758030018, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1808, Loss: 0.42682450110323256, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1809, Loss: 0.2629499257283317, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1810, Loss: 0.2785907449917487, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1811, Loss: 0.2424851623794972, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1812, Loss: 0.4228778693627434, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1813, Loss: 0.2516937703312869, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1814, Loss: 0.30902469697414303, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1815, Loss: 0.2589012192576058, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1816, Loss: 0.25837391470367443, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1817, Loss: 0.27523940661331336, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1818, Loss: 0.4093402228998376, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1819, Loss: 0.24896669962778384, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1820, Loss: 0.33242159579712893, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1821, Loss: 0.2813903603809029, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1822, Loss: 0.2897188392104338, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1823, Loss: 0.2780911421024588, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1824, Loss: 0.27991897919098613, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1825, Loss: 0.32269574464993606, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1826, Loss: 0.4458707149490698, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1827, Loss: 0.25651259487437905, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1828, Loss: 0.3893401657079252, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1829, Loss: 0.43581149515546264, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1830, Loss: 0.30678533733280355, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1831, Loss: 0.32096358855567947, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1832, Loss: 0.28888768718633195, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1833, Loss: 0.27572876341956337, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1834, Loss: 0.2564084681399444, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1835, Loss: 0.29751244993371506, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1836, Loss: 0.25315296523295805, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1837, Loss: 0.2621138446806698, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1838, Loss: 0.2712990648969445, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1839, Loss: 0.24127056674966776, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1840, Loss: 0.38170679043878697, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1841, Loss: 0.32481892866352613, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1842, Loss: 0.32586033120873725, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1843, Loss: 0.23947085968579662, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1844, Loss: 0.2647724968057831, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1845, Loss: 0.24112555956171855, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1846, Loss: 0.27724210201440597, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1847, Loss: 0.28876843488306203, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1848, Loss: 0.2604488250202988, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1849, Loss: 0.47451130605365177, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1850, Loss: 0.2990180567852633, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1851, Loss: 0.44203227798370703, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1852, Loss: 0.4196591611716561, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1853, Loss: 0.2788358345387547, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1854, Loss: 0.33337161419052636, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1855, Loss: 0.2963251671215221, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1856, Loss: 0.35788198548555816, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1857, Loss: 0.40396322904826343, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1858, Loss: 0.2457139006784124, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1859, Loss: 0.35841486505266873, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1860, Loss: 0.5109945977705767, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1861, Loss: 0.4791599087181717, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1862, Loss: 0.2528767343742383, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1863, Loss: 0.4562121090113124, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1864, Loss: 0.29778942609290404, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1865, Loss: 0.41301212322057096, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1866, Loss: 0.24604609522169735, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1867, Loss: 0.4405121245631226, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1868, Loss: 0.27910479894742235, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1869, Loss: 0.24831448941766796, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1870, Loss: 0.3065530498586397, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1871, Loss: 0.2853889102761548, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1872, Loss: 0.296185467539544, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1873, Loss: 0.27875465014350903, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1874, Loss: 0.24803410247344157, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Batch 1875, Loss: 0.3916489443505169, Batch Size: 32, Learning Rate: 1.8934026990196294e-05\n",
      "Epoch 19, Updated Learning Rate: 1.609392294166685e-05\n",
      "Epoch 19, Average Loss: 0.3234545572369945, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1, Loss: 0.6209666551785404, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 2, Loss: 0.4366625423026702, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 3, Loss: 0.27174925070919986, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 4, Loss: 0.5595221906247063, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 5, Loss: 0.3883955364167183, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 6, Loss: 0.3337194700913556, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 7, Loss: 0.2788661006974723, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 8, Loss: 0.4452458680763419, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 9, Loss: 0.24652298002846842, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 10, Loss: 0.36318785949813975, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 11, Loss: 0.2562826517809749, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 12, Loss: 0.5808810962619204, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 13, Loss: 0.29195875930741444, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 14, Loss: 0.4003703062189001, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 15, Loss: 0.25806273501501625, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 16, Loss: 0.5073188362479552, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 17, Loss: 0.3447151751458941, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 18, Loss: 0.26718343052178756, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 19, Loss: 0.2682635349215837, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 20, Loss: 0.27457100854161526, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 21, Loss: 0.576893145831819, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 22, Loss: 0.3768047580855465, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 23, Loss: 0.34884895338354144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 24, Loss: 0.3516627742323068, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 25, Loss: 0.43449482058510724, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 26, Loss: 0.2589557309298518, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 27, Loss: 0.27077950367143494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 28, Loss: 0.25059347204344296, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 29, Loss: 0.5843815314897381, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 30, Loss: 0.39101696279434756, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 31, Loss: 0.26527998356420085, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 32, Loss: 0.2554785878528319, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 33, Loss: 0.31886298565543963, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 34, Loss: 0.24034582121138773, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 35, Loss: 0.24157103236155947, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 36, Loss: 0.2590348067971586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 37, Loss: 0.24573455514823694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 38, Loss: 0.3566950664084527, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 39, Loss: 0.24221762268471758, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 40, Loss: 0.4361518851425279, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 41, Loss: 0.2695042992959751, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 42, Loss: 0.22837377544148546, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 43, Loss: 0.5919117466983722, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 44, Loss: 0.3489278205468619, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 45, Loss: 0.24227989105151654, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 46, Loss: 0.4223999908675672, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 47, Loss: 0.2869945659379026, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 48, Loss: 0.2547483802832326, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 49, Loss: 0.5285824905708261, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 50, Loss: 0.2976166880863619, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 51, Loss: 0.42733961367275597, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 52, Loss: 0.23491536713980707, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 53, Loss: 0.324139245321215, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 54, Loss: 0.2909720695562824, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 55, Loss: 0.2648645752163058, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 56, Loss: 0.34388291486157607, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 57, Loss: 0.41531855183688426, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 58, Loss: 0.3073284974070472, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 59, Loss: 0.26581780691523044, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 60, Loss: 0.3453998678391496, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 61, Loss: 0.24873815657907955, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 62, Loss: 0.33363746787049914, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 63, Loss: 0.30442407801690186, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 64, Loss: 0.272697146111595, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 65, Loss: 0.3474974184236217, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 66, Loss: 0.3308556969453256, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 67, Loss: 0.4233822599542168, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 68, Loss: 0.4003737367419903, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 69, Loss: 0.2998757436667313, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 70, Loss: 0.586293605316524, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 71, Loss: 0.5070831898325343, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 72, Loss: 0.23380296080419913, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 73, Loss: 0.23128258835862212, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 74, Loss: 0.3849842228152036, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 75, Loss: 0.29737274055194296, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 76, Loss: 0.25957938289735855, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 77, Loss: 0.25766184032369677, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 78, Loss: 0.4167862023229748, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 79, Loss: 0.26670794720524654, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 80, Loss: 0.42493952958960673, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 81, Loss: 0.3867557651576562, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 82, Loss: 0.5428790853285956, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 83, Loss: 0.24034201639106126, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 84, Loss: 0.26808264808728804, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 85, Loss: 0.2831724226480694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 86, Loss: 0.29168284190089977, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 87, Loss: 0.30460505968851725, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 88, Loss: 0.26279019098737166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 89, Loss: 0.24809691348852694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 90, Loss: 0.2638778052734206, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 91, Loss: 0.25019225770900827, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 92, Loss: 0.2531190147343813, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 93, Loss: 0.3246962043637897, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 94, Loss: 0.24039653659686178, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 95, Loss: 0.6593174515689861, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 96, Loss: 0.4644329999682844, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 97, Loss: 0.27312883194846665, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 98, Loss: 0.318297764056298, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 99, Loss: 0.31344692725564144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 100, Loss: 0.37364480328855865, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 101, Loss: 0.30211251824936536, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 102, Loss: 0.26539349598517664, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 103, Loss: 0.2850523757764959, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 104, Loss: 0.26443702353103166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 105, Loss: 0.26164912935804946, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 106, Loss: 0.24990724683718807, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 107, Loss: 0.25914192642279404, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 108, Loss: 0.24340560210811418, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 109, Loss: 0.35160934728077364, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 110, Loss: 0.2381895750959978, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 111, Loss: 0.3587816318048209, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 112, Loss: 0.298728476777653, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 113, Loss: 0.28811184225293923, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 114, Loss: 0.33530571575305, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 115, Loss: 0.28179686808151255, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 116, Loss: 0.3691325403803109, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 117, Loss: 0.4472268816320314, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 118, Loss: 0.28451666923132785, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 119, Loss: 0.3219576914900928, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 120, Loss: 0.30944469092515803, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 121, Loss: 0.28284243914950113, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 122, Loss: 0.3022644664732143, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 123, Loss: 0.3229145087056221, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 124, Loss: 0.24724627242445008, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 125, Loss: 0.3596509550477968, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 126, Loss: 0.23604783605128335, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 127, Loss: 0.31447816574893417, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 128, Loss: 0.3368242462332699, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 129, Loss: 0.379838208938836, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 130, Loss: 0.2643692625532495, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 131, Loss: 0.24987937740717137, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 132, Loss: 0.33153847905562883, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 133, Loss: 0.49674516083879994, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 134, Loss: 0.3280477778436436, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 135, Loss: 0.3690345331272603, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 136, Loss: 0.503649520931381, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 137, Loss: 0.2492712904758739, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 138, Loss: 0.2763242596940385, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 139, Loss: 0.2554716082624347, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 140, Loss: 0.36414148206574626, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 141, Loss: 0.2926107602850406, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 142, Loss: 0.2490046714832043, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 143, Loss: 0.37183539849091174, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 144, Loss: 0.31579524416728744, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 145, Loss: 0.2573518374059985, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 146, Loss: 0.280623127916407, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 147, Loss: 0.38039534489284504, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 148, Loss: 0.283299152734191, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 149, Loss: 0.26380872049379156, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 150, Loss: 0.27953164492868615, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 151, Loss: 0.362469912402348, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 152, Loss: 0.34755775834858266, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 153, Loss: 0.26513987945209494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 154, Loss: 0.3083448494243969, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 155, Loss: 0.29135672711092975, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 156, Loss: 0.31485897290454384, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 157, Loss: 0.25498425466538793, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 158, Loss: 0.2437263574526723, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 159, Loss: 0.23399038059411068, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 160, Loss: 0.37495122455075913, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 161, Loss: 0.29948927442596024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 162, Loss: 0.25971867669974863, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 163, Loss: 0.26428860817374245, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 164, Loss: 0.3355863216933581, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 165, Loss: 0.23134366627046654, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 166, Loss: 0.3079818445475403, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 167, Loss: 0.2509999658382482, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 168, Loss: 0.35233568094101275, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 169, Loss: 0.2755282910287931, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 170, Loss: 0.2666593759644817, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 171, Loss: 0.4016360294629543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 172, Loss: 0.2502409486823448, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 173, Loss: 0.2990209136957359, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 174, Loss: 0.2840828810715666, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 175, Loss: 0.25092819978815173, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 176, Loss: 0.2437104838680277, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 177, Loss: 0.3112400105945453, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 178, Loss: 0.2656219057521419, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 179, Loss: 0.2683656603324735, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 180, Loss: 0.2695454205596032, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 181, Loss: 0.33387751526964676, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 182, Loss: 0.2688396155027115, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 183, Loss: 0.2609379107372098, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 184, Loss: 0.29090938395174915, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 185, Loss: 0.2683778405787607, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 186, Loss: 0.25199042479030737, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 187, Loss: 0.4189381978489899, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 188, Loss: 0.2511538113934853, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 189, Loss: 0.3047988274234518, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 190, Loss: 0.2908755622917909, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 191, Loss: 0.29491292333427777, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 192, Loss: 0.39002571157239413, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 193, Loss: 0.26886851193141115, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 194, Loss: 0.3070662353957489, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 195, Loss: 0.24829066784736256, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 196, Loss: 0.32293216887996834, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 197, Loss: 0.2928526934444477, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 198, Loss: 0.25169556659067543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 199, Loss: 0.5096643730555341, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 200, Loss: 0.28590162613339404, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 201, Loss: 0.2733467803699851, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 202, Loss: 0.33014452782804926, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 203, Loss: 0.2894832688476099, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 204, Loss: 0.3337947442675264, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 205, Loss: 0.24031632225116387, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 206, Loss: 0.2554497270007683, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 207, Loss: 0.4432090426636873, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 208, Loss: 0.2930728435687975, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 209, Loss: 0.4434382708278016, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 210, Loss: 0.2699503571257962, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 211, Loss: 0.2594125182608135, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 212, Loss: 0.34099766805863974, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 213, Loss: 0.2510617691188037, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 214, Loss: 0.43213900283028317, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 215, Loss: 0.3252524190383951, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 216, Loss: 0.42183901578994176, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 217, Loss: 0.2991205260575678, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 218, Loss: 0.3421133800275332, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 219, Loss: 0.24356772070759553, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 220, Loss: 0.2548251419928282, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 221, Loss: 0.2475027570777136, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 222, Loss: 0.29856438128238044, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 223, Loss: 0.3128991695632642, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 224, Loss: 0.26273427801180727, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 225, Loss: 0.372391071978878, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 226, Loss: 0.4506643749433922, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 227, Loss: 0.27144517551260616, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 228, Loss: 0.4118509337047035, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 229, Loss: 0.33593856511677467, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 230, Loss: 0.2920565742815563, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 231, Loss: 0.34622082334696547, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 232, Loss: 0.609768803607529, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 233, Loss: 0.28683685515845814, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 234, Loss: 0.29205288290133724, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 235, Loss: 0.4127317183002117, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 236, Loss: 0.2550473837190232, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 237, Loss: 0.32770801508324116, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 238, Loss: 0.2427092499535635, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 239, Loss: 0.28778876050798163, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 240, Loss: 0.2642732343772709, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 241, Loss: 0.23621754371475054, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 242, Loss: 0.2639315647141751, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 243, Loss: 0.27348547976678006, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 244, Loss: 0.26430082600030913, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 245, Loss: 0.3669273248826784, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 246, Loss: 0.33780062744076345, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 247, Loss: 0.26704283315437594, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 248, Loss: 0.27438010062773266, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 249, Loss: 0.36354314863591103, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 250, Loss: 0.5269280982779004, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 251, Loss: 0.3271208094804422, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 252, Loss: 0.41851037883869613, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 253, Loss: 0.23789633614244415, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 254, Loss: 0.2329440333705116, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 255, Loss: 0.25120049181883364, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 256, Loss: 0.2606873367477352, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 257, Loss: 0.47285935946281815, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 258, Loss: 0.29985959307720866, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 259, Loss: 0.2708457193334808, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 260, Loss: 0.24541854620459252, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 261, Loss: 0.33201628009329154, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 262, Loss: 0.28148979380875505, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 263, Loss: 0.2937982025029392, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 264, Loss: 0.3241237467539893, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 265, Loss: 0.3092328313466695, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 266, Loss: 0.3024032850436661, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 267, Loss: 0.2499791168449105, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 268, Loss: 0.25238008135527645, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 269, Loss: 0.3944813574770376, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 270, Loss: 0.2716443402857998, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 271, Loss: 0.386904121409815, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 272, Loss: 0.28223995757710657, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 273, Loss: 0.2721887951824663, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 274, Loss: 0.2617417695593429, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 275, Loss: 0.29874106095854824, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 276, Loss: 0.2848585732647942, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 277, Loss: 0.239704751637082, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 278, Loss: 0.43396231016070463, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 279, Loss: 0.287562309880774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 280, Loss: 0.2930920412059038, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 281, Loss: 0.27334050073426874, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 282, Loss: 0.29629397953983805, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 283, Loss: 0.4062650371440525, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 284, Loss: 0.3269094700444001, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 285, Loss: 0.24667281574166575, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 286, Loss: 0.25591320640706244, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 287, Loss: 0.2533370851976013, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 288, Loss: 0.5341621401315763, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 289, Loss: 0.4178371009691952, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 290, Loss: 0.29231996818309885, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 291, Loss: 0.33267373846338494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 292, Loss: 0.29419044822622853, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 293, Loss: 0.3452229348574032, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 294, Loss: 0.37125322077266976, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 295, Loss: 0.28001436460055185, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 296, Loss: 0.2692570642878186, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 297, Loss: 0.28326364378683033, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 298, Loss: 0.273564280278388, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 299, Loss: 0.3047357833445873, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 300, Loss: 0.2617309127223172, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 301, Loss: 0.4343231017737974, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 302, Loss: 0.2477134873028205, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 303, Loss: 0.31124832627805366, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 304, Loss: 0.3423347913679074, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 305, Loss: 0.239912992026889, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 306, Loss: 0.353672838398331, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 307, Loss: 0.37920383629594656, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 308, Loss: 0.2588009007254173, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 309, Loss: 0.36542297547291275, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 310, Loss: 0.30841106735129, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 311, Loss: 0.2719158776286536, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 312, Loss: 0.25991548581214957, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 313, Loss: 0.3737386674830076, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 314, Loss: 0.29065109184430105, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 315, Loss: 0.3153425309473542, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 316, Loss: 0.23782158840094286, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 317, Loss: 0.45105376254611584, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 318, Loss: 0.26878071215219684, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 319, Loss: 0.2702328153142014, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 320, Loss: 0.2853143936594181, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 321, Loss: 0.42242347889088816, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 322, Loss: 0.2783102576440914, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 323, Loss: 0.26214446142850983, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 324, Loss: 0.32155164971236505, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 325, Loss: 0.29821498745884734, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 326, Loss: 0.24985792640817767, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 327, Loss: 0.25785041995538366, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 328, Loss: 0.24690082722278386, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 329, Loss: 0.2762158289717399, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 330, Loss: 0.2395604237916968, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 331, Loss: 0.4004039235525294, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 332, Loss: 0.36321863130824505, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 333, Loss: 0.27467756539968324, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 334, Loss: 0.2788690282449557, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 335, Loss: 0.38058057424269687, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 336, Loss: 0.2643298542389489, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 337, Loss: 0.3013715936141034, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 338, Loss: 0.34677524619026134, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 339, Loss: 0.3841992825590762, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 340, Loss: 0.2515926780227958, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 341, Loss: 0.519386674617317, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 342, Loss: 0.27910554773131224, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 343, Loss: 0.28948559404893875, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 344, Loss: 0.2505861070518206, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 345, Loss: 0.2617301317466692, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 346, Loss: 0.28946266311899194, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 347, Loss: 0.351565774289072, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 348, Loss: 0.25766037071262965, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 349, Loss: 0.33737076130182664, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 350, Loss: 0.4579265656244854, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 351, Loss: 0.3222963721929281, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 352, Loss: 0.24297392100120102, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 353, Loss: 0.27295430271329507, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 354, Loss: 0.2453827406045098, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 355, Loss: 0.2450029955097096, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 356, Loss: 0.33646034723388407, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 357, Loss: 0.24301847769028814, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 358, Loss: 0.25679863579873496, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 359, Loss: 0.3380910890186586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 360, Loss: 0.2464036889881136, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 361, Loss: 0.28602727416011575, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 362, Loss: 0.4307999352955531, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 363, Loss: 0.3963300029109323, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 364, Loss: 0.27578082372541646, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 365, Loss: 0.26976742308128476, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 366, Loss: 0.2627321023103113, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 367, Loss: 0.33103389538808925, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 368, Loss: 0.24451131375104895, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 369, Loss: 0.5341290438527457, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 370, Loss: 0.2521290871014355, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 371, Loss: 0.45897647840917555, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 372, Loss: 0.4651824236967974, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 373, Loss: 0.43318873968678584, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 374, Loss: 0.2464941464381707, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 375, Loss: 0.25596899101123494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 376, Loss: 0.23828892214173478, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 377, Loss: 0.4877211215029236, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 378, Loss: 0.24098565927918023, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 379, Loss: 0.2780952557115441, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 380, Loss: 0.31887364976969235, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 381, Loss: 0.23367872960647304, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 382, Loss: 0.4433144441276585, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 383, Loss: 0.5533521057491353, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 384, Loss: 0.31403360876841424, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 385, Loss: 0.28248412928382033, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 386, Loss: 0.2848869905690906, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 387, Loss: 0.321237634715336, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 388, Loss: 0.5603034485179079, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 389, Loss: 0.4623213575745242, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 390, Loss: 0.27039225353075247, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 391, Loss: 0.32381396678773106, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 392, Loss: 0.23386548781768388, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 393, Loss: 0.2495141392519119, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 394, Loss: 0.2749500194217095, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 395, Loss: 0.270738425854706, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 396, Loss: 0.32167645670380823, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 397, Loss: 0.3000637283350815, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 398, Loss: 0.231746955013543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 399, Loss: 0.2857593672863278, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 400, Loss: 0.2895020819178027, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 401, Loss: 0.2512656993980906, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 402, Loss: 0.25634894335659464, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 403, Loss: 0.2978543064556337, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 404, Loss: 0.2710962080959077, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 405, Loss: 0.36323525600821904, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 406, Loss: 0.4317007295226454, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 407, Loss: 0.25432649318781775, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 408, Loss: 0.2629599396242396, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 409, Loss: 0.2516679065668482, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 410, Loss: 0.3930798087542473, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 411, Loss: 0.3965186313643658, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 412, Loss: 0.3131623793676048, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 413, Loss: 0.45673641696679657, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 414, Loss: 0.7014141292645046, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 415, Loss: 0.39391509232212824, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 416, Loss: 0.3181837450646857, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 417, Loss: 0.2557848848883298, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 418, Loss: 0.25859853309428327, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 419, Loss: 0.422228137659316, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 420, Loss: 0.4583358114170064, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 421, Loss: 0.32439654161249876, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 422, Loss: 0.31349510241149287, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 423, Loss: 0.29029211591248855, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 424, Loss: 0.3074178407507755, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 425, Loss: 0.2713337973624599, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 426, Loss: 0.322632524202669, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 427, Loss: 0.2631056896565277, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 428, Loss: 0.3554887368370181, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 429, Loss: 0.35699795577590404, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 430, Loss: 0.36232299227669607, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 431, Loss: 0.3139280997476055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 432, Loss: 0.3585639148182144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 433, Loss: 0.265598806803469, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 434, Loss: 0.2753121417639171, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 435, Loss: 0.26046828017923124, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 436, Loss: 0.29673943058084623, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 437, Loss: 0.2956669405273342, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 438, Loss: 0.3142203440592519, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 439, Loss: 0.24150538326324006, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 440, Loss: 0.2503664273065594, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 441, Loss: 0.27507995006883046, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 442, Loss: 0.24336230103489606, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 443, Loss: 0.35035108280490285, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 444, Loss: 0.2454181120605657, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 445, Loss: 0.23649157198087822, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 446, Loss: 0.5773468559252906, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 447, Loss: 0.3200914680167428, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 448, Loss: 0.26927887575610565, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 449, Loss: 0.26161130458636084, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 450, Loss: 0.5491865764100794, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 451, Loss: 0.36469800500304306, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 452, Loss: 0.26799953794706743, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 453, Loss: 0.24460564083268632, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 454, Loss: 0.3481203537769719, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 455, Loss: 0.2583744623375245, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 456, Loss: 0.26065786991637113, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 457, Loss: 0.34618821709014114, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 458, Loss: 0.2914007976993905, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 459, Loss: 0.3387677817718838, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 460, Loss: 0.3091247321471273, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 461, Loss: 0.26430850872924655, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 462, Loss: 0.3248020367299025, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 463, Loss: 0.2409751678161377, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 464, Loss: 0.27139063227266247, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 465, Loss: 0.44762589936098585, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 466, Loss: 0.24875798582292394, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 467, Loss: 0.3802854616513427, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 468, Loss: 0.30009034558736786, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 469, Loss: 0.2499805678808764, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 470, Loss: 0.3180195780112403, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 471, Loss: 0.25998635168667716, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 472, Loss: 0.4371114192259141, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 473, Loss: 0.48033402285087123, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 474, Loss: 0.44310803026469936, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 475, Loss: 0.25257180513329225, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 476, Loss: 0.2730636621929408, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 477, Loss: 0.3746163407948727, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 478, Loss: 0.44861426829900597, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 479, Loss: 0.37935469022004403, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 480, Loss: 0.26566171774885083, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 481, Loss: 0.3279154830323633, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 482, Loss: 0.27900165512391734, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 483, Loss: 0.4305906456980231, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 484, Loss: 0.22990273653183535, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 485, Loss: 0.6181776464472137, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 486, Loss: 0.23774449108997417, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 487, Loss: 0.2544216760018388, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 488, Loss: 0.5983544404351688, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 489, Loss: 0.2941921399487083, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 490, Loss: 0.2613152608254853, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 491, Loss: 0.3065614843974942, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 492, Loss: 0.3500406794456298, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 493, Loss: 0.3398405982821306, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 494, Loss: 0.321237596933123, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 495, Loss: 0.4126807729194645, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 496, Loss: 0.24992199604680518, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 497, Loss: 0.2748311256546149, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 498, Loss: 0.2681209399682042, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 499, Loss: 0.3089608905080821, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 500, Loss: 0.41142320998212123, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 501, Loss: 0.2516224588952969, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 502, Loss: 0.2800309049634161, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 503, Loss: 0.43923519547637396, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 504, Loss: 0.3210146243977048, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 505, Loss: 0.3316863054579188, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 506, Loss: 0.30275708962412906, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 507, Loss: 0.31335611047878714, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 508, Loss: 0.35526898938620327, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 509, Loss: 0.3424162152622972, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 510, Loss: 0.37839119376310276, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 511, Loss: 0.31270171798219665, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 512, Loss: 0.29627898240568235, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 513, Loss: 0.2906857587122907, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 514, Loss: 0.37999605890409793, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 515, Loss: 0.26572991353092024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 516, Loss: 0.2575442542171492, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 517, Loss: 0.2768110967948921, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 518, Loss: 0.37171896261031523, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 519, Loss: 0.24808676059952783, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 520, Loss: 0.3148735468185065, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 521, Loss: 0.35818282991420747, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 522, Loss: 0.3286352367879999, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 523, Loss: 0.23926867633025492, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 524, Loss: 0.3140334788977358, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 525, Loss: 0.2471466912149976, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 526, Loss: 0.3624731082207763, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 527, Loss: 0.3050753318448027, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 528, Loss: 0.3287482921734775, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 529, Loss: 0.40036873869307965, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 530, Loss: 0.3072921239273817, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 531, Loss: 0.4178028532575927, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 532, Loss: 0.2466809039222014, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 533, Loss: 0.35315192808254814, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 534, Loss: 0.3827358030269479, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 535, Loss: 0.27132025587360004, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 536, Loss: 0.42379758140596024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 537, Loss: 0.35740842632583986, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 538, Loss: 0.5356905224740526, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 539, Loss: 0.4353536570444575, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 540, Loss: 0.2522619164829032, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 541, Loss: 0.353405055608386, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 542, Loss: 0.24930279494048874, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 543, Loss: 0.2906161559651999, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 544, Loss: 0.28199265393796363, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 545, Loss: 0.2688470014623273, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 546, Loss: 0.40171126130613877, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 547, Loss: 0.2584232851589281, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 548, Loss: 0.3073056100459225, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 549, Loss: 0.3155962146135562, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 550, Loss: 0.2755160114952023, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 551, Loss: 0.26909189105342274, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 552, Loss: 0.4362726944367551, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 553, Loss: 0.24433159899573503, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 554, Loss: 0.38492403231952904, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 555, Loss: 0.31169550809348356, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 556, Loss: 0.3937463637177126, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 557, Loss: 0.2603767677013525, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 558, Loss: 0.3462397999021601, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 559, Loss: 0.3625109448887723, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 560, Loss: 0.27557304475668465, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 561, Loss: 0.2908158930823315, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 562, Loss: 0.2535643542281632, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 563, Loss: 0.32982436821820915, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 564, Loss: 0.23005365361722693, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 565, Loss: 0.3321221151200836, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 566, Loss: 0.48306723511528005, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 567, Loss: 0.2324827059967135, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 568, Loss: 0.38037656814209425, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 569, Loss: 0.2943685508876709, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 570, Loss: 0.243349504539226, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 571, Loss: 0.28188104608725306, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 572, Loss: 0.29001602597453574, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 573, Loss: 0.24790868160296742, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 574, Loss: 0.4718091202044533, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 575, Loss: 0.2999438423054427, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 576, Loss: 0.24142233114532471, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 577, Loss: 0.2570912340000667, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 578, Loss: 0.2639109392107134, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 579, Loss: 0.3825003187154007, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 580, Loss: 0.3941607824041554, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 581, Loss: 0.3103155811582915, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 582, Loss: 0.519176071872922, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 583, Loss: 0.24967494019069342, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 584, Loss: 0.44312594072957046, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 585, Loss: 0.43009231273548554, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 586, Loss: 0.3342354947104516, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 587, Loss: 0.4711085559531077, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 588, Loss: 0.3213935600705894, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 589, Loss: 0.28295726022746137, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 590, Loss: 0.24552819954661118, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 591, Loss: 0.5991533633888101, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 592, Loss: 0.323786732926843, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 593, Loss: 0.42032248401239763, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 594, Loss: 0.33977077462262595, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 595, Loss: 0.25470785358570486, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 596, Loss: 0.2527696836932402, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 597, Loss: 0.3293734489746032, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 598, Loss: 0.32452741805504937, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 599, Loss: 0.23720050813837731, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 600, Loss: 0.36643259159373587, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 601, Loss: 0.3702335533928536, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 602, Loss: 0.3003792009606986, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 603, Loss: 0.2912696426925668, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 604, Loss: 0.23754251581070582, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 605, Loss: 0.6247636486583954, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 606, Loss: 0.23672326220233172, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 607, Loss: 0.2830111655740427, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 608, Loss: 0.28610518905380944, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 609, Loss: 0.34952646734008774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 610, Loss: 0.24196250769551214, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 611, Loss: 0.23925175645462518, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 612, Loss: 0.29609917715865774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 613, Loss: 0.3027215979350118, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 614, Loss: 0.30440401145409945, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 615, Loss: 0.44082008310870147, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 616, Loss: 0.3623220690219454, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 617, Loss: 0.29840227818804993, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 618, Loss: 0.23544903944853326, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 619, Loss: 0.2525855436892015, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 620, Loss: 0.28474718475541605, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 621, Loss: 0.2507978851626531, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 622, Loss: 0.2756581333951671, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 623, Loss: 0.30942778423583983, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 624, Loss: 0.2517174075010083, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 625, Loss: 0.24153282658538386, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 626, Loss: 0.2760656400609711, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 627, Loss: 0.24104733312262522, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 628, Loss: 0.29460929511839773, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 629, Loss: 0.3038718999315897, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 630, Loss: 0.25212364280212884, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 631, Loss: 0.2730454500084012, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 632, Loss: 0.2593770585481492, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 633, Loss: 0.2972497588639485, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 634, Loss: 0.2717827038908808, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 635, Loss: 0.39219599765313984, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 636, Loss: 0.3184320803957546, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 637, Loss: 0.25544736379770655, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 638, Loss: 0.2938703611932803, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 639, Loss: 0.37635171861673683, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 640, Loss: 0.2473638425372124, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 641, Loss: 0.2619799162593262, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 642, Loss: 0.306129560882811, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 643, Loss: 0.37885401918300543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 644, Loss: 0.3657989167628639, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 645, Loss: 0.28115699586163984, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 646, Loss: 0.3232711930100052, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 647, Loss: 0.23657749103770803, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 648, Loss: 0.2888130617234955, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 649, Loss: 0.23437081703855275, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 650, Loss: 0.33050005111828334, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 651, Loss: 0.3766711095286418, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 652, Loss: 0.25669046794056855, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 653, Loss: 0.24633409819717716, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 654, Loss: 0.2613908552044142, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 655, Loss: 0.2794424566834439, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 656, Loss: 0.3347255719841039, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 657, Loss: 0.26859572414946675, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 658, Loss: 0.3708747615406849, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 659, Loss: 0.2503591838180306, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 660, Loss: 0.2672076766323743, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 661, Loss: 0.3842229615995668, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 662, Loss: 0.25772956264614283, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 663, Loss: 0.3294382321821281, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 664, Loss: 0.3263818129210272, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 665, Loss: 0.26228347209448166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 666, Loss: 0.3271412194319595, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 667, Loss: 0.3105315061321547, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 668, Loss: 0.4826739713648047, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 669, Loss: 0.2633914424506457, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 670, Loss: 0.28419003782708624, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 671, Loss: 0.2589342183318066, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 672, Loss: 0.30277206347130214, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 673, Loss: 0.285740520885658, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 674, Loss: 0.4262770458219809, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 675, Loss: 0.5276717602868187, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 676, Loss: 0.3999527484737768, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 677, Loss: 0.3495231822957719, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 678, Loss: 0.33524159301243195, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 679, Loss: 0.25016522389572493, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 680, Loss: 0.2428293621019072, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 681, Loss: 0.4529242344565975, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 682, Loss: 0.5070565057248653, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 683, Loss: 0.4403056865465073, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 684, Loss: 0.2601526234356138, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 685, Loss: 0.4140084596222978, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 686, Loss: 0.37194742704498274, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 687, Loss: 0.3056328123577635, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 688, Loss: 0.26308977984947135, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 689, Loss: 0.27230771457772346, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 690, Loss: 0.4235690219566072, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 691, Loss: 0.2566326072811646, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 692, Loss: 0.38541353528792166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 693, Loss: 0.3833173253351204, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 694, Loss: 0.4727106370563614, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 695, Loss: 0.4682738434984774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 696, Loss: 0.3188836929117345, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 697, Loss: 0.32367118016440183, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 698, Loss: 0.2641260033615691, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 699, Loss: 0.24926881867342376, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 700, Loss: 0.34025330944836796, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 701, Loss: 0.3227026981243679, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 702, Loss: 0.2615013133314332, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 703, Loss: 0.3718281039633035, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 704, Loss: 0.47627687429259236, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 705, Loss: 0.38124204551122065, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 706, Loss: 0.39937160230204694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 707, Loss: 0.2598221473462416, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 708, Loss: 0.34156221870905923, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 709, Loss: 0.2781381898843627, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 710, Loss: 0.23415431290893557, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 711, Loss: 0.6294176345603057, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 712, Loss: 0.41257929683762473, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 713, Loss: 0.3639989389569507, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 714, Loss: 0.4268420627886407, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 715, Loss: 0.23408197998783545, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 716, Loss: 0.2754988831795902, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 717, Loss: 0.2927243038473958, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 718, Loss: 0.6150734750082172, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 719, Loss: 0.29268711148214743, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 720, Loss: 0.3333129060845797, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 721, Loss: 0.38562178997721963, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 722, Loss: 0.2903725077285385, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 723, Loss: 0.37925645184298157, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 724, Loss: 0.23427028858369942, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 725, Loss: 0.320517619629656, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 726, Loss: 0.31834387686170607, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 727, Loss: 0.38914478502467464, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 728, Loss: 0.2872999041324531, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 729, Loss: 0.286161810410167, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 730, Loss: 0.3455879375368741, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 731, Loss: 0.3015758756489724, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 732, Loss: 0.2685222900703722, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 733, Loss: 0.34126331875552895, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 734, Loss: 0.3166037079929241, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 735, Loss: 0.2912389370004538, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 736, Loss: 0.3559079708792187, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 737, Loss: 0.26337151680289494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 738, Loss: 0.31769367451191244, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 739, Loss: 0.47322776726707616, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 740, Loss: 0.242332088628502, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 741, Loss: 0.37428797249584034, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 742, Loss: 0.33028258372791836, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 743, Loss: 0.43933173031122985, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 744, Loss: 0.25389966164879346, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 745, Loss: 0.25339006020863125, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 746, Loss: 0.5143569051213033, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 747, Loss: 0.7551341703111335, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 748, Loss: 0.3026752570257265, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 749, Loss: 0.349979483339547, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 750, Loss: 0.23428138673006124, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 751, Loss: 0.28715163923859593, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 752, Loss: 0.3574043925282847, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 753, Loss: 0.30978494622410113, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 754, Loss: 0.44389115582112615, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 755, Loss: 0.26719080854472566, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 756, Loss: 0.29310825991605144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 757, Loss: 0.27131661105359794, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 758, Loss: 0.2999322774443745, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 759, Loss: 0.26808888553087956, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 760, Loss: 0.31502182967759307, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 761, Loss: 0.33907466222512284, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 762, Loss: 0.3057096187989844, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 763, Loss: 0.2617530032400939, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 764, Loss: 0.23160364467489114, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 765, Loss: 0.3932672439467453, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 766, Loss: 0.2554410979665268, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 767, Loss: 0.4404496364038576, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 768, Loss: 0.31352056873960904, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 769, Loss: 0.47432930108810656, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 770, Loss: 0.2860674270796935, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 771, Loss: 0.4625685879291188, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 772, Loss: 0.29541077673633215, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 773, Loss: 0.2783769332294268, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 774, Loss: 0.2782634657858269, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 775, Loss: 0.3010815752620586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 776, Loss: 0.27877442811262587, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 777, Loss: 0.2935598804159995, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 778, Loss: 0.26004140648944674, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 779, Loss: 0.26675684666553545, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 780, Loss: 0.28536809667606794, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 781, Loss: 0.5831730001128703, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 782, Loss: 0.27465503455625534, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 783, Loss: 0.3964659476762312, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 784, Loss: 0.2661929015681125, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 785, Loss: 0.2563571410861738, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 786, Loss: 0.44044647568945916, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 787, Loss: 0.3405055613752427, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 788, Loss: 0.24401479100855422, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 789, Loss: 0.3467909565496726, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 790, Loss: 0.24851296082404348, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 791, Loss: 0.26998200842249015, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 792, Loss: 0.3477933311645674, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 793, Loss: 0.5711956498518094, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 794, Loss: 0.2881143851978926, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 795, Loss: 0.26499141963444683, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 796, Loss: 0.4711692187105836, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 797, Loss: 0.40347830341270213, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 798, Loss: 0.23779632636251488, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 799, Loss: 0.3131904178001595, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 800, Loss: 0.40709247232400825, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 801, Loss: 0.3194195628444032, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 802, Loss: 0.266720167556722, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 803, Loss: 0.26062682977643037, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 804, Loss: 0.34434424501750216, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 805, Loss: 0.3257990135293468, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 806, Loss: 0.2787820456446497, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 807, Loss: 0.48018284685190593, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 808, Loss: 0.2520975634320757, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 809, Loss: 0.2572849353632068, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 810, Loss: 0.26275249959162356, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 811, Loss: 0.4431633392304698, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 812, Loss: 0.3263745511864108, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 813, Loss: 0.2507600329044723, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 814, Loss: 0.25665246830304606, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 815, Loss: 0.3081809528354107, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 816, Loss: 0.48521400636851925, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 817, Loss: 0.23877758718646783, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 818, Loss: 0.365822901051041, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 819, Loss: 0.35110375722607207, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 820, Loss: 0.26037009888373674, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 821, Loss: 0.4158184791270749, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 822, Loss: 0.2518644352435646, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 823, Loss: 0.33965015739485765, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 824, Loss: 0.2535166600188257, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 825, Loss: 0.2501546839932564, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 826, Loss: 0.4059297111975183, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 827, Loss: 0.31597040840049095, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 828, Loss: 0.2576613772883166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 829, Loss: 0.3003075087656095, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 830, Loss: 0.31669541585474736, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 831, Loss: 0.28195270721779525, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 832, Loss: 0.2774187404846664, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 833, Loss: 0.27785167267522637, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 834, Loss: 0.25875234362771793, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 835, Loss: 0.3289483348804491, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 836, Loss: 0.276741991373804, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 837, Loss: 0.35663018101222504, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 838, Loss: 0.259376267958536, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 839, Loss: 0.2755561492570428, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 840, Loss: 0.3731070992202997, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 841, Loss: 0.37587284517477654, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 842, Loss: 0.3001185845406752, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 843, Loss: 0.41899764485889907, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 844, Loss: 0.39821628545372667, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 845, Loss: 0.2934357961110267, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 846, Loss: 0.3672038406629998, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 847, Loss: 0.2679507776772596, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 848, Loss: 0.319160024022008, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 849, Loss: 0.32731775808885466, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 850, Loss: 0.5122161573306155, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 851, Loss: 0.2651542279061246, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 852, Loss: 0.30138261001198136, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 853, Loss: 0.28703970719095834, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 854, Loss: 0.3145192658407316, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 855, Loss: 0.37079811294160336, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 856, Loss: 0.4198425295361087, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 857, Loss: 0.4420787171563352, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 858, Loss: 0.44113599538114967, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 859, Loss: 0.26507650980558034, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 860, Loss: 0.2660175589971327, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 861, Loss: 0.4352888275381004, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 862, Loss: 0.23278294143312805, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 863, Loss: 0.2704097718848146, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 864, Loss: 0.3991625504484453, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 865, Loss: 0.2831545200078893, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 866, Loss: 0.2715510884203204, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 867, Loss: 0.277309331665078, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 868, Loss: 0.24290709241198055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 869, Loss: 0.28711844010284554, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 870, Loss: 0.27726541173786673, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 871, Loss: 0.24112410615825278, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 872, Loss: 0.24267464908946332, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 873, Loss: 0.3673562926689199, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 874, Loss: 0.24246119132542523, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 875, Loss: 0.2749861424172078, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 876, Loss: 0.2660710314848935, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 877, Loss: 0.23142271155194924, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 878, Loss: 0.2634514403684158, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 879, Loss: 0.3316918419383739, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 880, Loss: 0.5046957939827362, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 881, Loss: 0.3042182956753456, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 882, Loss: 0.3728469649215169, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 883, Loss: 0.26776529907773977, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 884, Loss: 0.28616154213670875, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 885, Loss: 0.2784173899452087, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 886, Loss: 0.24019143860003903, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 887, Loss: 0.34976878499629793, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 888, Loss: 0.4272496439550042, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 889, Loss: 0.2843689638490263, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 890, Loss: 0.2767070761103434, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 891, Loss: 0.26815454824732016, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 892, Loss: 0.3820246579239833, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 893, Loss: 0.2325477922550747, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 894, Loss: 0.2783619058974491, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 895, Loss: 0.29586048933281484, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 896, Loss: 0.3091142657297964, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 897, Loss: 0.23348648002966874, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 898, Loss: 0.2449781998434753, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 899, Loss: 0.2679900646644929, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 900, Loss: 0.3300916003874541, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 901, Loss: 0.3006326552135703, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 902, Loss: 0.29454793257515544, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 903, Loss: 0.48076154937810267, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 904, Loss: 0.31162545449457313, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 905, Loss: 0.358740282183427, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 906, Loss: 0.3333111323403059, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 907, Loss: 0.3361625692728617, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 908, Loss: 0.24501069277712278, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 909, Loss: 0.3541902706763843, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 910, Loss: 0.44268072147982374, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 911, Loss: 0.2471786218721998, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 912, Loss: 0.3240320639944281, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 913, Loss: 0.26710993860441073, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 914, Loss: 0.26152129492603415, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 915, Loss: 0.5912809334608491, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 916, Loss: 0.2592486025853582, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 917, Loss: 0.28222583929312395, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 918, Loss: 0.23939092462123318, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 919, Loss: 0.3133349878690166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 920, Loss: 0.36325697173675464, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 921, Loss: 0.24142612707534117, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 922, Loss: 0.2563041941131771, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 923, Loss: 0.2615162451376829, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 924, Loss: 0.3314418647977219, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 925, Loss: 0.6854668956230877, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 926, Loss: 0.4455803570796828, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 927, Loss: 0.39413173817510694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 928, Loss: 0.3145717754651822, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 929, Loss: 0.2446695356421068, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 930, Loss: 0.33639144216607014, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 931, Loss: 0.3376123204182717, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 932, Loss: 0.2498625211077646, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 933, Loss: 0.24439630648561733, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 934, Loss: 0.2788704782148253, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 935, Loss: 0.31488472173218807, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 936, Loss: 0.2481543993033027, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 937, Loss: 0.27950857885591024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 938, Loss: 0.5036810350794727, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 939, Loss: 0.23422413065334846, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 940, Loss: 0.28038493141174403, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 941, Loss: 0.33231656962167155, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 942, Loss: 0.36080453889503183, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 943, Loss: 0.3817306651438024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 944, Loss: 0.2990839946945549, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 945, Loss: 0.3737395625443665, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 946, Loss: 0.2629178664807086, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 947, Loss: 0.2588086149029143, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 948, Loss: 0.24267404743204563, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 949, Loss: 0.33344242665092094, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 950, Loss: 0.26576874834531816, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 951, Loss: 0.25868753478985845, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 952, Loss: 0.3412690540130572, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 953, Loss: 0.3320779636192622, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 954, Loss: 0.2584086737712094, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 955, Loss: 0.2707162428564224, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 956, Loss: 0.23519117362466366, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 957, Loss: 0.2754397615551827, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 958, Loss: 0.2759424046058655, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 959, Loss: 0.2682101408965518, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 960, Loss: 0.31182578361375984, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 961, Loss: 0.30660539269588266, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 962, Loss: 0.24882263032318247, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 963, Loss: 0.2727797158763149, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 964, Loss: 0.2399494027172278, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 965, Loss: 0.2975744340751189, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 966, Loss: 0.29053707415316554, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 967, Loss: 0.34517876926467606, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 968, Loss: 0.2763955811499281, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 969, Loss: 0.28293877519706745, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 970, Loss: 0.2799370660795625, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 971, Loss: 0.315532586786091, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 972, Loss: 0.36125778423236443, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 973, Loss: 0.35905722285640973, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 974, Loss: 0.30424133991403746, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 975, Loss: 0.2702866940645082, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 976, Loss: 0.25813172843007587, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 977, Loss: 0.26099242164373976, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 978, Loss: 0.3123654069476627, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 979, Loss: 0.28222855388031676, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 980, Loss: 0.2821208795593908, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 981, Loss: 0.2614933800004662, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 982, Loss: 0.23236758110388275, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 983, Loss: 0.25648454895772554, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 984, Loss: 0.25374264160196774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 985, Loss: 0.26408640889866786, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 986, Loss: 0.3542346599623021, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 987, Loss: 0.2769523151714755, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 988, Loss: 0.3967844191802515, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 989, Loss: 0.3021394799611026, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 990, Loss: 0.6222343922456162, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 991, Loss: 0.3868329846768467, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 992, Loss: 0.23862693897564694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 993, Loss: 0.23148622056182439, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 994, Loss: 0.24825523528995083, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 995, Loss: 0.24134747675822107, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 996, Loss: 0.4104022121391636, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 997, Loss: 0.24330323741170007, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 998, Loss: 0.2488106797222822, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 999, Loss: 0.27794383675259227, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1000, Loss: 0.26747277195872327, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1001, Loss: 0.4507849097972759, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1002, Loss: 0.3997371004117387, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1003, Loss: 0.2555089262782713, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1004, Loss: 0.24759494627122708, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1005, Loss: 0.35847607043900087, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1006, Loss: 0.279731088321317, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1007, Loss: 0.2817247897036813, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1008, Loss: 0.2374174219704303, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1009, Loss: 0.3295429794070479, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1010, Loss: 0.3564471986911193, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1011, Loss: 0.28391189394463373, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1012, Loss: 0.35886720409754436, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1013, Loss: 0.29987402249573986, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1014, Loss: 0.27835341545561565, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1015, Loss: 0.35908802996358785, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1016, Loss: 0.2548027752957516, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1017, Loss: 0.3270314607955542, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1018, Loss: 0.42976582064746094, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1019, Loss: 0.30877523702931076, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1020, Loss: 0.29542089198044436, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1021, Loss: 0.3376739550769552, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1022, Loss: 0.31423987966778205, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1023, Loss: 0.27462471770715713, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1024, Loss: 0.34841927431353154, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1025, Loss: 0.36565535636512586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1026, Loss: 0.30574681481496335, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1027, Loss: 0.3235169650751215, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1028, Loss: 0.3487234683830226, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1029, Loss: 0.2986339383708261, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1030, Loss: 0.2716036214189261, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1031, Loss: 0.34370727010770075, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1032, Loss: 0.28864404015397216, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1033, Loss: 0.3686827006815306, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1034, Loss: 0.344339145139551, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1035, Loss: 0.4077555330020277, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1036, Loss: 0.36225493887132665, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1037, Loss: 0.302351567844517, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1038, Loss: 0.2769972139869736, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1039, Loss: 0.27091314673149175, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1040, Loss: 0.39648308914302866, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1041, Loss: 0.28958752552044087, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1042, Loss: 0.25911654713462795, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1043, Loss: 0.30223913569350613, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1044, Loss: 0.2539778663453913, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1045, Loss: 0.3209353809137787, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1046, Loss: 0.30837739751491117, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1047, Loss: 0.46749426023339774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1048, Loss: 0.3054822459821679, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1049, Loss: 0.40293449544538373, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1050, Loss: 0.4134660694879829, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1051, Loss: 0.32675399050281434, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1052, Loss: 0.37822246546351934, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1053, Loss: 0.2971748306265899, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1054, Loss: 0.409856625050393, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1055, Loss: 0.3124723035416744, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1056, Loss: 0.2730808312203583, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1057, Loss: 0.30168967331199165, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1058, Loss: 0.5134944033769697, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1059, Loss: 0.2996292819871086, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1060, Loss: 0.3403301470902433, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1061, Loss: 0.3197230507214405, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1062, Loss: 0.47109690705376295, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1063, Loss: 0.3356021996312033, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1064, Loss: 0.3457713551707233, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1065, Loss: 0.36206986495599736, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1066, Loss: 0.30803099412319995, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1067, Loss: 0.2495760613013965, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1068, Loss: 0.3273240506291196, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1069, Loss: 0.3273515918805704, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1070, Loss: 0.2565458705913953, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1071, Loss: 0.2785758471028416, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1072, Loss: 0.24947309844867305, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1073, Loss: 0.3122819438266362, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1074, Loss: 0.2826034611344122, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1075, Loss: 0.3983324619903815, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1076, Loss: 0.23878623571332283, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1077, Loss: 0.4326399933805437, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1078, Loss: 0.33170695029047886, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1079, Loss: 0.3660324900387055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1080, Loss: 0.25188414605801984, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1081, Loss: 0.28296732986516754, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1082, Loss: 0.25119638182537407, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1083, Loss: 0.268129148910559, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1084, Loss: 0.27146600366547596, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1085, Loss: 0.3230421916771524, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1086, Loss: 0.36242074909211763, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1087, Loss: 0.3690143328604091, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1088, Loss: 0.40030754846100547, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1089, Loss: 0.2378849170169217, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1090, Loss: 0.2901264420541343, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1091, Loss: 0.3284634378193333, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1092, Loss: 0.5235701681355037, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1093, Loss: 0.25362308296591735, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1094, Loss: 0.5404293373737106, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1095, Loss: 0.23859391500531468, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1096, Loss: 0.39168144451015147, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1097, Loss: 0.28556671809057116, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1098, Loss: 0.40710294442202144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1099, Loss: 0.24449569014314343, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1100, Loss: 0.25851987946369104, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1101, Loss: 0.3939721132694562, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1102, Loss: 0.26094227369970874, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1103, Loss: 0.30266498337893066, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1104, Loss: 0.3255567640246265, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1105, Loss: 0.2530162276271582, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1106, Loss: 0.2743518428436536, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1107, Loss: 0.2973693467281451, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1108, Loss: 0.24130223053103092, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1109, Loss: 0.314533138494232, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1110, Loss: 0.31490181961544494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1111, Loss: 0.654578612168731, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1112, Loss: 0.34732219875249326, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1113, Loss: 0.23868604402637994, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1114, Loss: 0.29372109359268056, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1115, Loss: 0.3385231371878626, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1116, Loss: 0.7279407227950379, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1117, Loss: 0.4412847153198629, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1118, Loss: 0.4326359043554266, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1119, Loss: 0.5601069779657473, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1120, Loss: 0.2701090451971965, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1121, Loss: 0.27644188796685853, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1122, Loss: 0.2621456344046839, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1123, Loss: 0.32977696520058636, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1124, Loss: 0.3823578964397202, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1125, Loss: 0.28717159059416664, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1126, Loss: 0.42408594908008623, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1127, Loss: 0.32704184892825006, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1128, Loss: 0.24451585223588923, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1129, Loss: 0.30192798969494977, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1130, Loss: 0.26947263037632574, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1131, Loss: 0.438928782596808, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1132, Loss: 0.24321020383297715, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1133, Loss: 0.26406823428282805, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1134, Loss: 0.3874666228113318, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1135, Loss: 0.36949460252672817, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1136, Loss: 0.29472884563474894, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1137, Loss: 0.4367618759169153, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1138, Loss: 0.32097497314098344, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1139, Loss: 0.258518138533865, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1140, Loss: 0.28088215545511674, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1141, Loss: 0.49117800248569893, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1142, Loss: 0.36457448264946196, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1143, Loss: 0.43185891205975757, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1144, Loss: 0.3807500238645371, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1145, Loss: 0.28930849882818055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1146, Loss: 0.38303301789182653, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1147, Loss: 0.34578998158902324, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1148, Loss: 0.24337036293943584, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1149, Loss: 0.557638794506243, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1150, Loss: 0.3474031780622222, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1151, Loss: 0.2766676360926452, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1152, Loss: 0.2684238992776903, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1153, Loss: 0.28165002535276884, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1154, Loss: 0.2525479877812989, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1155, Loss: 0.3778728737671705, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1156, Loss: 0.3013081438197653, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1157, Loss: 0.30004214088317704, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1158, Loss: 0.34131973757655687, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1159, Loss: 0.31449080023255055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1160, Loss: 0.4290035354506462, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1161, Loss: 0.3145849677510226, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1162, Loss: 0.2643286962881503, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1163, Loss: 0.25971350838961327, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1164, Loss: 0.3223380138126142, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1165, Loss: 0.25269976959442997, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1166, Loss: 0.24011034153515712, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1167, Loss: 0.3082097140109331, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1168, Loss: 0.318547057335788, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1169, Loss: 0.3406718992635751, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1170, Loss: 0.2509165818160407, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1171, Loss: 0.5370906325181215, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1172, Loss: 0.25478010656638683, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1173, Loss: 0.33737492834539967, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1174, Loss: 0.26822124241615924, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1175, Loss: 0.32148366186476385, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1176, Loss: 0.24890069557240657, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1177, Loss: 0.2798783312243379, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1178, Loss: 0.2545513173891934, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1179, Loss: 0.32503896989762393, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1180, Loss: 0.27908230198637896, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1181, Loss: 0.2565918594357269, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1182, Loss: 0.5540791976660931, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1183, Loss: 0.31375002485495357, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1184, Loss: 0.5028451949753477, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1185, Loss: 0.2668066855879614, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1186, Loss: 0.24489646318490227, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1187, Loss: 0.2872469660213718, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1188, Loss: 0.6122651378633802, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1189, Loss: 0.2965323041238128, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1190, Loss: 0.3847077455954546, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1191, Loss: 0.3531112781435001, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1192, Loss: 0.29065701777178976, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1193, Loss: 0.3970756660627658, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1194, Loss: 0.35355076894252524, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1195, Loss: 0.23931174308241157, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1196, Loss: 0.2764434223239323, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1197, Loss: 0.24206557909785123, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1198, Loss: 0.3210122134945387, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1199, Loss: 0.42318628872445013, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1200, Loss: 0.2964596022738705, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1201, Loss: 0.24126610531461157, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1202, Loss: 0.26516020682776026, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1203, Loss: 0.3311865310984996, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1204, Loss: 0.25285479691445556, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1205, Loss: 0.2549090380888249, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1206, Loss: 0.26290155252884734, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1207, Loss: 0.3127078241405504, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1208, Loss: 0.25909855889353195, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1209, Loss: 0.23006361124444177, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1210, Loss: 0.28601593538550096, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1211, Loss: 0.40304459336747683, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1212, Loss: 0.29538227221283286, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1213, Loss: 0.27099926646100103, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1214, Loss: 0.2778380198568135, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1215, Loss: 0.33605035729450766, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1216, Loss: 0.26950636076208323, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1217, Loss: 0.2750018756434199, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1218, Loss: 0.5352453183509912, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1219, Loss: 0.23501086559326542, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1220, Loss: 0.38115917704978763, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1221, Loss: 0.5527608541922543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1222, Loss: 0.5828064230728885, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1223, Loss: 0.29314853239499805, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1224, Loss: 0.3045703173284311, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1225, Loss: 0.2549296188763873, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1226, Loss: 0.32724030496076817, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1227, Loss: 0.32680702154873753, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1228, Loss: 0.2777511990793312, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1229, Loss: 0.36903116360626, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1230, Loss: 0.23601534232349572, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1231, Loss: 0.3024845563721469, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1232, Loss: 0.261844664607547, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1233, Loss: 0.26268052241797907, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1234, Loss: 0.2914758729528084, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1235, Loss: 0.32363742863953493, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1236, Loss: 0.46097576616864583, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1237, Loss: 0.25397392302201577, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1238, Loss: 0.3743455007390809, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1239, Loss: 0.28193887159632214, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1240, Loss: 0.38623087577333814, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1241, Loss: 0.24497698388004385, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1242, Loss: 0.4151135752844988, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1243, Loss: 0.31578706563870657, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1244, Loss: 0.25754828262788027, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1245, Loss: 0.32708686677569576, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1246, Loss: 0.24471882869073272, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1247, Loss: 0.24319313791394234, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1248, Loss: 0.40319095485494794, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1249, Loss: 0.23036118007390544, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1250, Loss: 0.34267772501998706, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1251, Loss: 0.29177544137030953, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1252, Loss: 0.3066764728114186, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1253, Loss: 0.2578001075700331, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1254, Loss: 0.32982674352422847, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1255, Loss: 0.2767109892753532, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1256, Loss: 0.2426478799866019, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1257, Loss: 0.2864256017418045, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1258, Loss: 0.2765087496630185, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1259, Loss: 0.30112808847636735, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1260, Loss: 0.3241010491539356, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1261, Loss: 0.28546783776340356, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1262, Loss: 0.4250846402544838, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1263, Loss: 0.3022153730388055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1264, Loss: 0.2511388549961685, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1265, Loss: 0.3025893796716118, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1266, Loss: 0.2916057775960016, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1267, Loss: 0.3306246037940288, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1268, Loss: 0.36362175486653453, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1269, Loss: 0.25362764301347546, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1270, Loss: 0.255533282286144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1271, Loss: 0.33738191448448573, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1272, Loss: 0.2683870373453954, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1273, Loss: 0.305678681796708, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1274, Loss: 0.48351996423383803, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1275, Loss: 0.30152420937145497, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1276, Loss: 0.5834995238549109, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1277, Loss: 0.2899265196973222, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1278, Loss: 0.40933275252471546, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1279, Loss: 0.2592624959045966, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1280, Loss: 0.29900861776779136, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1281, Loss: 0.27081728162127106, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1282, Loss: 0.2791989162613182, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1283, Loss: 0.24752758197567992, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1284, Loss: 0.27408076403732345, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1285, Loss: 0.29147550242507586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1286, Loss: 0.39560504215656545, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1287, Loss: 0.4912558273568073, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1288, Loss: 0.5114173360033586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1289, Loss: 0.24713342154964485, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1290, Loss: 0.3446222666882325, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1291, Loss: 0.39777282681659437, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1292, Loss: 0.23811715386991175, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1293, Loss: 0.4401143701782745, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1294, Loss: 0.3881504867396467, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1295, Loss: 0.24967956533126853, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1296, Loss: 0.33973913578177534, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1297, Loss: 0.3194626054484702, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1298, Loss: 0.6059567899823186, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1299, Loss: 0.3479294926979564, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1300, Loss: 0.308390745785833, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1301, Loss: 0.2725690379043489, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1302, Loss: 0.3076371399670068, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1303, Loss: 0.2880283523332994, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1304, Loss: 0.25854560751274985, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1305, Loss: 0.34773226396713985, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1306, Loss: 0.2557804289829363, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1307, Loss: 0.42439309661117947, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1308, Loss: 0.25576298600820424, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1309, Loss: 0.37296495260502094, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1310, Loss: 0.2654369929678465, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1311, Loss: 0.28155590460601554, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1312, Loss: 0.33519615440947226, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1313, Loss: 0.36078631023474916, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1314, Loss: 0.3940803818528692, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1315, Loss: 0.278392154013891, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1316, Loss: 0.2775766122796434, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1317, Loss: 0.24133666514690075, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1318, Loss: 0.26770405944894776, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1319, Loss: 0.25486983979276445, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1320, Loss: 0.23224374961615335, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1321, Loss: 0.40451880012927666, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1322, Loss: 0.41462903341923557, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1323, Loss: 0.2832951215326307, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1324, Loss: 0.4104364971936934, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1325, Loss: 0.3076029011118313, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1326, Loss: 0.2934042217502932, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1327, Loss: 0.4156202716652244, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1328, Loss: 0.35110167056889563, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1329, Loss: 0.29585943806170967, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1330, Loss: 0.2504620135474092, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1331, Loss: 0.26889094618362125, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1332, Loss: 0.28872423316661633, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1333, Loss: 0.3347480838236815, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1334, Loss: 0.3535882340399019, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1335, Loss: 0.26851953967565195, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1336, Loss: 0.4454936880788444, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1337, Loss: 0.2702205294336508, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1338, Loss: 0.28849317977145034, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1339, Loss: 0.3433961637727717, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1340, Loss: 0.45367986912107894, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1341, Loss: 0.27539735665728243, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1342, Loss: 0.3948768238079404, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1343, Loss: 0.23529131651124507, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1344, Loss: 0.2924398565070011, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1345, Loss: 0.3116015875537162, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1346, Loss: 0.2323759269070641, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1347, Loss: 0.47305615409194357, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1348, Loss: 0.28567357574456015, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1349, Loss: 0.45773587844996955, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1350, Loss: 0.234337751177219, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1351, Loss: 0.24406119356115796, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1352, Loss: 0.27793427927590414, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1353, Loss: 0.32181534657141847, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1354, Loss: 0.3082673788328852, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1355, Loss: 0.23762230376359822, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1356, Loss: 0.29224929111097453, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1357, Loss: 0.25879855634267734, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1358, Loss: 0.28091039115187166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1359, Loss: 0.23665780675201878, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1360, Loss: 0.29913844635672726, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1361, Loss: 0.23709373286773783, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1362, Loss: 0.279830309290737, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1363, Loss: 0.35730777788697754, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1364, Loss: 0.3191251017041758, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1365, Loss: 0.27931429274189584, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1366, Loss: 0.24026542333808187, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1367, Loss: 0.3019852478799169, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1368, Loss: 0.36647702694320694, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1369, Loss: 0.3337001159225236, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1370, Loss: 0.4052202923834187, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1371, Loss: 0.3226694290295019, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1372, Loss: 0.32109103183263726, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1373, Loss: 0.3255487251936624, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1374, Loss: 0.5289113572359363, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1375, Loss: 0.23326828409719097, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1376, Loss: 0.30256907461757865, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1377, Loss: 0.4714862574763484, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1378, Loss: 0.29702841953210596, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1379, Loss: 0.300824931779307, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1380, Loss: 0.4324692500025199, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1381, Loss: 0.5829929580326896, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1382, Loss: 0.3555357465453418, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1383, Loss: 0.3667137297531489, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1384, Loss: 0.23503613260739045, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1385, Loss: 0.26559053524021825, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1386, Loss: 0.33486764153396253, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1387, Loss: 0.24183921510301803, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1388, Loss: 0.2968599834941589, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1389, Loss: 0.3983171357790801, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1390, Loss: 0.38262290620989686, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1391, Loss: 0.2829635810972262, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1392, Loss: 0.31267186937439034, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1393, Loss: 0.271387433004665, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1394, Loss: 0.3287911007938369, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1395, Loss: 0.390204261788299, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1396, Loss: 0.25044640755127806, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1397, Loss: 0.27803426106865214, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1398, Loss: 0.36667234667642923, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1399, Loss: 0.47690039516332317, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1400, Loss: 0.30631418726740206, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1401, Loss: 0.2601025820520903, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1402, Loss: 0.2878571229185251, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1403, Loss: 0.23493535598827614, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1404, Loss: 0.283650120113637, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1405, Loss: 0.27646142175693833, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1406, Loss: 0.26791700020690884, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1407, Loss: 0.2679433853386527, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1408, Loss: 0.3024933475581429, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1409, Loss: 0.23058633596856465, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1410, Loss: 0.3376719202111409, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1411, Loss: 0.24215323916019946, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1412, Loss: 0.4422564495768505, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1413, Loss: 0.37576557167926683, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1414, Loss: 0.34976264195686724, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1415, Loss: 0.36160788494754437, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1416, Loss: 0.3938177740023499, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1417, Loss: 0.24741769413933581, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1418, Loss: 0.2496888909214004, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1419, Loss: 0.25805862980013533, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1420, Loss: 0.29019602631752295, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1421, Loss: 0.24438183406890807, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1422, Loss: 0.34570557503360816, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1423, Loss: 0.36180806758823675, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1424, Loss: 0.4342887802615296, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1425, Loss: 0.4194314476913863, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1426, Loss: 0.26959139230401713, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1427, Loss: 0.30862243383734805, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1428, Loss: 0.3594005005850778, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1429, Loss: 0.30559562417051567, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1430, Loss: 0.3260476914167425, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1431, Loss: 0.23244631581622283, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1432, Loss: 0.4239953557876164, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1433, Loss: 0.2879628623667218, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1434, Loss: 0.24303635149793906, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1435, Loss: 0.29148013621015323, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1436, Loss: 0.27099183045126923, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1437, Loss: 0.33519966337484614, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1438, Loss: 0.29887068345026047, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1439, Loss: 0.2415805612990473, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1440, Loss: 0.367663318967453, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1441, Loss: 0.36111163618295283, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1442, Loss: 0.38778370799350503, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1443, Loss: 0.330993326557483, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1444, Loss: 0.26714109568303734, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1445, Loss: 0.3546301233941046, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1446, Loss: 0.24026717497671563, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1447, Loss: 0.2324598312195913, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1448, Loss: 0.24888887820482883, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1449, Loss: 0.27806872672139793, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1450, Loss: 0.29712359429505625, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1451, Loss: 0.2451690046260414, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1452, Loss: 0.2718493171752884, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1453, Loss: 0.2576400645472543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1454, Loss: 0.26857919327601637, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1455, Loss: 0.3081690533380279, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1456, Loss: 0.2695579329725625, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1457, Loss: 0.24830567837374254, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1458, Loss: 0.2863920841343234, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1459, Loss: 0.3222921048187124, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1460, Loss: 0.24359799163624518, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1461, Loss: 0.29475490329692955, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1462, Loss: 0.2529264194117118, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1463, Loss: 0.24922173280084817, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1464, Loss: 0.26796432026193207, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1465, Loss: 0.2629476046688638, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1466, Loss: 0.2740367055844669, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1467, Loss: 0.31217168925073047, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1468, Loss: 0.390458102151896, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1469, Loss: 0.3097572749884702, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1470, Loss: 0.5098837578969365, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1471, Loss: 0.23653168330240532, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1472, Loss: 0.4169083518348473, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1473, Loss: 0.27181383266329384, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1474, Loss: 0.2813124125424525, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1475, Loss: 0.308261638479044, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1476, Loss: 0.3415754034361002, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1477, Loss: 0.427020186748924, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1478, Loss: 0.31650570753427204, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1479, Loss: 0.33481485496975505, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1480, Loss: 0.3226908721868429, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1481, Loss: 0.333127016226198, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1482, Loss: 0.3627065852670978, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1483, Loss: 0.2423683787217498, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1484, Loss: 0.29760648780395016, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1485, Loss: 0.2783688274158126, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1486, Loss: 0.3024332655820568, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1487, Loss: 0.25332912108120215, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1488, Loss: 0.2806741033175576, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1489, Loss: 0.24575287891979994, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1490, Loss: 0.2834741367904835, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1491, Loss: 0.3035199334780719, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1492, Loss: 0.2472606325282046, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1493, Loss: 0.24582345992853186, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1494, Loss: 0.2915924082314672, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1495, Loss: 0.29546771867756977, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1496, Loss: 0.2429140001261264, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1497, Loss: 0.578781845255979, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1498, Loss: 0.2559716480023764, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1499, Loss: 0.38550083227096676, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1500, Loss: 0.3353593563741628, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1501, Loss: 0.3782063997748196, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1502, Loss: 0.2619717225412725, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1503, Loss: 0.2674572033919912, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1504, Loss: 0.2570388878681864, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1505, Loss: 0.26191211163730366, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1506, Loss: 0.39900745729315756, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1507, Loss: 0.2552367185090401, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1508, Loss: 0.4379065599679798, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1509, Loss: 0.25275705788761066, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1510, Loss: 0.2977469202960783, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1511, Loss: 0.2856239954759192, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1512, Loss: 0.5781315919142338, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1513, Loss: 0.28631064524518807, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1514, Loss: 0.26047506597090075, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1515, Loss: 0.38864163472103624, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1516, Loss: 0.40160935213838045, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1517, Loss: 0.29366609192831095, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1518, Loss: 0.3452112415277133, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1519, Loss: 0.3036758627979885, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1520, Loss: 0.5130922219462126, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1521, Loss: 0.3641242998422086, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1522, Loss: 0.45592366148690266, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1523, Loss: 0.2689614726051881, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1524, Loss: 0.27843154598544956, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1525, Loss: 0.33207762626938797, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1526, Loss: 0.2774420997950754, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1527, Loss: 0.307556186401438, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1528, Loss: 0.3747563552617955, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1529, Loss: 0.357265161757667, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1530, Loss: 0.283172731954724, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1531, Loss: 0.2441394040054808, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1532, Loss: 0.2485468063100748, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1533, Loss: 0.3648370017404129, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1534, Loss: 0.257399252262887, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1535, Loss: 0.3085717839083549, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1536, Loss: 0.2766915213697714, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1537, Loss: 0.3193530671655548, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1538, Loss: 0.27019671387357463, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1539, Loss: 0.23466140572103927, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1540, Loss: 0.27531183534009684, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1541, Loss: 0.3161191456872817, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1542, Loss: 0.2792671299227025, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1543, Loss: 0.3049282004628729, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1544, Loss: 0.3836295603311417, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1545, Loss: 0.34702058687125437, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1546, Loss: 0.23758751378327123, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1547, Loss: 0.35856476387580305, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1548, Loss: 0.2710759352586837, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1549, Loss: 0.2592629288151719, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1550, Loss: 0.2745439058732169, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1551, Loss: 0.3774605013076441, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1552, Loss: 0.2995575832046107, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1553, Loss: 0.3540726426930698, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1554, Loss: 0.3719162649753588, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1555, Loss: 0.3201013674803264, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1556, Loss: 0.36862616839105267, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1557, Loss: 0.31850741749331424, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1558, Loss: 0.3002685016878629, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1559, Loss: 0.2969011678472449, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1560, Loss: 0.3800874715524165, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1561, Loss: 0.26012808174225965, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1562, Loss: 0.6992862600510984, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1563, Loss: 0.32333698441380926, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1564, Loss: 0.3357602733153937, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1565, Loss: 0.2644568353880333, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1566, Loss: 0.2713499783031654, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1567, Loss: 0.278680448722465, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1568, Loss: 0.2702434115123091, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1569, Loss: 0.4629211333435723, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1570, Loss: 0.3010291448268225, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1571, Loss: 0.2444834761364117, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1572, Loss: 0.322094496827532, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1573, Loss: 0.32153960387648434, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1574, Loss: 0.2882020186383658, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1575, Loss: 0.3340426469064927, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1576, Loss: 0.43910361304064, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1577, Loss: 0.23088800153866434, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1578, Loss: 0.3201981813333403, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1579, Loss: 0.458911713591096, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1580, Loss: 0.356842244111827, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1581, Loss: 0.3080987304210282, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1582, Loss: 0.4150812368356155, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1583, Loss: 0.35613146216031843, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1584, Loss: 0.31514166034322966, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1585, Loss: 0.2443293779677213, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1586, Loss: 0.26647894303963543, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1587, Loss: 0.26153068801778134, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1588, Loss: 0.24251311278898388, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1589, Loss: 0.37923701249039077, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1590, Loss: 0.30053874195975094, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1591, Loss: 0.2587879988979193, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1592, Loss: 0.324231596558709, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1593, Loss: 0.23711164112837216, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1594, Loss: 0.2773627570462502, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1595, Loss: 0.2604227595223609, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1596, Loss: 0.34668413973069223, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1597, Loss: 0.24365977434804587, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1598, Loss: 0.23460961309885467, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1599, Loss: 0.26792389305284514, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1600, Loss: 0.31307112099476886, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1601, Loss: 0.3923167358212014, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1602, Loss: 0.3081245545850318, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1603, Loss: 0.33040888088215115, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1604, Loss: 0.27709603679568307, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1605, Loss: 0.31768597428293244, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1606, Loss: 0.28427287978179283, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1607, Loss: 0.2558663356618791, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1608, Loss: 0.27105542384095394, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1609, Loss: 0.3675286643786143, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1610, Loss: 0.3058247309295039, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1611, Loss: 0.3097810208122885, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1612, Loss: 0.5152561882207654, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1613, Loss: 0.2695730652165189, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1614, Loss: 0.3324772350095371, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1615, Loss: 0.31163268911192055, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1616, Loss: 0.5401085028360509, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1617, Loss: 0.2754339221574562, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1618, Loss: 0.3538756250713222, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1619, Loss: 0.2877273714715679, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1620, Loss: 0.32063345382288444, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1621, Loss: 0.3227574126153586, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1622, Loss: 0.24207692312165313, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1623, Loss: 0.3669225655724942, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1624, Loss: 0.3100143556581608, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1625, Loss: 0.31744373054829905, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1626, Loss: 0.26619471492640057, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1627, Loss: 0.24699375248162883, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1628, Loss: 0.331285660427703, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1629, Loss: 0.26683587054922536, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1630, Loss: 0.23834646673977253, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1631, Loss: 0.2511110252515965, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1632, Loss: 0.26514530123338065, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1633, Loss: 0.24987892323052577, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1634, Loss: 0.3796381922586729, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1635, Loss: 0.23586435515242232, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1636, Loss: 0.35049591812608294, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1637, Loss: 0.27139390177802086, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1638, Loss: 0.2838961188986083, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1639, Loss: 0.31287549663469083, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1640, Loss: 0.3551616139149705, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1641, Loss: 0.3383543184185432, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1642, Loss: 0.38432665339093725, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1643, Loss: 0.2501355698482231, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1644, Loss: 0.6305903561160957, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1645, Loss: 0.30532473035600144, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1646, Loss: 0.2679394833635723, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1647, Loss: 0.28949374783139004, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1648, Loss: 0.2910325903685056, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1649, Loss: 0.23644527385246514, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1650, Loss: 0.5412086881955722, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1651, Loss: 0.45454684236408294, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1652, Loss: 0.30411875456095677, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1653, Loss: 0.2652753474981167, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1654, Loss: 0.273969383411105, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1655, Loss: 0.3710752616630857, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1656, Loss: 0.3071937110614179, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1657, Loss: 0.29583067617765607, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1658, Loss: 0.25827422928358706, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1659, Loss: 0.3315355869356297, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1660, Loss: 0.27526562140772715, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1661, Loss: 0.3171428908667365, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1662, Loss: 0.3567660289537878, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1663, Loss: 0.3451515839694036, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1664, Loss: 0.3317303175187, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1665, Loss: 0.35182932640926323, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1666, Loss: 0.37353594031698595, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1667, Loss: 0.29333291460835187, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1668, Loss: 0.47362548779444413, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1669, Loss: 0.3520625784241701, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1670, Loss: 0.23818896451754137, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1671, Loss: 0.3418515856460266, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1672, Loss: 0.5447570686642168, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1673, Loss: 0.31318557554032234, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1674, Loss: 0.37200122839075933, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1675, Loss: 0.2711904320788202, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1676, Loss: 0.36105591266157155, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1677, Loss: 0.5134814722244235, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1678, Loss: 0.4958552791255989, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1679, Loss: 0.2696066076325293, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1680, Loss: 0.262723320427177, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1681, Loss: 0.268768963305663, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1682, Loss: 0.24486550103102608, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1683, Loss: 0.2718671905976268, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1684, Loss: 0.32265591703244534, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1685, Loss: 0.3104435660857062, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1686, Loss: 0.539054681466941, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1687, Loss: 0.23793377118945633, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1688, Loss: 0.2512451246337542, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1689, Loss: 0.39605186884626964, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1690, Loss: 0.2712365000480791, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1691, Loss: 0.3386748892165426, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1692, Loss: 0.30367778425661596, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1693, Loss: 0.29384830447379584, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1694, Loss: 0.40627296481976205, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1695, Loss: 0.24021119781439756, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1696, Loss: 0.36920895161228773, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1697, Loss: 0.640461850307107, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1698, Loss: 0.28432147159579024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1699, Loss: 0.3363231364489204, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1700, Loss: 0.2648750265433327, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1701, Loss: 0.260218121856375, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1702, Loss: 0.25781948008278777, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1703, Loss: 0.2699825480876481, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1704, Loss: 0.2731459772586701, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1705, Loss: 0.28492902904209483, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1706, Loss: 0.28094493595018977, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1707, Loss: 0.4924887002253219, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1708, Loss: 0.2713543291107215, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1709, Loss: 0.23441861744763873, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1710, Loss: 0.37740127430169507, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1711, Loss: 0.37733352746109783, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1712, Loss: 0.42700088872219966, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1713, Loss: 0.3872069875941428, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1714, Loss: 0.42642390937677865, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1715, Loss: 0.3198903121195993, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1716, Loss: 0.3204100298354772, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1717, Loss: 0.25799785755772153, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1718, Loss: 0.5344494847183902, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1719, Loss: 0.2713265920796489, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1720, Loss: 0.337434634670446, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1721, Loss: 0.387070000983111, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1722, Loss: 0.3539481576912399, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1723, Loss: 0.3547765413983191, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1724, Loss: 0.2567123818153438, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1725, Loss: 0.3225354187119448, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1726, Loss: 0.23222148926595254, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1727, Loss: 0.3518035955853079, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1728, Loss: 0.34924805961215755, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1729, Loss: 0.48392838306511754, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1730, Loss: 0.28778506730282793, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1731, Loss: 0.25531399917837827, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1732, Loss: 0.26027342882720333, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1733, Loss: 0.49187046427640385, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1734, Loss: 0.2757977442294008, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1735, Loss: 0.28535355468072165, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1736, Loss: 0.33344352247259623, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1737, Loss: 0.36902369551366926, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1738, Loss: 0.23000077375629271, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1739, Loss: 0.24500000603862968, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1740, Loss: 0.2631215795857024, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1741, Loss: 0.2837656717609335, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1742, Loss: 0.27848144806629943, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1743, Loss: 0.315722839854846, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1744, Loss: 0.4061986010900904, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1745, Loss: 0.3877811864546873, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1746, Loss: 0.24817596530257197, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1747, Loss: 0.2860312046876087, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1748, Loss: 0.3923853236426008, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1749, Loss: 0.2536127207817181, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1750, Loss: 0.36224613068086176, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1751, Loss: 0.2623434553522501, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1752, Loss: 0.23724646891874213, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1753, Loss: 0.268380919259225, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1754, Loss: 0.24775223886004996, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1755, Loss: 0.2699874902225686, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1756, Loss: 0.2346084102100823, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1757, Loss: 0.4046700144519133, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1758, Loss: 0.265250227820493, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1759, Loss: 0.2785351577721727, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1760, Loss: 0.3487869235987018, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1761, Loss: 0.23771544803737732, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1762, Loss: 0.5366899646062008, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1763, Loss: 0.3944573067061396, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1764, Loss: 0.2700640426934976, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1765, Loss: 0.25706861352818944, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1766, Loss: 0.23489257627768223, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1767, Loss: 0.3272622268861704, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1768, Loss: 0.32171215734306285, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1769, Loss: 0.2552971880569929, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1770, Loss: 0.3637260326431342, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1771, Loss: 0.41529688585968483, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1772, Loss: 0.30456038063802804, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1773, Loss: 0.3596094173388951, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1774, Loss: 0.25314348796990277, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1775, Loss: 0.3238585852138358, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1776, Loss: 0.23870526921089097, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1777, Loss: 0.28158801726309557, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1778, Loss: 0.33287122172780226, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1779, Loss: 0.2811910344640451, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1780, Loss: 0.24117469632437197, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1781, Loss: 0.3714820183863691, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1782, Loss: 0.4552505472561385, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1783, Loss: 0.37780518915587785, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1784, Loss: 0.36303059803277393, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1785, Loss: 0.2847731388069381, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1786, Loss: 0.2932173093265612, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1787, Loss: 0.2882234093681535, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1788, Loss: 0.33129542922639993, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1789, Loss: 0.35793651671100946, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1790, Loss: 0.25267624498921804, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1791, Loss: 0.2531332079219313, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1792, Loss: 0.25044125951806384, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1793, Loss: 0.3460139965374095, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1794, Loss: 0.2542558631894355, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1795, Loss: 0.2501397840006344, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1796, Loss: 0.36655510230725213, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1797, Loss: 0.26171022969079105, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1798, Loss: 0.24024725195966892, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1799, Loss: 0.29037635800172357, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1800, Loss: 0.2725893704673166, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1801, Loss: 0.25272691867512853, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1802, Loss: 0.4598726357421729, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1803, Loss: 0.2313928136365494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1804, Loss: 0.32838979860778295, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1805, Loss: 0.32357197605936, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1806, Loss: 0.3902138159145192, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1807, Loss: 0.548959238428473, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1808, Loss: 0.5926637022113774, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1809, Loss: 0.29310694063917575, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1810, Loss: 0.2552159632817158, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1811, Loss: 0.2857085161123214, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1812, Loss: 0.30142654588089507, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1813, Loss: 0.23924653895317538, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1814, Loss: 0.33423036647172305, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1815, Loss: 0.2623976614771948, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1816, Loss: 0.2851484075079575, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1817, Loss: 0.271964963396171, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1818, Loss: 0.3851641796265356, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1819, Loss: 0.30745277353876005, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1820, Loss: 0.37182289447077366, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1821, Loss: 0.25918284264553504, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1822, Loss: 0.26537495656703874, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1823, Loss: 0.36533788180852134, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1824, Loss: 0.23787595232795827, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1825, Loss: 0.3120253256662346, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1826, Loss: 0.48788622206711224, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1827, Loss: 0.25157633321931583, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1828, Loss: 0.39846442291151013, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1829, Loss: 0.4866456119873065, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1830, Loss: 0.26564478181891527, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1831, Loss: 0.25971777119863243, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1832, Loss: 0.2563425733279264, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1833, Loss: 0.2449047045269703, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1834, Loss: 0.2475764577259894, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1835, Loss: 0.30931254906167394, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1836, Loss: 0.26267490706989494, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1837, Loss: 0.27778158548873183, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1838, Loss: 0.24626742904355856, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1839, Loss: 0.2991059929031299, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1840, Loss: 0.4481008490091145, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1841, Loss: 0.3209155476404968, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1842, Loss: 0.3269941974426482, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1843, Loss: 0.24608458293722213, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1844, Loss: 0.24991079613936557, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1845, Loss: 0.30279713992468493, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1846, Loss: 0.2794801428463206, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1847, Loss: 0.2451025664057768, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1848, Loss: 0.2592812185453114, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1849, Loss: 0.35408628596433944, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1850, Loss: 0.3004715552349337, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1851, Loss: 0.3227564677550042, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1852, Loss: 0.40198605396079196, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1853, Loss: 0.26149517964995705, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1854, Loss: 0.2652636589975782, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1855, Loss: 0.29717322790278106, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1856, Loss: 0.28176508934239947, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1857, Loss: 0.2943156498515323, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1858, Loss: 0.26509089344884884, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1859, Loss: 0.38961871437515616, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1860, Loss: 0.43068829990470414, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1861, Loss: 0.4716258586196257, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1862, Loss: 0.2630850480376672, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1863, Loss: 0.4234831492222675, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1864, Loss: 0.31737331720941353, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1865, Loss: 0.38586563425967557, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1866, Loss: 0.28635684133982436, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1867, Loss: 0.2641362174647815, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1868, Loss: 0.23163433673006495, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1869, Loss: 0.35951382714962504, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1870, Loss: 0.25471229277515306, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1871, Loss: 0.24297923620586892, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1872, Loss: 0.28782377986419533, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1873, Loss: 0.3044765328222905, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1874, Loss: 0.265506045614524, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Batch 1875, Loss: 0.5142864514027335, Batch Size: 32, Learning Rate: 1.609392294166685e-05\n",
      "Epoch 20, Updated Learning Rate: 1.3679834500416823e-05\n",
      "Epoch 20, Average Loss: 0.32098095749390426, Learning Rate: 1.3679834500416823e-05\n"
     ]
    }
   ],
   "source": [
    "params = model.get_params()\n",
    "optimizer = Adam(params, lr=0.0003)\n",
    "scheduler = ExponentialLearningRateScheduler(optimizer, decay_rate=0.85)\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)\n",
    "# # Train the model\n",
    "# train(train_batches, model, optimizer, scheduler, early_stopping)\n",
    "\n",
    "train(train_batches, model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3304b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(test_batches, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8983ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with MC Dropout: 97.97%\n"
     ]
    }
   ],
   "source": [
    "test_mc(test_batches, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "915cae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda : 0.005 and alpha : 0.0003 : 93.78\n",
    "# Lambda : 0.0005 and alpha : 0.0003 : 94.59 epoch 20 : 94.89 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
